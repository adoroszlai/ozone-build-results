No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2024-02-01 18:04:48,190 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = scm/172.19.0.4
STARTUP_MSG:   args = [--init]
STARTUP_MSG:   version = 1.5.0-SNAPSHOT
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-4.7.5.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.14.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.3.2.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-api-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-dropwizard3-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-3.0.1.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk18on-1.77.jar:/opt/hadoop/share/ozone/lib/bcutil-jdk18on-1.77.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.9.22.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.58.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.25.0.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.9.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.9.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.12.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.16.jar:/opt/hadoop/share/ozone/lib/commons-net-3.10.0.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/ozone/lib/re2j-1.7.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.25.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-2.0.10.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.8.0.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.16.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.16.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.16.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk18on-1.77.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/slf4j-api-2.0.10.jar:/opt/hadoop/share/ozone/lib/commons-text-1.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/okhttp-4.12.0.jar:/opt/hadoop/share/ozone/lib/okio-3.6.0.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.6.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.9.22.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.9.22.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.9.22.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.5.0-SNAPSHOT.jar
STARTUP_MSG:   build = https://github.com/apache/ozone/d5742ef724eeda88681a6ed5e26fae2ba3fd2f74 ; compiled by 'runner' on 2024-02-01T17:35Z
STARTUP_MSG:   java = 11.0.19
STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=4MB, dfs.container.ratis.segment.size=64MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.command.worker.interval=2s, hdds.datanode.block.delete.max.lock.wait.timeout=100ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.max.lock.holding.time=1s, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=19864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.close.threads.max=3, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.file.size=100B, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/dn.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/dn@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.queue.limit=4096, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.slow.op.warning.threshold=500ms, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=100MB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.log.appender.wait-time.min=0us, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/scm.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=5s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=5s, hdds.scm.replication.under.replicated.interval=5s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=5, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=30s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.expired.certificate.check.interval=P1D, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=1MB, ozone.chunk.read.mapped.buffer.threshold=32KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.grpc.write.timeout=30s, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.ec.grpc.zerocopy.enabled=true, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/om.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.max.buckets=100000, ozone.om.multitenancy.enabled=true, ozone.om.multitenancy.ranger.sync.interval=30s, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.open.mpu.cleanup.service.interval=24h, ozone.om.open.mpu.cleanup.service.timeout=300s, ozone.om.open.mpu.expire.threshold=30d, ozone.om.open.mpu.parts.cleanup.limit.per.task=1000, ozone.om.ranger.https-address=https://ranger:6182, ozone.om.ranger.https.admin.api.passwd=Passwd1, ozone.om.ranger.https.admin.api.user=admin, ozone.om.ranger.service=cm_ozone, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.checkpoint.dir.creation.poll.timeout=20s, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.load.native.lib=true, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.tenant.dev.skip.ranger=true, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=6000, ozone.recon.address=recon:9891, ozone.recon.administrators=testuser2, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.scmclient.failover.max.retry=3, ozone.recon.scmclient.max.retry.timeout=6s, ozone.recon.scmclient.rpc.timeout=1m, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3.administrators=testuser,s3g, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/s3g.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.list-keys.shallow.enabled=true, ozone.s3g.metrics.percentiles.intervals.seconds=60, ozone.s3g.secret.http.auth.type=kerberos, ozone.s3g.secret.http.enabled=true, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.handler.count.key=100, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.handler.count.key=100, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.admin.monitor.logging.limit=1000, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.handler.count.key=100, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=1, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=45s, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.raft.server.log.appender.wait-time.min=0ms, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=scm:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=30s, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.directory.service.interval=24h, ozone.snapshot.directory.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
************************************************************/
2024-02-01 18:04:48,309 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
2024-02-01 18:04:49,034 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2024-02-01 18:04:50,922 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
2024-02-01 18:04:51,043 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2024-02-01 18:04:51,415 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
2024-02-01 18:04:57,791 [main] INFO client.SCMCertificateClient: Certificate serial ID set to null
2024-02-01 18:04:57,800 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
2024-02-01 18:04:57,808 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
2024-02-01 18:04:57,814 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
2024-02-01 18:05:01,279 [main] INFO client.SCMCertificateClient: Init response: GETCERT
2024-02-01 18:05:05,122 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.19.0.4,host:scm
2024-02-01 18:05:05,123 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
2024-02-01 18:05:05,153 [main] ERROR utils.SelfSignedCertificate: Invalid domain scm
2024-02-01 18:05:06,109 [main] INFO utils.SelfSignedCertificate: Certificate 1 is issued by CN=scm@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=1 to CN=scm@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=1, valid from Thu Feb 01 18:05:04 UTC 2024 to Sun Mar 11 18:05:04 UTC 2029
2024-02-01 18:05:06,147 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/ca/certs/certificate.crt
2024-02-01 18:05:06,147 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkNzIzYzZmMDQtYzVmNC00MmEzLWE0ZDktMmY0YTgxMjk5
ODg5MTEwLwYDVQQKDChDSUQtN2Y5MGMzNTQtNjIzMy00MWI2LWE1ODYtZDhlNmRj
ZTBlN2Y2MQowCAYDVQQFEwExMB4XDTI0MDIwMTE4MDUwNFoXDTI5MDMxMTE4MDUw
NFowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJDcyM2M2ZjA0LWM1ZjQt
NDJhMy1hNGQ5LTJmNGE4MTI5OTg4OTExMC8GA1UECgwoQ0lELTdmOTBjMzU0LTYy
MzMtNDFiNi1hNTg2LWQ4ZTZkY2UwZTdmNjEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAKMc3jKRSWA/Bd/P7YZakAeyFwnZoQbQS2sT
BLSUMCgz+gMpn504P9zMH84n5IA9X9CrI1NQsci7qf/tVoACBf8NaRW2Y46DUNx5
cDPeKc++UnMcJBrtzu7gDQJFftmTdDByu5ik0gXUSWF0Rxc4Vxhj0adOwwAy5LuR
ozlZgobqyyKRP3mNONL3c6QHceS8ks3wF9LI2Utf2N7Bafz1u+Q10krhbE/YwDsg
7Wi2QCC8+uM9fRCEEs+zSEaKozw2gtrrOlWUdJjY0QmWmQOoNgt6GcnMP4leB9XF
AqHxYckdkh34jWPK5KYCWtFbaud2r15hAVHK64SlCbFDC/WcXfUCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBMA
BDANBgkqhkiG9w0BAQsFAAOCAQEAl54azzQdv2+wRG2FoJht6t8/GomFaTbIcP+y
FtG7KEGSni6fO+/q3pvO7pBkZn8B5jkLQhIDMfQO0DJlhbCPEIcOY/HJrYqW9o7V
di7pSI2RvSnVoFySDT0V2qvr5tCz7BjFnXOjtxFGAIStzmnLffCOeWT0GOr+OZ4a
VR9rAYRD0ZAm3cEyyBIMSGv/w8G/XLEXInFxUxEcailMWx683RdsogSwlhR84sC5
WW8lzlJpYCKv0SBPWiCa4OtcWV3dj0rg3jIUvGZNgruDGs7kx/E9om3MMQns3Ep5
Wtaz4eHP4A8VP80l5eSprpvM/f7szN0loKQEqzC1d2dJZNilvw==
-----END CERTIFICATE-----

2024-02-01 18:05:06,277 [main] INFO client.SCMCertificateClient: Creating csr for SCM->hostName:scm,scmId:723c6f04-c5f4-42a3-a4d9-2f4a81299889,clusterId:CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,subject:scm-sub@scm
2024-02-01 18:05:06,279 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.19.0.4,host:scm
2024-02-01 18:05:06,281 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
2024-02-01 18:05:06,282 [main] ERROR utils.CertificateSignRequest: Invalid domain scm
2024-02-01 18:05:06,659 [main] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.19, 2.5.29.15, 2.5.29.17
2024-02-01 18:05:06,673 [main] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2024-02-01 18:05:06,747 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/CA-1.crt
2024-02-01 18:05:06,756 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkNzIzYzZmMDQtYzVmNC00MmEzLWE0ZDktMmY0YTgxMjk5
ODg5MTEwLwYDVQQKDChDSUQtN2Y5MGMzNTQtNjIzMy00MWI2LWE1ODYtZDhlNmRj
ZTBlN2Y2MQowCAYDVQQFEwExMB4XDTI0MDIwMTE4MDUwNFoXDTI5MDMxMTE4MDUw
NFowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJDcyM2M2ZjA0LWM1ZjQt
NDJhMy1hNGQ5LTJmNGE4MTI5OTg4OTExMC8GA1UECgwoQ0lELTdmOTBjMzU0LTYy
MzMtNDFiNi1hNTg2LWQ4ZTZkY2UwZTdmNjEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAKMc3jKRSWA/Bd/P7YZakAeyFwnZoQbQS2sT
BLSUMCgz+gMpn504P9zMH84n5IA9X9CrI1NQsci7qf/tVoACBf8NaRW2Y46DUNx5
cDPeKc++UnMcJBrtzu7gDQJFftmTdDByu5ik0gXUSWF0Rxc4Vxhj0adOwwAy5LuR
ozlZgobqyyKRP3mNONL3c6QHceS8ks3wF9LI2Utf2N7Bafz1u+Q10krhbE/YwDsg
7Wi2QCC8+uM9fRCEEs+zSEaKozw2gtrrOlWUdJjY0QmWmQOoNgt6GcnMP4leB9XF
AqHxYckdkh34jWPK5KYCWtFbaud2r15hAVHK64SlCbFDC/WcXfUCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBMA
BDANBgkqhkiG9w0BAQsFAAOCAQEAl54azzQdv2+wRG2FoJht6t8/GomFaTbIcP+y
FtG7KEGSni6fO+/q3pvO7pBkZn8B5jkLQhIDMfQO0DJlhbCPEIcOY/HJrYqW9o7V
di7pSI2RvSnVoFySDT0V2qvr5tCz7BjFnXOjtxFGAIStzmnLffCOeWT0GOr+OZ4a
VR9rAYRD0ZAm3cEyyBIMSGv/w8G/XLEXInFxUxEcailMWx683RdsogSwlhR84sC5
WW8lzlJpYCKv0SBPWiCa4OtcWV3dj0rg3jIUvGZNgruDGs7kx/E9om3MMQns3Ep5
Wtaz4eHP4A8VP80l5eSprpvM/f7szN0loKQEqzC1d2dJZNilvw==
-----END CERTIFICATE-----

2024-02-01 18:05:06,769 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/2.crt
2024-02-01 18:05:06,773 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDtTCCAp2gAwIBAgIBAjANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkNzIzYzZmMDQtYzVmNC00MmEzLWE0ZDktMmY0YTgxMjk5
ODg5MTEwLwYDVQQKDChDSUQtN2Y5MGMzNTQtNjIzMy00MWI2LWE1ODYtZDhlNmRj
ZTBlN2Y2MQowCAYDVQQFEwExMB4XDTI0MDIwMTE4MDUwNloXDTI5MDMxMTE4MDUw
NlowgYQxFDASBgNVBAMMC3NjbS1zdWJAc2NtMS0wKwYDVQQLDCQ3MjNjNmYwNC1j
NWY0LTQyYTMtYTRkOS0yZjRhODEyOTk4ODkxMTAvBgNVBAoMKENJRC03ZjkwYzM1
NC02MjMzLTQxYjYtYTU4Ni1kOGU2ZGNlMGU3ZjYxCjAIBgNVBAUTATIwggEiMA0G
CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC3HcVwGajBedmN1LipSXMXgS3/4n9w
6zJ4EJgOFWAe76L7/zbcd4VQIIzeMtnJIRerlDm1C5czcQF+HD6aBEccNot8NnT+
OyQ7qCD5EMIFBtdOezQlVHIU+mqYBhrXIG/4rgU1TuiGfgwjb7OplzDmch4L25He
C+QV3aaoJxn7dqeB1q0r1G4siSAnW6OPKuBRWA+tVRpMMQfiIg3dcIJ8z6sO9a1A
ps9I9hxgbSvljr1qphCvVNctspKcqkflI6pTtZzGBXIm/AP/QMyynzgWQq5pQO3v
kJVOM9Gmnt9Qe1mf79yoXy5ku8cG7MNAQlXjQzQC+uZ9y1kd3vxjniTRAgMBAAGj
NDAyMA8GA1UdEQQIMAaHBKwTAAQwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8E
BAMCAb4wDQYJKoZIhvcNAQELBQADggEBAI1dn4V5ROFRqerehYNNnv8zfHLQpGit
W/t47dA5xfLFT3UMPuHIJvyB49lJiop0Xb7XjZE8wfaov5+bVWa3qZ5iSrXSp9XO
G9rVVUZHU0qnTKGVcfvAnyLcynykK5knm/7Xm2/c7POq48YK8hovrngjSzUieSTO
i5kEKzQkmcdbHkfTgdZyXhOEsI5i9BEKeC9GuGlG4/0R5S1BzChcUHkcpQy1gZQu
dKE8gDkekQCLEHP565QnaV7Fv6dyw4c33vA5zuhoLz4g0dJnfZnOhm37MWliYWA7
ZcPbckelug4sgqy4BaJoXkGdTemX9FsoUx1y5muqvIuQE0Scl56Fy48=
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkNzIzYzZmMDQtYzVmNC00MmEzLWE0ZDktMmY0YTgxMjk5
ODg5MTEwLwYDVQQKDChDSUQtN2Y5MGMzNTQtNjIzMy00MWI2LWE1ODYtZDhlNmRj
ZTBlN2Y2MQowCAYDVQQFEwExMB4XDTI0MDIwMTE4MDUwNFoXDTI5MDMxMTE4MDUw
NFowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJDcyM2M2ZjA0LWM1ZjQt
NDJhMy1hNGQ5LTJmNGE4MTI5OTg4OTExMC8GA1UECgwoQ0lELTdmOTBjMzU0LTYy
MzMtNDFiNi1hNTg2LWQ4ZTZkY2UwZTdmNjEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAKMc3jKRSWA/Bd/P7YZakAeyFwnZoQbQS2sT
BLSUMCgz+gMpn504P9zMH84n5IA9X9CrI1NQsci7qf/tVoACBf8NaRW2Y46DUNx5
cDPeKc++UnMcJBrtzu7gDQJFftmTdDByu5ik0gXUSWF0Rxc4Vxhj0adOwwAy5LuR
ozlZgobqyyKRP3mNONL3c6QHceS8ks3wF9LI2Utf2N7Bafz1u+Q10krhbE/YwDsg
7Wi2QCC8+uM9fRCEEs+zSEaKozw2gtrrOlWUdJjY0QmWmQOoNgt6GcnMP4leB9XF
AqHxYckdkh34jWPK5KYCWtFbaud2r15hAVHK64SlCbFDC/WcXfUCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBMA
BDANBgkqhkiG9w0BAQsFAAOCAQEAl54azzQdv2+wRG2FoJht6t8/GomFaTbIcP+y
FtG7KEGSni6fO+/q3pvO7pBkZn8B5jkLQhIDMfQO0DJlhbCPEIcOY/HJrYqW9o7V
di7pSI2RvSnVoFySDT0V2qvr5tCz7BjFnXOjtxFGAIStzmnLffCOeWT0GOr+OZ4a
VR9rAYRD0ZAm3cEyyBIMSGv/w8G/XLEXInFxUxEcailMWx683RdsogSwlhR84sC5
WW8lzlJpYCKv0SBPWiCa4OtcWV3dj0rg3jIUvGZNgruDGs7kx/E9om3MMQns3Ep5
Wtaz4eHP4A8VP80l5eSprpvM/f7szN0loKQEqzC1d2dJZNilvw==
-----END CERTIFICATE-----

2024-02-01 18:05:06,774 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/certificate.crt
2024-02-01 18:05:06,774 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDtTCCAp2gAwIBAgIBAjANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkNzIzYzZmMDQtYzVmNC00MmEzLWE0ZDktMmY0YTgxMjk5
ODg5MTEwLwYDVQQKDChDSUQtN2Y5MGMzNTQtNjIzMy00MWI2LWE1ODYtZDhlNmRj
ZTBlN2Y2MQowCAYDVQQFEwExMB4XDTI0MDIwMTE4MDUwNloXDTI5MDMxMTE4MDUw
NlowgYQxFDASBgNVBAMMC3NjbS1zdWJAc2NtMS0wKwYDVQQLDCQ3MjNjNmYwNC1j
NWY0LTQyYTMtYTRkOS0yZjRhODEyOTk4ODkxMTAvBgNVBAoMKENJRC03ZjkwYzM1
NC02MjMzLTQxYjYtYTU4Ni1kOGU2ZGNlMGU3ZjYxCjAIBgNVBAUTATIwggEiMA0G
CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC3HcVwGajBedmN1LipSXMXgS3/4n9w
6zJ4EJgOFWAe76L7/zbcd4VQIIzeMtnJIRerlDm1C5czcQF+HD6aBEccNot8NnT+
OyQ7qCD5EMIFBtdOezQlVHIU+mqYBhrXIG/4rgU1TuiGfgwjb7OplzDmch4L25He
C+QV3aaoJxn7dqeB1q0r1G4siSAnW6OPKuBRWA+tVRpMMQfiIg3dcIJ8z6sO9a1A
ps9I9hxgbSvljr1qphCvVNctspKcqkflI6pTtZzGBXIm/AP/QMyynzgWQq5pQO3v
kJVOM9Gmnt9Qe1mf79yoXy5ku8cG7MNAQlXjQzQC+uZ9y1kd3vxjniTRAgMBAAGj
NDAyMA8GA1UdEQQIMAaHBKwTAAQwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8E
BAMCAb4wDQYJKoZIhvcNAQELBQADggEBAI1dn4V5ROFRqerehYNNnv8zfHLQpGit
W/t47dA5xfLFT3UMPuHIJvyB49lJiop0Xb7XjZE8wfaov5+bVWa3qZ5iSrXSp9XO
G9rVVUZHU0qnTKGVcfvAnyLcynykK5knm/7Xm2/c7POq48YK8hovrngjSzUieSTO
i5kEKzQkmcdbHkfTgdZyXhOEsI5i9BEKeC9GuGlG4/0R5S1BzChcUHkcpQy1gZQu
dKE8gDkekQCLEHP565QnaV7Fv6dyw4c33vA5zuhoLz4g0dJnfZnOhm37MWliYWA7
ZcPbckelug4sgqy4BaJoXkGdTemX9FsoUx1y5muqvIuQE0Scl56Fy48=
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkNzIzYzZmMDQtYzVmNC00MmEzLWE0ZDktMmY0YTgxMjk5
ODg5MTEwLwYDVQQKDChDSUQtN2Y5MGMzNTQtNjIzMy00MWI2LWE1ODYtZDhlNmRj
ZTBlN2Y2MQowCAYDVQQFEwExMB4XDTI0MDIwMTE4MDUwNFoXDTI5MDMxMTE4MDUw
NFowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJDcyM2M2ZjA0LWM1ZjQt
NDJhMy1hNGQ5LTJmNGE4MTI5OTg4OTExMC8GA1UECgwoQ0lELTdmOTBjMzU0LTYy
MzMtNDFiNi1hNTg2LWQ4ZTZkY2UwZTdmNjEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAKMc3jKRSWA/Bd/P7YZakAeyFwnZoQbQS2sT
BLSUMCgz+gMpn504P9zMH84n5IA9X9CrI1NQsci7qf/tVoACBf8NaRW2Y46DUNx5
cDPeKc++UnMcJBrtzu7gDQJFftmTdDByu5ik0gXUSWF0Rxc4Vxhj0adOwwAy5LuR
ozlZgobqyyKRP3mNONL3c6QHceS8ks3wF9LI2Utf2N7Bafz1u+Q10krhbE/YwDsg
7Wi2QCC8+uM9fRCEEs+zSEaKozw2gtrrOlWUdJjY0QmWmQOoNgt6GcnMP4leB9XF
AqHxYckdkh34jWPK5KYCWtFbaud2r15hAVHK64SlCbFDC/WcXfUCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBMA
BDANBgkqhkiG9w0BAQsFAAOCAQEAl54azzQdv2+wRG2FoJht6t8/GomFaTbIcP+y
FtG7KEGSni6fO+/q3pvO7pBkZn8B5jkLQhIDMfQO0DJlhbCPEIcOY/HJrYqW9o7V
di7pSI2RvSnVoFySDT0V2qvr5tCz7BjFnXOjtxFGAIStzmnLffCOeWT0GOr+OZ4a
VR9rAYRD0ZAm3cEyyBIMSGv/w8G/XLEXInFxUxEcailMWx683RdsogSwlhR84sC5
WW8lzlJpYCKv0SBPWiCa4OtcWV3dj0rg3jIUvGZNgruDGs7kx/E9om3MMQns3Ep5
Wtaz4eHP4A8VP80l5eSprpvM/f7szN0loKQEqzC1d2dJZNilvw==
-----END CERTIFICATE-----

2024-02-01 18:05:06,775 [main] INFO client.SCMCertificateClient: Successfully stored SCM signed certificate.
2024-02-01 18:05:07,293 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
2024-02-01 18:05:07,537 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2024-02-01 18:05:07,551 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
2024-02-01 18:05:07,566 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2024-02-01 18:05:07,567 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
2024-02-01 18:05:07,567 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
2024-02-01 18:05:07,568 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2024-02-01 18:05:07,568 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
2024-02-01 18:05:07,570 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2024-02-01 18:05:07,570 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2024-02-01 18:05:07,581 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2024-02-01 18:05:07,609 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2024-02-01 18:05:07,686 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
2024-02-01 18:05:07,686 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
2024-02-01 18:05:08,174 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
2024-02-01 18:05:08,176 [main] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2024-02-01 18:05:08,184 [main] INFO server.RaftServerConfigKeys: raft.server.close.threshold = 60s (default)
2024-02-01 18:05:08,184 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2024-02-01 18:05:08,189 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2024-02-01 18:05:08,190 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
2024-02-01 18:05:08,190 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
2024-02-01 18:05:08,207 [main] INFO server.RaftServer: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: addNew group-D8E6DCE0E7F6:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894] returns group-D8E6DCE0E7F6:java.util.concurrent.CompletableFuture@e49437[Not completed]
2024-02-01 18:05:08,231 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: new RaftServerImpl for group-D8E6DCE0E7F6:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894] with SCMStateMachine:uninitialized
2024-02-01 18:05:08,238 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2024-02-01 18:05:08,238 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
2024-02-01 18:05:08,239 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
2024-02-01 18:05:08,239 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
2024-02-01 18:05:08,239 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2024-02-01 18:05:08,240 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.member.majority-add = false (default)
2024-02-01 18:05:08,240 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2024-02-01 18:05:08,254 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: ConfigurationManager, init=-1: peers:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894]|listeners:[], old=null, confs=<EMPTY_MAP>
2024-02-01 18:05:08,260 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
2024-02-01 18:05:08,265 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
2024-02-01 18:05:08,268 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
2024-02-01 18:05:08,268 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100ms (default)
2024-02-01 18:05:08,272 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
2024-02-01 18:05:08,272 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.read-after-write-consistent.write-index-cache.expiry-time = 60s (default)
2024-02-01 18:05:08,283 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.dropwizard3.Dm3MetricRegistriesImpl
2024-02-01 18:05:08,404 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2024-02-01 18:05:08,406 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2024-02-01 18:05:08,406 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
2024-02-01 18:05:08,407 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
2024-02-01 18:05:08,407 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
2024-02-01 18:05:08,407 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
2024-02-01 18:05:08,409 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
2024-02-01 18:05:08,409 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
2024-02-01 18:05:08,410 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2024-02-01 18:05:08,418 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6 does not exist. Creating ...
2024-02-01 18:05:08,429 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6/in_use.lock acquired by nodename 12@scm
2024-02-01 18:05:08,439 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6 has been successfully formatted.
2024-02-01 18:05:08,439 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] WARN util.FileUtils: Failed to Files.newInputStream /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6/current/raft-meta.conf with options []: java.nio.file.NoSuchFileException: /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6/current/raft-meta.conf
2024-02-01 18:05:08,449 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
2024-02-01 18:05:08,468 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
2024-02-01 18:05:08,468 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2024-02-01 18:05:08,475 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2024-02-01 18:05:08,476 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
2024-02-01 18:05:08,479 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2024-02-01 18:05:08,491 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
2024-02-01 18:05:08,491 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2024-02-01 18:05:08,491 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2024-02-01 18:05:08,500 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO util.AwaitToRun: Thread[723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-cacheEviction-AwaitToRun,5,main] started
2024-02-01 18:05:08,504 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6
2024-02-01 18:05:08,504 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2024-02-01 18:05:08,508 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
2024-02-01 18:05:08,510 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2024-02-01 18:05:08,510 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
2024-02-01 18:05:08,511 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
2024-02-01 18:05:08,511 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
2024-02-01 18:05:08,511 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
2024-02-01 18:05:08,512 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2024-02-01 18:05:08,513 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554440 (custom)
2024-02-01 18:05:08,529 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2024-02-01 18:05:08,530 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
2024-02-01 18:05:08,530 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
2024-02-01 18:05:08,531 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
2024-02-01 18:05:08,539 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2024-02-01 18:05:08,539 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2024-02-01 18:05:08,549 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: start as a follower, conf=-1: peers:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894]|listeners:[], old=null
2024-02-01 18:05:08,563 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: changes role from      null to FOLLOWER at term 0 for startAsFollower
2024-02-01 18:05:08,564 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO impl.RoleInfo: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: start 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState
2024-02-01 18:05:08,570 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D8E6DCE0E7F6,id=723c6f04-c5f4-42a3-a4d9-2f4a81299889
2024-02-01 18:05:08,572 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.trigger-when-stop.enabled = true (default)
2024-02-01 18:05:08,572 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
2024-02-01 18:05:08,573 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
2024-02-01 18:05:08,574 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
2024-02-01 18:05:08,574 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
2024-02-01 18:05:08,576 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
2024-02-01 18:05:08,577 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2024-02-01 18:05:08,579 [main] INFO server.RaftServer: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: start RPC server
2024-02-01 18:05:08,625 [main] INFO server.GrpcService: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: GrpcService started, listening on 9894
2024-02-01 18:05:08,637 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-723c6f04-c5f4-42a3-a4d9-2f4a81299889: Started
2024-02-01 18:05:13,680 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO impl.FollowerState: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5116180025ns, electionTimeout:5102ms
2024-02-01 18:05:13,680 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO impl.RoleInfo: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: shutdown 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState
2024-02-01 18:05:13,681 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2024-02-01 18:05:13,683 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
2024-02-01 18:05:13,683 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO impl.RoleInfo: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: start 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1
2024-02-01 18:05:13,686 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO impl.LeaderElection: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894]|listeners:[], old=null
2024-02-01 18:05:13,686 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO impl.LeaderElection: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
2024-02-01 18:05:13,689 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO impl.LeaderElection: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894]|listeners:[], old=null
2024-02-01 18:05:13,689 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO impl.LeaderElection: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1 ELECTION round 0: result PASSED (term=1)
2024-02-01 18:05:13,689 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO impl.RoleInfo: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: shutdown 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1
2024-02-01 18:05:13,689 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2024-02-01 18:05:13,693 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
2024-02-01 18:05:13,695 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2024-02-01 18:05:13,696 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
2024-02-01 18:05:13,698 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
2024-02-01 18:05:13,698 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
2024-02-01 18:05:13,699 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
2024-02-01 18:05:13,703 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.enabled = false (default)
2024-02-01 18:05:13,705 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.timeout.ratio = 0.9 (default)
2024-02-01 18:05:13,705 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2024-02-01 18:05:13,705 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2024-02-01 18:05:13,705 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
2024-02-01 18:05:13,707 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO impl.RoleInfo: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: start 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderStateImpl
2024-02-01 18:05:13,707 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: set firstElectionSinceStartup to false for becomeLeader
2024-02-01 18:05:13,707 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: change Leader from null to 723c6f04-c5f4-42a3-a4d9-2f4a81299889 at term 1 for becomeLeader, leader elected after 5447ms
2024-02-01 18:05:13,720 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker: Starting segment from index:0
2024-02-01 18:05:13,749 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: set configuration 0: peers:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894]|listeners:[], old=null
2024-02-01 18:05:13,759 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker] INFO segmented.BufferedWriteChannel: open log_inprogress_0 at position 0
2024-02-01 18:05:13,767 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6/current/log_inprogress_0
2024-02-01 18:05:13,779 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO server.RaftServer$Division: leader is ready since appliedIndex == 0 >= startIndex == 0
2024-02-01 18:05:14,637 [main] INFO server.RaftServer: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: close
2024-02-01 18:05:14,638 [main] INFO server.GrpcService: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: shutdown server GrpcServerProtocolService now
2024-02-01 18:05:14,638 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: shutdown
2024-02-01 18:05:14,638 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-D8E6DCE0E7F6,id=723c6f04-c5f4-42a3-a4d9-2f4a81299889
2024-02-01 18:05:14,639 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO impl.RoleInfo: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: shutdown 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderStateImpl
2024-02-01 18:05:14,642 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO impl.PendingRequests: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-PendingRequests: sendNotLeaderResponses
2024-02-01 18:05:14,651 [main] INFO server.GrpcService: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: shutdown server GrpcServerProtocolService successfully
2024-02-01 18:05:14,652 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO impl.StateMachineUpdater: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater: set stopIndex = 0
2024-02-01 18:05:14,653 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO impl.StateMachineUpdater: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater: Took a snapshot at index 0
2024-02-01 18:05:14,653 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO impl.StateMachineUpdater: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2024-02-01 18:05:14,657 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: applyIndex: 0
2024-02-01 18:05:14,657 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-cacheEviction-AwaitToRun] INFO util.AwaitToRun: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-cacheEviction-AwaitToRun-AwaitForSignal is interrupted
2024-02-01 18:05:14,773 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker close()
2024-02-01 18:05:14,774 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-723c6f04-c5f4-42a3-a4d9-2f4a81299889: Stopped
2024-02-01 18:05:14,774 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2024-02-01 18:05:14,776 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6; layoutVersion=7; scmId=723c6f04-c5f4-42a3-a4d9-2f4a81299889
2024-02-01 18:05:14,780 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down StorageContainerManager at scm/172.19.0.4
************************************************************/
No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2024-02-01 18:05:16,274 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = scm/172.19.0.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 1.5.0-SNAPSHOT
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-4.7.5.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.14.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.3.2.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-api-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-dropwizard3-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-3.0.1.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk18on-1.77.jar:/opt/hadoop/share/ozone/lib/bcutil-jdk18on-1.77.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.9.22.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.58.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.25.0.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.9.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.9.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.12.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.16.jar:/opt/hadoop/share/ozone/lib/commons-net-3.10.0.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/ozone/lib/re2j-1.7.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.25.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-2.0.10.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.8.0.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.16.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.16.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.16.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.5.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk18on-1.77.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/slf4j-api-2.0.10.jar:/opt/hadoop/share/ozone/lib/commons-text-1.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/okhttp-4.12.0.jar:/opt/hadoop/share/ozone/lib/okio-3.6.0.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.6.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.9.22.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.9.22.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.9.22.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.5.0-SNAPSHOT.jar
STARTUP_MSG:   build = https://github.com/apache/ozone/d5742ef724eeda88681a6ed5e26fae2ba3fd2f74 ; compiled by 'runner' on 2024-02-01T17:35Z
STARTUP_MSG:   java = 11.0.19
STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=4MB, dfs.container.ratis.segment.size=64MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.command.worker.interval=2s, hdds.datanode.block.delete.max.lock.wait.timeout=100ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.max.lock.holding.time=1s, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=19864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.close.threads.max=3, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.file.size=100B, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/dn.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/dn@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.queue.limit=4096, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.slow.op.warning.threshold=500ms, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=100MB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.log.appender.wait-time.min=0us, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/scm.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=5s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=5s, hdds.scm.replication.under.replicated.interval=5s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=5, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=30s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.expired.certificate.check.interval=P1D, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=1MB, ozone.chunk.read.mapped.buffer.threshold=32KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.grpc.write.timeout=30s, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.ec.grpc.zerocopy.enabled=true, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/om.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.max.buckets=100000, ozone.om.multitenancy.enabled=true, ozone.om.multitenancy.ranger.sync.interval=30s, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.open.mpu.cleanup.service.interval=24h, ozone.om.open.mpu.cleanup.service.timeout=300s, ozone.om.open.mpu.expire.threshold=30d, ozone.om.open.mpu.parts.cleanup.limit.per.task=1000, ozone.om.ranger.https-address=https://ranger:6182, ozone.om.ranger.https.admin.api.passwd=Passwd1, ozone.om.ranger.https.admin.api.user=admin, ozone.om.ranger.service=cm_ozone, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.checkpoint.dir.creation.poll.timeout=20s, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.load.native.lib=true, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.tenant.dev.skip.ranger=true, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=6000, ozone.recon.address=recon:9891, ozone.recon.administrators=testuser2, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.scmclient.failover.max.retry=3, ozone.recon.scmclient.max.retry.timeout=6s, ozone.recon.scmclient.rpc.timeout=1m, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3.administrators=testuser,s3g, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/s3g.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.list-keys.shallow.enabled=true, ozone.s3g.metrics.percentiles.intervals.seconds=60, ozone.s3g.secret.http.auth.type=kerberos, ozone.s3g.secret.http.enabled=true, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.handler.count.key=100, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.handler.count.key=100, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.admin.monitor.logging.limit=1000, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.handler.count.key=100, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=1, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=45s, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.raft.server.log.appender.wait-time.min=0ms, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=scm:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=30s, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.directory.service.interval=24h, ozone.snapshot.directory.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
************************************************************/
2024-02-01 18:05:16,285 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
2024-02-01 18:05:16,334 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2024-02-01 18:05:16,614 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
2024-02-01 18:05:16,628 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2024-02-01 18:05:16,869 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
2024-02-01 18:05:16,869 [main] INFO server.StorageContainerManager: SCM login successful.
2024-02-01 18:05:17,397 [main] INFO client.SCMCertificateClient: Certificate serial ID set to 2
2024-02-01 18:05:17,547 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=1
           Start Date: Thu Feb 01 18:05:06 UTC 2024
           Final Date: Sun Mar 11 18:05:06 UTC 2029
            SubjectDN: CN=scm-sub@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=2
           Public Key: RSA Public Key [af:9a:88:3c:ff:2e:a1:97:27:86:c1:c4:01:63:74:94:be:15:15:1e],[56:66:d1:a4]
        modulus: b71dc57019a8c179d98dd4b8a9497317812dffe27f70eb327810980e15601eefa2fbff36dc778550208cde32d9c92117ab9439b50b973371017e1c3e9a04471c368b7c3674fe3b243ba820f910c20506d74e7b3425547214fa6a98061ad7206ff8ae05354ee8867e0c236fb3a99730e6721e0bdb91de0be415dda6a82719fb76a781d6ad2bd46e2c8920275ba38f2ae051580fad551a4c3107e2220ddd70827ccfab0ef5ad40a6cf48f61c606d2be58ebd6aa610af54d72db2929caa47e523aa53b59cc6057226fc03ff40ccb29f381642ae6940edef90954e33d1a69edf507b599fefdca85f2e64bbc706ecc3404255e3433402fae67dcb591ddefc639e24d1
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 8d5d9f857944e151a9eade85834d9eff337c72d0
                       a468ad5bfb78edd039c5f2c54f750c3ee1c826fc
                       81e3d9498a8a745dbed78d913cc1f6a8bf9f9b55
                       66b7a99e624ab5d2a7d5ce1bdad5554647534aa7
                       4ca19571fbc09f22dcca7ca42b99279bfed79b6f
                       dcecf3aae3c60af21a2fae78234b35227924ce8b
                       99042b342499c75b1e47d381d6725e1384b08e62
                       f4110a782f46b86946e3fd11e52d41cc285c5079
                       1ca50cb581942e74a13c80391e91008b1073f9eb
                       9427695ec5bfa772c38737def039cee8682f3e20
                       d1d2677d99ce866dfb31696261603b65c3db7247
                       a5ba0e2c82acb805a2685e419d4de997f45b2853
                       1d72e66baabc8b9013449c979e85cb8f
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [CONTEXT 7] IMPLICIT 
        DER Octet String[4] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe
 from file: /data/metadata/scm/sub-ca/certs/2.crt.
2024-02-01 18:05:17,584 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=1
           Start Date: Thu Feb 01 18:05:04 UTC 2024
           Final Date: Sun Mar 11 18:05:04 UTC 2029
            SubjectDN: CN=scm@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=1
           Public Key: RSA Public Key [d0:ba:0f:0d:fd:96:d6:cd:32:65:a8:b8:76:22:12:0e:73:8a:a0:49],[56:66:d1:a4]
        modulus: a31cde329149603f05dfcfed865a9007b21709d9a106d04b6b1304b494302833fa03299f9d383fdccc1fce27e4803d5fd0ab235350b1c8bba9ffed56800205ff0d6915b6638e8350dc797033de29cfbe52731c241aedceeee00d02457ed993743072bb98a4d205d4496174471738571863d1a74ec30032e4bb91a339598286eacb22913f798d38d2f773a40771e4bc92cdf017d2c8d94b5fd8dec169fcf5bbe435d24ae16c4fd8c03b20ed68b64020bcfae33d7d108412cfb348468aa33c3682daeb3a55947498d8d109969903a8360b7a19c9cc3f895e07d5c502a1f161c91d921df88d63cae4a6025ad15b6ae776af5e610151caeb84a509b1430bf59c5df5
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 979e1acf341dbf6fb0446d85a0986deadf3f1a89
                       856936c870ffb216d1bb2841929e2e9f3befeade
                       9bceee9064667f01e6390b42120331f40ed03265
                       85b08f10870e63f1c9ad8a96f68ed5762ee9488d
                       91bd29d5a05c920d3d15daabebe6d0b3ec18c59d
                       73a3b711460084adce69cb7df08e7964f418eafe
                       399e1a551f6b018443d19026ddc132c8120c486b
                       ffc3c1bf5cb11722717153111c6a294c5b1ebcdd
                       176ca204b096147ce2c0b9596f25ce52696022af
                       d1204f5a209ae0eb5c595ddd8f4ae0de3214bc66
                       4d82bb831acee4c7f13da26dcc3109ecdc4a795a
                       d6b3e1e1cfe00f153fcd25e5e4a9ae9bccfdfeec
                       ccdd25a0a404ab30b577674964d8a5bf
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [CONTEXT 7] IMPLICIT 
        DER Octet String[4] 

 from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
2024-02-01 18:05:17,588 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=1
           Start Date: Thu Feb 01 18:05:06 UTC 2024
           Final Date: Sun Mar 11 18:05:06 UTC 2029
            SubjectDN: CN=scm-sub@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=2
           Public Key: RSA Public Key [af:9a:88:3c:ff:2e:a1:97:27:86:c1:c4:01:63:74:94:be:15:15:1e],[56:66:d1:a4]
        modulus: b71dc57019a8c179d98dd4b8a9497317812dffe27f70eb327810980e15601eefa2fbff36dc778550208cde32d9c92117ab9439b50b973371017e1c3e9a04471c368b7c3674fe3b243ba820f910c20506d74e7b3425547214fa6a98061ad7206ff8ae05354ee8867e0c236fb3a99730e6721e0bdb91de0be415dda6a82719fb76a781d6ad2bd46e2c8920275ba38f2ae051580fad551a4c3107e2220ddd70827ccfab0ef5ad40a6cf48f61c606d2be58ebd6aa610af54d72db2929caa47e523aa53b59cc6057226fc03ff40ccb29f381642ae6940edef90954e33d1a69edf507b599fefdca85f2e64bbc706ecc3404255e3433402fae67dcb591ddefc639e24d1
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 8d5d9f857944e151a9eade85834d9eff337c72d0
                       a468ad5bfb78edd039c5f2c54f750c3ee1c826fc
                       81e3d9498a8a745dbed78d913cc1f6a8bf9f9b55
                       66b7a99e624ab5d2a7d5ce1bdad5554647534aa7
                       4ca19571fbc09f22dcca7ca42b99279bfed79b6f
                       dcecf3aae3c60af21a2fae78234b35227924ce8b
                       99042b342499c75b1e47d381d6725e1384b08e62
                       f4110a782f46b86946e3fd11e52d41cc285c5079
                       1ca50cb581942e74a13c80391e91008b1073f9eb
                       9427695ec5bfa772c38737def039cee8682f3e20
                       d1d2677d99ce866dfb31696261603b65c3db7247
                       a5ba0e2c82acb805a2685e419d4de997f45b2853
                       1d72e66baabc8b9013449c979e85cb8f
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [CONTEXT 7] IMPLICIT 
        DER Octet String[4] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe
 from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
2024-02-01 18:05:17,588 [main] INFO client.SCMCertificateClient: CertificateRenewerService and root ca rotation polling is disabled for scm/sub-ca
2024-02-01 18:05:17,657 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2024-02-01 18:05:17,763 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2024-02-01 18:05:17,773 [main] INFO utils.LeakDetector: Starting leak detector thread ManagedRocksObject0.
2024-02-01 18:05:17,965 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.5.0-SNAPSHOT.jar!/network-topology-default.xml]
2024-02-01 18:05:17,966 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
2024-02-01 18:05:18,011 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.dropwizard3.Dm3MetricRegistriesImpl
2024-02-01 18:05:18,244 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:723c6f04-c5f4-42a3-a4d9-2f4a81299889
2024-02-01 18:05:18,267 [main] INFO ssl.ReloadingX509KeyManager: Key manager is loaded with certificate chain
2024-02-01 18:05:18,272 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=1
           Start Date: Thu Feb 01 18:05:06 UTC 2024
           Final Date: Sun Mar 11 18:05:06 UTC 2029
            SubjectDN: CN=scm-sub@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=2
           Public Key: RSA Public Key [af:9a:88:3c:ff:2e:a1:97:27:86:c1:c4:01:63:74:94:be:15:15:1e],[56:66:d1:a4]
        modulus: b71dc57019a8c179d98dd4b8a9497317812dffe27f70eb327810980e15601eefa2fbff36dc778550208cde32d9c92117ab9439b50b973371017e1c3e9a04471c368b7c3674fe3b243ba820f910c20506d74e7b3425547214fa6a98061ad7206ff8ae05354ee8867e0c236fb3a99730e6721e0bdb91de0be415dda6a82719fb76a781d6ad2bd46e2c8920275ba38f2ae051580fad551a4c3107e2220ddd70827ccfab0ef5ad40a6cf48f61c606d2be58ebd6aa610af54d72db2929caa47e523aa53b59cc6057226fc03ff40ccb29f381642ae6940edef90954e33d1a69edf507b599fefdca85f2e64bbc706ecc3404255e3433402fae67dcb591ddefc639e24d1
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 8d5d9f857944e151a9eade85834d9eff337c72d0
                       a468ad5bfb78edd039c5f2c54f750c3ee1c826fc
                       81e3d9498a8a745dbed78d913cc1f6a8bf9f9b55
                       66b7a99e624ab5d2a7d5ce1bdad5554647534aa7
                       4ca19571fbc09f22dcca7ca42b99279bfed79b6f
                       dcecf3aae3c60af21a2fae78234b35227924ce8b
                       99042b342499c75b1e47d381d6725e1384b08e62
                       f4110a782f46b86946e3fd11e52d41cc285c5079
                       1ca50cb581942e74a13c80391e91008b1073f9eb
                       9427695ec5bfa772c38737def039cee8682f3e20
                       d1d2677d99ce866dfb31696261603b65c3db7247
                       a5ba0e2c82acb805a2685e419d4de997f45b2853
                       1d72e66baabc8b9013449c979e85cb8f
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [CONTEXT 7] IMPLICIT 
        DER Octet String[4] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe

2024-02-01 18:05:18,275 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=1
           Start Date: Thu Feb 01 18:05:04 UTC 2024
           Final Date: Sun Mar 11 18:05:04 UTC 2029
            SubjectDN: CN=scm@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=1
           Public Key: RSA Public Key [d0:ba:0f:0d:fd:96:d6:cd:32:65:a8:b8:76:22:12:0e:73:8a:a0:49],[56:66:d1:a4]
        modulus: a31cde329149603f05dfcfed865a9007b21709d9a106d04b6b1304b494302833fa03299f9d383fdccc1fce27e4803d5fd0ab235350b1c8bba9ffed56800205ff0d6915b6638e8350dc797033de29cfbe52731c241aedceeee00d02457ed993743072bb98a4d205d4496174471738571863d1a74ec30032e4bb91a339598286eacb22913f798d38d2f773a40771e4bc92cdf017d2c8d94b5fd8dec169fcf5bbe435d24ae16c4fd8c03b20ed68b64020bcfae33d7d108412cfb348468aa33c3682daeb3a55947498d8d109969903a8360b7a19c9cc3f895e07d5c502a1f161c91d921df88d63cae4a6025ad15b6ae776af5e610151caeb84a509b1430bf59c5df5
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 979e1acf341dbf6fb0446d85a0986deadf3f1a89
                       856936c870ffb216d1bb2841929e2e9f3befeade
                       9bceee9064667f01e6390b42120331f40ed03265
                       85b08f10870e63f1c9ad8a96f68ed5762ee9488d
                       91bd29d5a05c920d3d15daabebe6d0b3ec18c59d
                       73a3b711460084adce69cb7df08e7964f418eafe
                       399e1a551f6b018443d19026ddc132c8120c486b
                       ffc3c1bf5cb11722717153111c6a294c5b1ebcdd
                       176ca204b096147ce2c0b9596f25ce52696022af
                       d1204f5a209ae0eb5c595ddd8f4ae0de3214bc66
                       4d82bb831acee4c7f13da26dcc3109ecdc4a795a
                       d6b3e1e1cfe00f153fcd25e5e4a9ae9bccfdfeec
                       ccdd25a0a404ab30b577674964d8a5bf
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [CONTEXT 7] IMPLICIT 
        DER Octet String[4] 


2024-02-01 18:05:18,280 [main] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2024-02-01 18:05:18,280 [main] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2024-02-01 18:05:18,281 [main] INFO ssl.ReloadingX509TrustManager: Trust manager is loaded with certificates
2024-02-01 18:05:18,284 [main] INFO ssl.ReloadingX509TrustManager:   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=1
           Start Date: Thu Feb 01 18:05:04 UTC 2024
           Final Date: Sun Mar 11 18:05:04 UTC 2029
            SubjectDN: CN=scm@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=1
           Public Key: RSA Public Key [d0:ba:0f:0d:fd:96:d6:cd:32:65:a8:b8:76:22:12:0e:73:8a:a0:49],[56:66:d1:a4]
        modulus: a31cde329149603f05dfcfed865a9007b21709d9a106d04b6b1304b494302833fa03299f9d383fdccc1fce27e4803d5fd0ab235350b1c8bba9ffed56800205ff0d6915b6638e8350dc797033de29cfbe52731c241aedceeee00d02457ed993743072bb98a4d205d4496174471738571863d1a74ec30032e4bb91a339598286eacb22913f798d38d2f773a40771e4bc92cdf017d2c8d94b5fd8dec169fcf5bbe435d24ae16c4fd8c03b20ed68b64020bcfae33d7d108412cfb348468aa33c3682daeb3a55947498d8d109969903a8360b7a19c9cc3f895e07d5c502a1f161c91d921df88d63cae4a6025ad15b6ae776af5e610151caeb84a509b1430bf59c5df5
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 979e1acf341dbf6fb0446d85a0986deadf3f1a89
                       856936c870ffb216d1bb2841929e2e9f3befeade
                       9bceee9064667f01e6390b42120331f40ed03265
                       85b08f10870e63f1c9ad8a96f68ed5762ee9488d
                       91bd29d5a05c920d3d15daabebe6d0b3ec18c59d
                       73a3b711460084adce69cb7df08e7964f418eafe
                       399e1a551f6b018443d19026ddc132c8120c486b
                       ffc3c1bf5cb11722717153111c6a294c5b1ebcdd
                       176ca204b096147ce2c0b9596f25ce52696022af
                       d1204f5a209ae0eb5c595ddd8f4ae0de3214bc66
                       4d82bb831acee4c7f13da26dcc3109ecdc4a795a
                       d6b3e1e1cfe00f153fcd25e5e4a9ae9bccfdfeec
                       ccdd25a0a404ab30b577674964d8a5bf
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [CONTEXT 7] IMPLICIT 
        DER Octet String[4] 


2024-02-01 18:05:18,297 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2024-02-01 18:05:18,300 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2024-02-01 18:05:18,349 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
2024-02-01 18:05:18,357 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2024-02-01 18:05:18,357 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
2024-02-01 18:05:18,358 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2024-02-01 18:05:18,358 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
2024-02-01 18:05:18,358 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
2024-02-01 18:05:18,358 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2024-02-01 18:05:18,359 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
2024-02-01 18:05:18,365 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2024-02-01 18:05:18,365 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2024-02-01 18:05:18,366 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2024-02-01 18:05:18,373 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2024-02-01 18:05:18,376 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
2024-02-01 18:05:18,377 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
2024-02-01 18:05:18,665 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
2024-02-01 18:05:18,666 [main] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2024-02-01 18:05:18,667 [main] INFO server.RaftServerConfigKeys: raft.server.close.threshold = 60s (default)
2024-02-01 18:05:18,667 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2024-02-01 18:05:18,670 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2024-02-01 18:05:18,671 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
2024-02-01 18:05:18,671 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
2024-02-01 18:05:18,677 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServer: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: found a subdirectory /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6
2024-02-01 18:05:18,683 [main] INFO server.RaftServer: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: addNew group-D8E6DCE0E7F6:[] returns group-D8E6DCE0E7F6:java.util.concurrent.CompletableFuture@669daa93[Not completed]
2024-02-01 18:05:18,701 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: new RaftServerImpl for group-D8E6DCE0E7F6:[] with SCMStateMachine:uninitialized
2024-02-01 18:05:18,703 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2024-02-01 18:05:18,704 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
2024-02-01 18:05:18,704 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
2024-02-01 18:05:18,704 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
2024-02-01 18:05:18,704 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2024-02-01 18:05:18,704 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.member.majority-add = false (default)
2024-02-01 18:05:18,705 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2024-02-01 18:05:18,712 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
2024-02-01 18:05:18,717 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
2024-02-01 18:05:18,720 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
2024-02-01 18:05:18,723 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
2024-02-01 18:05:18,723 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100ms (default)
2024-02-01 18:05:18,726 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
2024-02-01 18:05:18,727 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.read-after-write-consistent.write-index-cache.expiry-time = 60s (default)
2024-02-01 18:05:18,815 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2024-02-01 18:05:18,817 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2024-02-01 18:05:18,818 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
2024-02-01 18:05:18,818 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
2024-02-01 18:05:18,818 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
2024-02-01 18:05:18,819 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
2024-02-01 18:05:18,820 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
2024-02-01 18:05:18,821 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
2024-02-01 18:05:18,821 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
2024-02-01 18:05:18,861 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
2024-02-01 18:05:18,922 [main] INFO ha.SequenceIdGenerator: upgrade localId to 113750153625600000
2024-02-01 18:05:18,929 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
2024-02-01 18:05:18,937 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
2024-02-01 18:05:18,940 [main] INFO ha.SequenceIdGenerator: upgrade CertificateId to 2
2024-02-01 18:05:18,942 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
2024-02-01 18:05:19,027 [main] WARN server.ServerUtils: ozone.scm.stale.node.interval value = 30000 is smaller than min = 90000 based on the key value of hdds.heartbeat.interval, reset to the min value 90000.
2024-02-01 18:05:19,027 [main] WARN server.ServerUtils: ozone.scm.stale.node.interval value = 30000 is smaller than min = 90000 based on the key value of hdds.heartbeat.interval, reset to the min value 90000.
2024-02-01 18:05:19,027 [main] WARN server.ServerUtils: ozone.scm.dead.node.interval value = 45000 is smaller than min = 180000 based on the key value of ozone.scm.stale.node.interval, reset to the min value 180000.
2024-02-01 18:05:19,032 [main] INFO node.SCMNodeManager: Entering startup safe mode.
2024-02-01 18:05:19,051 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
2024-02-01 18:05:19,056 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2024-02-01 18:05:19,065 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
2024-02-01 18:05:19,092 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
2024-02-01 18:05:19,092 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2024-02-01 18:05:19,100 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
2024-02-01 18:05:19,100 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
2024-02-01 18:05:19,103 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
2024-02-01 18:05:19,104 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
2024-02-01 18:05:19,110 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
2024-02-01 18:05:19,111 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
2024-02-01 18:05:19,135 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
2024-02-01 18:05:19,135 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
2024-02-01 18:05:19,157 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
2024-02-01 18:05:19,219 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
2024-02-01 18:05:19,220 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:05:19,222 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
2024-02-01 18:05:19,232 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
2024-02-01 18:05:19,235 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:19,236 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
2024-02-01 18:05:19,402 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT10M
2024-02-01 18:05:19,402 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
2024-02-01 18:05:19,423 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
2024-02-01 18:05:19,427 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
2024-02-01 18:05:19,428 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 2 on primary SCM
2024-02-01 18:05:19,438 [main] INFO server.SCMCertStore: Scm certificate 2 for CN=scm-sub@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=2 is stored
2024-02-01 18:05:19,438 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
2024-02-01 18:05:19,440 [main] INFO server.SCMCertStore: Scm certificate 1 for CN=scm@scm,OU=723c6f04-c5f4-42a3-a4d9-2f4a81299889,O=CID-7f90c354-6233-41b6-a586-d8e6dce0e7f6,SERIALNUMBER=1 is stored
2024-02-01 18:05:19,444 [main] INFO ha.SequenceIdGenerator: upgrade CertificateId to 2
2024-02-01 18:05:19,466 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2024-02-01 18:05:19,500 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
2024-02-01 18:05:19,501 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
2024-02-01 18:05:19,533 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
2024-02-01 18:05:20,002 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2024-02-01 18:05:20,009 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2024-02-01 18:05:20,009 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
2024-02-01 18:05:20,010 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
2024-02-01 18:05:20,048 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2024-02-01 18:05:20,054 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2024-02-01 18:05:20,054 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
2024-02-01 18:05:20,055 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
2024-02-01 18:05:20,120 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2024-02-01 18:05:20,149 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2024-02-01 18:05:20,151 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
2024-02-01 18:05:20,164 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
2024-02-01 18:05:20,215 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
2024-02-01 18:05:20,216 [main] INFO server.StorageContainerManager: 
Container Balancer status:
Key                            Value
Running                        false
Container Balancer Configuration values:
Key                                                Value
Threshold                                          10
Max Datanodes to Involve per Iteration(percent)    20
Max Size to Move per Iteration                     500GB
Max Size Entering Target per Iteration             26GB
Max Size Leaving Source per Iteration              26GB
Number of Iterations                               10
Time Limit for Single Container's Movement         65min
Time Limit for Single Container's Replication      50min
Interval between each Iteration                    70min
Whether to Enable Network Topology                 false
Whether to Trigger Refresh Datanode Usage Info     false
Container IDs to Exclude from Balancing            None
Datanodes Specified to be Balanced                 None
Datanodes Excluded from Balancing                  None

2024-02-01 18:05:20,216 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
2024-02-01 18:05:20,239 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
2024-02-01 18:05:20,241 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
2024-02-01 18:05:20,244 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
2024-02-01 18:05:20,244 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
2024-02-01 18:05:20,244 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2024-02-01 18:05:20,264 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6/in_use.lock acquired by nodename 6@scm
2024-02-01 18:05:20,270 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=723c6f04-c5f4-42a3-a4d9-2f4a81299889} from /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6/current/raft-meta
2024-02-01 18:05:20,296 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: set configuration 0: peers:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894]|listeners:[], old=null
2024-02-01 18:05:20,298 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
2024-02-01 18:05:20,307 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
2024-02-01 18:05:20,307 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2024-02-01 18:05:20,309 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2024-02-01 18:05:20,309 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
2024-02-01 18:05:20,313 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2024-02-01 18:05:20,317 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
2024-02-01 18:05:20,317 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2024-02-01 18:05:20,318 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2024-02-01 18:05:20,319 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO util.AwaitToRun: Thread[723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-cacheEviction-AwaitToRun,5,main] started
2024-02-01 18:05:20,323 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6
2024-02-01 18:05:20,323 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2024-02-01 18:05:20,324 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
2024-02-01 18:05:20,325 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2024-02-01 18:05:20,327 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
2024-02-01 18:05:20,327 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
2024-02-01 18:05:20,328 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
2024-02-01 18:05:20,328 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
2024-02-01 18:05:20,328 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2024-02-01 18:05:20,330 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554440 (custom)
2024-02-01 18:05:20,337 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2024-02-01 18:05:20,338 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
2024-02-01 18:05:20,339 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
2024-02-01 18:05:20,339 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
2024-02-01 18:05:20,362 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: set configuration 0: peers:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894]|listeners:[], old=null
2024-02-01 18:05:20,363 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6/current/log_inprogress_0
2024-02-01 18:05:20,365 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2024-02-01 18:05:20,377 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO segmented.BufferedWriteChannel: open log_inprogress_0 (append) at position 69
2024-02-01 18:05:20,378 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: start as a follower, conf=0: peers:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894]|listeners:[], old=null
2024-02-01 18:05:20,379 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: changes role from      null to FOLLOWER at term 1 for startAsFollower
2024-02-01 18:05:20,380 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO impl.RoleInfo: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: start 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState
2024-02-01 18:05:20,382 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D8E6DCE0E7F6,id=723c6f04-c5f4-42a3-a4d9-2f4a81299889
2024-02-01 18:05:20,383 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.trigger-when-stop.enabled = true (default)
2024-02-01 18:05:20,384 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
2024-02-01 18:05:20,384 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
2024-02-01 18:05:20,384 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
2024-02-01 18:05:20,385 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
2024-02-01 18:05:20,387 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2024-02-01 18:05:20,387 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
2024-02-01 18:05:20,396 [main] INFO server.RaftServer: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: start RPC server
2024-02-01 18:05:20,458 [main] INFO server.GrpcService: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: GrpcService started, listening on 9894
2024-02-01 18:05:20,460 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-723c6f04-c5f4-42a3-a4d9-2f4a81299889: Started
2024-02-01 18:05:20,467 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894]
2024-02-01 18:05:20,467 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
2024-02-01 18:05:20,470 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
2024-02-01 18:05:20,470 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
2024-02-01 18:05:20,470 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
2024-02-01 18:05:20,586 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2024-02-01 18:05:20,603 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2024-02-01 18:05:20,603 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
2024-02-01 18:05:20,789 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
2024-02-01 18:05:20,805 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2024-02-01 18:05:20,805 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
2024-02-01 18:05:20,819 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
2024-02-01 18:05:20,820 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
2024-02-01 18:05:20,820 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2024-02-01 18:05:20,820 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
2024-02-01 18:05:20,847 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
2024-02-01 18:05:20,864 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2024-02-01 18:05:20,868 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
2024-02-01 18:05:20,869 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
2024-02-01 18:05:21,025 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
2024-02-01 18:05:21,055 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
2024-02-01 18:05:21,056 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
2024-02-01 18:05:21,118 [main] INFO util.log: Logging initialized @5828ms to org.eclipse.jetty.util.log.Slf4jLog
2024-02-01 18:05:21,312 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:34885 / 172.19.0.13:34885
2024-02-01 18:05:21,314 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:38323 / 172.19.0.9:38323
2024-02-01 18:05:21,358 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2024-02-01 18:05:21,361 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2024-02-01 18:05:21,605 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:35199 / 172.19.0.6:35199
2024-02-01 18:05:21,686 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:42883 / 172.19.0.2:42883
2024-02-01 18:05:21,704 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2024-02-01 18:05:21,705 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
2024-02-01 18:05:21,709 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:45153 / 172.19.0.8:45153
2024-02-01 18:05:21,754 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2024-02-01 18:05:21,758 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2024-02-01 18:05:21,767 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
2024-02-01 18:05:21,768 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
2024-02-01 18:05:21,768 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
2024-02-01 18:05:21,770 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:42861 / 172.19.0.5:42861
2024-02-01 18:05:21,805 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2024-02-01 18:05:21,832 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
2024-02-01 18:05:21,904 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2024-02-01 18:05:21,955 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
2024-02-01 18:05:21,957 [main] INFO http.HttpServer2: Jetty bound to port 9876
2024-02-01 18:05:21,960 [main] INFO server.Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 11.0.19+7-LTS
2024-02-01 18:05:21,966 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_5.ozonesecure_default:38323 / 172.19.0.9:38323
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:21,967 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_recon_1.ozonesecure_default:34885 / 172.19.0.13:34885
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:22,004 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_3.ozonesecure_default:35199 / 172.19.0.6:35199
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:22,042 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_2.ozonesecure_default:42883 / 172.19.0.2:42883
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:22,048 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_4.ozonesecure_default:42861 / 172.19.0.5:42861
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:22,056 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_1.ozonesecure_default:45153 / 172.19.0.8:45153
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:22,075 [main] INFO server.session: DefaultSessionIdManager workerName=node0
2024-02-01 18:05:22,076 [main] INFO server.session: No SessionScavenger set, using defaults
2024-02-01 18:05:22,079 [main] INFO server.session: node0 Scavenging every 660000ms
2024-02-01 18:05:22,097 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/scm.keytab, for principal HTTP/scm@EXAMPLE.COM
2024-02-01 18:05:22,102 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@57e1bc88{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
2024-02-01 18:05:22,102 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@27d7ee42{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2024-02-01 18:05:22,119 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:44779 / 172.19.0.11:44779
2024-02-01 18:05:22,141 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2024-02-01 18:05:22,248 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/scm.keytab, for principal HTTP/scm@EXAMPLE.COM
2024-02-01 18:05:22,258 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@70661538{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_5_0-SNAPSHOT_jar-_-any-8495637465159364845/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.5.0-SNAPSHOT.jar!/webapps/scm}
2024-02-01 18:05:22,269 [main] INFO server.AbstractConnector: Started ServerConnector@e7283dd{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
2024-02-01 18:05:22,269 [main] INFO server.Server: Started @6979ms
2024-02-01 18:05:22,271 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
2024-02-01 18:05:22,271 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
2024-02-01 18:05:22,273 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
2024-02-01 18:05:22,391 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm:43989 / 172.19.0.4:43989
2024-02-01 18:05:22,403 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2024-02-01 18:05:22,404 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#0 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from scm:43989 / 172.19.0.4:43989
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:23,984 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_5.ozonesecure_default:38323 / 172.19.0.9:38323
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:23,994 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_recon_1.ozonesecure_default:34885 / 172.19.0.13:34885
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:24,031 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#9 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_3.ozonesecure_default:35199 / 172.19.0.6:35199
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:24,058 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_2.ozonesecure_default:42883 / 172.19.0.2:42883
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:24,068 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_1.ozonesecure_default:45153 / 172.19.0.8:45153
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:24,070 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_4.ozonesecure_default:42861 / 172.19.0.5:42861
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:24,223 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:05:24,411 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#1 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from scm:43989 / 172.19.0.4:43989
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2024-02-01 18:05:24,413 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-scm/sub-ca-refreshCACertificates] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:723c6f04-c5f4-42a3-a4d9-2f4a81299889 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:105)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
, while invoking $Proxy15.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.4:9961 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
2024-02-01 18:05:25,146 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm:42397 / 172.19.0.4:42397
2024-02-01 18:05:25,163 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:05:25,544 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO impl.FollowerState: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5163998846ns, electionTimeout:5156ms
2024-02-01 18:05:25,544 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO impl.RoleInfo: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: shutdown 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState
2024-02-01 18:05:25,545 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
2024-02-01 18:05:25,547 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
2024-02-01 18:05:25,547 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-FollowerState] INFO impl.RoleInfo: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: start 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1
2024-02-01 18:05:25,548 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO impl.LeaderElection: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894]|listeners:[], old=null
2024-02-01 18:05:25,549 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO impl.LeaderElection: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
2024-02-01 18:05:25,553 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO impl.LeaderElection: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894]|listeners:[], old=null
2024-02-01 18:05:25,553 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO impl.LeaderElection: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1 ELECTION round 0: result PASSED (term=2)
2024-02-01 18:05:25,553 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO impl.RoleInfo: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: shutdown 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1
2024-02-01 18:05:25,553 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
2024-02-01 18:05:25,557 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
2024-02-01 18:05:25,559 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2024-02-01 18:05:25,560 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
2024-02-01 18:05:25,562 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
2024-02-01 18:05:25,562 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
2024-02-01 18:05:25,562 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
2024-02-01 18:05:25,566 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.enabled = false (default)
2024-02-01 18:05:25,567 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.timeout.ratio = 0.9 (default)
2024-02-01 18:05:25,567 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2024-02-01 18:05:25,567 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2024-02-01 18:05:25,567 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
2024-02-01 18:05:25,568 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO impl.RoleInfo: 723c6f04-c5f4-42a3-a4d9-2f4a81299889: start 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderStateImpl
2024-02-01 18:05:25,568 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: set firstElectionSinceStartup to false for becomeLeader
2024-02-01 18:05:25,568 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
2024-02-01 18:05:25,569 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
2024-02-01 18:05:25,571 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: change Leader from null to 723c6f04-c5f4-42a3-a4d9-2f4a81299889 at term 2 for becomeLeader, leader elected after 6851ms
2024-02-01 18:05:25,576 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
2024-02-01 18:05:25,578 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6/current/log_inprogress_0 to /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6/current/log_0-0
2024-02-01 18:05:25,579 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-LeaderElection1] INFO server.RaftServer$Division: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6: set configuration 1: peers:[723c6f04-c5f4-42a3-a4d9-2f4a81299889|scm:9894]|listeners:[], old=null
2024-02-01 18:05:25,580 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker] INFO segmented.BufferedWriteChannel: open log_inprogress_1 at position 0
2024-02-01 18:05:25,591 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/7f90c354-6233-41b6-a586-d8e6dce0e7f6/current/log_inprogress_1
2024-02-01 18:05:25,597 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO server.RaftServer$Division: leader is ready since appliedIndex == 1 >= startIndex == 1
2024-02-01 18:05:25,597 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
2024-02-01 18:05:25,598 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
2024-02-01 18:05:25,599 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: Initializing SecretKeys.
2024-02-01 18:05:25,600 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: No valid key has been loaded. A new key is generated: SecretKey(id = 41f4e9ac-f679-4a30-8abd-3993a4ba2453, creation at: 2024-02-01T18:05:25.599645Z, expire at: 2024-02-08T18:05:25.599645Z)
2024-02-01 18:05:25,606 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:25,606 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
2024-02-01 18:05:25,607 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
2024-02-01 18:05:25,607 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
2024-02-01 18:05:25,609 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
2024-02-01 18:05:25,628 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2024-02-01 18:05:25,694 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 41f4e9ac-f679-4a30-8abd-3993a4ba2453, creation at: 2024-02-01T18:05:25.599Z, expire at: 2024-02-08T18:05:25.599Z)]
2024-02-01 18:05:25,694 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 41f4e9ac-f679-4a30-8abd-3993a4ba2453, creation at: 2024-02-01T18:05:25.599Z, expire at: 2024-02-08T18:05:25.599Z)
2024-02-01 18:05:25,730 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 41f4e9ac-f679-4a30-8abd-3993a4ba2453, creation at: 2024-02-01T18:05:25.599Z, expire at: 2024-02-08T18:05:25.599Z)] to file /data/metadata/scm/keys/secret_keys.json
2024-02-01 18:05:25,731 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:25,731 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
2024-02-01 18:05:25,731 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
2024-02-01 18:05:26,010 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for RECON recon, UUID: 23a7423c-367e-424d-b5ff-0e85936d833c
2024-02-01 18:05:26,010 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: 6197f77b-f550-485b-8efa-1fede16758a8
2024-02-01 18:05:26,020 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for CertificateId, expected lastId is 0, actual lastId is 2.
2024-02-01 18:05:26,024 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:26,027 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:26,027 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 2 to 3.
2024-02-01 18:05:26,168 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2024-02-01 18:05:26,168 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2024-02-01 18:05:26,194 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2024-02-01 18:05:26,720 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:26,774 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:26,775 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 3 to 4.
2024-02-01 18:05:26,794 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2024-02-01 18:05:26,794 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2024-02-01 18:05:26,807 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2024-02-01 18:05:26,860 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:26,879 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: c3350086-6cc4-474e-91c0-301184edbd03
2024-02-01 18:05:26,885 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:26,887 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 4 to 5.
2024-02-01 18:05:26,911 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2024-02-01 18:05:26,916 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2024-02-01 18:05:26,928 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2024-02-01 18:05:26,988 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:27,006 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: f86d5d8b-5e92-40d9-89ae-845d527031b5
2024-02-01 18:05:27,028 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b
2024-02-01 18:05:27,033 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:27,044 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 5 to 6.
2024-02-01 18:05:27,097 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2024-02-01 18:05:27,098 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2024-02-01 18:05:27,149 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2024-02-01 18:05:27,258 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:27,279 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:27,280 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 6 to 7.
2024-02-01 18:05:27,289 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: 00dd29b0-a872-4d37-95d3-e85543918f3f
2024-02-01 18:05:27,306 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2024-02-01 18:05:27,306 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2024-02-01 18:05:27,332 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2024-02-01 18:05:27,521 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:27,553 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:27,554 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 7 to 8.
2024-02-01 18:05:27,596 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2024-02-01 18:05:27,597 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2024-02-01 18:05:27,607 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2024-02-01 18:05:27,661 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:41407 / 172.19.0.11:41407
2024-02-01 18:05:27,669 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2024-02-01 18:05:27,681 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:27,714 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2024-02-01 18:05:27,714 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2024-02-01 18:05:27,717 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2024-02-01 18:05:27,717 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2024-02-01 18:05:27,718 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2024-02-01 18:05:27,718 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2024-02-01 18:05:27,718 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2024-02-01 18:05:27,721 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2024-02-01 18:05:27,722 [723c6f04-c5f4-42a3-a4d9-2f4a81299889-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: CA certificates are not changed.
2024-02-01 18:05:27,728 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2024-02-01 18:05:27,730 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2024-02-01 18:05:27,731 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2024-02-01 18:05:27,731 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2024-02-01 18:05:27,748 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om, UUID: 5a0d1b1a-7bad-4fc4-a54d-a755f5d6c84e
2024-02-01 18:05:27,773 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:27,775 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 8 to 9.
2024-02-01 18:05:27,949 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2024-02-01 18:05:27,965 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2024-02-01 18:05:28,053 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2024-02-01 18:05:28,227 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:28,379 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2024-02-01 18:05:28,379 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2024-02-01 18:05:28,380 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2024-02-01 18:05:28,383 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2024-02-01 18:05:28,694 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:46113 / 172.19.0.8:46113
2024-02-01 18:05:28,696 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2024-02-01 18:05:28,700 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2024-02-01 18:05:28,780 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:38357 / 172.19.0.6:38357
2024-02-01 18:05:28,796 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:05:28,860 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:05:28,921 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:34203 / 172.19.0.9:34203
2024-02-01 18:05:28,981 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:05:29,223 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:05:29,349 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:36905 / 172.19.0.2:36905
2024-02-01 18:05:29,383 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:05:29,387 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:44041 / 172.19.0.5:44041
2024-02-01 18:05:29,461 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:05:34,224 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:05:39,224 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:05:44,090 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:40979 / 172.19.0.11:40979
2024-02-01 18:05:44,131 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2024-02-01 18:05:44,224 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:05:44,957 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:40413 / 172.19.0.5:40413
2024-02-01 18:05:44,982 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:05:45,100 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:38325 / 172.19.0.9:38325
2024-02-01 18:05:45,161 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:05:45,184 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:37359 / 172.19.0.6:37359
2024-02-01 18:05:45,219 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:05:45,996 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:34031 / 172.19.0.8:34031
2024-02-01 18:05:46,016 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:05:46,160 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:38567 / 172.19.0.2:38567
2024-02-01 18:05:46,185 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:05:46,982 [IPC Server handler 41 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/00dd29b0-a872-4d37-95d3-e85543918f3f
2024-02-01 18:05:46,991 [IPC Server handler 41 on default port 9861] INFO node.SCMNodeManager: Registered datanode: 00dd29b0-a872-4d37-95d3-e85543918f3f{ip: 172.19.0.5, host: ozonesecure_datanode_4.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 8, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2024-02-01 18:05:46,998 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 5 required.
2024-02-01 18:05:47,000 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2024-02-01 18:05:47,029 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=99010d61-4807-4c38-b353-61aa4809c03a to datanode:00dd29b0-a872-4d37-95d3-e85543918f3f
2024-02-01 18:05:47,093 [IPC Server handler 40 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/6197f77b-f550-485b-8efa-1fede16758a8
2024-02-01 18:05:47,107 [IPC Server handler 40 on default port 9861] INFO node.SCMNodeManager: Registered datanode: 6197f77b-f550-485b-8efa-1fede16758a8{ip: 172.19.0.9, host: ozonesecure_datanode_5.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 4, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2024-02-01 18:05:47,102 [IPC Server handler 36 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c3350086-6cc4-474e-91c0-301184edbd03
2024-02-01 18:05:47,127 [IPC Server handler 36 on default port 9861] INFO node.SCMNodeManager: Registered datanode: c3350086-6cc4-474e-91c0-301184edbd03{ip: 172.19.0.6, host: ozonesecure_datanode_3.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 5, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2024-02-01 18:05:47,109 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 5 required.
2024-02-01 18:05:47,108 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2024-02-01 18:05:47,128 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2024-02-01 18:05:47,137 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 5 required.
2024-02-01 18:05:47,146 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:47,149 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 99010d61-4807-4c38-b353-61aa4809c03a, Nodes: 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:05:47.028852Z[UTC]]
2024-02-01 18:05:47,188 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e0dea549-7be8-448a-8201-a5bbcfbc6cde to datanode:c3350086-6cc4-474e-91c0-301184edbd03
2024-02-01 18:05:47,197 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:47,199 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: e0dea549-7be8-448a-8201-a5bbcfbc6cde, Nodes: c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:05:47.188836Z[UTC]]
2024-02-01 18:05:47,200 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=19d6b066-a82f-41e8-a5a6-575432061236 to datanode:6197f77b-f550-485b-8efa-1fede16758a8
2024-02-01 18:05:47,217 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:47,217 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 19d6b066-a82f-41e8-a5a6-575432061236, Nodes: 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:05:47.200277Z[UTC]]
2024-02-01 18:05:48,021 [IPC Server handler 41 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/040c5bc7-c3de-4dd7-aa13-fc40341a6f9b
2024-02-01 18:05:48,021 [IPC Server handler 41 on default port 9861] INFO node.SCMNodeManager: Registered datanode: 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b{ip: 172.19.0.8, host: ozonesecure_datanode_1.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 6, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2024-02-01 18:05:48,021 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 4 DataNodes registered, 5 required.
2024-02-01 18:05:48,022 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2024-02-01 18:05:48,023 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3592f4d3-7110-4966-93d1-1a3de731c961 to datanode:040c5bc7-c3de-4dd7-aa13-fc40341a6f9b
2024-02-01 18:05:48,026 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:48,026 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 3592f4d3-7110-4966-93d1-1a3de731c961, Nodes: 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:05:48.023442Z[UTC]]
2024-02-01 18:05:48,210 [IPC Server handler 25 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f86d5d8b-5e92-40d9-89ae-845d527031b5
2024-02-01 18:05:48,210 [IPC Server handler 25 on default port 9861] INFO node.SCMNodeManager: Registered datanode: f86d5d8b-5e92-40d9-89ae-845d527031b5{ip: 172.19.0.2, host: ozonesecure_datanode_2.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 7, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2024-02-01 18:05:48,210 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 5 DataNodes registered, 5 required.
2024-02-01 18:05:48,210 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
2024-02-01 18:05:48,210 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
2024-02-01 18:05:48,210 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2024-02-01 18:05:48,210 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
2024-02-01 18:05:48,211 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2024-02-01 18:05:48,211 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7177a92c-4d64-4a30-b4f7-04fe3683da6c to datanode:f86d5d8b-5e92-40d9-89ae-845d527031b5
2024-02-01 18:05:48,214 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:48,215 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 7177a92c-4d64-4a30-b4f7-04fe3683da6c, Nodes: f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:05:48.211429Z[UTC]]
2024-02-01 18:05:48,219 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57 to datanode:6197f77b-f550-485b-8efa-1fede16758a8
2024-02-01 18:05:48,228 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57 to datanode:f86d5d8b-5e92-40d9-89ae-845d527031b5
2024-02-01 18:05:48,228 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57 to datanode:040c5bc7-c3de-4dd7-aa13-fc40341a6f9b
2024-02-01 18:05:48,232 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:48,233 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: a6071b12-cba5-48f6-8c89-887a5f3e3d57, Nodes: 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:05:48.219519Z[UTC]]
2024-02-01 18:05:49,224 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:05:49,582 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:45797 / 172.19.0.11:45797
2024-02-01 18:05:49,607 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
2024-02-01 18:05:49,757 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:32925 / 172.19.0.13:32925
2024-02-01 18:05:49,761 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:05:50,923 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:50,923 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=99010d61-4807-4c38-b353-61aa4809c03a
2024-02-01 18:05:50,936 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:51,020 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:51,021 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=19d6b066-a82f-41e8-a5a6-575432061236
2024-02-01 18:05:51,021 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:51,070 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:51,071 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=e0dea549-7be8-448a-8201-a5bbcfbc6cde
2024-02-01 18:05:51,071 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:51,147 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:51,251 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:51,252 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=3592f4d3-7110-4966-93d1-1a3de731c961
2024-02-01 18:05:51,253 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:51,477 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:51,588 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2024-02-01 18:05:51,589 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=7177a92c-4d64-4a30-b4f7-04fe3683da6c
2024-02-01 18:05:51,589 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:51,839 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:54,225 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:05:55,705 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:56,005 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:56,131 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:56,522 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:56,658 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2024-02-01 18:05:56,664 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
2024-02-01 18:05:56,667 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57
2024-02-01 18:05:56,667 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
2024-02-01 18:05:56,667 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
2024-02-01 18:05:56,667 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
2024-02-01 18:05:56,667 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
2024-02-01 18:05:56,667 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
2024-02-01 18:05:56,667 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
2024-02-01 18:05:56,668 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
2024-02-01 18:05:56,668 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO block.SCMBlockDeletingService: notifyStatusChanged:RUNNING
2024-02-01 18:05:56,668 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
2024-02-01 18:05:56,673 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
2024-02-01 18:05:56,673 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
2024-02-01 18:05:59,225 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:06:04,225 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:06:09,225 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:06:14,225 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:06:14,719 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:36623 / 172.19.0.11:36623
2024-02-01 18:06:14,723 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2024-02-01 18:06:14,730 [IPC Server handler 74 on default port 9863] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5), c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6), f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2), 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)]. isPolicySatisfied: true.
2024-02-01 18:06:14,737 [IPC Server handler 74 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
2024-02-01 18:06:14,748 [IPC Server handler 74 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: 78131eda-c94d-4bce-b16d-40e77cb3d806, Nodes: 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:06:14.730962Z[UTC]]
2024-02-01 18:06:14,750 [723c6f04-c5f4-42a3-a4d9-2f4a81299889@group-D8E6DCE0E7F6-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 113750153625600000.
2024-02-01 18:06:14,752 [IPC Server handler 74 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 113750153625600000 to 113750153625601000.
2024-02-01 18:06:15,874 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:39021 / 172.19.0.9:39021
2024-02-01 18:06:15,880 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:06:15,929 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:42857 / 172.19.0.8:42857
2024-02-01 18:06:15,932 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:06:15,949 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:36125 / 172.19.0.5:36125
2024-02-01 18:06:15,967 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:06:15,982 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:37313 / 172.19.0.9:37313
2024-02-01 18:06:15,997 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:06:16,056 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:35671 / 172.19.0.8:35671
2024-02-01 18:06:16,064 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:44293 / 172.19.0.13:44293
2024-02-01 18:06:16,100 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:06:16,122 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:44655 / 172.19.0.5:44655
2024-02-01 18:06:16,133 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:06:16,160 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:06:16,415 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:35735 / 172.19.0.2:35735
2024-02-01 18:06:16,417 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:06:16,498 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:40213 / 172.19.0.2:40213
2024-02-01 18:06:16,564 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:06:16,643 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:46051 / 172.19.0.6:46051
2024-02-01 18:06:16,658 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:06:16,750 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:34915 / 172.19.0.6:34915
2024-02-01 18:06:16,755 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:06:17,375 [IPC Server handler 74 on default port 9863] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9), c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6), f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2), 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)]. isPolicySatisfied: true.
2024-02-01 18:06:17,384 [IPC Server handler 74 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: 3ec162d6-2472-47e2-b227-0af8745fa14b, Nodes: 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:06:17.375524Z[UTC]]
2024-02-01 18:06:18,115 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:33569 / 172.19.0.11:33569
2024-02-01 18:06:18,122 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:06:19,226 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:06:19,686 [IPC Server handler 48 on default port 9863] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9), 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5), f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2), c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6)]. isPolicySatisfied: true.
2024-02-01 18:06:19,692 [IPC Server handler 48 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: 8b6304f6-7ecd-42f8-94b1-e59513c88279, Nodes: 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:06:19.686624Z[UTC]]
2024-02-01 18:06:20,941 [IPC Server handler 93 on default port 9863] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6), 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9), f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2), 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5), 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)]. isPolicySatisfied: true.
2024-02-01 18:06:20,949 [IPC Server handler 93 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: 3e32c5bc-dfdf-4fa6-b04c-14737f549e26, Nodes: c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6)6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:06:20.942335Z[UTC]]
2024-02-01 18:06:21,147 [IPC Server handler 1 on default port 9863] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6), 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9), 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5), f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)]. isPolicySatisfied: true.
2024-02-01 18:06:21,154 [IPC Server handler 1 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: 533b24af-c82a-40aa-884f-929b72588b74, Nodes: 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6)6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:06:21.147717Z[UTC]]
2024-02-01 18:06:23,024 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:06:23,323 [IPC Server handler 74 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:06:24,202 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:06:24,226 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2024-02-01 18:06:29,230 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 4 milliseconds for processing 5 containers.
2024-02-01 18:06:34,231 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2024-02-01 18:06:39,232 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2024-02-01 18:06:44,233 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 5 containers.
2024-02-01 18:06:48,229 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:43823 / 172.19.0.11:43823
2024-02-01 18:06:48,235 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2024-02-01 18:06:49,233 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 5 containers.
2024-02-01 18:06:49,675 [IPC Server handler 9 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
2024-02-01 18:06:50,178 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:06:50,425 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:34603 / 172.19.0.13:34603
2024-02-01 18:06:50,436 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:06:51,209 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:44457 / 172.19.0.2:44457
2024-02-01 18:06:51,232 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:46265 / 172.19.0.5:46265
2024-02-01 18:06:51,240 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:06:51,241 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:06:51,242 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:43707 / 172.19.0.8:43707
2024-02-01 18:06:51,265 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:06:51,315 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:38661 / 172.19.0.6:38661
2024-02-01 18:06:51,330 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:06:51,353 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:41901 / 172.19.0.9:41901
2024-02-01 18:06:51,366 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:06:54,234 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2024-02-01 18:06:55,657 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:46461 / 172.19.0.11:46461
2024-02-01 18:06:55,660 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:06:55,663 [IPC Server handler 9 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:06:56,279 [IPC Server handler 74 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:06:56,889 [IPC Server handler 71 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:06:59,234 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 5 containers.
2024-02-01 18:07:04,235 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 5 containers.
2024-02-01 18:07:04,677 [IPC Server handler 47 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:05,296 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:05,909 [IPC Server handler 37 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:09,236 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2024-02-01 18:07:13,941 [IPC Server handler 96 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:13,943 [IPC Server handler 93 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:13,943 [IPC Server handler 96 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:14,237 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2024-02-01 18:07:16,539 [IPC Server handler 9 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:17,794 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:19,237 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 5 containers.
2024-02-01 18:07:19,741 [IPC Server handler 72 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:20,691 [IPC Server handler 72 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:21,234 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:39845 / 172.19.0.2:39845
2024-02-01 18:07:21,248 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:37033 / 172.19.0.5:37033
2024-02-01 18:07:21,253 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:07:21,262 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:07:21,280 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:36845 / 172.19.0.8:36845
2024-02-01 18:07:21,282 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:07:21,321 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:37751 / 172.19.0.6:37751
2024-02-01 18:07:21,340 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:35621 / 172.19.0.9:35621
2024-02-01 18:07:21,342 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:07:21,348 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:07:21,776 [IPC Server handler 23 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:24,238 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2024-02-01 18:07:25,405 [IPC Server handler 55 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:25,856 [IPC Server handler 32 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:26,316 [IPC Server handler 55 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:27,324 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:27,894 [IPC Server handler 80 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:29,228 [IPC Server handler 74 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:29,239 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2024-02-01 18:07:32,893 [IPC Server handler 71 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:32,898 [IPC Server handler 80 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:33,148 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:35847 / 172.19.0.9:35847
2024-02-01 18:07:33,149 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:37779 / 172.19.0.2:37779
2024-02-01 18:07:33,168 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:07:33,180 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:36553 / 172.19.0.8:36553
2024-02-01 18:07:33,190 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:07:33,203 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:07:33,212 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:36727 / 172.19.0.13:36727
2024-02-01 18:07:33,236 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:42935 / 172.19.0.11:42935
2024-02-01 18:07:33,238 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:07:33,246 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:07:33,252 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:34,170 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:34,174 [IPC Server handler 74 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:34,222 [IPC Server handler 55 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:34,239 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:07:39,240 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:07:40,833 [IPC Server handler 32 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:40,837 [IPC Server handler 68 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:40,931 [IPC Server handler 93 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:07:44,241 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:07:49,241 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:07:51,234 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:34147 / 172.19.0.5:34147
2024-02-01 18:07:51,237 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:07:51,319 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:39097 / 172.19.0.6:39097
2024-02-01 18:07:51,327 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:07:54,242 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:07:59,243 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:08:03,152 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:38335 / 172.19.0.9:38335
2024-02-01 18:08:03,153 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:41867 / 172.19.0.8:41867
2024-02-01 18:08:03,154 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:45935 / 172.19.0.2:45935
2024-02-01 18:08:03,168 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:08:03,173 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:08:03,181 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:08:04,243 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:08:08,327 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:08,829 [IPC Server handler 68 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:09,244 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:08:09,295 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:09,766 [IPC Server handler 23 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:10,239 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:10,695 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:11,152 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:11,639 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:12,186 [IPC Server handler 55 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:12,667 [IPC Server handler 23 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:13,134 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:13,610 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:14,077 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:14,245 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:08:19,245 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:08:21,239 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:41863 / 172.19.0.5:41863
2024-02-01 18:08:21,245 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:08:21,362 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:34729 / 172.19.0.6:34729
2024-02-01 18:08:21,373 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:08:23,947 [IPC Server handler 96 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:08:24,246 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:08:26,744 [IPC Server handler 68 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:28,232 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:29,247 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:08:30,146 [IPC Server handler 55 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:30,147 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.3
2024-02-01 18:08:33,185 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:43897 / 172.19.0.8:43897
2024-02-01 18:08:33,214 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:08:33,215 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:42183 / 172.19.0.9:42183
2024-02-01 18:08:33,215 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:42103 / 172.19.0.2:42103
2024-02-01 18:08:33,221 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:08:33,224 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:08:34,248 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:08:39,254 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:08:44,255 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:08:49,256 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:08:49,661 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:44477 / 172.19.0.11:44477
2024-02-01 18:08:49,679 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2024-02-01 18:08:51,238 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:43639 / 172.19.0.5:43639
2024-02-01 18:08:51,267 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:08:51,318 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:37515 / 172.19.0.6:37515
2024-02-01 18:08:51,336 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:08:54,256 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:08:59,257 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:09:03,191 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:45049 / 172.19.0.2:45049
2024-02-01 18:09:03,194 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:34357 / 172.19.0.8:34357
2024-02-01 18:09:03,198 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:40719 / 172.19.0.9:40719
2024-02-01 18:09:03,221 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:09:03,231 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:09:03,237 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:09:04,257 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:09:09,258 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:09:14,258 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:09:19,259 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:09:21,236 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:44983 / 172.19.0.5:44983
2024-02-01 18:09:21,272 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:09:21,315 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:35139 / 172.19.0.6:35139
2024-02-01 18:09:21,320 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:09:24,260 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:09:29,261 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:09:33,130 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:42005 / 172.19.0.8:42005
2024-02-01 18:09:33,151 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:09:33,156 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:33045 / 172.19.0.2:33045
2024-02-01 18:09:33,162 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:35479 / 172.19.0.9:35479
2024-02-01 18:09:33,162 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:09:33,172 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:09:34,262 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:09:39,263 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:09:44,264 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:09:49,265 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:09:51,235 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:46623 / 172.19.0.5:46623
2024-02-01 18:09:51,246 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:09:51,334 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:45063 / 172.19.0.6:45063
2024-02-01 18:09:51,344 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:09:54,265 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:09:59,266 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:10:03,183 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:45505 / 172.19.0.2:45505
2024-02-01 18:10:03,184 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:37545 / 172.19.0.8:37545
2024-02-01 18:10:03,191 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:44077 / 172.19.0.9:44077
2024-02-01 18:10:03,191 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:10:03,196 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:10:03,199 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:10:04,267 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:10:09,267 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:10:14,268 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:10:19,268 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:10:21,244 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:43335 / 172.19.0.5:43335
2024-02-01 18:10:21,254 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:10:21,331 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:34997 / 172.19.0.6:34997
2024-02-01 18:10:21,336 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:10:24,269 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:10:29,270 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:10:33,170 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:33187 / 172.19.0.8:33187
2024-02-01 18:10:33,184 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:10:33,185 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:38283 / 172.19.0.2:38283
2024-02-01 18:10:33,230 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:10:33,233 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:33051 / 172.19.0.9:33051
2024-02-01 18:10:33,239 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:10:34,271 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:10:39,272 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:10:44,272 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:10:49,273 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:10:51,226 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:39309 / 172.19.0.5:39309
2024-02-01 18:10:51,232 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:10:51,343 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:43351 / 172.19.0.6:43351
2024-02-01 18:10:51,346 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:10:54,275 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:10:59,276 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:11:03,156 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:33545 / 172.19.0.9:33545
2024-02-01 18:11:03,166 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:40341 / 172.19.0.8:40341
2024-02-01 18:11:03,167 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:44765 / 172.19.0.2:44765
2024-02-01 18:11:03,182 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:11:03,193 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:11:03,193 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:11:04,277 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:11:09,278 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:11:11,376 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:41031 / 172.19.0.11:41031
2024-02-01 18:11:11,379 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2024-02-01 18:11:14,278 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:11:16,960 [IPC Server handler 93 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:11:19,279 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:11:20,553 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:41187 / 172.19.0.13:41187
2024-02-01 18:11:20,557 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:11:21,259 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:41613 / 172.19.0.5:41613
2024-02-01 18:11:21,278 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:11:21,318 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:43753 / 172.19.0.6:43753
2024-02-01 18:11:21,333 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:11:22,272 [IPC Server handler 9 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:11:24,280 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:11:29,281 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:11:33,154 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:39453 / 172.19.0.8:39453
2024-02-01 18:11:33,164 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:43759 / 172.19.0.9:43759
2024-02-01 18:11:33,165 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:35987 / 172.19.0.2:35987
2024-02-01 18:11:33,179 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:11:33,182 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:11:33,189 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:11:34,282 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:11:35,663 [IPC Server handler 23 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:11:39,282 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:11:40,763 [IPC Server handler 68 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:11:44,283 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:11:49,283 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:11:51,238 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:46263 / 172.19.0.5:46263
2024-02-01 18:11:51,249 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:11:51,343 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:35233 / 172.19.0.6:35233
2024-02-01 18:11:51,351 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:11:53,951 [IPC Server handler 96 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:11:54,284 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:11:59,284 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:11:59,447 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:12:03,159 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:36075 / 172.19.0.8:36075
2024-02-01 18:12:03,173 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:46237 / 172.19.0.9:46237
2024-02-01 18:12:03,179 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:12:03,183 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:35563 / 172.19.0.2:35563
2024-02-01 18:12:03,185 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:12:03,199 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:12:04,286 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:12:09,286 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:12:14,287 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:12:15,922 [IPC Server handler 81 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:12:19,287 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:12:21,230 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:41953 / 172.19.0.5:41953
2024-02-01 18:12:21,239 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:12:21,325 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:35775 / 172.19.0.6:35775
2024-02-01 18:12:21,330 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:12:22,500 [IPC Server handler 23 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:12:24,288 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:12:29,288 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:12:33,170 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:39047 / 172.19.0.8:39047
2024-02-01 18:12:33,171 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:34663 / 172.19.0.2:34663
2024-02-01 18:12:33,180 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:12:33,183 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:12:33,183 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:33093 / 172.19.0.9:33093
2024-02-01 18:12:33,186 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:12:34,289 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:12:35,659 [IPC Server handler 32 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:12:39,290 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:12:40,924 [IPC Server handler 76 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:12:44,290 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:12:49,291 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:12:51,220 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:44495 / 172.19.0.5:44495
2024-02-01 18:12:51,227 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:12:51,327 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:40951 / 172.19.0.6:40951
2024-02-01 18:12:51,330 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:12:54,291 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:12:54,672 [IPC Server handler 68 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:12:59,293 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:12:59,773 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:13:03,172 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:46001 / 172.19.0.8:46001
2024-02-01 18:13:03,198 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:13:03,205 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:41625 / 172.19.0.9:41625
2024-02-01 18:13:03,206 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:35509 / 172.19.0.2:35509
2024-02-01 18:13:03,213 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:13:03,219 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:13:04,293 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:13:09,294 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:13:13,513 [IPC Server handler 32 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:13:14,294 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:13:18,637 [IPC Server handler 68 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:13:19,295 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:13:21,226 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:44381 / 172.19.0.5:44381
2024-02-01 18:13:21,244 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:13:21,317 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:37537 / 172.19.0.6:37537
2024-02-01 18:13:21,336 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:13:24,295 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:13:29,296 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:13:32,093 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:13:33,132 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:42025 / 172.19.0.8:42025
2024-02-01 18:13:33,145 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:40861 / 172.19.0.9:40861
2024-02-01 18:13:33,150 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:13:33,154 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:13:33,176 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:41149 / 172.19.0.2:41149
2024-02-01 18:13:33,184 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:13:34,296 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:13:37,203 [IPC Server handler 9 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:13:39,297 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:13:44,298 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:13:49,300 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:13:51,233 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:46207 / 172.19.0.5:46207
2024-02-01 18:13:51,240 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:13:51,330 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:35209 / 172.19.0.6:35209
2024-02-01 18:13:51,337 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:13:54,302 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:13:59,303 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:14:03,146 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:35287 / 172.19.0.8:35287
2024-02-01 18:14:03,164 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:14:03,176 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:46201 / 172.19.0.9:46201
2024-02-01 18:14:03,179 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:43527 / 172.19.0.2:43527
2024-02-01 18:14:03,188 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:14:03,194 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:14:04,303 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:14:09,304 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:14:11,112 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:37093 / 172.19.0.11:37093
2024-02-01 18:14:11,115 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2024-02-01 18:14:12,446 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:38203 / 172.19.0.11:38203
2024-02-01 18:14:12,452 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:14:14,305 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:14:19,305 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:14:21,220 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:43851 / 172.19.0.5:43851
2024-02-01 18:14:21,236 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:14:21,319 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:33083 / 172.19.0.6:33083
2024-02-01 18:14:21,329 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:14:24,306 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:14:25,800 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:39519 / 172.19.0.11:39519
2024-02-01 18:14:25,802 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2024-02-01 18:14:25,802 [IPC Server handler 11 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:14:29,306 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:14:30,998 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:14:33,136 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:44587 / 172.19.0.8:44587
2024-02-01 18:14:33,145 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:14:33,152 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:37937 / 172.19.0.2:37937
2024-02-01 18:14:33,156 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:14:34,307 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:14:39,307 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:14:44,308 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:14:49,308 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:14:51,213 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:40307 / 172.19.0.5:40307
2024-02-01 18:14:51,218 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:14:51,317 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:42933 / 172.19.0.6:42933
2024-02-01 18:14:51,326 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:14:54,308 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:14:59,309 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:15:03,127 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:37027 / 172.19.0.8:37027
2024-02-01 18:15:03,140 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:35485 / 172.19.0.2:35485
2024-02-01 18:15:03,144 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:15:03,151 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:15:04,309 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:15:09,309 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:15:14,310 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:15:19,310 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:15:20,694 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:40195 / 172.19.0.11:40195
2024-02-01 18:15:20,696 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:15:20,704 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:33169 / 172.19.0.11:33169
2024-02-01 18:15:20,705 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2024-02-01 18:15:20,705 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:15:21,214 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:34453 / 172.19.0.5:34453
2024-02-01 18:15:21,220 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:15:21,314 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:40629 / 172.19.0.6:40629
2024-02-01 18:15:21,320 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:15:23,748 [IPC Server handler 11 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:15:24,310 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:15:27,766 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:15:29,311 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:15:31,911 [IPC Server handler 51 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:15:33,137 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:46103 / 172.19.0.8:46103
2024-02-01 18:15:33,140 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:44889 / 172.19.0.2:44889
2024-02-01 18:15:33,145 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:15:33,147 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:15:34,148 [EventQueue-StaleNodeForStaleNodeHandler] INFO node.StaleNodeHandler: Datanode 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) moved to stale state. Finalizing its pipelines [PipelineID=19d6b066-a82f-41e8-a5a6-575432061236, PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, PipelineID=533b24af-c82a-40aa-884f-929b72588b74]
2024-02-01 18:15:34,151 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline PipelineID=19d6b066-a82f-41e8-a5a6-575432061236 moved to CLOSED state
2024-02-01 18:15:34,156 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Container #1 closed for pipeline=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806
2024-02-01 18:15:34,156 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1, current state: CLOSING
2024-02-01 18:15:34,158 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806 moved to CLOSED state
2024-02-01 18:15:34,160 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Container #3 closed for pipeline=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279
2024-02-01 18:15:34,162 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279 moved to CLOSED state
2024-02-01 18:15:34,164 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Container #4 closed for pipeline=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26
2024-02-01 18:15:34,166 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26 moved to CLOSED state
2024-02-01 18:15:34,168 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Container #6 closed for pipeline=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57
2024-02-01 18:15:34,170 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57 moved to CLOSED state
2024-02-01 18:15:34,173 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Container #2 closed for pipeline=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b
2024-02-01 18:15:34,173 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #3, current state: CLOSING
2024-02-01 18:15:34,173 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #4, current state: CLOSING
2024-02-01 18:15:34,174 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #6, current state: CLOSING
2024-02-01 18:15:34,174 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2, current state: CLOSING
2024-02-01 18:15:34,174 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b moved to CLOSED state
2024-02-01 18:15:34,176 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Container #5 closed for pipeline=PipelineID=533b24af-c82a-40aa-884f-929b72588b74
2024-02-01 18:15:34,177 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #5, current state: CLOSING
2024-02-01 18:15:34,177 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline PipelineID=533b24af-c82a-40aa-884f-929b72588b74 moved to CLOSED state
2024-02-01 18:15:34,311 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811904311 and scm deadline 1706811934311
2024-02-01 18:15:34,312 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811904312 and scm deadline 1706811934312
2024-02-01 18:15:34,312 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811904312 and scm deadline 1706811934312
2024-02-01 18:15:34,312 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811904312 and scm deadline 1706811934312
2024-02-01 18:15:34,312 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811904312 and scm deadline 1706811934312
2024-02-01 18:15:34,313 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811904313 and scm deadline 1706811934313
2024-02-01 18:15:34,313 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811904313 and scm deadline 1706811934313
2024-02-01 18:15:34,313 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811904313 and scm deadline 1706811934313
2024-02-01 18:15:34,313 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811904313 and scm deadline 1706811934313
2024-02-01 18:15:34,313 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811904313 and scm deadline 1706811934313
2024-02-01 18:15:34,314 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811904314 and scm deadline 1706811934314
2024-02-01 18:15:34,314 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811904314 and scm deadline 1706811934314
2024-02-01 18:15:34,314 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811904314 and scm deadline 1706811934314
2024-02-01 18:15:34,314 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811904314 and scm deadline 1706811934314
2024-02-01 18:15:34,314 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811904314 and scm deadline 1706811934314
2024-02-01 18:15:34,315 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811904315 and scm deadline 1706811934315
2024-02-01 18:15:34,315 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811904315 and scm deadline 1706811934315
2024-02-01 18:15:34,315 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811904315 and scm deadline 1706811934315
2024-02-01 18:15:34,315 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811904315 and scm deadline 1706811934315
2024-02-01 18:15:34,315 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811904315 and scm deadline 1706811934315
2024-02-01 18:15:34,316 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811904316 and scm deadline 1706811934316
2024-02-01 18:15:34,316 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811904316 and scm deadline 1706811934316
2024-02-01 18:15:34,316 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811904316 and scm deadline 1706811934316
2024-02-01 18:15:34,316 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811904316 and scm deadline 1706811934316
2024-02-01 18:15:34,316 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811904316 and scm deadline 1706811934316
2024-02-01 18:15:34,317 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811904317 and scm deadline 1706811934317
2024-02-01 18:15:34,317 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811904317 and scm deadline 1706811934317
2024-02-01 18:15:34,317 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811904317 and scm deadline 1706811934317
2024-02-01 18:15:34,317 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 6 milliseconds for processing 6 containers.
2024-02-01 18:15:34,979 [IPC Server handler 43 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:15:35,495 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:42797 / 172.19.0.13:42797
2024-02-01 18:15:35,496 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:15:38,052 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:15:39,318 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811909318 and scm deadline 1706811939318
2024-02-01 18:15:39,318 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811909318 and scm deadline 1706811939318
2024-02-01 18:15:39,318 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811909318 and scm deadline 1706811939318
2024-02-01 18:15:39,318 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811909318 and scm deadline 1706811939318
2024-02-01 18:15:39,319 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811909319 and scm deadline 1706811939319
2024-02-01 18:15:39,319 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811909319 and scm deadline 1706811939319
2024-02-01 18:15:39,319 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811909319 and scm deadline 1706811939319
2024-02-01 18:15:39,319 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811909319 and scm deadline 1706811939319
2024-02-01 18:15:39,319 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811909319 and scm deadline 1706811939319
2024-02-01 18:15:39,320 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811909320 and scm deadline 1706811939320
2024-02-01 18:15:39,320 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811909320 and scm deadline 1706811939320
2024-02-01 18:15:39,320 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811909320 and scm deadline 1706811939320
2024-02-01 18:15:39,320 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811909320 and scm deadline 1706811939320
2024-02-01 18:15:39,320 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811909320 and scm deadline 1706811939320
2024-02-01 18:15:39,321 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811909321 and scm deadline 1706811939321
2024-02-01 18:15:39,322 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811909322 and scm deadline 1706811939322
2024-02-01 18:15:39,323 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811909323 and scm deadline 1706811939323
2024-02-01 18:15:39,323 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811909323 and scm deadline 1706811939323
2024-02-01 18:15:39,323 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811909323 and scm deadline 1706811939323
2024-02-01 18:15:39,324 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811909324 and scm deadline 1706811939324
2024-02-01 18:15:39,325 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811909325 and scm deadline 1706811939325
2024-02-01 18:15:39,325 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811909325 and scm deadline 1706811939325
2024-02-01 18:15:39,326 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811909326 and scm deadline 1706811939326
2024-02-01 18:15:39,326 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811909326 and scm deadline 1706811939326
2024-02-01 18:15:39,326 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811909326 and scm deadline 1706811939326
2024-02-01 18:15:39,331 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811909331 and scm deadline 1706811939331
2024-02-01 18:15:39,332 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811909332 and scm deadline 1706811939332
2024-02-01 18:15:39,333 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811909333 and scm deadline 1706811939333
2024-02-01 18:15:39,333 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 16 milliseconds for processing 6 containers.
2024-02-01 18:15:42,235 [IPC Server handler 47 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:15:44,333 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811914333 and scm deadline 1706811944333
2024-02-01 18:15:44,334 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811914334 and scm deadline 1706811944334
2024-02-01 18:15:44,334 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811914334 and scm deadline 1706811944334
2024-02-01 18:15:44,334 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811914334 and scm deadline 1706811944334
2024-02-01 18:15:44,334 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811914334 and scm deadline 1706811944334
2024-02-01 18:15:44,334 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811914334 and scm deadline 1706811944334
2024-02-01 18:15:44,334 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811914334 and scm deadline 1706811944334
2024-02-01 18:15:44,335 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811914335 and scm deadline 1706811944335
2024-02-01 18:15:44,335 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811914335 and scm deadline 1706811944335
2024-02-01 18:15:44,335 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811914335 and scm deadline 1706811944335
2024-02-01 18:15:44,335 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811914335 and scm deadline 1706811944335
2024-02-01 18:15:44,335 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811914335 and scm deadline 1706811944335
2024-02-01 18:15:44,336 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811914336 and scm deadline 1706811944336
2024-02-01 18:15:44,336 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811914336 and scm deadline 1706811944336
2024-02-01 18:15:44,336 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811914336 and scm deadline 1706811944336
2024-02-01 18:15:44,336 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811914336 and scm deadline 1706811944336
2024-02-01 18:15:44,336 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811914336 and scm deadline 1706811944336
2024-02-01 18:15:44,337 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811914336 and scm deadline 1706811944336
2024-02-01 18:15:44,337 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811914337 and scm deadline 1706811944337
2024-02-01 18:15:44,337 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811914337 and scm deadline 1706811944337
2024-02-01 18:15:44,338 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811914338 and scm deadline 1706811944338
2024-02-01 18:15:44,338 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811914338 and scm deadline 1706811944338
2024-02-01 18:15:44,338 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811914338 and scm deadline 1706811944338
2024-02-01 18:15:44,338 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811914338 and scm deadline 1706811944338
2024-02-01 18:15:44,339 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811914339 and scm deadline 1706811944339
2024-02-01 18:15:44,339 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811914339 and scm deadline 1706811944339
2024-02-01 18:15:44,339 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811914339 and scm deadline 1706811944339
2024-02-01 18:15:44,339 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811914339 and scm deadline 1706811944339
2024-02-01 18:15:44,339 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 6 milliseconds for processing 6 containers.
2024-02-01 18:15:46,567 [IPC Server handler 68 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:15:48,263 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=aedbc0f3-603e-4dd1-9e35-b0e2d7631a28 to datanode:c3350086-6cc4-474e-91c0-301184edbd03
2024-02-01 18:15:48,263 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=aedbc0f3-603e-4dd1-9e35-b0e2d7631a28 to datanode:040c5bc7-c3de-4dd7-aa13-fc40341a6f9b
2024-02-01 18:15:48,263 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=aedbc0f3-603e-4dd1-9e35-b0e2d7631a28 to datanode:f86d5d8b-5e92-40d9-89ae-845d527031b5
2024-02-01 18:15:48,266 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: aedbc0f3-603e-4dd1-9e35-b0e2d7631a28, Nodes: c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6)040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:15:48.263735Z[UTC]]
2024-02-01 18:15:49,340 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811919340 and scm deadline 1706811949340
2024-02-01 18:15:49,340 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811919340 and scm deadline 1706811949340
2024-02-01 18:15:49,340 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811919340 and scm deadline 1706811949340
2024-02-01 18:15:49,340 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811919340 and scm deadline 1706811949340
2024-02-01 18:15:49,341 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.155270Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811919341 and scm deadline 1706811949341
2024-02-01 18:15:49,341 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811919341 and scm deadline 1706811949341
2024-02-01 18:15:49,341 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811919341 and scm deadline 1706811949341
2024-02-01 18:15:49,341 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811919341 and scm deadline 1706811949341
2024-02-01 18:15:49,341 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811919341 and scm deadline 1706811949341
2024-02-01 18:15:49,341 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.172624Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811919341 and scm deadline 1706811949341
2024-02-01 18:15:49,342 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811919342 and scm deadline 1706811949342
2024-02-01 18:15:49,342 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811919342 and scm deadline 1706811949342
2024-02-01 18:15:49,342 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811919342 and scm deadline 1706811949342
2024-02-01 18:15:49,342 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811919342 and scm deadline 1706811949342
2024-02-01 18:15:49,342 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.159916Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811919342 and scm deadline 1706811949342
2024-02-01 18:15:49,342 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811919342 and scm deadline 1706811949342
2024-02-01 18:15:49,342 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811919342 and scm deadline 1706811949342
2024-02-01 18:15:49,343 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811919343 and scm deadline 1706811949343
2024-02-01 18:15:49,343 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811919343 and scm deadline 1706811949343
2024-02-01 18:15:49,343 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.163865Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811919343 and scm deadline 1706811949343
2024-02-01 18:15:49,343 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) with datanode deadline 1706811919343 and scm deadline 1706811949343
2024-02-01 18:15:49,343 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811919343 and scm deadline 1706811949343
2024-02-01 18:15:49,343 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706811919343 and scm deadline 1706811949343
2024-02-01 18:15:49,343 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811919343 and scm deadline 1706811949343
2024-02-01 18:15:49,344 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.175902Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811919344 and scm deadline 1706811949344
2024-02-01 18:15:49,344 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811919344 and scm deadline 1706811949344
2024-02-01 18:15:49,344 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811919344 and scm deadline 1706811949344
2024-02-01 18:15:49,344 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811919344 and scm deadline 1706811949344
2024-02-01 18:15:49,344 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 5 milliseconds for processing 6 containers.
2024-02-01 18:15:49,635 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:15:51,213 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:39807 / 172.19.0.5:39807
2024-02-01 18:15:51,223 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:15:51,315 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:44155 / 172.19.0.6:44155
2024-02-01 18:15:51,324 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:15:52,225 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) reported CLOSED replica with index 1.
2024-02-01 18:15:52,237 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) reported CLOSED replica with index 5.
2024-02-01 18:15:52,250 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #5 to CLOSED state, datanode 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) reported CLOSED replica with index 4.
2024-02-01 18:15:52,255 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #4 to CLOSED state, datanode 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) reported CLOSED replica with index 4.
2024-02-01 18:15:52,346 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #3 to CLOSED state, datanode c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) reported CLOSED replica with index 5.
2024-02-01 18:15:52,361 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:41093 / 172.19.0.13:41093
2024-02-01 18:15:52,363 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:15:52,559 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:35777 / 172.19.0.8:35777
2024-02-01 18:15:52,588 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:15:52,642 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:40985 / 172.19.0.2:40985
2024-02-01 18:15:52,656 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:15:52,710 [IPC Server handler 11 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:15:54,346 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811924346 and scm deadline 1706811954346
2024-02-01 18:15:54,346 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811924346 and scm deadline 1706811954346
2024-02-01 18:15:54,346 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811924346 and scm deadline 1706811954346
2024-02-01 18:15:54,354 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811924354 and scm deadline 1706811954354
2024-02-01 18:15:54,354 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811924354 and scm deadline 1706811954354
2024-02-01 18:15:54,355 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811924355 and scm deadline 1706811954355
2024-02-01 18:15:54,355 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811924355 and scm deadline 1706811954355
2024-02-01 18:15:54,356 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811924355 and scm deadline 1706811954355
2024-02-01 18:15:54,356 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811924356 and scm deadline 1706811954356
2024-02-01 18:15:54,363 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811924362 and scm deadline 1706811954362
2024-02-01 18:15:54,363 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811924363 and scm deadline 1706811954363
2024-02-01 18:15:54,363 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811924363 and scm deadline 1706811954363
2024-02-01 18:15:54,364 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811924364 and scm deadline 1706811954364
2024-02-01 18:15:54,365 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811924365 and scm deadline 1706811954365
2024-02-01 18:15:54,365 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811924365 and scm deadline 1706811954365
2024-02-01 18:15:54,366 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811924366 and scm deadline 1706811954366
2024-02-01 18:15:54,366 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811924366 and scm deadline 1706811954366
2024-02-01 18:15:54,366 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811924366 and scm deadline 1706811954366
2024-02-01 18:15:54,366 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 21 milliseconds for processing 6 containers.
2024-02-01 18:15:57,741 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=aedbc0f3-603e-4dd1-9e35-b0e2d7631a28
2024-02-01 18:15:58,754 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:15:59,367 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811929367 and scm deadline 1706811959367
2024-02-01 18:15:59,367 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811929367 and scm deadline 1706811959367
2024-02-01 18:15:59,368 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811929368 and scm deadline 1706811959368
2024-02-01 18:15:59,368 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811929368 and scm deadline 1706811959368
2024-02-01 18:15:59,369 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811929369 and scm deadline 1706811959369
2024-02-01 18:15:59,369 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811929369 and scm deadline 1706811959369
2024-02-01 18:15:59,370 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811929369 and scm deadline 1706811959369
2024-02-01 18:15:59,370 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811929370 and scm deadline 1706811959370
2024-02-01 18:15:59,370 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811929370 and scm deadline 1706811959370
2024-02-01 18:15:59,371 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811929370 and scm deadline 1706811959370
2024-02-01 18:15:59,371 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811929371 and scm deadline 1706811959371
2024-02-01 18:15:59,371 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811929371 and scm deadline 1706811959371
2024-02-01 18:15:59,371 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811929371 and scm deadline 1706811959371
2024-02-01 18:15:59,372 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811929372 and scm deadline 1706811959372
2024-02-01 18:15:59,372 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811929372 and scm deadline 1706811959372
2024-02-01 18:15:59,373 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) with datanode deadline 1706811929373 and scm deadline 1706811959373
2024-02-01 18:15:59,373 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811929373 and scm deadline 1706811959373
2024-02-01 18:15:59,374 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: false] for container ContainerInfo{id=#6, state=CLOSING, stateEnterTime=2024-02-01T18:15:34.167634Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811929374 and scm deadline 1706811959374
2024-02-01 18:15:59,375 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 8 milliseconds for processing 6 containers.
2024-02-01 18:16:04,172 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:36073 / 172.19.0.2:36073
2024-02-01 18:16:04,196 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:16:04,255 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #6 to CLOSED state, datanode 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) reported CLOSED replica with index 0.
2024-02-01 18:16:04,375 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811934375 and scm deadline 1706811964375
2024-02-01 18:16:04,376 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811934376 and scm deadline 1706811964376
2024-02-01 18:16:04,376 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811934376 and scm deadline 1706811964376
2024-02-01 18:16:04,376 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811934376 and scm deadline 1706811964376
2024-02-01 18:16:04,377 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811934377 and scm deadline 1706811964377
2024-02-01 18:16:04,377 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: true] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811934377 and scm deadline 1706811964377
2024-02-01 18:16:04,378 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 6 containers.
2024-02-01 18:16:09,379 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811939379 and scm deadline 1706811969379
2024-02-01 18:16:09,381 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811939380 and scm deadline 1706811969380
2024-02-01 18:16:09,382 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811939382 and scm deadline 1706811969382
2024-02-01 18:16:09,382 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811939382 and scm deadline 1706811969382
2024-02-01 18:16:09,383 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811939383 and scm deadline 1706811969383
2024-02-01 18:16:09,383 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: true] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811939383 and scm deadline 1706811969383
2024-02-01 18:16:09,387 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 9 milliseconds for processing 6 containers.
2024-02-01 18:16:10,574 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:43139 / 172.19.0.11:43139
2024-02-01 18:16:10,578 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2024-02-01 18:16:10,578 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:14,388 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811944388 and scm deadline 1706811974388
2024-02-01 18:16:14,388 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811944388 and scm deadline 1706811974388
2024-02-01 18:16:14,389 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811944389 and scm deadline 1706811974389
2024-02-01 18:16:14,391 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811944391 and scm deadline 1706811974391
2024-02-01 18:16:14,392 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811944392 and scm deadline 1706811974392
2024-02-01 18:16:14,393 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: true] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811944392 and scm deadline 1706811974392
2024-02-01 18:16:14,393 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 6 milliseconds for processing 6 containers.
2024-02-01 18:16:15,643 [IPC Server handler 11 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:19,394 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811949394 and scm deadline 1706811979394
2024-02-01 18:16:19,395 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811949395 and scm deadline 1706811979395
2024-02-01 18:16:19,395 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811949395 and scm deadline 1706811979395
2024-02-01 18:16:19,395 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811949395 and scm deadline 1706811979395
2024-02-01 18:16:19,395 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811949395 and scm deadline 1706811979395
2024-02-01 18:16:19,396 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: true] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811949396 and scm deadline 1706811979396
2024-02-01 18:16:19,396 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 6 containers.
2024-02-01 18:16:19,785 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:38651 / 172.19.0.11:38651
2024-02-01 18:16:19,787 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:16:19,789 [IPC Server handler 51 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:20,601 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:36375 / 172.19.0.13:36375
2024-02-01 18:16:20,605 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:16:22,388 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:39285 / 172.19.0.6:39285
2024-02-01 18:16:22,393 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:16:22,852 [IPC Server handler 52 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:24,396 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811954396 and scm deadline 1706811984396
2024-02-01 18:16:24,397 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811954397 and scm deadline 1706811984397
2024-02-01 18:16:24,397 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811954397 and scm deadline 1706811984397
2024-02-01 18:16:24,397 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811954397 and scm deadline 1706811984397
2024-02-01 18:16:24,398 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811954398 and scm deadline 1706811984398
2024-02-01 18:16:24,398 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: true] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811954398 and scm deadline 1706811984398
2024-02-01 18:16:24,398 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2024-02-01 18:16:25,923 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:29,399 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811959398 and scm deadline 1706811989398
2024-02-01 18:16:29,399 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811959399 and scm deadline 1706811989399
2024-02-01 18:16:29,399 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811959399 and scm deadline 1706811989399
2024-02-01 18:16:29,399 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811959399 and scm deadline 1706811989399
2024-02-01 18:16:29,400 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811959400 and scm deadline 1706811989400
2024-02-01 18:16:29,400 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: true] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811959400 and scm deadline 1706811989400
2024-02-01 18:16:29,400 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2024-02-01 18:16:30,166 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:34,263 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:37165 / 172.19.0.8:37165
2024-02-01 18:16:34,267 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:16:34,277 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:40825 / 172.19.0.2:40825
2024-02-01 18:16:34,293 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:16:34,374 [IPC Server handler 72 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:34,400 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811964400 and scm deadline 1706811994400
2024-02-01 18:16:34,401 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811964401 and scm deadline 1706811994401
2024-02-01 18:16:34,401 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811964401 and scm deadline 1706811994401
2024-02-01 18:16:34,401 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811964401 and scm deadline 1706811994401
2024-02-01 18:16:34,402 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811964402 and scm deadline 1706811994402
2024-02-01 18:16:34,402 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: true] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811964402 and scm deadline 1706811994402
2024-02-01 18:16:34,402 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2024-02-01 18:16:37,444 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:39,403 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811969403 and scm deadline 1706811999403
2024-02-01 18:16:39,403 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811969403 and scm deadline 1706811999403
2024-02-01 18:16:39,403 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811969403 and scm deadline 1706811999403
2024-02-01 18:16:39,404 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811969404 and scm deadline 1706811999404
2024-02-01 18:16:39,404 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811969404 and scm deadline 1706811999404
2024-02-01 18:16:39,404 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: true] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811969404 and scm deadline 1706811999404
2024-02-01 18:16:39,404 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2024-02-01 18:16:40,516 [IPC Server handler 23 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:44,405 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811974405 and scm deadline 1706812004405
2024-02-01 18:16:44,406 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811974406 and scm deadline 1706812004406
2024-02-01 18:16:44,406 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811974406 and scm deadline 1706812004406
2024-02-01 18:16:44,407 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811974407 and scm deadline 1706812004407
2024-02-01 18:16:44,407 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811974407 and scm deadline 1706812004407
2024-02-01 18:16:44,408 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: true] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811974408 and scm deadline 1706812004408
2024-02-01 18:16:44,408 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 4 milliseconds for processing 6 containers.
2024-02-01 18:16:44,792 [IPC Server handler 51 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:49,030 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:49,409 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811979409 and scm deadline 1706812009409
2024-02-01 18:16:49,410 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811979410 and scm deadline 1706812009410
2024-02-01 18:16:49,410 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811979410 and scm deadline 1706812009410
2024-02-01 18:16:49,410 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811979410 and scm deadline 1706812009410
2024-02-01 18:16:49,411 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811979411 and scm deadline 1706812009411
2024-02-01 18:16:49,411 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: true] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811979411 and scm deadline 1706812009411
2024-02-01 18:16:49,411 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 6 containers.
2024-02-01 18:16:52,100 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:52,383 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:44255 / 172.19.0.6:44255
2024-02-01 18:16:52,389 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:16:54,412 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811984412 and scm deadline 1706812014412
2024-02-01 18:16:54,412 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811984412 and scm deadline 1706812014412
2024-02-01 18:16:54,412 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811984412 and scm deadline 1706812014412
2024-02-01 18:16:54,413 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811984413 and scm deadline 1706812014413
2024-02-01 18:16:54,413 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811984413 and scm deadline 1706812014413
2024-02-01 18:16:54,413 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: true] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811984413 and scm deadline 1706812014413
2024-02-01 18:16:54,413 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2024-02-01 18:16:55,174 [IPC Server handler 9 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:16:59,414 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811989414 and scm deadline 1706812019414
2024-02-01 18:16:59,415 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811989415 and scm deadline 1706812019415
2024-02-01 18:16:59,415 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811989415 and scm deadline 1706812019415
2024-02-01 18:16:59,415 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811989415 and scm deadline 1706812019415
2024-02-01 18:16:59,415 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=533b24af-c82a-40aa-884f-929b72588b74, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811989415 and scm deadline 1706812019415
2024-02-01 18:16:59,416 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 6, pipelineID: PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, force: true] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706811989416 and scm deadline 1706812019416
2024-02-01 18:16:59,416 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 6 containers.
2024-02-01 18:17:00,920 [IPC Server handler 58 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.4
2024-02-01 18:17:04,152 [EventQueue-DeadNodeForDeadNodeHandler] INFO node.DeadNodeHandler: A dead datanode is detected. 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)
2024-02-01 18:17:04,153 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=19d6b066-a82f-41e8-a5a6-575432061236 close command to datanode 6197f77b-f550-485b-8efa-1fede16758a8
2024-02-01 18:17:04,157 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 19d6b066-a82f-41e8-a5a6-575432061236, Nodes: 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:6197f77b-f550-485b-8efa-1fede16758a8, CreationTimestamp2024-02-01T18:05:47.200Z[UTC]] removed.
2024-02-01 18:17:04,161 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 78131eda-c94d-4bce-b16d-40e77cb3d806, Nodes: 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2024-02-01T18:06:14.730Z[UTC]] removed.
2024-02-01 18:17:04,162 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8b6304f6-7ecd-42f8-94b1-e59513c88279, Nodes: 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2024-02-01T18:06:19.686Z[UTC]] removed.
2024-02-01 18:17:04,163 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3e32c5bc-dfdf-4fa6-b04c-14737f549e26, Nodes: c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6)6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2024-02-01T18:06:20.942Z[UTC]] removed.
2024-02-01 18:17:04,163 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57 close command to datanode 6197f77b-f550-485b-8efa-1fede16758a8
2024-02-01 18:17:04,163 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57 close command to datanode f86d5d8b-5e92-40d9-89ae-845d527031b5
2024-02-01 18:17:04,163 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57 close command to datanode 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b
2024-02-01 18:17:04,165 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a6071b12-cba5-48f6-8c89-887a5f3e3d57, Nodes: 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:040c5bc7-c3de-4dd7-aa13-fc40341a6f9b, CreationTimestamp2024-02-01T18:05:48.219Z[UTC]] removed.
2024-02-01 18:17:04,166 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3ec162d6-2472-47e2-b227-0af8745fa14b, Nodes: 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2024-02-01T18:06:17.375Z[UTC]] removed.
2024-02-01 18:17:04,167 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 533b24af-c82a-40aa-884f-929b72588b74, Nodes: 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6)6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2024-02-01T18:06:21.147Z[UTC]] removed.
2024-02-01 18:17:04,169 [EventQueue-DeadNodeForDeadNodeHandler] INFO node.DeadNodeHandler: Clearing command queue of size 116 for DN 6197f77b-f550-485b-8efa-1fede16758a8(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)
2024-02-01 18:17:04,169 [EventQueue-DeadNodeForDeadNodeHandler] INFO net.NetworkTopologyImpl: Removed a node: /default-rack/6197f77b-f550-485b-8efa-1fede16758a8
2024-02-01 18:17:04,265 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:34537 / 172.19.0.2:34537
2024-02-01 18:17:04,270 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:39233 / 172.19.0.8:39233
2024-02-01 18:17:04,278 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:17:04,283 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:17:04,286 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57 is not found
2024-02-01 18:17:04,418 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2024-02-01 18:17:05,520 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:41867 / 172.19.0.13:41867
2024-02-01 18:17:05,524 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:17:09,319 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:09,337 [UnderReplicatedProcessor] INFO net.NetworkTopologyImpl: No available node in (scope="/" excludedScope="[/default-rack/040c5bc7-c3de-4dd7-aa13-fc40341a6f9b, /default-rack/f86d5d8b-5e92-40d9-89ae-845d527031b5]" excludedNodes="[040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)]"  ancestorGen="1").
2024-02-01 18:17:09,337 [UnderReplicatedProcessor] WARN algorithms.SCMContainerPlacementRackAware: Failed to find the datanode for container. excludedNodes:[040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2)], affinityNode:
2024-02-01 18:17:09,342 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [replicateContainerCommand: containerId=6, replicaIndex=0, targetNode=00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5), priority=NORMAL] for container ContainerInfo{id=#6, state=CLOSED, stateEnterTime=2024-02-01T18:16:04.258248Z, pipelineID=PipelineID=a6071b12-cba5-48f6-8c89-887a5f3e3d57, owner=omServiceIdDefault} to 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1706811999342 and scm deadline 1706812029342
2024-02-01 18:17:09,351 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:09,355 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:09,356 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:09,360 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:09,360 [UnderReplicatedProcessor] INFO replication.UnhealthyReplicationProcessor: Processed 1 containers with health state counts {UNDER_REPLICATED=1}, failed processing 5, deferred due to load 0
2024-02-01 18:17:09,420 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:17:12,253 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:45301 / 172.19.0.5:45301
2024-02-01 18:17:12,301 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2024-02-01 18:17:12,302 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: 268732fa-8685-4c37-8862-b55170c79ab1
2024-02-01 18:17:12,306 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 9 to 10.
2024-02-01 18:17:12,321 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:42127 / 172.19.0.9:42127
2024-02-01 18:17:12,337 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2024-02-01 18:17:12,344 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2024-02-01 18:17:12,351 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2024-02-01 18:17:12,362 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2024-02-01 18:17:12,365 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: d485da85-a987-4ce2-b4a8-26adb0f1b82e
2024-02-01 18:17:12,490 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 10 to 11.
2024-02-01 18:17:12,563 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2024-02-01 18:17:12,564 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2024-02-01 18:17:12,575 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2024-02-01 18:17:12,762 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2024-02-01 18:17:12,762 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2024-02-01 18:17:12,838 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2024-02-01 18:17:12,838 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2024-02-01 18:17:13,252 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:35063 / 172.19.0.9:35063
2024-02-01 18:17:13,288 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:17:13,335 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:36353 / 172.19.0.5:36353
2024-02-01 18:17:13,347 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:17:14,362 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:14,364 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:14,365 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:14,367 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:14,372 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:14,374 [UnderReplicatedProcessor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {}, failed processing 5, deferred due to load 0
2024-02-01 18:17:14,421 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:17:15,576 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:33553 / 172.19.0.4:33553
2024-02-01 18:17:15,608 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:17:19,375 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:19,376 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:19,376 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:19,377 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:19,377 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:101)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:782)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2024-02-01 18:17:19,377 [UnderReplicatedProcessor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {}, failed processing 5, deferred due to load 0
2024-02-01 18:17:19,422 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:17:20,238 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:37873 / 172.19.0.5:37873
2024-02-01 18:17:20,242 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:17:20,458 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:42171 / 172.19.0.9:42171
2024-02-01 18:17:20,483 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:17:22,237 [IPC Server handler 45 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/268732fa-8685-4c37-8862-b55170c79ab1
2024-02-01 18:17:22,237 [IPC Server handler 45 on default port 9861] INFO node.SCMNodeManager: Registered datanode: 268732fa-8685-4c37-8862-b55170c79ab1{ip: 172.19.0.5, host: ozonesecure_datanode_4.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 10, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2024-02-01 18:17:22,237 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2024-02-01 18:17:22,239 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b93d4ffd-b298-4119-9b4c-d62248e8035d to datanode:268732fa-8685-4c37-8862-b55170c79ab1
2024-02-01 18:17:22,242 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: b93d4ffd-b298-4119-9b4c-d62248e8035d, Nodes: 268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:17:22.239688Z[UTC]]
2024-02-01 18:17:22,381 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:34237 / 172.19.0.6:34237
2024-02-01 18:17:22,388 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:17:22,413 [IPC Server handler 8 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d485da85-a987-4ce2-b4a8-26adb0f1b82e
2024-02-01 18:17:22,414 [IPC Server handler 8 on default port 9861] INFO node.SCMNodeManager: Registered datanode: d485da85-a987-4ce2-b4a8-26adb0f1b82e{ip: 172.19.0.9, host: ozonesecure_datanode_5.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 11, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2024-02-01 18:17:22,415 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2024-02-01 18:17:22,416 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=f7e04e5f-30fb-46de-ae55-67d43a459d51 to datanode:d485da85-a987-4ce2-b4a8-26adb0f1b82e
2024-02-01 18:17:22,421 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: f7e04e5f-30fb-46de-ae55-67d43a459d51, Nodes: d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:17:22.416157Z[UTC]]
2024-02-01 18:17:22,421 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c00184ae-b9bc-428e-a273-cbd47308834a to datanode:268732fa-8685-4c37-8862-b55170c79ab1
2024-02-01 18:17:22,421 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c00184ae-b9bc-428e-a273-cbd47308834a to datanode:d485da85-a987-4ce2-b4a8-26adb0f1b82e
2024-02-01 18:17:22,422 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c00184ae-b9bc-428e-a273-cbd47308834a to datanode:00dd29b0-a872-4d37-95d3-e85543918f3f
2024-02-01 18:17:22,423 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: c00184ae-b9bc-428e-a273-cbd47308834a, Nodes: 268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2024-02-01T18:17:22.421854Z[UTC]]
2024-02-01 18:17:23,488 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:38255 / 172.19.0.13:38255
2024-02-01 18:17:23,489 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:17:24,378 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)]. isPolicySatisfied: true.
2024-02-01 18:17:24,380 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 1, replicationConfig: EC{rs-3-2-1024k}, sources: [00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) replicaIndex: 1, c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) replicaIndex: 2, f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) replicaIndex: 3, 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 4], targets: [268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)], missingIndexes: [5]] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to 268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706812014380 and scm deadline 1706812044380
2024-02-01 18:17:24,381 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)]. isPolicySatisfied: true.
2024-02-01 18:17:24,381 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 5, replicationConfig: EC{rs-3-2-1024k}, sources: [040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 1, c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) replicaIndex: 2, 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) replicaIndex: 4, f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) replicaIndex: 5], targets: [268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)], missingIndexes: [3]] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to 268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706812014381 and scm deadline 1706812044381
2024-02-01 18:17:24,381 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)]. isPolicySatisfied: true.
2024-02-01 18:17:24,381 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) and command replicateContainerCommand. Assuming zero
2024-02-01 18:17:24,381 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) and command reconstructECContainersCommand. Assuming zero
2024-02-01 18:17:24,381 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 4, replicationConfig: EC{rs-3-2-1024k}, sources: [c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) replicaIndex: 1, f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) replicaIndex: 3, 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) replicaIndex: 4, 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 5], targets: [d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)], missingIndexes: [2]] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706812014381 and scm deadline 1706812044381
2024-02-01 18:17:24,382 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)]. isPolicySatisfied: true.
2024-02-01 18:17:24,382 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) and command replicateContainerCommand. Assuming zero
2024-02-01 18:17:24,382 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) and command reconstructECContainersCommand. Assuming zero
2024-02-01 18:17:24,382 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 3, replicationConfig: EC{rs-3-2-1024k}, sources: [040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 1, 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) replicaIndex: 3, f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) replicaIndex: 4, c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) replicaIndex: 5], targets: [d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)], missingIndexes: [2]] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706812014382 and scm deadline 1706812044382
2024-02-01 18:17:24,382 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)]. isPolicySatisfied: true.
2024-02-01 18:17:24,382 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) and command replicateContainerCommand. Assuming zero
2024-02-01 18:17:24,382 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) and command reconstructECContainersCommand. Assuming zero
2024-02-01 18:17:24,382 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 2, replicationConfig: EC{rs-3-2-1024k}, sources: [040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 1, c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) replicaIndex: 3, f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) replicaIndex: 4, 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) replicaIndex: 5], targets: [d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)], missingIndexes: [2]] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706812014382 and scm deadline 1706812044382
2024-02-01 18:17:24,382 [UnderReplicatedProcessor] INFO replication.UnhealthyReplicationProcessor: Processed 5 containers with health state counts {UNDER_REPLICATED=5}, failed processing 0, deferred due to load 0
2024-02-01 18:17:24,424 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:17:25,155 [EventQueue-StaleNodeForStaleNodeHandler] INFO node.StaleNodeHandler: Datanode 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) moved to stale state. Finalizing its pipelines [PipelineID=99010d61-4807-4c38-b353-61aa4809c03a, PipelineID=c00184ae-b9bc-428e-a273-cbd47308834a]
2024-02-01 18:17:25,157 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline PipelineID=99010d61-4807-4c38-b353-61aa4809c03a moved to CLOSED state
2024-02-01 18:17:25,158 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline PipelineID=c00184ae-b9bc-428e-a273-cbd47308834a moved to CLOSED state
2024-02-01 18:17:25,340 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=b93d4ffd-b298-4119-9b4c-d62248e8035d
2024-02-01 18:17:25,674 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=f7e04e5f-30fb-46de-ae55-67d43a459d51
2024-02-01 18:17:26,332 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:39241 / 172.19.0.5:39241
2024-02-01 18:17:26,353 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:17:29,426 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:17:29,676 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:40321 / 172.19.0.4:40321
2024-02-01 18:17:29,696 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:17:34,263 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:40837 / 172.19.0.2:40837
2024-02-01 18:17:34,266 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:17:34,269 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:42335 / 172.19.0.8:42335
2024-02-01 18:17:34,274 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:17:34,427 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:17:39,428 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:17:43,025 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:34973 / 172.19.0.4:34973
2024-02-01 18:17:43,039 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:17:44,429 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:17:46,195 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:34613 / 172.19.0.9:34613
2024-02-01 18:17:46,216 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:17:49,430 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:17:52,385 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:42329 / 172.19.0.6:42329
2024-02-01 18:17:52,389 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:17:54,431 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:17:56,248 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:46847 / 172.19.0.4:46847
2024-02-01 18:17:56,260 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:17:59,432 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:18:04,264 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:42471 / 172.19.0.8:42471
2024-02-01 18:18:04,273 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:41893 / 172.19.0.2:41893
2024-02-01 18:18:04,276 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:18:04,278 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:18:04,433 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:18:05,533 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:34103 / 172.19.0.5:34103
2024-02-01 18:18:05,536 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:18:09,434 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:18:09,660 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:45209 / 172.19.0.4:45209
2024-02-01 18:18:09,671 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:18:11,907 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:41929 / 172.19.0.9:41929
2024-02-01 18:18:11,925 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:18:14,436 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:18:19,436 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:18:22,382 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:37771 / 172.19.0.6:37771
2024-02-01 18:18:22,392 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:18:22,800 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:33863 / 172.19.0.4:33863
2024-02-01 18:18:22,812 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:18:24,437 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:18:29,438 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:18:34,316 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:43099 / 172.19.0.2:43099
2024-02-01 18:18:34,316 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:43333 / 172.19.0.8:43333
2024-02-01 18:18:34,320 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:18:34,323 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:18:34,439 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:18:35,532 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:41453 / 172.19.0.5:41453
2024-02-01 18:18:35,548 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:18:35,963 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:42943 / 172.19.0.4:42943
2024-02-01 18:18:35,974 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:18:39,440 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:18:41,907 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:46773 / 172.19.0.9:46773
2024-02-01 18:18:41,918 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:18:44,441 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:18:49,056 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:46249 / 172.19.0.4:46249
2024-02-01 18:18:49,069 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:18:49,441 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:18:52,382 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:35969 / 172.19.0.6:35969
2024-02-01 18:18:52,385 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:18:53,497 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:39623 / 172.19.0.13:39623
2024-02-01 18:18:53,499 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:18:54,442 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:18:55,160 [EventQueue-DeadNodeForDeadNodeHandler] INFO node.DeadNodeHandler: A dead datanode is detected. 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)
2024-02-01 18:18:55,161 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99010d61-4807-4c38-b353-61aa4809c03a close command to datanode 00dd29b0-a872-4d37-95d3-e85543918f3f
2024-02-01 18:18:55,163 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 99010d61-4807-4c38-b353-61aa4809c03a, Nodes: 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:00dd29b0-a872-4d37-95d3-e85543918f3f, CreationTimestamp2024-02-01T18:05:47.028Z[UTC]] removed.
2024-02-01 18:18:55,163 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=c00184ae-b9bc-428e-a273-cbd47308834a close command to datanode 268732fa-8685-4c37-8862-b55170c79ab1
2024-02-01 18:18:55,163 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=c00184ae-b9bc-428e-a273-cbd47308834a close command to datanode d485da85-a987-4ce2-b4a8-26adb0f1b82e
2024-02-01 18:18:55,163 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=c00184ae-b9bc-428e-a273-cbd47308834a close command to datanode 00dd29b0-a872-4d37-95d3-e85543918f3f
2024-02-01 18:18:55,165 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: c00184ae-b9bc-428e-a273-cbd47308834a, Nodes: 268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:d485da85-a987-4ce2-b4a8-26adb0f1b82e, CreationTimestamp2024-02-01T18:17:22.421Z[UTC]] removed.
2024-02-01 18:18:55,165 [EventQueue-DeadNodeForDeadNodeHandler] INFO node.DeadNodeHandler: Clearing command queue of size 3 for DN 00dd29b0-a872-4d37-95d3-e85543918f3f(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)
2024-02-01 18:18:55,165 [EventQueue-DeadNodeForDeadNodeHandler] INFO net.NetworkTopologyImpl: Removed a node: /default-rack/00dd29b0-a872-4d37-95d3-e85543918f3f
2024-02-01 18:18:59,443 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:19:02,348 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:43217 / 172.19.0.4:43217
2024-02-01 18:19:02,364 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:19:03,268 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:35259 / 172.19.0.9:35259
2024-02-01 18:19:03,274 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:19:03,275 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=c00184ae-b9bc-428e-a273-cbd47308834a is not found
2024-02-01 18:19:04,262 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:45533 / 172.19.0.8:45533
2024-02-01 18:19:04,266 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:19:04,284 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:46419 / 172.19.0.2:46419
2024-02-01 18:19:04,290 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:19:04,390 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)]. isPolicySatisfied: true.
2024-02-01 18:19:04,391 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 1, replicationConfig: EC{rs-3-2-1024k}, sources: [c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) replicaIndex: 2, f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) replicaIndex: 3, 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 4], targets: [d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)], missingIndexes: [1]] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.227416Z, pipelineID=PipelineID=78131eda-c94d-4bce-b16d-40e77cb3d806, owner=omServiceIdDefault} to d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706812114391 and scm deadline 1706812144391
2024-02-01 18:19:04,391 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)]. isPolicySatisfied: true.
2024-02-01 18:19:04,391 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 5, replicationConfig: EC{rs-3-2-1024k}, sources: [040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 1, c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) replicaIndex: 2, f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) replicaIndex: 5], targets: [d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9)], missingIndexes: [4]] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.252440Z, pipelineID=PipelineID=533b24af-c82a-40aa-884f-929b72588b74, owner=omServiceIdDefault} to d485da85-a987-4ce2-b4a8-26adb0f1b82e(ozonesecure_datanode_5.ozonesecure_default/172.19.0.9) with datanode deadline 1706812114391 and scm deadline 1706812144391
2024-02-01 18:19:04,392 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)]. isPolicySatisfied: true.
2024-02-01 18:19:04,392 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 4, replicationConfig: EC{rs-3-2-1024k}, sources: [c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) replicaIndex: 1, f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) replicaIndex: 3, 040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 5], targets: [268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)], missingIndexes: [4]] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.257007Z, pipelineID=PipelineID=3e32c5bc-dfdf-4fa6-b04c-14737f549e26, owner=omServiceIdDefault} to 268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706812114392 and scm deadline 1706812144392
2024-02-01 18:19:04,392 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)]. isPolicySatisfied: true.
2024-02-01 18:19:04,392 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 3, replicationConfig: EC{rs-3-2-1024k}, sources: [040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 1, f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) replicaIndex: 4, c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) replicaIndex: 5], targets: [268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)], missingIndexes: [3]] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.350775Z, pipelineID=PipelineID=8b6304f6-7ecd-42f8-94b1-e59513c88279, owner=omServiceIdDefault} to 268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706812114392 and scm deadline 1706812144392
2024-02-01 18:19:04,392 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)]. isPolicySatisfied: true.
2024-02-01 18:19:04,392 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 2, replicationConfig: EC{rs-3-2-1024k}, sources: [040c5bc7-c3de-4dd7-aa13-fc40341a6f9b(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 1, c3350086-6cc4-474e-91c0-301184edbd03(ozonesecure_datanode_3.ozonesecure_default/172.19.0.6) replicaIndex: 3, f86d5d8b-5e92-40d9-89ae-845d527031b5(ozonesecure_datanode_2.ozonesecure_default/172.19.0.2) replicaIndex: 4], targets: [268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5)], missingIndexes: [5]] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2024-02-01T18:15:52.240541Z, pipelineID=PipelineID=3ec162d6-2472-47e2-b227-0af8745fa14b, owner=omServiceIdDefault} to 268732fa-8685-4c37-8862-b55170c79ab1(ozonesecure_datanode_4.ozonesecure_default/172.19.0.5) with datanode deadline 1706812114392 and scm deadline 1706812144392
2024-02-01 18:19:04,393 [UnderReplicatedProcessor] INFO replication.UnhealthyReplicationProcessor: Processed 5 containers with health state counts {UNDER_REPLICATED=5}, failed processing 0, deferred due to load 0
2024-02-01 18:19:04,444 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:19:05,533 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:32995 / 172.19.0.5:32995
2024-02-01 18:19:05,536 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:19:09,446 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:19:14,446 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:19:15,493 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:35193 / 172.19.0.4:35193
2024-02-01 18:19:15,504 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:19:19,447 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:19:22,380 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:34781 / 172.19.0.6:34781
2024-02-01 18:19:22,386 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:19:24,448 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:19:28,621 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:35765 / 172.19.0.4:35765
2024-02-01 18:19:28,635 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:19:29,448 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:19:33,250 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:33585 / 172.19.0.9:33585
2024-02-01 18:19:33,253 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:19:34,271 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:43813 / 172.19.0.2:43813
2024-02-01 18:19:34,278 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:41439 / 172.19.0.8:41439
2024-02-01 18:19:34,288 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:19:34,306 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:19:34,389 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:35479 / 172.19.0.9:35479
2024-02-01 18:19:34,397 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2024-02-01 18:19:34,449 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:19:38,626 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:46697 / 172.19.0.5:46697
2024-02-01 18:19:38,628 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:19:39,450 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:19:41,804 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:33561 / 172.19.0.4:33561
2024-02-01 18:19:41,817 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:19:44,451 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:19:49,451 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:19:52,388 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:42327 / 172.19.0.6:42327
2024-02-01 18:19:52,394 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:19:54,452 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:19:54,864 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:35975 / 172.19.0.4:35975
2024-02-01 18:19:54,875 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:19:59,453 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:20:04,269 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:42673 / 172.19.0.2:42673
2024-02-01 18:20:04,269 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:43477 / 172.19.0.8:43477
2024-02-01 18:20:04,282 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:20:04,284 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:20:04,454 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:20:07,308 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:33849 / 172.19.0.9:33849
2024-02-01 18:20:07,326 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:20:08,106 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:39041 / 172.19.0.4:39041
2024-02-01 18:20:08,118 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:20:08,629 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:41425 / 172.19.0.5:41425
2024-02-01 18:20:08,644 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:20:09,454 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:20:14,455 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:20:19,457 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:20:21,206 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:46091 / 172.19.0.4:46091
2024-02-01 18:20:21,218 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:20:22,389 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:36755 / 172.19.0.6:36755
2024-02-01 18:20:22,406 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:20:24,457 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:20:29,458 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:20:34,264 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:42933 / 172.19.0.8:42933
2024-02-01 18:20:34,273 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:20:34,275 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:32879 / 172.19.0.2:32879
2024-02-01 18:20:34,281 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:20:34,363 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:43585 / 172.19.0.4:43585
2024-02-01 18:20:34,374 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:20:34,459 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:20:37,295 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:36931 / 172.19.0.9:36931
2024-02-01 18:20:37,301 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:20:38,627 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:40001 / 172.19.0.5:40001
2024-02-01 18:20:38,634 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:20:39,460 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:20:44,460 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:20:47,566 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:40991 / 172.19.0.4:40991
2024-02-01 18:20:47,580 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:20:49,461 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:20:52,388 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:44405 / 172.19.0.6:44405
2024-02-01 18:20:52,394 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:20:54,462 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:20:59,462 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:21:00,826 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:41349 / 172.19.0.4:41349
2024-02-01 18:21:00,846 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:21:04,265 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:39351 / 172.19.0.8:39351
2024-02-01 18:21:04,272 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:40617 / 172.19.0.2:40617
2024-02-01 18:21:04,276 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:21:04,289 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:21:04,463 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:21:07,294 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:36447 / 172.19.0.9:36447
2024-02-01 18:21:07,298 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:21:08,633 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:37499 / 172.19.0.5:37499
2024-02-01 18:21:08,637 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:21:09,464 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:21:14,047 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:42403 / 172.19.0.4:42403
2024-02-01 18:21:14,059 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:21:14,464 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:21:19,465 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:21:20,628 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:36891 / 172.19.0.13:36891
2024-02-01 18:21:20,630 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:21:22,384 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:33857 / 172.19.0.6:33857
2024-02-01 18:21:22,386 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:21:24,466 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:21:27,346 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:44869 / 172.19.0.4:44869
2024-02-01 18:21:27,357 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:21:29,467 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:21:34,261 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:42169 / 172.19.0.8:42169
2024-02-01 18:21:34,265 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:46301 / 172.19.0.2:46301
2024-02-01 18:21:34,268 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:21:34,277 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:21:34,467 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:21:37,295 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:37875 / 172.19.0.9:37875
2024-02-01 18:21:37,301 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:21:38,625 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:38167 / 172.19.0.5:38167
2024-02-01 18:21:38,651 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:21:39,468 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:21:40,502 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:42473 / 172.19.0.4:42473
2024-02-01 18:21:40,516 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:21:44,469 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:21:49,470 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:21:52,390 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:38047 / 172.19.0.6:38047
2024-02-01 18:21:52,398 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:21:53,740 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:40461 / 172.19.0.4:40461
2024-02-01 18:21:53,754 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:21:54,470 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:21:59,471 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:22:04,262 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:42789 / 172.19.0.8:42789
2024-02-01 18:22:04,273 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:41505 / 172.19.0.2:41505
2024-02-01 18:22:04,274 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:22:04,277 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:22:04,472 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:22:06,967 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:44347 / 172.19.0.4:44347
2024-02-01 18:22:06,984 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2024-02-01 18:22:07,295 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:35311 / 172.19.0.9:35311
2024-02-01 18:22:07,311 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:22:08,626 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:40661 / 172.19.0.5:40661
2024-02-01 18:22:08,635 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:22:09,472 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2024-02-01 18:22:14,473 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:22:19,474 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2024-02-01 18:22:22,385 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:44321 / 172.19.0.6:44321
2024-02-01 18:22:22,413 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2024-02-01 18:22:24,474 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
