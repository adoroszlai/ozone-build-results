No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2023-11-27 12:35:47,568 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = scm/172.20.0.8
STARTUP_MSG:   args = [--init]
STARTUP_MSG:   version = 1.4.0-SNAPSHOT
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/java-uuid-generator-4.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-3.0.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-3.0.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/picocli-4.7.5.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.16.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-dropwizard3-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-3.0.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-3.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.58.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-3.0.0.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
STARTUP_MSG:   build = https://github.com/apache/ozone/7dcae6715cc449b5ffd41b31fdda38fa0560dcc3 ; compiled by 'runner' on 2023-11-27T11:55Z
STARTUP_MSG:   java = 11.0.19
STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.command.worker.interval=2s, hdds.datanode.block.delete.max.lock.wait.timeout=100ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.max.lock.holding.time=1s, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=19864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.close.threads.max=3, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.file.size=100B, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/dn@EXAMPLE.COM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.queue.limit=4096, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=100MB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.log.appender.wait-time.min=1ms, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/scm.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.expired.certificate.check.interval=P1D, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.administrators=*, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.grpc.write.timeout=30s, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/om.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.open.mpu.cleanup.service.interval=24h, ozone.om.open.mpu.cleanup.service.timeout=300s, ozone.om.open.mpu.expire.threshold=30d, ozone.om.open.mpu.parts.cleanup.limit.per.task=1000, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.checkpoint.dir.creation.poll.timeout=20s, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=6000, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3.administrators=s3g, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/s3g.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.list-keys.shallow.enabled=true, ozone.s3g.secret.http.auth.type=kerberos, ozone.s3g.secret.http.enabled=true, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.raft.server.log.appender.wait-time.min=0ms, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=scm:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
************************************************************/
2023-11-27 12:35:47,627 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
2023-11-27 12:35:47,955 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-11-27 12:35:48,971 [main] INFO reflections.Reflections: Reflections took 813 ms to scan 3 urls, producing 134 keys and 291 values 
2023-11-27 12:35:49,458 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
2023-11-27 12:35:49,469 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2023-11-27 12:35:49,670 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
2023-11-27 12:35:53,894 [main] INFO client.SCMCertificateClient: Certificate serial ID set to null
2023-11-27 12:35:53,914 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
2023-11-27 12:35:53,915 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
2023-11-27 12:35:53,917 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
2023-11-27 12:35:58,625 [main] INFO client.SCMCertificateClient: Init response: GETCERT
2023-11-27 12:36:00,328 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.20.0.8,host:scm
2023-11-27 12:36:00,328 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
2023-11-27 12:36:00,330 [main] ERROR utils.SelfSignedCertificate: Invalid domain scm
2023-11-27 12:36:00,407 [main] INFO utils.SelfSignedCertificate: Certificate 1 is issued by CN=scm@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=1 to CN=scm@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=1, valid from Mon Nov 27 12:36:00 UTC 2023 to Thu Jan 04 12:36:00 UTC 2029
2023-11-27 12:36:00,425 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/ca/certs/certificate.crt
2023-11-27 12:36:00,425 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkZWZkMTQxNTItZTM1Zi00MzIyLThlMzctOGJiYWM3MTI0
ODA4MTEwLwYDVQQKDChDSUQtNTg4NDllN2MtNmFhMy00MGY2LTk1YjAtNzg0NGYw
YWI1MTNiMQowCAYDVQQFEwExMB4XDTIzMTEyNzEyMzYwMFoXDTI5MDEwNDEyMzYw
MFowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJGVmZDE0MTUyLWUzNWYt
NDMyMi04ZTM3LThiYmFjNzEyNDgwODExMC8GA1UECgwoQ0lELTU4ODQ5ZTdjLTZh
YTMtNDBmNi05NWIwLTc4NDRmMGFiNTEzYjEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAJSxT7HeaSt1A7IdjalsPEwyj6VYTyEkj27W
RYrKeVUXMN/m/ibFmRNvFTXpCQv0VEx4OTAtA9f3tUMpaIEBgfokCbarU2yA5TCZ
0LVli4WGrzfJgWNTbaOWvys6bh4idTd7j8ARxN3yLanjp21NHIVtYQoiQ0+sbZ0J
7xtIv9IO1PfKmuAjJ8erw/I6KS0stdgMoNvBt6G4AbPSplS0w3sBopEoT0CCvKmb
xhbJcHd2opUZ5VkfUtKvZg5VfNhvsDYkJbQuZ7i5+RrkUNeuvmpuJdK1CZWt1YmS
2qJ93heXnKrUwaxWNd84KIZSIIyuqg098jagEhPtVVBNHPNenGECAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBQA
CDANBgkqhkiG9w0BAQsFAAOCAQEAVfV7QfyXkhbmouVoKiL+zVnmXhYpRxOO/no2
zHzx1W51zUQZTtVWZdP7R7RQ5NARfUzg0ZhkYT4w0Jx5OR6ZAXgSzbJxEB5lV6qY
FYvzwOf4f5haQjjcb5854wBWNNtTK9+2Sldwx+D4kINl8sEVVCultFx4l2mPs83H
tPNGVIllhfNFSzWBq8ansFVOsDDVOZO3+/UM6r22NX2IQibqfnH7B2Vv3pz2qCeT
kUfqaXGp7xX09y3vO4lUP0NlaAaSVtE4pL2n/+ysw5zjzzcfyknl7Y7VDAyT6vNy
VVDBwIziQguSj8Na+okLi7oq1TGmULo5IFbEj/bLq3htTRy6HA==
-----END CERTIFICATE-----

2023-11-27 12:36:00,458 [main] INFO client.SCMCertificateClient: Creating csr for SCM->hostName:scm,scmId:efd14152-e35f-4322-8e37-8bbac7124808,clusterId:CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,subject:scm-sub@scm
2023-11-27 12:36:00,460 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.20.0.8,host:scm
2023-11-27 12:36:00,461 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
2023-11-27 12:36:00,461 [main] ERROR utils.CertificateSignRequest: Invalid domain scm
2023-11-27 12:36:00,519 [main] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.19, 2.5.29.15, 2.5.29.17
2023-11-27 12:36:00,519 [main] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-27 12:36:00,532 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/CA-1.crt
2023-11-27 12:36:00,532 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkZWZkMTQxNTItZTM1Zi00MzIyLThlMzctOGJiYWM3MTI0
ODA4MTEwLwYDVQQKDChDSUQtNTg4NDllN2MtNmFhMy00MGY2LTk1YjAtNzg0NGYw
YWI1MTNiMQowCAYDVQQFEwExMB4XDTIzMTEyNzEyMzYwMFoXDTI5MDEwNDEyMzYw
MFowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJGVmZDE0MTUyLWUzNWYt
NDMyMi04ZTM3LThiYmFjNzEyNDgwODExMC8GA1UECgwoQ0lELTU4ODQ5ZTdjLTZh
YTMtNDBmNi05NWIwLTc4NDRmMGFiNTEzYjEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAJSxT7HeaSt1A7IdjalsPEwyj6VYTyEkj27W
RYrKeVUXMN/m/ibFmRNvFTXpCQv0VEx4OTAtA9f3tUMpaIEBgfokCbarU2yA5TCZ
0LVli4WGrzfJgWNTbaOWvys6bh4idTd7j8ARxN3yLanjp21NHIVtYQoiQ0+sbZ0J
7xtIv9IO1PfKmuAjJ8erw/I6KS0stdgMoNvBt6G4AbPSplS0w3sBopEoT0CCvKmb
xhbJcHd2opUZ5VkfUtKvZg5VfNhvsDYkJbQuZ7i5+RrkUNeuvmpuJdK1CZWt1YmS
2qJ93heXnKrUwaxWNd84KIZSIIyuqg098jagEhPtVVBNHPNenGECAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBQA
CDANBgkqhkiG9w0BAQsFAAOCAQEAVfV7QfyXkhbmouVoKiL+zVnmXhYpRxOO/no2
zHzx1W51zUQZTtVWZdP7R7RQ5NARfUzg0ZhkYT4w0Jx5OR6ZAXgSzbJxEB5lV6qY
FYvzwOf4f5haQjjcb5854wBWNNtTK9+2Sldwx+D4kINl8sEVVCultFx4l2mPs83H
tPNGVIllhfNFSzWBq8ansFVOsDDVOZO3+/UM6r22NX2IQibqfnH7B2Vv3pz2qCeT
kUfqaXGp7xX09y3vO4lUP0NlaAaSVtE4pL2n/+ysw5zjzzcfyknl7Y7VDAyT6vNy
VVDBwIziQguSj8Na+okLi7oq1TGmULo5IFbEj/bLq3htTRy6HA==
-----END CERTIFICATE-----

2023-11-27 12:36:00,534 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/2.crt
2023-11-27 12:36:00,534 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDtTCCAp2gAwIBAgIBAjANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkZWZkMTQxNTItZTM1Zi00MzIyLThlMzctOGJiYWM3MTI0
ODA4MTEwLwYDVQQKDChDSUQtNTg4NDllN2MtNmFhMy00MGY2LTk1YjAtNzg0NGYw
YWI1MTNiMQowCAYDVQQFEwExMB4XDTIzMTEyNzEyMzYwMFoXDTI5MDEwNDEyMzYw
MFowgYQxFDASBgNVBAMMC3NjbS1zdWJAc2NtMS0wKwYDVQQLDCRlZmQxNDE1Mi1l
MzVmLTQzMjItOGUzNy04YmJhYzcxMjQ4MDgxMTAvBgNVBAoMKENJRC01ODg0OWU3
Yy02YWEzLTQwZjYtOTViMC03ODQ0ZjBhYjUxM2IxCjAIBgNVBAUTATIwggEiMA0G
CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCndCkyvIW9M4fGC3EAVAN/vAuFGAcK
82kOBusM18Kjm/OEYD7B6V4PemWpd39L3TfCmQ47ggjOAWPaGB2F96OmcEGw0zbA
ZzA5wXZAgFqctQdf25KEY6CpX8JzAMgpiHQIvlRz59cU0//v914d2i6VvCypymD4
9NffOEBzIVsilz+ETDOCg8+ehG7wL3NO/d1Y5oyxfTDioPJUOVFt0OOpCAlq9yEy
sk0PQCgVPSGAJek9h6jp95OvO24K0LU4yGsj98mvvlf0bp1QjGCxQxrvzn9Sw0Wq
UwAPpd1tkHAWfP64JQZvMYqb74CF0zC2L3rOZOmo9cwcUy62y2pdbM8fAgMBAAGj
NDAyMA8GA1UdEQQIMAaHBKwUAAgwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8E
BAMCAb4wDQYJKoZIhvcNAQELBQADggEBACS8JOzAmiV/+1F6DMByvf/gTawz62gW
d1+Zb7eIafib0Yjdf92BIsTTRzPNAKB+tzujgf/BoVRRR0/K4rlqfDsdbPTJ6ylb
Hl7MrQ2Ioy/Abi7L/biEDj8mnhrlACd1OKcY/Iyej05qMkRoFPFloeFyhkqiKSVC
oK+zp6IOiEUyrVJ8wmVGdscmeHmdQj1lyLfTeW7AIsEpqXv+XLkO3F7qDLylAibU
Hyms6/iw+thYJk7kj0RUJxViAZvuEX0IlQZndcUN8yJRfGXAwEr9P/Rp2PBak0+r
CjkvvP3RPlFUVsNONHF2xeGU7hbsliifZRRMpm/+ve/CuLrH6ZdNoqU=
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkZWZkMTQxNTItZTM1Zi00MzIyLThlMzctOGJiYWM3MTI0
ODA4MTEwLwYDVQQKDChDSUQtNTg4NDllN2MtNmFhMy00MGY2LTk1YjAtNzg0NGYw
YWI1MTNiMQowCAYDVQQFEwExMB4XDTIzMTEyNzEyMzYwMFoXDTI5MDEwNDEyMzYw
MFowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJGVmZDE0MTUyLWUzNWYt
NDMyMi04ZTM3LThiYmFjNzEyNDgwODExMC8GA1UECgwoQ0lELTU4ODQ5ZTdjLTZh
YTMtNDBmNi05NWIwLTc4NDRmMGFiNTEzYjEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAJSxT7HeaSt1A7IdjalsPEwyj6VYTyEkj27W
RYrKeVUXMN/m/ibFmRNvFTXpCQv0VEx4OTAtA9f3tUMpaIEBgfokCbarU2yA5TCZ
0LVli4WGrzfJgWNTbaOWvys6bh4idTd7j8ARxN3yLanjp21NHIVtYQoiQ0+sbZ0J
7xtIv9IO1PfKmuAjJ8erw/I6KS0stdgMoNvBt6G4AbPSplS0w3sBopEoT0CCvKmb
xhbJcHd2opUZ5VkfUtKvZg5VfNhvsDYkJbQuZ7i5+RrkUNeuvmpuJdK1CZWt1YmS
2qJ93heXnKrUwaxWNd84KIZSIIyuqg098jagEhPtVVBNHPNenGECAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBQA
CDANBgkqhkiG9w0BAQsFAAOCAQEAVfV7QfyXkhbmouVoKiL+zVnmXhYpRxOO/no2
zHzx1W51zUQZTtVWZdP7R7RQ5NARfUzg0ZhkYT4w0Jx5OR6ZAXgSzbJxEB5lV6qY
FYvzwOf4f5haQjjcb5854wBWNNtTK9+2Sldwx+D4kINl8sEVVCultFx4l2mPs83H
tPNGVIllhfNFSzWBq8ansFVOsDDVOZO3+/UM6r22NX2IQibqfnH7B2Vv3pz2qCeT
kUfqaXGp7xX09y3vO4lUP0NlaAaSVtE4pL2n/+ysw5zjzzcfyknl7Y7VDAyT6vNy
VVDBwIziQguSj8Na+okLi7oq1TGmULo5IFbEj/bLq3htTRy6HA==
-----END CERTIFICATE-----

2023-11-27 12:36:00,535 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/certificate.crt
2023-11-27 12:36:00,535 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDtTCCAp2gAwIBAgIBAjANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkZWZkMTQxNTItZTM1Zi00MzIyLThlMzctOGJiYWM3MTI0
ODA4MTEwLwYDVQQKDChDSUQtNTg4NDllN2MtNmFhMy00MGY2LTk1YjAtNzg0NGYw
YWI1MTNiMQowCAYDVQQFEwExMB4XDTIzMTEyNzEyMzYwMFoXDTI5MDEwNDEyMzYw
MFowgYQxFDASBgNVBAMMC3NjbS1zdWJAc2NtMS0wKwYDVQQLDCRlZmQxNDE1Mi1l
MzVmLTQzMjItOGUzNy04YmJhYzcxMjQ4MDgxMTAvBgNVBAoMKENJRC01ODg0OWU3
Yy02YWEzLTQwZjYtOTViMC03ODQ0ZjBhYjUxM2IxCjAIBgNVBAUTATIwggEiMA0G
CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCndCkyvIW9M4fGC3EAVAN/vAuFGAcK
82kOBusM18Kjm/OEYD7B6V4PemWpd39L3TfCmQ47ggjOAWPaGB2F96OmcEGw0zbA
ZzA5wXZAgFqctQdf25KEY6CpX8JzAMgpiHQIvlRz59cU0//v914d2i6VvCypymD4
9NffOEBzIVsilz+ETDOCg8+ehG7wL3NO/d1Y5oyxfTDioPJUOVFt0OOpCAlq9yEy
sk0PQCgVPSGAJek9h6jp95OvO24K0LU4yGsj98mvvlf0bp1QjGCxQxrvzn9Sw0Wq
UwAPpd1tkHAWfP64JQZvMYqb74CF0zC2L3rOZOmo9cwcUy62y2pdbM8fAgMBAAGj
NDAyMA8GA1UdEQQIMAaHBKwUAAgwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8E
BAMCAb4wDQYJKoZIhvcNAQELBQADggEBACS8JOzAmiV/+1F6DMByvf/gTawz62gW
d1+Zb7eIafib0Yjdf92BIsTTRzPNAKB+tzujgf/BoVRRR0/K4rlqfDsdbPTJ6ylb
Hl7MrQ2Ioy/Abi7L/biEDj8mnhrlACd1OKcY/Iyej05qMkRoFPFloeFyhkqiKSVC
oK+zp6IOiEUyrVJ8wmVGdscmeHmdQj1lyLfTeW7AIsEpqXv+XLkO3F7qDLylAibU
Hyms6/iw+thYJk7kj0RUJxViAZvuEX0IlQZndcUN8yJRfGXAwEr9P/Rp2PBak0+r
CjkvvP3RPlFUVsNONHF2xeGU7hbsliifZRRMpm/+ve/CuLrH6ZdNoqU=
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkZWZkMTQxNTItZTM1Zi00MzIyLThlMzctOGJiYWM3MTI0
ODA4MTEwLwYDVQQKDChDSUQtNTg4NDllN2MtNmFhMy00MGY2LTk1YjAtNzg0NGYw
YWI1MTNiMQowCAYDVQQFEwExMB4XDTIzMTEyNzEyMzYwMFoXDTI5MDEwNDEyMzYw
MFowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJGVmZDE0MTUyLWUzNWYt
NDMyMi04ZTM3LThiYmFjNzEyNDgwODExMC8GA1UECgwoQ0lELTU4ODQ5ZTdjLTZh
YTMtNDBmNi05NWIwLTc4NDRmMGFiNTEzYjEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAJSxT7HeaSt1A7IdjalsPEwyj6VYTyEkj27W
RYrKeVUXMN/m/ibFmRNvFTXpCQv0VEx4OTAtA9f3tUMpaIEBgfokCbarU2yA5TCZ
0LVli4WGrzfJgWNTbaOWvys6bh4idTd7j8ARxN3yLanjp21NHIVtYQoiQ0+sbZ0J
7xtIv9IO1PfKmuAjJ8erw/I6KS0stdgMoNvBt6G4AbPSplS0w3sBopEoT0CCvKmb
xhbJcHd2opUZ5VkfUtKvZg5VfNhvsDYkJbQuZ7i5+RrkUNeuvmpuJdK1CZWt1YmS
2qJ93heXnKrUwaxWNd84KIZSIIyuqg098jagEhPtVVBNHPNenGECAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBQA
CDANBgkqhkiG9w0BAQsFAAOCAQEAVfV7QfyXkhbmouVoKiL+zVnmXhYpRxOO/no2
zHzx1W51zUQZTtVWZdP7R7RQ5NARfUzg0ZhkYT4w0Jx5OR6ZAXgSzbJxEB5lV6qY
FYvzwOf4f5haQjjcb5854wBWNNtTK9+2Sldwx+D4kINl8sEVVCultFx4l2mPs83H
tPNGVIllhfNFSzWBq8ansFVOsDDVOZO3+/UM6r22NX2IQibqfnH7B2Vv3pz2qCeT
kUfqaXGp7xX09y3vO4lUP0NlaAaSVtE4pL2n/+ysw5zjzzcfyknl7Y7VDAyT6vNy
VVDBwIziQguSj8Na+okLi7oq1TGmULo5IFbEj/bLq3htTRy6HA==
-----END CERTIFICATE-----

2023-11-27 12:36:00,536 [main] INFO client.SCMCertificateClient: Successfully stored SCM signed certificate.
2023-11-27 12:36:00,693 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
2023-11-27 12:36:00,814 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-11-27 12:36:00,815 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
2023-11-27 12:36:00,816 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-11-27 12:36:00,816 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
2023-11-27 12:36:00,816 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
2023-11-27 12:36:00,816 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2023-11-27 12:36:00,817 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
2023-11-27 12:36:00,828 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-27 12:36:00,830 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2023-11-27 12:36:00,831 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-11-27 12:36:00,839 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2023-11-27 12:36:00,841 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-11-27 12:36:00,842 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-11-27 12:36:01,049 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
2023-11-27 12:36:01,051 [main] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-11-27 12:36:01,051 [main] INFO server.RaftServerConfigKeys: raft.server.close.threshold = 60s (default)
2023-11-27 12:36:01,051 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-11-27 12:36:01,054 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-11-27 12:36:01,055 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
2023-11-27 12:36:01,056 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
2023-11-27 12:36:01,062 [main] INFO server.RaftServer: efd14152-e35f-4322-8e37-8bbac7124808: addNew group-7844F0AB513B:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894] returns group-7844F0AB513B:java.util.concurrent.CompletableFuture@4bf8b77[Not completed]
2023-11-27 12:36:01,081 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808: new RaftServerImpl for group-7844F0AB513B:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894] with SCMStateMachine:uninitialized
2023-11-27 12:36:01,083 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-11-27 12:36:01,083 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
2023-11-27 12:36:01,084 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
2023-11-27 12:36:01,084 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
2023-11-27 12:36:01,084 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-11-27 12:36:01,084 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.member.majority-add = false (default)
2023-11-27 12:36:01,085 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-11-27 12:36:01,090 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: ConfigurationManager, init=-1: peers:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-11-27 12:36:01,095 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
2023-11-27 12:36:01,098 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
2023-11-27 12:36:01,101 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
2023-11-27 12:36:01,101 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100ms (default)
2023-11-27 12:36:01,105 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
2023-11-27 12:36:01,106 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.read-after-write-consistent.write-index-cache.expiry-time = 60s (default)
2023-11-27 12:36:01,111 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.dropwizard3.Dm3MetricRegistriesImpl
2023-11-27 12:36:01,217 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-11-27 12:36:01,220 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-11-27 12:36:01,220 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
2023-11-27 12:36:01,220 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
2023-11-27 12:36:01,220 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
2023-11-27 12:36:01,221 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
2023-11-27 12:36:01,222 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
2023-11-27 12:36:01,222 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
2023-11-27 12:36:01,222 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-11-27 12:36:01,230 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b does not exist. Creating ...
2023-11-27 12:36:01,234 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b/in_use.lock acquired by nodename 13@scm
2023-11-27 12:36:01,242 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b has been successfully formatted.
2023-11-27 12:36:01,245 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
2023-11-27 12:36:01,252 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
2023-11-27 12:36:01,253 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-27 12:36:01,254 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-11-27 12:36:01,255 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
2023-11-27 12:36:01,258 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-11-27 12:36:01,262 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
2023-11-27 12:36:01,262 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-11-27 12:36:01,263 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-27 12:36:01,264 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO util.AwaitToRun: Thread[efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-cacheEviction-AwaitToRun,5,main] started
2023-11-27 12:36:01,268 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b
2023-11-27 12:36:01,268 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2023-11-27 12:36:01,268 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
2023-11-27 12:36:01,270 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-11-27 12:36:01,270 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
2023-11-27 12:36:01,271 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
2023-11-27 12:36:01,272 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
2023-11-27 12:36:01,272 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-11-27 12:36:01,272 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-11-27 12:36:01,274 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
2023-11-27 12:36:01,274 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-27 12:36:01,276 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
2023-11-27 12:36:01,277 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
2023-11-27 12:36:01,277 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
2023-11-27 12:36:01,288 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO segmented.SegmentedRaftLogWorker: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-11-27 12:36:01,288 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO segmented.SegmentedRaftLogWorker: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-11-27 12:36:01,292 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: start as a follower, conf=-1: peers:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894]|listeners:[], old=null
2023-11-27 12:36:01,293 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-11-27 12:36:01,294 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO impl.RoleInfo: efd14152-e35f-4322-8e37-8bbac7124808: start efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState
2023-11-27 12:36:01,294 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
2023-11-27 12:36:01,294 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-11-27 12:36:01,299 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7844F0AB513B,id=efd14152-e35f-4322-8e37-8bbac7124808
2023-11-27 12:36:01,301 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.trigger-when-stop.enabled = true (default)
2023-11-27 12:36:01,301 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-11-27 12:36:01,301 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
2023-11-27 12:36:01,302 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
2023-11-27 12:36:01,302 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
2023-11-27 12:36:01,306 [main] INFO server.RaftServer: efd14152-e35f-4322-8e37-8bbac7124808: start RPC server
2023-11-27 12:36:01,340 [main] INFO server.GrpcService: efd14152-e35f-4322-8e37-8bbac7124808: GrpcService started, listening on 9894
2023-11-27 12:36:01,341 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-efd14152-e35f-4322-8e37-8bbac7124808: Started
2023-11-27 12:36:06,370 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO impl.FollowerState: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5076590284ns, electionTimeout:5072ms
2023-11-27 12:36:06,370 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO impl.RoleInfo: efd14152-e35f-4322-8e37-8bbac7124808: shutdown efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState
2023-11-27 12:36:06,370 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-11-27 12:36:06,373 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
2023-11-27 12:36:06,373 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO impl.RoleInfo: efd14152-e35f-4322-8e37-8bbac7124808: start efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1
2023-11-27 12:36:06,376 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO impl.LeaderElection: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894]|listeners:[], old=null
2023-11-27 12:36:06,376 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO impl.LeaderElection: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
2023-11-27 12:36:06,379 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO impl.LeaderElection: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894]|listeners:[], old=null
2023-11-27 12:36:06,380 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO impl.LeaderElection: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1 ELECTION round 0: result PASSED (term=1)
2023-11-27 12:36:06,380 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO impl.RoleInfo: efd14152-e35f-4322-8e37-8bbac7124808: shutdown efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1
2023-11-27 12:36:06,380 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-11-27 12:36:06,384 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
2023-11-27 12:36:06,387 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-11-27 12:36:06,388 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
2023-11-27 12:36:06,390 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
2023-11-27 12:36:06,391 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
2023-11-27 12:36:06,391 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
2023-11-27 12:36:06,395 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.enabled = false (default)
2023-11-27 12:36:06,396 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.timeout.ratio = 0.9 (default)
2023-11-27 12:36:06,396 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-11-27 12:36:06,397 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-11-27 12:36:06,397 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-11-27 12:36:06,398 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO impl.RoleInfo: efd14152-e35f-4322-8e37-8bbac7124808: start efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderStateImpl
2023-11-27 12:36:06,398 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: set firstElectionSinceStartup to false for becomeLeader
2023-11-27 12:36:06,398 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: change Leader from null to efd14152-e35f-4322-8e37-8bbac7124808 at term 1 for becomeLeader, leader elected after 5302ms
2023-11-27 12:36:06,435 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker: Starting segment from index:0
2023-11-27 12:36:06,452 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: set configuration 0: peers:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894]|listeners:[], old=null
2023-11-27 12:36:06,500 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b/current/log_inprogress_0
2023-11-27 12:36:06,506 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO server.RaftServer$Division: leader is ready since appliedIndex == 0 >= startIndex == 0
2023-11-27 12:36:07,342 [main] INFO server.RaftServer: efd14152-e35f-4322-8e37-8bbac7124808: close
2023-11-27 12:36:07,343 [main] INFO server.GrpcService: efd14152-e35f-4322-8e37-8bbac7124808: shutdown server GrpcServerProtocolService now
2023-11-27 12:36:07,343 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: shutdown
2023-11-27 12:36:07,344 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-7844F0AB513B,id=efd14152-e35f-4322-8e37-8bbac7124808
2023-11-27 12:36:07,344 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO impl.RoleInfo: efd14152-e35f-4322-8e37-8bbac7124808: shutdown efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderStateImpl
2023-11-27 12:36:07,347 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO impl.PendingRequests: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-PendingRequests: sendNotLeaderResponses
2023-11-27 12:36:07,350 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO impl.StateMachineUpdater: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater: set stopIndex = 0
2023-11-27 12:36:07,350 [main] INFO server.GrpcService: efd14152-e35f-4322-8e37-8bbac7124808: shutdown server GrpcServerProtocolService successfully
2023-11-27 12:36:07,350 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO impl.StateMachineUpdater: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater: Took a snapshot at index 0
2023-11-27 12:36:07,351 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO impl.StateMachineUpdater: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-11-27 12:36:07,354 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: applyIndex: 0
2023-11-27 12:36:07,354 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-cacheEviction-AwaitToRun] INFO util.AwaitToRun: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-cacheEviction-AwaitToRun-AwaitForSignal is interrupted
2023-11-27 12:36:07,504 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO segmented.SegmentedRaftLogWorker: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker close()
2023-11-27 12:36:07,505 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-efd14152-e35f-4322-8e37-8bbac7124808: Stopped
2023-11-27 12:36:07,505 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-11-27 12:36:07,507 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b; layoutVersion=7; scmId=efd14152-e35f-4322-8e37-8bbac7124808
2023-11-27 12:36:07,510 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down StorageContainerManager at scm/172.20.0.8
************************************************************/
No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2023-11-27 12:36:09,098 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = scm/172.20.0.8
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 1.4.0-SNAPSHOT
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/java-uuid-generator-4.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-3.0.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-3.0.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/picocli-4.7.5.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.16.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-dropwizard3-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-3.0.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-3.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.58.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-3.0.0.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
STARTUP_MSG:   build = https://github.com/apache/ozone/7dcae6715cc449b5ffd41b31fdda38fa0560dcc3 ; compiled by 'runner' on 2023-11-27T11:55Z
STARTUP_MSG:   java = 11.0.19
STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.command.worker.interval=2s, hdds.datanode.block.delete.max.lock.wait.timeout=100ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.max.lock.holding.time=1s, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=19864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.close.threads.max=3, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.file.size=100B, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/dn@EXAMPLE.COM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.queue.limit=4096, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=100MB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.log.appender.wait-time.min=1ms, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/scm.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.expired.certificate.check.interval=P1D, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.administrators=*, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.grpc.write.timeout=30s, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/om.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.open.mpu.cleanup.service.interval=24h, ozone.om.open.mpu.cleanup.service.timeout=300s, ozone.om.open.mpu.expire.threshold=30d, ozone.om.open.mpu.parts.cleanup.limit.per.task=1000, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.checkpoint.dir.creation.poll.timeout=20s, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=6000, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3.administrators=s3g, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/s3g.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.list-keys.shallow.enabled=true, ozone.s3g.secret.http.auth.type=kerberos, ozone.s3g.secret.http.enabled=true, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.raft.server.log.appender.wait-time.min=0ms, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=scm:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
************************************************************/
2023-11-27 12:36:09,105 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
2023-11-27 12:36:09,150 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-11-27 12:36:09,252 [main] INFO reflections.Reflections: Reflections took 75 ms to scan 3 urls, producing 134 keys and 291 values 
2023-11-27 12:36:09,325 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
2023-11-27 12:36:09,333 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2023-11-27 12:36:09,480 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
2023-11-27 12:36:09,480 [main] INFO server.StorageContainerManager: SCM login successful.
2023-11-27 12:36:09,961 [main] INFO client.SCMCertificateClient: Certificate serial ID set to 2
2023-11-27 12:36:10,075 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=1
           Start Date: Mon Nov 27 12:36:00 UTC 2023
           Final Date: Thu Jan 04 12:36:00 UTC 2029
            SubjectDN: CN=scm-sub@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=2
           Public Key: RSA Public Key [19:cd:24:a4:5a:ec:0b:f6:d5:7d:de:8c:5f:71:6c:94:8f:2b:04:ce],[56:66:d1:a4]
        modulus: a7742932bc85bd3387c60b710054037fbc0b8518070af3690e06eb0cd7c2a39bf384603ec1e95e0f7a65a9777f4bdd37c2990e3b8208ce0163da181d85f7a3a67041b0d336c0673039c17640805a9cb5075fdb928463a0a95fc27300c829887408be5473e7d714d3ffeff75e1dda2e95bc2ca9ca60f8f4d7df384073215b22973f844c338283cf9e846ef02f734efddd58e68cb17d30e2a0f25439516dd0e3a908096af72132b24d0f4028153d218025e93d87a8e9f793af3b6e0ad0b538c86b23f7c9afbe57f46e9d508c60b1431aefce7f52c345aa53000fa5dd6d9070167cfeb825066f318a9bef8085d330b62f7ace64e9a8f5cc1c532eb6cb6a5d6ccf1f
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 24bc24ecc09a257ffb517a0cc072bdffe04dac33
                       eb6816775f996fb78869f89bd188dd7fdd8122c4
                       d34733cd00a07eb73ba381ffc1a15451474fcae2
                       b96a7c3b1d6cf4c9eb295b1e5eccad0d88a32fc0
                       6e2ecbfdb8840e3f269e1ae500277538a718fc8c
                       9e8f4e6a32446814f165a1e172864aa2292542a0
                       afb3a7a20e884532ad527cc2654676c72678799d
                       423d65c8b7d3796ec022c129a97bfe5cb90edc5e
                       ea0cbca50226d41f29acebf8b0fad858264ee48f
                       4454271562019bee117d0895066775c50df32251
                       7c65c0c04afd3ff469d8f05a934fab0a392fbcfd
                       d13e515456c34e347176c5e194ee16ec96289f65
                       144ca66ffebdefc2b8bac7e9974da2a5
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe
 from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
2023-11-27 12:36:10,084 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=1
           Start Date: Mon Nov 27 12:36:00 UTC 2023
           Final Date: Thu Jan 04 12:36:00 UTC 2029
            SubjectDN: CN=scm@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=1
           Public Key: RSA Public Key [72:10:cf:f5:6e:cd:d0:c5:5c:08:c5:3a:fe:e7:2e:32:63:67:dd:26],[56:66:d1:a4]
        modulus: 94b14fb1de692b7503b21d8da96c3c4c328fa5584f21248f6ed6458aca79551730dfe6fe26c599136f1535e9090bf4544c7839302d03d7f7b5432968810181fa2409b6ab536c80e53099d0b5658b8586af37c98163536da396bf2b3a6e1e2275377b8fc011c4ddf22da9e3a76d4d1c856d610a22434fac6d9d09ef1b48bfd20ed4f7ca9ae02327c7abc3f23a292d2cb5d80ca0dbc1b7a1b801b3d2a654b4c37b01a291284f4082bca99bc616c9707776a29519e5591f52d2af660e557cd86fb0362425b42e67b8b9f91ae450d7aebe6a6e25d2b50995add58992daa27dde17979caad4c1ac5635df38288652208caeaa0d3df236a01213ed55504d1cf35e9c61
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 55f57b41fc979216e6a2e5682a22fecd59e65e16
                       2947138efe7a36cc7cf1d56e75cd44194ed55665
                       d3fb47b450e4d0117d4ce0d19864613e30d09c79
                       391e99017812cdb271101e6557aa98158bf3c0e7
                       f87f985a4238dc6f9f39e3005634db532bdfb64a
                       5770c7e0f8908365f2c115542ba5b45c7897698f
                       b3cdc7b4f34654896585f3454b3581abc6a7b055
                       4eb030d53993b7fbf50ceabdb6357d884226ea7e
                       71fb07656fde9cf6a827939147ea6971a9ef15f4
                       f72def3b89543f436568069256d138a4bda7ffec
                       acc39ce3cf371fca49e5ed8ed50c0c93eaf37255
                       50c1c08ce2420b928fc35afa890b8bba2ad531a6
                       50ba392056c48ff6cbab786d4d1cba1c
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 

 from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
2023-11-27 12:36:10,126 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=1
           Start Date: Mon Nov 27 12:36:00 UTC 2023
           Final Date: Thu Jan 04 12:36:00 UTC 2029
            SubjectDN: CN=scm-sub@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=2
           Public Key: RSA Public Key [19:cd:24:a4:5a:ec:0b:f6:d5:7d:de:8c:5f:71:6c:94:8f:2b:04:ce],[56:66:d1:a4]
        modulus: a7742932bc85bd3387c60b710054037fbc0b8518070af3690e06eb0cd7c2a39bf384603ec1e95e0f7a65a9777f4bdd37c2990e3b8208ce0163da181d85f7a3a67041b0d336c0673039c17640805a9cb5075fdb928463a0a95fc27300c829887408be5473e7d714d3ffeff75e1dda2e95bc2ca9ca60f8f4d7df384073215b22973f844c338283cf9e846ef02f734efddd58e68cb17d30e2a0f25439516dd0e3a908096af72132b24d0f4028153d218025e93d87a8e9f793af3b6e0ad0b538c86b23f7c9afbe57f46e9d508c60b1431aefce7f52c345aa53000fa5dd6d9070167cfeb825066f318a9bef8085d330b62f7ace64e9a8f5cc1c532eb6cb6a5d6ccf1f
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 24bc24ecc09a257ffb517a0cc072bdffe04dac33
                       eb6816775f996fb78869f89bd188dd7fdd8122c4
                       d34733cd00a07eb73ba381ffc1a15451474fcae2
                       b96a7c3b1d6cf4c9eb295b1e5eccad0d88a32fc0
                       6e2ecbfdb8840e3f269e1ae500277538a718fc8c
                       9e8f4e6a32446814f165a1e172864aa2292542a0
                       afb3a7a20e884532ad527cc2654676c72678799d
                       423d65c8b7d3796ec022c129a97bfe5cb90edc5e
                       ea0cbca50226d41f29acebf8b0fad858264ee48f
                       4454271562019bee117d0895066775c50df32251
                       7c65c0c04afd3ff469d8f05a934fab0a392fbcfd
                       d13e515456c34e347176c5e194ee16ec96289f65
                       144ca66ffebdefc2b8bac7e9974da2a5
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe
 from file: /data/metadata/scm/sub-ca/certs/2.crt.
2023-11-27 12:36:10,127 [main] INFO client.SCMCertificateClient: CertificateRenewerService and root ca rotation polling is disabled for scm/sub-ca
2023-11-27 12:36:10,210 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-11-27 12:36:10,339 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-11-27 12:36:10,567 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
2023-11-27 12:36:10,568 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
2023-11-27 12:36:10,634 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.dropwizard3.Dm3MetricRegistriesImpl
2023-11-27 12:36:10,788 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:efd14152-e35f-4322-8e37-8bbac7124808
2023-11-27 12:36:10,808 [main] INFO ssl.ReloadingX509KeyManager: Key manager is loaded with certificate chain
2023-11-27 12:36:10,810 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=1
           Start Date: Mon Nov 27 12:36:00 UTC 2023
           Final Date: Thu Jan 04 12:36:00 UTC 2029
            SubjectDN: CN=scm-sub@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=2
           Public Key: RSA Public Key [19:cd:24:a4:5a:ec:0b:f6:d5:7d:de:8c:5f:71:6c:94:8f:2b:04:ce],[56:66:d1:a4]
        modulus: a7742932bc85bd3387c60b710054037fbc0b8518070af3690e06eb0cd7c2a39bf384603ec1e95e0f7a65a9777f4bdd37c2990e3b8208ce0163da181d85f7a3a67041b0d336c0673039c17640805a9cb5075fdb928463a0a95fc27300c829887408be5473e7d714d3ffeff75e1dda2e95bc2ca9ca60f8f4d7df384073215b22973f844c338283cf9e846ef02f734efddd58e68cb17d30e2a0f25439516dd0e3a908096af72132b24d0f4028153d218025e93d87a8e9f793af3b6e0ad0b538c86b23f7c9afbe57f46e9d508c60b1431aefce7f52c345aa53000fa5dd6d9070167cfeb825066f318a9bef8085d330b62f7ace64e9a8f5cc1c532eb6cb6a5d6ccf1f
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 24bc24ecc09a257ffb517a0cc072bdffe04dac33
                       eb6816775f996fb78869f89bd188dd7fdd8122c4
                       d34733cd00a07eb73ba381ffc1a15451474fcae2
                       b96a7c3b1d6cf4c9eb295b1e5eccad0d88a32fc0
                       6e2ecbfdb8840e3f269e1ae500277538a718fc8c
                       9e8f4e6a32446814f165a1e172864aa2292542a0
                       afb3a7a20e884532ad527cc2654676c72678799d
                       423d65c8b7d3796ec022c129a97bfe5cb90edc5e
                       ea0cbca50226d41f29acebf8b0fad858264ee48f
                       4454271562019bee117d0895066775c50df32251
                       7c65c0c04afd3ff469d8f05a934fab0a392fbcfd
                       d13e515456c34e347176c5e194ee16ec96289f65
                       144ca66ffebdefc2b8bac7e9974da2a5
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe

2023-11-27 12:36:10,817 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=1
           Start Date: Mon Nov 27 12:36:00 UTC 2023
           Final Date: Thu Jan 04 12:36:00 UTC 2029
            SubjectDN: CN=scm@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=1
           Public Key: RSA Public Key [72:10:cf:f5:6e:cd:d0:c5:5c:08:c5:3a:fe:e7:2e:32:63:67:dd:26],[56:66:d1:a4]
        modulus: 94b14fb1de692b7503b21d8da96c3c4c328fa5584f21248f6ed6458aca79551730dfe6fe26c599136f1535e9090bf4544c7839302d03d7f7b5432968810181fa2409b6ab536c80e53099d0b5658b8586af37c98163536da396bf2b3a6e1e2275377b8fc011c4ddf22da9e3a76d4d1c856d610a22434fac6d9d09ef1b48bfd20ed4f7ca9ae02327c7abc3f23a292d2cb5d80ca0dbc1b7a1b801b3d2a654b4c37b01a291284f4082bca99bc616c9707776a29519e5591f52d2af660e557cd86fb0362425b42e67b8b9f91ae450d7aebe6a6e25d2b50995add58992daa27dde17979caad4c1ac5635df38288652208caeaa0d3df236a01213ed55504d1cf35e9c61
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 55f57b41fc979216e6a2e5682a22fecd59e65e16
                       2947138efe7a36cc7cf1d56e75cd44194ed55665
                       d3fb47b450e4d0117d4ce0d19864613e30d09c79
                       391e99017812cdb271101e6557aa98158bf3c0e7
                       f87f985a4238dc6f9f39e3005634db532bdfb64a
                       5770c7e0f8908365f2c115542ba5b45c7897698f
                       b3cdc7b4f34654896585f3454b3581abc6a7b055
                       4eb030d53993b7fbf50ceabdb6357d884226ea7e
                       71fb07656fde9cf6a827939147ea6971a9ef15f4
                       f72def3b89543f436568069256d138a4bda7ffec
                       acc39ce3cf371fca49e5ed8ed50c0c93eaf37255
                       50c1c08ce2420b928fc35afa890b8bba2ad531a6
                       50ba392056c48ff6cbab786d4d1cba1c
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 


2023-11-27 12:36:10,820 [main] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-27 12:36:10,821 [main] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-27 12:36:10,821 [main] INFO ssl.ReloadingX509TrustManager: Trust manager is loaded with certificates
2023-11-27 12:36:10,825 [main] INFO ssl.ReloadingX509TrustManager:   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=1
           Start Date: Mon Nov 27 12:36:00 UTC 2023
           Final Date: Thu Jan 04 12:36:00 UTC 2029
            SubjectDN: CN=scm@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=1
           Public Key: RSA Public Key [72:10:cf:f5:6e:cd:d0:c5:5c:08:c5:3a:fe:e7:2e:32:63:67:dd:26],[56:66:d1:a4]
        modulus: 94b14fb1de692b7503b21d8da96c3c4c328fa5584f21248f6ed6458aca79551730dfe6fe26c599136f1535e9090bf4544c7839302d03d7f7b5432968810181fa2409b6ab536c80e53099d0b5658b8586af37c98163536da396bf2b3a6e1e2275377b8fc011c4ddf22da9e3a76d4d1c856d610a22434fac6d9d09ef1b48bfd20ed4f7ca9ae02327c7abc3f23a292d2cb5d80ca0dbc1b7a1b801b3d2a654b4c37b01a291284f4082bca99bc616c9707776a29519e5591f52d2af660e557cd86fb0362425b42e67b8b9f91ae450d7aebe6a6e25d2b50995add58992daa27dde17979caad4c1ac5635df38288652208caeaa0d3df236a01213ed55504d1cf35e9c61
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 55f57b41fc979216e6a2e5682a22fecd59e65e16
                       2947138efe7a36cc7cf1d56e75cd44194ed55665
                       d3fb47b450e4d0117d4ce0d19864613e30d09c79
                       391e99017812cdb271101e6557aa98158bf3c0e7
                       f87f985a4238dc6f9f39e3005634db532bdfb64a
                       5770c7e0f8908365f2c115542ba5b45c7897698f
                       b3cdc7b4f34654896585f3454b3581abc6a7b055
                       4eb030d53993b7fbf50ceabdb6357d884226ea7e
                       71fb07656fde9cf6a827939147ea6971a9ef15f4
                       f72def3b89543f436568069256d138a4bda7ffec
                       acc39ce3cf371fca49e5ed8ed50c0c93eaf37255
                       50c1c08ce2420b928fc35afa890b8bba2ad531a6
                       50ba392056c48ff6cbab786d4d1cba1c
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 


2023-11-27 12:36:10,839 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-27 12:36:10,841 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-27 12:36:10,916 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
2023-11-27 12:36:10,926 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-11-27 12:36:10,928 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
2023-11-27 12:36:10,929 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-11-27 12:36:10,930 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
2023-11-27 12:36:10,930 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
2023-11-27 12:36:10,931 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2023-11-27 12:36:10,931 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
2023-11-27 12:36:10,933 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-27 12:36:10,934 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2023-11-27 12:36:10,943 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-11-27 12:36:10,951 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2023-11-27 12:36:10,953 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-11-27 12:36:10,954 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-11-27 12:36:11,224 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
2023-11-27 12:36:11,226 [main] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-11-27 12:36:11,226 [main] INFO server.RaftServerConfigKeys: raft.server.close.threshold = 60s (default)
2023-11-27 12:36:11,226 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-11-27 12:36:11,229 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-11-27 12:36:11,230 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
2023-11-27 12:36:11,230 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
2023-11-27 12:36:11,232 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServer: efd14152-e35f-4322-8e37-8bbac7124808: found a subdirectory /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b
2023-11-27 12:36:11,239 [main] INFO server.RaftServer: efd14152-e35f-4322-8e37-8bbac7124808: addNew group-7844F0AB513B:[] returns group-7844F0AB513B:java.util.concurrent.CompletableFuture@5e0442dd[Not completed]
2023-11-27 12:36:11,259 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808: new RaftServerImpl for group-7844F0AB513B:[] with SCMStateMachine:uninitialized
2023-11-27 12:36:11,262 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-11-27 12:36:11,262 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
2023-11-27 12:36:11,263 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
2023-11-27 12:36:11,263 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
2023-11-27 12:36:11,263 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-11-27 12:36:11,264 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.member.majority-add = false (default)
2023-11-27 12:36:11,264 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-11-27 12:36:11,272 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-11-27 12:36:11,277 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
2023-11-27 12:36:11,280 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
2023-11-27 12:36:11,283 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
2023-11-27 12:36:11,283 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100ms (default)
2023-11-27 12:36:11,287 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
2023-11-27 12:36:11,288 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.read-after-write-consistent.write-index-cache.expiry-time = 60s (default)
2023-11-27 12:36:11,369 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-11-27 12:36:11,371 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-11-27 12:36:11,371 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
2023-11-27 12:36:11,371 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
2023-11-27 12:36:11,372 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
2023-11-27 12:36:11,372 [efd14152-e35f-4322-8e37-8bbac7124808-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
2023-11-27 12:36:11,373 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
2023-11-27 12:36:11,373 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
2023-11-27 12:36:11,374 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
2023-11-27 12:36:11,402 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
2023-11-27 12:36:11,434 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
2023-11-27 12:36:11,435 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
2023-11-27 12:36:11,440 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
2023-11-27 12:36:11,442 [main] INFO ha.SequenceIdGenerator: upgrade CertificateId to 2
2023-11-27 12:36:11,444 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
2023-11-27 12:36:11,539 [main] INFO node.SCMNodeManager: Entering startup safe mode.
2023-11-27 12:36:11,554 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
2023-11-27 12:36:11,558 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2023-11-27 12:36:11,567 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
2023-11-27 12:36:11,581 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
2023-11-27 12:36:11,581 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2023-11-27 12:36:11,587 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
2023-11-27 12:36:11,588 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
2023-11-27 12:36:11,592 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
2023-11-27 12:36:11,594 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
2023-11-27 12:36:11,600 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
2023-11-27 12:36:11,600 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
2023-11-27 12:36:11,623 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
2023-11-27 12:36:11,623 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
2023-11-27 12:36:11,641 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
2023-11-27 12:36:11,711 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
2023-11-27 12:36:11,714 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
2023-11-27 12:36:11,714 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
2023-11-27 12:36:11,727 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
2023-11-27 12:36:11,730 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:11,732 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
2023-11-27 12:36:11,883 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT10M
2023-11-27 12:36:11,883 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
2023-11-27 12:36:11,906 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
2023-11-27 12:36:11,911 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
2023-11-27 12:36:11,913 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 2 on primary SCM
2023-11-27 12:36:11,932 [main] INFO server.SCMCertStore: Scm certificate 2 for CN=scm-sub@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=2 is stored
2023-11-27 12:36:11,933 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
2023-11-27 12:36:11,936 [main] INFO server.SCMCertStore: Scm certificate 1 for CN=scm@scm,OU=efd14152-e35f-4322-8e37-8bbac7124808,O=CID-58849e7c-6aa3-40f6-95b0-7844f0ab513b,SERIALNUMBER=1 is stored
2023-11-27 12:36:11,942 [main] INFO ha.SequenceIdGenerator: upgrade CertificateId to 2
2023-11-27 12:36:11,961 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-11-27 12:36:11,993 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
2023-11-27 12:36:11,995 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
2023-11-27 12:36:12,024 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [*, scm]
2023-11-27 12:36:12,418 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2023-11-27 12:36:12,433 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-11-27 12:36:12,433 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
2023-11-27 12:36:12,435 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
2023-11-27 12:36:12,470 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2023-11-27 12:36:12,476 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-11-27 12:36:12,479 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
2023-11-27 12:36:12,479 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
2023-11-27 12:36:12,544 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2023-11-27 12:36:12,635 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-11-27 12:36:12,636 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
2023-11-27 12:36:12,637 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
2023-11-27 12:36:12,705 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
2023-11-27 12:36:12,706 [main] INFO server.StorageContainerManager: 
Container Balancer status:
Key                            Value
Running                        false
Container Balancer Configuration values:
Key                                                Value
Threshold                                          10
Max Datanodes to Involve per Iteration(percent)    20
Max Size to Move per Iteration                     500GB
Max Size Entering Target per Iteration             26GB
Max Size Leaving Source per Iteration              26GB
Number of Iterations                               10
Time Limit for Single Container's Movement         65min
Time Limit for Single Container's Replication      50min
Interval between each Iteration                    70min
Whether to Enable Network Topology                 false
Whether to Trigger Refresh Datanode Usage Info     false
Container IDs to Exclude from Balancing            None
Datanodes Specified to be Balanced                 None
Datanodes Excluded from Balancing                  None

2023-11-27 12:36:12,706 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
2023-11-27 12:36:12,709 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
2023-11-27 12:36:12,712 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
2023-11-27 12:36:12,714 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
2023-11-27 12:36:12,714 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
2023-11-27 12:36:12,715 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-11-27 12:36:12,750 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b/in_use.lock acquired by nodename 7@scm
2023-11-27 12:36:12,754 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=efd14152-e35f-4322-8e37-8bbac7124808} from /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b/current/raft-meta
2023-11-27 12:36:12,803 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: set configuration 0: peers:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894]|listeners:[], old=null
2023-11-27 12:36:12,806 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
2023-11-27 12:36:12,814 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
2023-11-27 12:36:12,814 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-27 12:36:12,815 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-11-27 12:36:12,816 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
2023-11-27 12:36:12,820 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-11-27 12:36:12,826 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
2023-11-27 12:36:12,826 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-11-27 12:36:12,827 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-27 12:36:12,828 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO util.AwaitToRun: Thread[efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-cacheEviction-AwaitToRun,5,main] started
2023-11-27 12:36:12,832 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b
2023-11-27 12:36:12,833 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2023-11-27 12:36:12,833 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
2023-11-27 12:36:12,835 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-11-27 12:36:12,835 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
2023-11-27 12:36:12,835 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
2023-11-27 12:36:12,836 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
2023-11-27 12:36:12,836 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-11-27 12:36:12,836 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-11-27 12:36:12,838 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
2023-11-27 12:36:12,838 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-27 12:36:12,843 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
2023-11-27 12:36:12,845 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
2023-11-27 12:36:12,845 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
2023-11-27 12:36:12,897 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: set configuration 0: peers:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894]|listeners:[], old=null
2023-11-27 12:36:12,898 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b/current/log_inprogress_0
2023-11-27 12:36:12,899 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO segmented.SegmentedRaftLogWorker: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-11-27 12:36:12,978 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: start as a follower, conf=0: peers:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894]|listeners:[], old=null
2023-11-27 12:36:12,978 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: changes role from      null to FOLLOWER at term 1 for startAsFollower
2023-11-27 12:36:12,979 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO impl.RoleInfo: efd14152-e35f-4322-8e37-8bbac7124808: start efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState
2023-11-27 12:36:12,984 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
2023-11-27 12:36:12,984 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-11-27 12:36:12,985 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7844F0AB513B,id=efd14152-e35f-4322-8e37-8bbac7124808
2023-11-27 12:36:12,987 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.trigger-when-stop.enabled = true (default)
2023-11-27 12:36:12,987 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-11-27 12:36:12,988 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
2023-11-27 12:36:12,988 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
2023-11-27 12:36:12,989 [efd14152-e35f-4322-8e37-8bbac7124808-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
2023-11-27 12:36:12,996 [main] INFO server.RaftServer: efd14152-e35f-4322-8e37-8bbac7124808: start RPC server
2023-11-27 12:36:13,034 [main] INFO server.GrpcService: efd14152-e35f-4322-8e37-8bbac7124808: GrpcService started, listening on 9894
2023-11-27 12:36:13,046 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [efd14152-e35f-4322-8e37-8bbac7124808|scm:9894]
2023-11-27 12:36:13,046 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
2023-11-27 12:36:13,048 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-efd14152-e35f-4322-8e37-8bbac7124808: Started
2023-11-27 12:36:13,049 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
2023-11-27 12:36:13,050 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
2023-11-27 12:36:13,050 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
2023-11-27 12:36:13,195 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2023-11-27 12:36:13,207 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-11-27 12:36:13,207 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
2023-11-27 12:36:13,263 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
2023-11-27 12:36:13,264 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-11-27 12:36:13,267 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
2023-11-27 12:36:13,298 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
2023-11-27 12:36:13,302 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
2023-11-27 12:36:13,306 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-11-27 12:36:13,306 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
2023-11-27 12:36:13,328 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
2023-11-27 12:36:13,331 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
2023-11-27 12:36:13,331 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-11-27 12:36:13,334 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
2023-11-27 12:36:13,489 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
2023-11-27 12:36:13,498 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
2023-11-27 12:36:13,499 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = simple
2023-11-27 12:36:13,592 [main] INFO util.log: Logging initialized @5578ms to org.eclipse.jetty.util.log.Slf4jLog
2023-11-27 12:36:13,647 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:33894 / 172.20.0.7:33894
2023-11-27 12:36:13,678 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:47836 / 172.20.0.2:47836
2023-11-27 12:36:13,746 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
2023-11-27 12:36:13,752 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-11-27 12:36:13,754 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
2023-11-27 12:36:13,754 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-11-27 12:36:13,754 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-11-27 12:36:13,796 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
2023-11-27 12:36:13,804 [main] INFO http.HttpServer2: Jetty bound to port 9876
2023-11-27 12:36:13,809 [main] INFO server.Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 11.0.19+7-LTS
2023-11-27 12:36:13,858 [main] INFO server.session: DefaultSessionIdManager workerName=node0
2023-11-27 12:36:13,858 [main] INFO server.session: No SessionScavenger set, using defaults
2023-11-27 12:36:13,862 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-mr_datanode_2.ozonesecure-mr_default:47836 / 172.20.0.2:47836
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-27 12:36:13,865 [main] INFO server.session: node0 Scavenging every 660000ms
2023-11-27 12:36:13,868 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-mr_datanode_3.ozonesecure-mr_default:33894 / 172.20.0.7:33894
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-27 12:36:13,886 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3ea95c22{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
2023-11-27 12:36:13,889 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@12d432ea{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-11-27 12:36:14,005 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7292470c{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-671743779953721847/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
2023-11-27 12:36:14,014 [main] INFO server.AbstractConnector: Started ServerConnector@1a60b0c4{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
2023-11-27 12:36:14,014 [main] INFO server.Server: Started @6000ms
2023-11-27 12:36:14,018 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
2023-11-27 12:36:14,018 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
2023-11-27 12:36:14,019 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
2023-11-27 12:36:14,060 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:57622 / 172.20.0.9:57622
2023-11-27 12:36:14,080 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-mr_datanode_1.ozonesecure-mr_default:57622 / 172.20.0.9:57622
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-27 12:36:14,160 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm:33243 / 172.20.0.8:33243
2023-11-27 12:36:14,177 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#0 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from scm:33243 / 172.20.0.8:33243
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-27 12:36:14,568 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:36295 / 172.20.0.5:36295
2023-11-27 12:36:15,880 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-mr_datanode_2.ozonesecure-mr_default:47836 / 172.20.0.2:47836
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-27 12:36:15,894 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-mr_datanode_3.ozonesecure-mr_default:33894 / 172.20.0.7:33894
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-27 12:36:16,089 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-mr_datanode_1.ozonesecure-mr_default:57622 / 172.20.0.9:57622
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-27 12:36:16,187 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#1 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from scm:33243 / 172.20.0.8:33243
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-27 12:36:16,189 [efd14152-e35f-4322-8e37-8bbac7124808-scm/sub-ca-refreshCACertificates] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
, while invoking $Proxy15.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.20.0.8:9961 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
2023-11-27 12:36:16,736 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm:42919 / 172.20.0.8:42919
2023-11-27 12:36:17,884 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#9 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-mr_datanode_2.ozonesecure-mr_default:47836 / 172.20.0.2:47836
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-27 12:36:17,897 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#9 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-mr_datanode_3.ozonesecure-mr_default:33894 / 172.20.0.7:33894
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-27 12:36:18,092 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#9 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure-mr_datanode_1.ozonesecure-mr_default:57622 / 172.20.0.9:57622
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-27 12:36:18,131 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO impl.FollowerState: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5151726334ns, electionTimeout:5145ms
2023-11-27 12:36:18,131 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO impl.RoleInfo: efd14152-e35f-4322-8e37-8bbac7124808: shutdown efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState
2023-11-27 12:36:18,131 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
2023-11-27 12:36:18,134 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
2023-11-27 12:36:18,134 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-FollowerState] INFO impl.RoleInfo: efd14152-e35f-4322-8e37-8bbac7124808: start efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1
2023-11-27 12:36:18,136 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO impl.LeaderElection: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894]|listeners:[], old=null
2023-11-27 12:36:18,136 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO impl.LeaderElection: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
2023-11-27 12:36:18,141 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO impl.LeaderElection: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894]|listeners:[], old=null
2023-11-27 12:36:18,141 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO impl.LeaderElection: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1 ELECTION round 0: result PASSED (term=2)
2023-11-27 12:36:18,141 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO impl.RoleInfo: efd14152-e35f-4322-8e37-8bbac7124808: shutdown efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1
2023-11-27 12:36:18,141 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
2023-11-27 12:36:18,146 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
2023-11-27 12:36:18,149 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-11-27 12:36:18,150 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
2023-11-27 12:36:18,152 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
2023-11-27 12:36:18,152 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
2023-11-27 12:36:18,153 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
2023-11-27 12:36:18,157 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.enabled = false (default)
2023-11-27 12:36:18,158 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.timeout.ratio = 0.9 (default)
2023-11-27 12:36:18,158 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-11-27 12:36:18,158 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-11-27 12:36:18,159 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-11-27 12:36:18,160 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO impl.RoleInfo: efd14152-e35f-4322-8e37-8bbac7124808: start efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderStateImpl
2023-11-27 12:36:18,160 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: set firstElectionSinceStartup to false for becomeLeader
2023-11-27 12:36:18,160 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
2023-11-27 12:36:18,160 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
2023-11-27 12:36:18,165 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: change Leader from null to efd14152-e35f-4322-8e37-8bbac7124808 at term 2 for becomeLeader, leader elected after 6882ms
2023-11-27 12:36:18,174 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
2023-11-27 12:36:18,189 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-LeaderElection1] INFO server.RaftServer$Division: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B: set configuration 1: peers:[efd14152-e35f-4322-8e37-8bbac7124808|scm:9894]|listeners:[], old=null
2023-11-27 12:36:18,190 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b/current/log_inprogress_0 to /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b/current/log_0-0
2023-11-27 12:36:18,197 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#2 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from scm:33243 / 172.20.0.8:33243
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-27 12:36:18,199 [efd14152-e35f-4322-8e37-8bbac7124808-scm/sub-ca-refreshCACertificates] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:efd14152-e35f-4322-8e37-8bbac7124808 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
, while invoking $Proxy15.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.20.0.8:9961 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
2023-11-27 12:36:18,200 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/58849e7c-6aa3-40f6-95b0-7844f0ab513b/current/log_inprogress_1
2023-11-27 12:36:18,204 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO server.RaftServer$Division: leader is ready since appliedIndex == 1 >= startIndex == 1
2023-11-27 12:36:18,204 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
2023-11-27 12:36:18,205 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
2023-11-27 12:36:18,205 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: Initializing SecretKeys.
2023-11-27 12:36:18,206 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:18,207 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: No valid key has been loaded. A new key is generated: SecretKey(id = 05ecf1e3-7c22-4297-89a6-5ce4b9bc42ce, creation at: 2023-11-27T12:36:18.206140Z, expire at: 2023-12-04T12:36:18.206140Z)
2023-11-27 12:36:18,211 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
2023-11-27 12:36:18,212 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
2023-11-27 12:36:18,213 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
2023-11-27 12:36:18,214 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
2023-11-27 12:36:18,223 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-11-27 12:36:18,277 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = 05ecf1e3-7c22-4297-89a6-5ce4b9bc42ce, creation at: 2023-11-27T12:36:18.206Z, expire at: 2023-12-04T12:36:18.206Z)]
2023-11-27 12:36:18,277 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = 05ecf1e3-7c22-4297-89a6-5ce4b9bc42ce, creation at: 2023-11-27T12:36:18.206Z, expire at: 2023-12-04T12:36:18.206Z)
2023-11-27 12:36:18,328 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = 05ecf1e3-7c22-4297-89a6-5ce4b9bc42ce, creation at: 2023-11-27T12:36:18.206Z, expire at: 2023-12-04T12:36:18.206Z)] to file /data/metadata/scm/keys/secret_keys.json
2023-11-27 12:36:18,329 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:18,329 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
2023-11-27 12:36:18,329 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
2023-11-27 12:36:19,626 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:37357 / 172.20.0.5:37357
2023-11-27 12:36:19,657 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om, UUID: 96cc05e9-423a-4270-8f46-ac313482b341
2023-11-27 12:36:19,672 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for CertificateId, expected lastId is 0, actual lastId is 2.
2023-11-27 12:36:19,675 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:19,679 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:19,679 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 2 to 3.
2023-11-27 12:36:19,729 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-27 12:36:19,729 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-27 12:36:19,755 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-27 12:36:19,887 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 32bbcd2cc758, UUID: 3e7184a9-5f44-4cfd-ac8a-4a6eec2b9210
2023-11-27 12:36:20,062 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:20,084 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:20,084 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 3 to 4.
2023-11-27 12:36:20,088 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-27 12:36:20,088 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-27 12:36:20,096 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-27 12:36:20,133 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:20,143 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn d9114ff972cf, UUID: 84226438-5cc1-4e21-9d63-d7d41b6032ab
2023-11-27 12:36:20,186 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 692c996c59a7, UUID: f0350e74-e232-4fa2-9e94-53f21a392d23
2023-11-27 12:36:20,199 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:20,204 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 4 to 5.
2023-11-27 12:36:20,215 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-27 12:36:20,215 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-27 12:36:20,222 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-27 12:36:20,281 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:20,298 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:20,298 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 5 to 6.
2023-11-27 12:36:20,303 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-27 12:36:20,303 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-27 12:36:20,308 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-27 12:36:20,346 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:20,369 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-27 12:36:20,369 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-27 12:36:20,370 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-27 12:36:20,370 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-27 12:36:20,375 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-27 12:36:20,375 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-27 12:36:20,376 [efd14152-e35f-4322-8e37-8bbac7124808-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-27 12:36:20,376 [efd14152-e35f-4322-8e37-8bbac7124808-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-27 12:36:20,377 [efd14152-e35f-4322-8e37-8bbac7124808-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: CA certificates are not changed.
2023-11-27 12:36:20,557 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-27 12:36:20,557 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-27 12:36:20,558 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-27 12:36:20,558 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-27 12:36:20,811 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:42774 / 172.20.0.2:42774
2023-11-27 12:36:21,135 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:44604 / 172.20.0.7:44604
2023-11-27 12:36:21,137 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:56006 / 172.20.0.9:56006
2023-11-27 12:36:29,211 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:38197 / 172.20.0.5:38197
2023-11-27 12:36:29,797 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:46108 / 172.20.0.7:46108
2023-11-27 12:36:29,875 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:46106 / 172.20.0.2:46106
2023-11-27 12:36:30,374 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:47876 / 172.20.0.9:47876
2023-11-27 12:36:31,835 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3e7184a9-5f44-4cfd-ac8a-4a6eec2b9210
2023-11-27 12:36:31,844 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3e7184a9-5f44-4cfd-ac8a-4a6eec2b9210{ip: 172.20.0.2, host: ozonesecure-mr_datanode_2.ozonesecure-mr_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 4, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-11-27 12:36:31,851 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/84226438-5cc1-4e21-9d63-d7d41b6032ab
2023-11-27 12:36:31,853 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 84226438-5cc1-4e21-9d63-d7d41b6032ab{ip: 172.20.0.7, host: ozonesecure-mr_datanode_3.ozonesecure-mr_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 5, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-11-27 12:36:31,886 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
2023-11-27 12:36:31,886 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
2023-11-27 12:36:31,887 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-11-27 12:36:31,887 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-11-27 12:36:31,894 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=bc5d41a3-dc01-4354-b038-f8e1a119173f to datanode:84226438-5cc1-4e21-9d63-d7d41b6032ab
2023-11-27 12:36:31,908 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:31,911 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: bc5d41a3-dc01-4354-b038-f8e1a119173f, Nodes: 84226438-5cc1-4e21-9d63-d7d41b6032ab(ozonesecure-mr_datanode_3.ozonesecure-mr_default/172.20.0.7), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-27T12:36:31.893703Z[UTC]]
2023-11-27 12:36:31,911 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=68a333a0-eebc-4c55-bf9c-913fbc60e0b0 to datanode:3e7184a9-5f44-4cfd-ac8a-4a6eec2b9210
2023-11-27 12:36:31,913 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:31,914 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 68a333a0-eebc-4c55-bf9c-913fbc60e0b0, Nodes: 3e7184a9-5f44-4cfd-ac8a-4a6eec2b9210(ozonesecure-mr_datanode_2.ozonesecure-mr_default/172.20.0.2), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-27T12:36:31.911565Z[UTC]]
2023-11-27 12:36:32,329 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f0350e74-e232-4fa2-9e94-53f21a392d23
2023-11-27 12:36:32,330 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f0350e74-e232-4fa2-9e94-53f21a392d23{ip: 172.20.0.9, host: ozonesecure-mr_datanode_1.ozonesecure-mr_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 6, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-11-27 12:36:32,331 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-11-27 12:36:32,332 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
2023-11-27 12:36:32,332 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
2023-11-27 12:36:32,332 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
2023-11-27 12:36:32,332 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
2023-11-27 12:36:32,333 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-11-27 12:36:32,334 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0eb38732-86f3-4118-81fd-a3ce766d5e92 to datanode:f0350e74-e232-4fa2-9e94-53f21a392d23
2023-11-27 12:36:32,338 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:32,339 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 0eb38732-86f3-4118-81fd-a3ce766d5e92, Nodes: f0350e74-e232-4fa2-9e94-53f21a392d23(ozonesecure-mr_datanode_1.ozonesecure-mr_default/172.20.0.9), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-27T12:36:32.334339Z[UTC]]
2023-11-27 12:36:32,349 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0fdf4524-a525-4742-b527-d445cca3d80b to datanode:f0350e74-e232-4fa2-9e94-53f21a392d23
2023-11-27 12:36:32,350 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0fdf4524-a525-4742-b527-d445cca3d80b to datanode:3e7184a9-5f44-4cfd-ac8a-4a6eec2b9210
2023-11-27 12:36:32,350 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0fdf4524-a525-4742-b527-d445cca3d80b to datanode:84226438-5cc1-4e21-9d63-d7d41b6032ab
2023-11-27 12:36:32,359 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:32,364 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 0fdf4524-a525-4742-b527-d445cca3d80b, Nodes: f0350e74-e232-4fa2-9e94-53f21a392d23(ozonesecure-mr_datanode_1.ozonesecure-mr_default/172.20.0.9)3e7184a9-5f44-4cfd-ac8a-4a6eec2b9210(ozonesecure-mr_datanode_2.ozonesecure-mr_default/172.20.0.2)84226438-5cc1-4e21-9d63-d7d41b6032ab(ozonesecure-mr_datanode_3.ozonesecure-mr_default/172.20.0.7), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-27T12:36:32.349243Z[UTC]]
2023-11-27 12:36:32,367 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5c9565b8-2bd2-4294-a61b-8bb19887f6bf to datanode:84226438-5cc1-4e21-9d63-d7d41b6032ab
2023-11-27 12:36:32,369 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5c9565b8-2bd2-4294-a61b-8bb19887f6bf to datanode:f0350e74-e232-4fa2-9e94-53f21a392d23
2023-11-27 12:36:32,369 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5c9565b8-2bd2-4294-a61b-8bb19887f6bf to datanode:3e7184a9-5f44-4cfd-ac8a-4a6eec2b9210
2023-11-27 12:36:32,373 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:32,376 [RatisPipelineUtilsThread-0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=5c9565b8-2bd2-4294-a61b-8bb19887f6bf contains same datanodes as previous pipelines: PipelineID=0fdf4524-a525-4742-b527-d445cca3d80b nodeIds: 84226438-5cc1-4e21-9d63-d7d41b6032ab, f0350e74-e232-4fa2-9e94-53f21a392d23, 3e7184a9-5f44-4cfd-ac8a-4a6eec2b9210
2023-11-27 12:36:32,376 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 5c9565b8-2bd2-4294-a61b-8bb19887f6bf, Nodes: 84226438-5cc1-4e21-9d63-d7d41b6032ab(ozonesecure-mr_datanode_3.ozonesecure-mr_default/172.20.0.7)f0350e74-e232-4fa2-9e94-53f21a392d23(ozonesecure-mr_datanode_1.ozonesecure-mr_default/172.20.0.9)3e7184a9-5f44-4cfd-ac8a-4a6eec2b9210(ozonesecure-mr_datanode_2.ozonesecure-mr_default/172.20.0.2), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-27T12:36:32.367316Z[UTC]]
2023-11-27 12:36:33,190 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:35439 / 172.20.0.5:35439
2023-11-27 12:36:34,980 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:34,981 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=68a333a0-eebc-4c55-bf9c-913fbc60e0b0
2023-11-27 12:36:34,983 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-27 12:36:35,051 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:35,052 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=bc5d41a3-dc01-4354-b038-f8e1a119173f
2023-11-27 12:36:35,052 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-27 12:36:35,057 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-27 12:36:35,150 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-27 12:36:35,477 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-27 12:36:35,478 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=0eb38732-86f3-4118-81fd-a3ce766d5e92
2023-11-27 12:36:35,478 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-27 12:36:35,700 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-27 12:36:36,572 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-27 12:36:36,615 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-27 12:36:36,735 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-27 12:36:40,049 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-27 12:36:40,327 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-27 12:36:40,444 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-27 12:36:40,447 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
2023-11-27 12:36:40,447 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=0fdf4524-a525-4742-b527-d445cca3d80b
2023-11-27 12:36:40,447 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
2023-11-27 12:36:40,447 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
2023-11-27 12:36:40,447 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
2023-11-27 12:36:40,448 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
2023-11-27 12:36:40,448 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
2023-11-27 12:36:40,448 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
2023-11-27 12:36:40,448 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
2023-11-27 12:36:40,448 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
2023-11-27 12:36:40,453 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
2023-11-27 12:36:40,453 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
2023-11-27 12:36:41,945 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=5c9565b8-2bd2-4294-a61b-8bb19887f6bf
2023-11-27 12:37:07,790 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:37835 / 172.20.0.5:37835
2023-11-27 12:37:07,817 [IPC Server handler 61 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
2023-11-27 12:37:07,828 [efd14152-e35f-4322-8e37-8bbac7124808@group-7844F0AB513B-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
2023-11-27 12:37:07,831 [IPC Server handler 61 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
2023-11-27 12:37:09,300 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:46256 / 172.20.0.7:46256
2023-11-27 12:37:09,460 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:39472 / 172.20.0.7:39472
2023-11-27 12:37:09,567 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:33136 / 172.20.0.2:33136
2023-11-27 12:37:09,662 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:38320 / 172.20.0.9:38320
2023-11-27 12:37:15,031 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:41168 / 172.20.0.9:41168
2023-11-27 12:37:36,970 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:34211 / 172.20.0.5:34211
2023-11-27 12:37:40,135 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:35001 / 172.20.0.5:35001
2023-11-27 12:37:41,923 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-11-27 12:37:45,101 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:49518 / 172.20.0.2:49518
2023-11-27 12:37:45,103 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:60358 / 172.20.0.7:60358
2023-11-27 12:37:45,123 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:41608 / 172.20.0.9:41608
2023-11-27 12:37:51,743 [IPC Server handler 61 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-11-27 12:37:52,642 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:58204 / 172.20.0.2:58204
2023-11-27 12:38:04,448 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:43825 / 172.20.0.5:43825
2023-11-27 12:38:10,154 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:34457 / 172.20.0.5:34457
2023-11-27 12:38:11,812 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-11-27 12:38:15,115 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:59918 / 172.20.0.7:59918
2023-11-27 12:38:15,139 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:37504 / 172.20.0.2:37504
2023-11-27 12:38:15,145 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:58878 / 172.20.0.9:58878
2023-11-27 12:38:22,373 [IPC Server handler 86 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:23,337 [IPC Server handler 22 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:23,424 [IPC Server handler 57 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:23,490 [IPC Server handler 61 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:25,748 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:32,921 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:33,884 [IPC Server handler 35 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:37,764 [IPC Server handler 35 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:38,700 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:42,823 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:43,767 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:45,091 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:43896 / 172.20.0.2:43896
2023-11-27 12:38:45,095 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:55910 / 172.20.0.7:55910
2023-11-27 12:38:45,096 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:40644 / 172.20.0.9:40644
2023-11-27 12:38:48,423 [IPC Server handler 57 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:48,572 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:50,095 [IPC Server handler 51 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-11-27 12:38:56,233 [IPC Server handler 22 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:56,304 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:56,365 [IPC Server handler 96 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:56,397 [IPC Server handler 91 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:38:58,673 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:05,857 [IPC Server handler 51 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:06,790 [IPC Server handler 53 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:10,690 [IPC Server handler 53 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:11,633 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:15,114 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:48284 / 172.20.0.9:48284
2023-11-27 12:39:15,117 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:46192 / 172.20.0.7:46192
2023-11-27 12:39:15,121 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:54578 / 172.20.0.2:54578
2023-11-27 12:39:15,698 [IPC Server handler 51 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:16,610 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:21,202 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:37061 / 172.20.0.5:37061
2023-11-27 12:39:21,226 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:21,326 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:32,049 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:40705 / 172.20.0.5:40705
2023-11-27 12:39:32,094 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-11-27 12:39:33,313 [IPC Server handler 60 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
2023-11-27 12:39:40,958 [IPC Server handler 22 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:41,007 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:41,077 [IPC Server handler 85 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:41,110 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:43,370 [IPC Server handler 87 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:45,105 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:49844 / 172.20.0.7:49844
2023-11-27 12:39:45,107 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:52218 / 172.20.0.9:52218
2023-11-27 12:39:45,110 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:54990 / 172.20.0.2:54990
2023-11-27 12:39:50,452 [IPC Server handler 35 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:51,368 [IPC Server handler 72 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:55,439 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:39:56,378 [IPC Server handler 86 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:00,412 [IPC Server handler 61 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:01,371 [IPC Server handler 21 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:05,913 [IPC Server handler 17 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:06,033 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:07,474 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-11-27 12:40:13,665 [IPC Server handler 51 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:13,718 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:13,787 [IPC Server handler 17 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:13,827 [IPC Server handler 85 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:15,099 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:45366 / 172.20.0.9:45366
2023-11-27 12:40:15,100 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:46428 / 172.20.0.2:46428
2023-11-27 12:40:15,105 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:35346 / 172.20.0.7:35346
2023-11-27 12:40:16,018 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:23,231 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:24,164 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:28,075 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:28,982 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:33,061 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:34,000 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:38,591 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:40439 / 172.20.0.5:40439
2023-11-27 12:40:38,626 [IPC Server handler 51 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:38,745 [IPC Server handler 17 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:40:45,102 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:43356 / 172.20.0.2:43356
2023-11-27 12:40:45,114 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:34806 / 172.20.0.7:34806
2023-11-27 12:40:45,117 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:34162 / 172.20.0.9:34162
2023-11-27 12:40:53,461 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:34695 / 172.20.0.5:34695
2023-11-27 12:40:54,852 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:33473 / 172.20.0.5:33473
2023-11-27 12:40:59,216 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-11-27 12:41:11,717 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
2023-11-27 12:41:11,898 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:13,008 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:13,120 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:13,177 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:15,108 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:52202 / 172.20.0.7:52202
2023-11-27 12:41:15,116 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:50544 / 172.20.0.9:50544
2023-11-27 12:41:15,118 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:45940 / 172.20.0.2:45940
2023-11-27 12:41:15,916 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:24,212 [IPC Server handler 54 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:25,330 [IPC Server handler 39 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:29,977 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:31,148 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:35,942 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:37,045 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:41,868 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:41,998 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:43,815 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-11-27 12:41:45,104 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:42732 / 172.20.0.2:42732
2023-11-27 12:41:45,108 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:37208 / 172.20.0.7:37208
2023-11-27 12:41:45,108 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:36366 / 172.20.0.9:36366
2023-11-27 12:41:49,756 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:49,825 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:49,910 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:49,953 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:41:52,614 [IPC Server handler 22 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:00,775 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:01,868 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:06,590 [IPC Server handler 78 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:07,740 [IPC Server handler 45 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:12,659 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:13,817 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:15,094 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:60204 / 172.20.0.9:60204
2023-11-27 12:42:15,097 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:34748 / 172.20.0.7:34748
2023-11-27 12:42:15,107 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:50042 / 172.20.0.2:50042
2023-11-27 12:42:19,430 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:34403 / 172.20.0.5:34403
2023-11-27 12:42:19,455 [IPC Server handler 35 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:19,578 [IPC Server handler 78 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:31,275 [IPC Server handler 41 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-11-27 12:42:40,262 [IPC Server handler 97 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:40,327 [IPC Server handler 36 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:40,388 [IPC Server handler 98 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:40,433 [IPC Server handler 57 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:43,119 [IPC Server handler 39 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:45,100 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:53314 / 172.20.0.9:53314
2023-11-27 12:42:45,106 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:58962 / 172.20.0.2:58962
2023-11-27 12:42:45,107 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:53758 / 172.20.0.7:53758
2023-11-27 12:42:51,324 [IPC Server handler 36 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:52,463 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:57,208 [IPC Server handler 37 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:42:58,365 [IPC Server handler 76 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:02,209 [IPC Server handler 52 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:03,336 [IPC Server handler 76 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:07,925 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:08,101 [IPC Server handler 39 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:09,555 [IPC Server handler 51 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.11
2023-11-27 12:43:15,085 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:35310 / 172.20.0.7:35310
2023-11-27 12:43:15,089 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:54640 / 172.20.0.2:54640
2023-11-27 12:43:15,092 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:39386 / 172.20.0.9:39386
2023-11-27 12:43:15,805 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:15,876 [IPC Server handler 54 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:15,926 [IPC Server handler 39 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:15,961 [IPC Server handler 28 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:18,697 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:26,788 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:27,979 [IPC Server handler 52 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:32,669 [IPC Server handler 85 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:33,884 [IPC Server handler 39 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:38,651 [IPC Server handler 85 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:39,752 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:44,573 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_om_1.ozonesecure-mr_default:44309 / 172.20.0.5:44309
2023-11-27 12:43:44,600 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:44,732 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.20.0.10
2023-11-27 12:43:45,088 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_3.ozonesecure-mr_default:58604 / 172.20.0.7:58604
2023-11-27 12:43:45,091 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_2.ozonesecure-mr_default:43668 / 172.20.0.2:43668
2023-11-27 12:43:45,094 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure-mr_datanode_1.ozonesecure-mr_default:36710 / 172.20.0.9:36710
