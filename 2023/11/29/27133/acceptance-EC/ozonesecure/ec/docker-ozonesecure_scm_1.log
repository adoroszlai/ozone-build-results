No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2023-11-29 08:35:06,889 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = scm/172.19.0.6
STARTUP_MSG:   args = [--init]
STARTUP_MSG:   version = 1.4.0-SNAPSHOT
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.6.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/java-uuid-generator-4.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-3.0.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-3.0.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/picocli-4.7.5.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.16.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-dropwizard3-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-3.0.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-3.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.58.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-3.0.0.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
STARTUP_MSG:   build = https://github.com/apache/ozone/799b20a08bf2e9746db2984da0cfcceabccce228 ; compiled by 'runner' on 2023-11-29T08:04Z
STARTUP_MSG:   java = 11.0.19
STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.buffer.size=2MB, dfs.container.ratis.segment.preallocated.size=4MB, dfs.container.ratis.segment.size=64MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.command.worker.interval=2s, hdds.datanode.block.delete.max.lock.wait.timeout=100ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.max.lock.holding.time=1s, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=19864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.close.threads.max=3, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.file.size=100B, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/dn.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/dn@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.queue.limit=4096, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=100MB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.log.appender.wait-time.min=1ms, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/scm.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=5s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=5s, hdds.scm.replication.under.replicated.interval=5s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=5, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=30s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.expired.certificate.check.interval=P1D, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.grpc.write.timeout=30s, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/om.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=true, ozone.om.multitenancy.ranger.sync.interval=30s, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.open.mpu.cleanup.service.interval=24h, ozone.om.open.mpu.cleanup.service.timeout=300s, ozone.om.open.mpu.expire.threshold=30d, ozone.om.open.mpu.parts.cleanup.limit.per.task=1000, ozone.om.ranger.https-address=https://ranger:6182, ozone.om.ranger.https.admin.api.passwd=Passwd1, ozone.om.ranger.https.admin.api.user=admin, ozone.om.ranger.service=cm_ozone, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.checkpoint.dir.creation.poll.timeout=20s, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.tenant.dev.skip.ranger=true, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=6000, ozone.recon.address=recon:9891, ozone.recon.administrators=testuser2, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3.administrators=testuser,s3g, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/s3g.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.list-keys.shallow.enabled=true, ozone.s3g.secret.http.auth.type=kerberos, ozone.s3g.secret.http.enabled=true, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=1, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=45s, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.raft.server.log.appender.wait-time.min=0ms, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=scm:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=30s, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
************************************************************/
2023-11-29 08:35:07,073 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
2023-11-29 08:35:07,743 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-11-29 08:35:08,989 [main] INFO reflections.Reflections: Reflections took 865 ms to scan 3 urls, producing 134 keys and 291 values 
2023-11-29 08:35:10,353 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
2023-11-29 08:35:10,499 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2023-11-29 08:35:10,775 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
2023-11-29 08:35:16,075 [main] INFO client.SCMCertificateClient: Certificate serial ID set to null
2023-11-29 08:35:16,075 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
2023-11-29 08:35:16,076 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
2023-11-29 08:35:16,078 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
2023-11-29 08:35:20,031 [main] INFO client.SCMCertificateClient: Init response: GETCERT
2023-11-29 08:35:21,802 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.19.0.6,host:scm
2023-11-29 08:35:21,831 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
2023-11-29 08:35:21,862 [main] ERROR utils.SelfSignedCertificate: Invalid domain scm
2023-11-29 08:35:22,569 [main] INFO utils.SelfSignedCertificate: Certificate 1 is issued by CN=scm@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=1 to CN=scm@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=1, valid from Wed Nov 29 08:35:21 UTC 2023 to Sat Jan 06 08:35:21 UTC 2029
2023-11-29 08:35:22,613 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/ca/certs/certificate.crt
2023-11-29 08:35:22,618 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkYWEyNjNlYzgtMDQ0Yy00N2I1LWE5YjQtZWJlYmZhMTYy
YjYwMTEwLwYDVQQKDChDSUQtNWE4YzVkNWYtMDg2My00YmNhLThlMzMtNmUwNTM3
MGUyOTk4MQowCAYDVQQFEwExMB4XDTIzMTEyOTA4MzUyMVoXDTI5MDEwNjA4MzUy
MVowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJGFhMjYzZWM4LTA0NGMt
NDdiNS1hOWI0LWViZWJmYTE2MmI2MDExMC8GA1UECgwoQ0lELTVhOGM1ZDVmLTA4
NjMtNGJjYS04ZTMzLTZlMDUzNzBlMjk5ODEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAJaP5RmCkbSVMD4FoQWbKcUQwZBfKPoW6Cle
vWlMNYuXJy0U36691VU4p8KcTQP5irftm0tGR2j5Un80l85GgbSmF3icsZHvSNj/
9ltMjFun0dNcTIFSwQ27PuJQWN2TEKFs9JzV+uFOHCo06h21d5Of4ft+hPLxK/s7
AJk+bYMowB0oWZv+pnwt/fdcNqoHcRL/O0Gw9NpnpIPJ0j/0iQkSiS6Bt372eAsT
ZvkUilyLuKMjHNDJHkFG1FRMHb/JQkJjwMO72W4ansv0CHWvbcQ09OF78kdAOk98
aBf/NT8ZlQef6seeCpFegwXzcbaIWZo46FGAdQziKjsRsxDBlKkCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBMA
BjANBgkqhkiG9w0BAQsFAAOCAQEAPxItDHCUtPMgxNsdomR687XCi2NbIyyel25F
bhU3qNLBZdjW1xB+ZTPM5Ahhy+rLvKWnKwP+cyZJzJiH1SXeQS18kH1Z08L5d6sr
T/+/flaUy0GhCx0Tz91vALaEXkyaPqU/ant6vMJELn+FRtSB0ns4WSiadBWajKFt
58lHKALdzvOoDDdnDdLBMdozAGJVZ1NzVThO0xGcODoWqWml01JVkgtnBL34QAFa
Qa4t8GQMxMZTTdTeLpHUB1LxXVAOzgyeB8SdyW7GrULn6fo+xLsYVWHCmvK+A71+
HB+JTxcJJY0m4K35kzuYjb5ZmHJI5zO8dQUBRuVXfemfcXeGng==
-----END CERTIFICATE-----

2023-11-29 08:35:22,749 [main] INFO client.SCMCertificateClient: Creating csr for SCM->hostName:scm,scmId:aa263ec8-044c-47b5-a9b4-ebebfa162b60,clusterId:CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,subject:scm-sub@scm
2023-11-29 08:35:22,767 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.19.0.6,host:scm
2023-11-29 08:35:22,768 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
2023-11-29 08:35:22,769 [main] ERROR utils.CertificateSignRequest: Invalid domain scm
2023-11-29 08:35:22,975 [main] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.19, 2.5.29.15, 2.5.29.17
2023-11-29 08:35:22,976 [main] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-29 08:35:23,053 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/CA-1.crt
2023-11-29 08:35:23,053 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkYWEyNjNlYzgtMDQ0Yy00N2I1LWE5YjQtZWJlYmZhMTYy
YjYwMTEwLwYDVQQKDChDSUQtNWE4YzVkNWYtMDg2My00YmNhLThlMzMtNmUwNTM3
MGUyOTk4MQowCAYDVQQFEwExMB4XDTIzMTEyOTA4MzUyMVoXDTI5MDEwNjA4MzUy
MVowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJGFhMjYzZWM4LTA0NGMt
NDdiNS1hOWI0LWViZWJmYTE2MmI2MDExMC8GA1UECgwoQ0lELTVhOGM1ZDVmLTA4
NjMtNGJjYS04ZTMzLTZlMDUzNzBlMjk5ODEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAJaP5RmCkbSVMD4FoQWbKcUQwZBfKPoW6Cle
vWlMNYuXJy0U36691VU4p8KcTQP5irftm0tGR2j5Un80l85GgbSmF3icsZHvSNj/
9ltMjFun0dNcTIFSwQ27PuJQWN2TEKFs9JzV+uFOHCo06h21d5Of4ft+hPLxK/s7
AJk+bYMowB0oWZv+pnwt/fdcNqoHcRL/O0Gw9NpnpIPJ0j/0iQkSiS6Bt372eAsT
ZvkUilyLuKMjHNDJHkFG1FRMHb/JQkJjwMO72W4ansv0CHWvbcQ09OF78kdAOk98
aBf/NT8ZlQef6seeCpFegwXzcbaIWZo46FGAdQziKjsRsxDBlKkCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBMA
BjANBgkqhkiG9w0BAQsFAAOCAQEAPxItDHCUtPMgxNsdomR687XCi2NbIyyel25F
bhU3qNLBZdjW1xB+ZTPM5Ahhy+rLvKWnKwP+cyZJzJiH1SXeQS18kH1Z08L5d6sr
T/+/flaUy0GhCx0Tz91vALaEXkyaPqU/ant6vMJELn+FRtSB0ns4WSiadBWajKFt
58lHKALdzvOoDDdnDdLBMdozAGJVZ1NzVThO0xGcODoWqWml01JVkgtnBL34QAFa
Qa4t8GQMxMZTTdTeLpHUB1LxXVAOzgyeB8SdyW7GrULn6fo+xLsYVWHCmvK+A71+
HB+JTxcJJY0m4K35kzuYjb5ZmHJI5zO8dQUBRuVXfemfcXeGng==
-----END CERTIFICATE-----

2023-11-29 08:35:23,077 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/2.crt
2023-11-29 08:35:23,081 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDtTCCAp2gAwIBAgIBAjANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkYWEyNjNlYzgtMDQ0Yy00N2I1LWE5YjQtZWJlYmZhMTYy
YjYwMTEwLwYDVQQKDChDSUQtNWE4YzVkNWYtMDg2My00YmNhLThlMzMtNmUwNTM3
MGUyOTk4MQowCAYDVQQFEwExMB4XDTIzMTEyOTA4MzUyMloXDTI5MDEwNjA4MzUy
MlowgYQxFDASBgNVBAMMC3NjbS1zdWJAc2NtMS0wKwYDVQQLDCRhYTI2M2VjOC0w
NDRjLTQ3YjUtYTliNC1lYmViZmExNjJiNjAxMTAvBgNVBAoMKENJRC01YThjNWQ1
Zi0wODYzLTRiY2EtOGUzMy02ZTA1MzcwZTI5OTgxCjAIBgNVBAUTATIwggEiMA0G
CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDDzYfLgz7hRfoc92n6H3qCo47Pk/yU
W8Q+jX/wywu9lDkyd9ou1vBGqKvIROCby5h1cBg7lABWK3sIl47AbKApD1Prqivi
8dONtbDbQrCInXetuBHYTHtjl/oKJKrWB2+Yd5dpcO0ezmpgbDXXiyic64tm+5Xu
NkxkBt8zFglJu4ro4F2eKscsq5tULl5QIza3VE4/Ic5tgTPn/NFxtSSQsST/z6zb
8bLxMIHKdC+D1LT/A70pYpq3kR58bCDqUCRrARRfs1IpGi8BMFILXjazdpEOYUAI
auV1OQA8qkhqgv7e8qoexDaWSBe5UR7Egtm5yioJUmkrtKcg7KrA9hYVAgMBAAGj
NDAyMA8GA1UdEQQIMAaHBKwTAAYwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8E
BAMCAb4wDQYJKoZIhvcNAQELBQADggEBACql8u36XTA+RsSbXj/y8FYECD20GNAv
ybw4RAOYvOLZbWcYBOU+x0n52Kd33hY8gd05eQa+xnFwP7jyMpy74cajRicPr2iR
Q0Ksib3PVSrj77pUrxDD6a53WxjggKVje3/1xg+zglLPetijXhGQCKSF3GaBkzft
EBTIQTSFSGAFZzl/06oLj3GxjqG/e5tQTHm/zqvpLSbPdtArKYGTf556g8V/Zw8m
AFAbBoWANd/6epwaskuYv5Go/kY0Muypmsu1jRp2BHMvsKvyJgLp98OvaHmaUJBi
hh0czQV2+QFofgduITmBfLit2mSOGftXlz9LDjE+BIm56Kn0exe9RTc=
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkYWEyNjNlYzgtMDQ0Yy00N2I1LWE5YjQtZWJlYmZhMTYy
YjYwMTEwLwYDVQQKDChDSUQtNWE4YzVkNWYtMDg2My00YmNhLThlMzMtNmUwNTM3
MGUyOTk4MQowCAYDVQQFEwExMB4XDTIzMTEyOTA4MzUyMVoXDTI5MDEwNjA4MzUy
MVowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJGFhMjYzZWM4LTA0NGMt
NDdiNS1hOWI0LWViZWJmYTE2MmI2MDExMC8GA1UECgwoQ0lELTVhOGM1ZDVmLTA4
NjMtNGJjYS04ZTMzLTZlMDUzNzBlMjk5ODEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAJaP5RmCkbSVMD4FoQWbKcUQwZBfKPoW6Cle
vWlMNYuXJy0U36691VU4p8KcTQP5irftm0tGR2j5Un80l85GgbSmF3icsZHvSNj/
9ltMjFun0dNcTIFSwQ27PuJQWN2TEKFs9JzV+uFOHCo06h21d5Of4ft+hPLxK/s7
AJk+bYMowB0oWZv+pnwt/fdcNqoHcRL/O0Gw9NpnpIPJ0j/0iQkSiS6Bt372eAsT
ZvkUilyLuKMjHNDJHkFG1FRMHb/JQkJjwMO72W4ansv0CHWvbcQ09OF78kdAOk98
aBf/NT8ZlQef6seeCpFegwXzcbaIWZo46FGAdQziKjsRsxDBlKkCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBMA
BjANBgkqhkiG9w0BAQsFAAOCAQEAPxItDHCUtPMgxNsdomR687XCi2NbIyyel25F
bhU3qNLBZdjW1xB+ZTPM5Ahhy+rLvKWnKwP+cyZJzJiH1SXeQS18kH1Z08L5d6sr
T/+/flaUy0GhCx0Tz91vALaEXkyaPqU/ant6vMJELn+FRtSB0ns4WSiadBWajKFt
58lHKALdzvOoDDdnDdLBMdozAGJVZ1NzVThO0xGcODoWqWml01JVkgtnBL34QAFa
Qa4t8GQMxMZTTdTeLpHUB1LxXVAOzgyeB8SdyW7GrULn6fo+xLsYVWHCmvK+A71+
HB+JTxcJJY0m4K35kzuYjb5ZmHJI5zO8dQUBRuVXfemfcXeGng==
-----END CERTIFICATE-----

2023-11-29 08:35:23,084 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/scm/sub-ca/certs/certificate.crt
2023-11-29 08:35:23,085 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDtTCCAp2gAwIBAgIBAjANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkYWEyNjNlYzgtMDQ0Yy00N2I1LWE5YjQtZWJlYmZhMTYy
YjYwMTEwLwYDVQQKDChDSUQtNWE4YzVkNWYtMDg2My00YmNhLThlMzMtNmUwNTM3
MGUyOTk4MQowCAYDVQQFEwExMB4XDTIzMTEyOTA4MzUyMloXDTI5MDEwNjA4MzUy
MlowgYQxFDASBgNVBAMMC3NjbS1zdWJAc2NtMS0wKwYDVQQLDCRhYTI2M2VjOC0w
NDRjLTQ3YjUtYTliNC1lYmViZmExNjJiNjAxMTAvBgNVBAoMKENJRC01YThjNWQ1
Zi0wODYzLTRiY2EtOGUzMy02ZTA1MzcwZTI5OTgxCjAIBgNVBAUTATIwggEiMA0G
CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDDzYfLgz7hRfoc92n6H3qCo47Pk/yU
W8Q+jX/wywu9lDkyd9ou1vBGqKvIROCby5h1cBg7lABWK3sIl47AbKApD1Prqivi
8dONtbDbQrCInXetuBHYTHtjl/oKJKrWB2+Yd5dpcO0ezmpgbDXXiyic64tm+5Xu
NkxkBt8zFglJu4ro4F2eKscsq5tULl5QIza3VE4/Ic5tgTPn/NFxtSSQsST/z6zb
8bLxMIHKdC+D1LT/A70pYpq3kR58bCDqUCRrARRfs1IpGi8BMFILXjazdpEOYUAI
auV1OQA8qkhqgv7e8qoexDaWSBe5UR7Egtm5yioJUmkrtKcg7KrA9hYVAgMBAAGj
NDAyMA8GA1UdEQQIMAaHBKwTAAYwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8E
BAMCAb4wDQYJKoZIhvcNAQELBQADggEBACql8u36XTA+RsSbXj/y8FYECD20GNAv
ybw4RAOYvOLZbWcYBOU+x0n52Kd33hY8gd05eQa+xnFwP7jyMpy74cajRicPr2iR
Q0Ksib3PVSrj77pUrxDD6a53WxjggKVje3/1xg+zglLPetijXhGQCKSF3GaBkzft
EBTIQTSFSGAFZzl/06oLj3GxjqG/e5tQTHm/zqvpLSbPdtArKYGTf556g8V/Zw8m
AFAbBoWANd/6epwaskuYv5Go/kY0Muypmsu1jRp2BHMvsKvyJgLp98OvaHmaUJBi
hh0czQV2+QFofgduITmBfLit2mSOGftXlz9LDjE+BIm56Kn0exe9RTc=
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkYWEyNjNlYzgtMDQ0Yy00N2I1LWE5YjQtZWJlYmZhMTYy
YjYwMTEwLwYDVQQKDChDSUQtNWE4YzVkNWYtMDg2My00YmNhLThlMzMtNmUwNTM3
MGUyOTk4MQowCAYDVQQFEwExMB4XDTIzMTEyOTA4MzUyMVoXDTI5MDEwNjA4MzUy
MVowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJGFhMjYzZWM4LTA0NGMt
NDdiNS1hOWI0LWViZWJmYTE2MmI2MDExMC8GA1UECgwoQ0lELTVhOGM1ZDVmLTA4
NjMtNGJjYS04ZTMzLTZlMDUzNzBlMjk5ODEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAJaP5RmCkbSVMD4FoQWbKcUQwZBfKPoW6Cle
vWlMNYuXJy0U36691VU4p8KcTQP5irftm0tGR2j5Un80l85GgbSmF3icsZHvSNj/
9ltMjFun0dNcTIFSwQ27PuJQWN2TEKFs9JzV+uFOHCo06h21d5Of4ft+hPLxK/s7
AJk+bYMowB0oWZv+pnwt/fdcNqoHcRL/O0Gw9NpnpIPJ0j/0iQkSiS6Bt372eAsT
ZvkUilyLuKMjHNDJHkFG1FRMHb/JQkJjwMO72W4ansv0CHWvbcQ09OF78kdAOk98
aBf/NT8ZlQef6seeCpFegwXzcbaIWZo46FGAdQziKjsRsxDBlKkCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBMA
BjANBgkqhkiG9w0BAQsFAAOCAQEAPxItDHCUtPMgxNsdomR687XCi2NbIyyel25F
bhU3qNLBZdjW1xB+ZTPM5Ahhy+rLvKWnKwP+cyZJzJiH1SXeQS18kH1Z08L5d6sr
T/+/flaUy0GhCx0Tz91vALaEXkyaPqU/ant6vMJELn+FRtSB0ns4WSiadBWajKFt
58lHKALdzvOoDDdnDdLBMdozAGJVZ1NzVThO0xGcODoWqWml01JVkgtnBL34QAFa
Qa4t8GQMxMZTTdTeLpHUB1LxXVAOzgyeB8SdyW7GrULn6fo+xLsYVWHCmvK+A71+
HB+JTxcJJY0m4K35kzuYjb5ZmHJI5zO8dQUBRuVXfemfcXeGng==
-----END CERTIFICATE-----

2023-11-29 08:35:23,085 [main] INFO client.SCMCertificateClient: Successfully stored SCM signed certificate.
2023-11-29 08:35:24,065 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
2023-11-29 08:35:24,940 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-11-29 08:35:24,949 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
2023-11-29 08:35:24,958 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-11-29 08:35:24,959 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
2023-11-29 08:35:24,960 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
2023-11-29 08:35:24,967 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2023-11-29 08:35:24,968 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
2023-11-29 08:35:24,985 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-29 08:35:24,998 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2023-11-29 08:35:25,002 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-11-29 08:35:25,062 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2023-11-29 08:35:25,087 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-11-29 08:35:25,087 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-11-29 08:35:26,041 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
2023-11-29 08:35:26,043 [main] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-11-29 08:35:26,043 [main] INFO server.RaftServerConfigKeys: raft.server.close.threshold = 60s (default)
2023-11-29 08:35:26,105 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-11-29 08:35:26,158 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-11-29 08:35:26,160 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
2023-11-29 08:35:26,168 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
2023-11-29 08:35:26,186 [main] INFO server.RaftServer: aa263ec8-044c-47b5-a9b4-ebebfa162b60: addNew group-6E05370E2998:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894] returns group-6E05370E2998:java.util.concurrent.CompletableFuture@28ce75ec[Not completed]
2023-11-29 08:35:26,240 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60: new RaftServerImpl for group-6E05370E2998:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894] with SCMStateMachine:uninitialized
2023-11-29 08:35:26,249 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-11-29 08:35:26,259 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
2023-11-29 08:35:26,259 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
2023-11-29 08:35:26,259 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
2023-11-29 08:35:26,260 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-11-29 08:35:26,261 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.member.majority-add = false (default)
2023-11-29 08:35:26,261 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-11-29 08:35:26,310 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: ConfigurationManager, init=-1: peers:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-11-29 08:35:26,337 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
2023-11-29 08:35:26,366 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
2023-11-29 08:35:26,383 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
2023-11-29 08:35:26,411 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100ms (default)
2023-11-29 08:35:26,417 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
2023-11-29 08:35:26,417 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.read-after-write-consistent.write-index-cache.expiry-time = 60s (default)
2023-11-29 08:35:26,462 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.dropwizard3.Dm3MetricRegistriesImpl
2023-11-29 08:35:26,929 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-11-29 08:35:26,931 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-11-29 08:35:26,931 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
2023-11-29 08:35:26,936 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
2023-11-29 08:35:26,936 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
2023-11-29 08:35:26,936 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
2023-11-29 08:35:26,938 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
2023-11-29 08:35:26,938 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
2023-11-29 08:35:26,938 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-11-29 08:35:26,967 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998 does not exist. Creating ...
2023-11-29 08:35:27,007 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998/in_use.lock acquired by nodename 14@scm
2023-11-29 08:35:27,022 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998 has been successfully formatted.
2023-11-29 08:35:27,043 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
2023-11-29 08:35:27,072 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
2023-11-29 08:35:27,078 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-29 08:35:27,080 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-11-29 08:35:27,086 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
2023-11-29 08:35:27,103 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-11-29 08:35:27,130 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
2023-11-29 08:35:27,133 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-11-29 08:35:27,133 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-29 08:35:27,135 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO util.AwaitToRun: Thread[aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-cacheEviction-AwaitToRun,5,main] started
2023-11-29 08:35:27,150 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998
2023-11-29 08:35:27,150 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2023-11-29 08:35:27,150 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
2023-11-29 08:35:27,152 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-11-29 08:35:27,153 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
2023-11-29 08:35:27,153 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
2023-11-29 08:35:27,155 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
2023-11-29 08:35:27,155 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-11-29 08:35:27,156 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-11-29 08:35:27,158 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
2023-11-29 08:35:27,159 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-29 08:35:27,291 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
2023-11-29 08:35:27,296 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
2023-11-29 08:35:27,297 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
2023-11-29 08:35:27,316 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO segmented.SegmentedRaftLogWorker: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-11-29 08:35:27,316 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO segmented.SegmentedRaftLogWorker: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-11-29 08:35:27,322 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: start as a follower, conf=-1: peers:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894]|listeners:[], old=null
2023-11-29 08:35:27,328 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-11-29 08:35:27,329 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO impl.RoleInfo: aa263ec8-044c-47b5-a9b4-ebebfa162b60: start aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState
2023-11-29 08:35:27,337 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
2023-11-29 08:35:27,337 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-11-29 08:35:27,372 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6E05370E2998,id=aa263ec8-044c-47b5-a9b4-ebebfa162b60
2023-11-29 08:35:27,374 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.trigger-when-stop.enabled = true (default)
2023-11-29 08:35:27,374 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-11-29 08:35:27,375 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
2023-11-29 08:35:27,375 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
2023-11-29 08:35:27,376 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
2023-11-29 08:35:27,391 [main] INFO server.RaftServer: aa263ec8-044c-47b5-a9b4-ebebfa162b60: start RPC server
2023-11-29 08:35:27,506 [main] INFO server.GrpcService: aa263ec8-044c-47b5-a9b4-ebebfa162b60: GrpcService started, listening on 9894
2023-11-29 08:35:27,511 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-aa263ec8-044c-47b5-a9b4-ebebfa162b60: Started
2023-11-29 08:35:32,461 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO impl.FollowerState: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5131960930ns, electionTimeout:5123ms
2023-11-29 08:35:32,461 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO impl.RoleInfo: aa263ec8-044c-47b5-a9b4-ebebfa162b60: shutdown aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState
2023-11-29 08:35:32,461 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-11-29 08:35:32,463 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
2023-11-29 08:35:32,463 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO impl.RoleInfo: aa263ec8-044c-47b5-a9b4-ebebfa162b60: start aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1
2023-11-29 08:35:32,466 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO impl.LeaderElection: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894]|listeners:[], old=null
2023-11-29 08:35:32,466 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO impl.LeaderElection: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
2023-11-29 08:35:32,469 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO impl.LeaderElection: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894]|listeners:[], old=null
2023-11-29 08:35:32,469 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO impl.LeaderElection: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1 ELECTION round 0: result PASSED (term=1)
2023-11-29 08:35:32,469 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO impl.RoleInfo: aa263ec8-044c-47b5-a9b4-ebebfa162b60: shutdown aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1
2023-11-29 08:35:32,469 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-11-29 08:35:32,473 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
2023-11-29 08:35:32,476 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-11-29 08:35:32,476 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
2023-11-29 08:35:32,478 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
2023-11-29 08:35:32,478 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
2023-11-29 08:35:32,479 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
2023-11-29 08:35:32,483 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.enabled = false (default)
2023-11-29 08:35:32,484 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.timeout.ratio = 0.9 (default)
2023-11-29 08:35:32,484 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-11-29 08:35:32,484 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-11-29 08:35:32,484 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-11-29 08:35:32,485 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO impl.RoleInfo: aa263ec8-044c-47b5-a9b4-ebebfa162b60: start aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderStateImpl
2023-11-29 08:35:32,485 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: set firstElectionSinceStartup to false for becomeLeader
2023-11-29 08:35:32,485 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: change Leader from null to aa263ec8-044c-47b5-a9b4-ebebfa162b60 at term 1 for becomeLeader, leader elected after 6149ms
2023-11-29 08:35:32,503 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker: Starting segment from index:0
2023-11-29 08:35:32,520 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: set configuration 0: peers:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894]|listeners:[], old=null
2023-11-29 08:35:32,552 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998/current/log_inprogress_0
2023-11-29 08:35:32,557 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO server.RaftServer$Division: leader is ready since appliedIndex == 0 >= startIndex == 0
2023-11-29 08:35:33,509 [main] INFO server.RaftServer: aa263ec8-044c-47b5-a9b4-ebebfa162b60: close
2023-11-29 08:35:33,509 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: shutdown
2023-11-29 08:35:33,510 [main] INFO server.GrpcService: aa263ec8-044c-47b5-a9b4-ebebfa162b60: shutdown server GrpcServerProtocolService now
2023-11-29 08:35:33,510 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-6E05370E2998,id=aa263ec8-044c-47b5-a9b4-ebebfa162b60
2023-11-29 08:35:33,510 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO impl.RoleInfo: aa263ec8-044c-47b5-a9b4-ebebfa162b60: shutdown aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderStateImpl
2023-11-29 08:35:33,514 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO impl.PendingRequests: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-PendingRequests: sendNotLeaderResponses
2023-11-29 08:35:33,516 [main] INFO server.GrpcService: aa263ec8-044c-47b5-a9b4-ebebfa162b60: shutdown server GrpcServerProtocolService successfully
2023-11-29 08:35:33,518 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO impl.StateMachineUpdater: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater: set stopIndex = 0
2023-11-29 08:35:33,519 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO impl.StateMachineUpdater: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater: Took a snapshot at index 0
2023-11-29 08:35:33,519 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO impl.StateMachineUpdater: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-11-29 08:35:33,522 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: applyIndex: 0
2023-11-29 08:35:33,523 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-cacheEviction-AwaitToRun] INFO util.AwaitToRun: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-cacheEviction-AwaitToRun-AwaitForSignal is interrupted
2023-11-29 08:35:33,555 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO segmented.SegmentedRaftLogWorker: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker close()
2023-11-29 08:35:33,556 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-aa263ec8-044c-47b5-a9b4-ebebfa162b60: Stopped
2023-11-29 08:35:33,557 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-11-29 08:35:33,559 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998; layoutVersion=7; scmId=aa263ec8-044c-47b5-a9b4-ebebfa162b60
2023-11-29 08:35:33,562 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down StorageContainerManager at scm/172.19.0.6
************************************************************/
No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2023-11-29 08:35:35,004 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = scm/172.19.0.6
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 1.4.0-SNAPSHOT
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.6.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/java-uuid-generator-4.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-3.0.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-3.0.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/picocli-4.7.5.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.16.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.53.v20231009.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.100.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-dropwizard3-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-3.0.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-3.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.58.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-3.0.0.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
STARTUP_MSG:   build = https://github.com/apache/ozone/799b20a08bf2e9746db2984da0cfcceabccce228 ; compiled by 'runner' on 2023-11-29T08:04Z
STARTUP_MSG:   java = 11.0.19
STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=true, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.buffer.size=2MB, dfs.container.ratis.segment.preallocated.size=4MB, dfs.container.ratis.segment.size=64MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60s, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=true, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.command.worker.interval=2s, hdds.datanode.block.delete.max.lock.wait.timeout=100ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.max.lock.holding.time=1s, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=19864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.close.threads.max=3, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.file.size=100B, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/dn.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/dn@EXAMPLE.COM, hdds.datanode.http.auth.type=kerberos, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.queue.limit=4096, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=100MB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.log.appender.wait-time.min=1ms, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/scm.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=kerberos, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=5s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=5s, hdds.scm.replication.under.replicated.interval=5s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=5, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=30s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.expired.certificate.check.interval=P1D, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneNativeAuthorizer, ozone.acl.enabled=true, ozone.administrators=testuser,recon,om, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.grpc.write.timeout=30s, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer, ozone.http.policy=HTTP_ONLY, ozone.httpfs.http.auth.kerberos.keytab=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.http.auth.kerberos.principal=HTTP/httpfs@EXAMPLE.COM, ozone.httpfs.http.auth.type=kerberos, ozone.httpfs.kerberos.keytab.file=/etc/security/keytabs/httpfs.keytab, ozone.httpfs.kerberos.principal=httpfs/httpfs@EXAMPLE.COM, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/om.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=kerberos, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=true, ozone.om.multitenancy.ranger.sync.interval=30s, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.open.mpu.cleanup.service.interval=24h, ozone.om.open.mpu.cleanup.service.timeout=300s, ozone.om.open.mpu.expire.threshold=30d, ozone.om.open.mpu.parts.cleanup.limit.per.task=1000, ozone.om.ranger.https-address=https://ranger:6182, ozone.om.ranger.https.admin.api.passwd=Passwd1, ozone.om.ranger.https.admin.api.user=admin, ozone.om.ranger.service=cm_ozone, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.checkpoint.dir.creation.poll.timeout=20s, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.tenant.dev.skip.ranger=true, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=6000, ozone.recon.address=recon:9891, ozone.recon.administrators=testuser2, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/recon.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/recon@EXAMPLE.COM, ozone.recon.http.auth.type=kerberos, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.kerberos.keytab.file=/etc/security/keytabs/recon.keytab, ozone.recon.kerberos.principal=recon/recon@EXAMPLE.COM, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=20s, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3.administrators=testuser,s3g, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/s3g.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=kerberos, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.list-keys.shallow.enabled=true, ozone.s3g.secret.http.auth.type=kerberos, ozone.s3g.secret.http.enabled=true, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=5s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=1, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=45s, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.raft.server.log.appender.wait-time.min=0ms, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=scm:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=30s, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
************************************************************/
2023-11-29 08:35:35,013 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
2023-11-29 08:35:35,057 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-11-29 08:35:35,179 [main] INFO reflections.Reflections: Reflections took 93 ms to scan 3 urls, producing 134 keys and 291 values 
2023-11-29 08:35:35,280 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
2023-11-29 08:35:35,288 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2023-11-29 08:35:35,501 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
2023-11-29 08:35:35,501 [main] INFO server.StorageContainerManager: SCM login successful.
2023-11-29 08:35:36,093 [main] INFO client.SCMCertificateClient: Certificate serial ID set to 2
2023-11-29 08:35:36,238 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=1
           Start Date: Wed Nov 29 08:35:22 UTC 2023
           Final Date: Sat Jan 06 08:35:22 UTC 2029
            SubjectDN: CN=scm-sub@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=2
           Public Key: RSA Public Key [ae:41:68:1b:07:d5:2b:a7:da:cd:63:b2:5f:7d:d2:7c:75:9b:92:67],[56:66:d1:a4]
        modulus: c3cd87cb833ee145fa1cf769fa1f7a82a38ecf93fc945bc43e8d7ff0cb0bbd94393277da2ed6f046a8abc844e09bcb987570183b9400562b7b08978ec06ca0290f53ebaa2be2f1d38db5b0db42b0889d77adb811d84c7b6397fa0a24aad6076f9877976970ed1ece6a606c35d78b289ceb8b66fb95ee364c6406df33160949bb8ae8e05d9e2ac72cab9b542e5e502336b7544e3f21ce6d8133e7fcd171b52490b124ffcfacdbf1b2f13081ca742f83d4b4ff03bd29629ab7911e7c6c20ea50246b01145fb352291a2f0130520b5e36b376910e6140086ae57539003caa486a82fedef2aa1ec436964817b9511ec482d9b9ca2a0952692bb4a720ecaac0f61615
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 2aa5f2edfa5d303e46c49b5e3ff2f05604083db4
                       18d02fc9bc38440398bce2d96d671804e53ec749
                       f9d8a777de163c81dd397906bec671703fb8f232
                       9cbbe1c6a346270faf68914342ac89bdcf552ae3
                       efba54af10c3e9ae775b18e080a5637b7ff5c60f
                       b38252cf7ad8a35e119008a485dc66819337ed10
                       14c841348548600567397fd3aa0b8f71b18ea1bf
                       7b9b504c79bfceabe92d26cf76d02b2981937f9e
                       7a83c57f670f2600501b06858035dffa7a9c1ab2
                       4b98bf91a8fe463432eca99acbb58d1a7604732f
                       b0abf22602e9f7c3af68799a509062861d1ccd05
                       76f901687e076e2139817cb8adda648e19fb5797
                       3f4b0e313e0489b9e8a9f47b17bd4537
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe
 from file: /data/metadata/scm/sub-ca/certs/certificate.crt.
2023-11-29 08:35:36,243 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=1
           Start Date: Wed Nov 29 08:35:21 UTC 2023
           Final Date: Sat Jan 06 08:35:21 UTC 2029
            SubjectDN: CN=scm@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=1
           Public Key: RSA Public Key [31:69:fa:e8:9d:a5:ba:51:c2:87:51:63:bf:8b:3d:41:e3:34:75:a9],[56:66:d1:a4]
        modulus: 968fe5198291b495303e05a1059b29c510c1905f28fa16e8295ebd694c358b97272d14dfaebdd55538a7c29c4d03f98ab7ed9b4b464768f9527f3497ce4681b4a617789cb191ef48d8fff65b4c8c5ba7d1d35c4c8152c10dbb3ee25058dd9310a16cf49cd5fae14e1c2a34ea1db577939fe1fb7e84f2f12bfb3b00993e6d8328c01d28599bfea67c2dfdf75c36aa077112ff3b41b0f4da67a483c9d23ff4890912892e81b77ef6780b1366f9148a5c8bb8a3231cd0c91e4146d4544c1dbfc9424263c0c3bbd96e1a9ecbf40875af6dc434f4e17bf247403a4f7c6817ff353f1995079feac79e0a915e8305f371b688599a38e85180750ce22a3b11b310c194a9
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 3f122d0c7094b4f320c4db1da2647af3b5c28b63
                       5b232c9e976e456e1537a8d2c165d8d6d7107e65
                       33cce40861cbeacbbca5a72b03fe732649cc9887
                       d525de412d7c907d59d3c2f977ab2b4fffbf7e56
                       94cb41a10b1d13cfdd6f00b6845e4c9a3ea53f6a
                       7b7abcc2442e7f8546d481d27b3859289a74159a
                       8ca16de7c9472802ddcef3a80c37670dd2c131da
                       3300625567537355384ed3119c383a16a969a5d3
                       5255920b6704bdf840015a41ae2df0640cc4c653
                       4dd4de2e91d40752f15d500ece0c9e07c49dc96e
                       c6ad42e7e9fa3ec4bb185561c29af2be03bd7e1c
                       1f894f1709258d26e0adf9933b988dbe59987248
                       e733bc75050146e5577de99f7177869e
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 

 from file: /data/metadata/scm/sub-ca/certs/CA-1.crt.
2023-11-29 08:35:36,247 [main] INFO client.SCMCertificateClient: Added certificate   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=1
           Start Date: Wed Nov 29 08:35:22 UTC 2023
           Final Date: Sat Jan 06 08:35:22 UTC 2029
            SubjectDN: CN=scm-sub@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=2
           Public Key: RSA Public Key [ae:41:68:1b:07:d5:2b:a7:da:cd:63:b2:5f:7d:d2:7c:75:9b:92:67],[56:66:d1:a4]
        modulus: c3cd87cb833ee145fa1cf769fa1f7a82a38ecf93fc945bc43e8d7ff0cb0bbd94393277da2ed6f046a8abc844e09bcb987570183b9400562b7b08978ec06ca0290f53ebaa2be2f1d38db5b0db42b0889d77adb811d84c7b6397fa0a24aad6076f9877976970ed1ece6a606c35d78b289ceb8b66fb95ee364c6406df33160949bb8ae8e05d9e2ac72cab9b542e5e502336b7544e3f21ce6d8133e7fcd171b52490b124ffcfacdbf1b2f13081ca742f83d4b4ff03bd29629ab7911e7c6c20ea50246b01145fb352291a2f0130520b5e36b376910e6140086ae57539003caa486a82fedef2aa1ec436964817b9511ec482d9b9ca2a0952692bb4a720ecaac0f61615
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 2aa5f2edfa5d303e46c49b5e3ff2f05604083db4
                       18d02fc9bc38440398bce2d96d671804e53ec749
                       f9d8a777de163c81dd397906bec671703fb8f232
                       9cbbe1c6a346270faf68914342ac89bdcf552ae3
                       efba54af10c3e9ae775b18e080a5637b7ff5c60f
                       b38252cf7ad8a35e119008a485dc66819337ed10
                       14c841348548600567397fd3aa0b8f71b18ea1bf
                       7b9b504c79bfceabe92d26cf76d02b2981937f9e
                       7a83c57f670f2600501b06858035dffa7a9c1ab2
                       4b98bf91a8fe463432eca99acbb58d1a7604732f
                       b0abf22602e9f7c3af68799a509062861d1ccd05
                       76f901687e076e2139817cb8adda648e19fb5797
                       3f4b0e313e0489b9e8a9f47b17bd4537
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe
 from file: /data/metadata/scm/sub-ca/certs/2.crt.
2023-11-29 08:35:36,247 [main] INFO client.SCMCertificateClient: CertificateRenewerService and root ca rotation polling is disabled for scm/sub-ca
2023-11-29 08:35:36,333 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-11-29 08:35:36,442 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-11-29 08:35:36,630 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
2023-11-29 08:35:36,632 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
2023-11-29 08:35:36,677 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.dropwizard3.Dm3MetricRegistriesImpl
2023-11-29 08:35:36,803 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:aa263ec8-044c-47b5-a9b4-ebebfa162b60
2023-11-29 08:35:36,822 [main] INFO ssl.ReloadingX509KeyManager: Key manager is loaded with certificate chain
2023-11-29 08:35:36,824 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=1
           Start Date: Wed Nov 29 08:35:22 UTC 2023
           Final Date: Sat Jan 06 08:35:22 UTC 2029
            SubjectDN: CN=scm-sub@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=2
           Public Key: RSA Public Key [ae:41:68:1b:07:d5:2b:a7:da:cd:63:b2:5f:7d:d2:7c:75:9b:92:67],[56:66:d1:a4]
        modulus: c3cd87cb833ee145fa1cf769fa1f7a82a38ecf93fc945bc43e8d7ff0cb0bbd94393277da2ed6f046a8abc844e09bcb987570183b9400562b7b08978ec06ca0290f53ebaa2be2f1d38db5b0db42b0889d77adb811d84c7b6397fa0a24aad6076f9877976970ed1ece6a606c35d78b289ceb8b66fb95ee364c6406df33160949bb8ae8e05d9e2ac72cab9b542e5e502336b7544e3f21ce6d8133e7fcd171b52490b124ffcfacdbf1b2f13081ca742f83d4b4ff03bd29629ab7911e7c6c20ea50246b01145fb352291a2f0130520b5e36b376910e6140086ae57539003caa486a82fedef2aa1ec436964817b9511ec482d9b9ca2a0952692bb4a720ecaac0f61615
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 2aa5f2edfa5d303e46c49b5e3ff2f05604083db4
                       18d02fc9bc38440398bce2d96d671804e53ec749
                       f9d8a777de163c81dd397906bec671703fb8f232
                       9cbbe1c6a346270faf68914342ac89bdcf552ae3
                       efba54af10c3e9ae775b18e080a5637b7ff5c60f
                       b38252cf7ad8a35e119008a485dc66819337ed10
                       14c841348548600567397fd3aa0b8f71b18ea1bf
                       7b9b504c79bfceabe92d26cf76d02b2981937f9e
                       7a83c57f670f2600501b06858035dffa7a9c1ab2
                       4b98bf91a8fe463432eca99acbb58d1a7604732f
                       b0abf22602e9f7c3af68799a509062861d1ccd05
                       76f901687e076e2139817cb8adda648e19fb5797
                       3f4b0e313e0489b9e8a9f47b17bd4537
       Extensions: 
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 

                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0xbe

2023-11-29 08:35:36,825 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=1
           Start Date: Wed Nov 29 08:35:21 UTC 2023
           Final Date: Sat Jan 06 08:35:21 UTC 2029
            SubjectDN: CN=scm@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=1
           Public Key: RSA Public Key [31:69:fa:e8:9d:a5:ba:51:c2:87:51:63:bf:8b:3d:41:e3:34:75:a9],[56:66:d1:a4]
        modulus: 968fe5198291b495303e05a1059b29c510c1905f28fa16e8295ebd694c358b97272d14dfaebdd55538a7c29c4d03f98ab7ed9b4b464768f9527f3497ce4681b4a617789cb191ef48d8fff65b4c8c5ba7d1d35c4c8152c10dbb3ee25058dd9310a16cf49cd5fae14e1c2a34ea1db577939fe1fb7e84f2f12bfb3b00993e6d8328c01d28599bfea67c2dfdf75c36aa077112ff3b41b0f4da67a483c9d23ff4890912892e81b77ef6780b1366f9148a5c8bb8a3231cd0c91e4146d4544c1dbfc9424263c0c3bbd96e1a9ecbf40875af6dc434f4e17bf247403a4f7c6817ff353f1995079feac79e0a915e8305f371b688599a38e85180750ce22a3b11b310c194a9
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 3f122d0c7094b4f320c4db1da2647af3b5c28b63
                       5b232c9e976e456e1537a8d2c165d8d6d7107e65
                       33cce40861cbeacbbca5a72b03fe732649cc9887
                       d525de412d7c907d59d3c2f977ab2b4fffbf7e56
                       94cb41a10b1d13cfdd6f00b6845e4c9a3ea53f6a
                       7b7abcc2442e7f8546d481d27b3859289a74159a
                       8ca16de7c9472802ddcef3a80c37670dd2c131da
                       3300625567537355384ed3119c383a16a969a5d3
                       5255920b6704bdf840015a41ae2df0640cc4c653
                       4dd4de2e91d40752f15d500ece0c9e07c49dc96e
                       c6ad42e7e9fa3ec4bb185561c29af2be03bd7e1c
                       1f894f1709258d26e0adf9933b988dbe59987248
                       e733bc75050146e5577de99f7177869e
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 


2023-11-29 08:35:36,828 [main] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-29 08:35:36,828 [main] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-29 08:35:36,828 [main] INFO ssl.ReloadingX509TrustManager: Trust manager is loaded with certificates
2023-11-29 08:35:36,830 [main] INFO ssl.ReloadingX509TrustManager:   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=1
           Start Date: Wed Nov 29 08:35:21 UTC 2023
           Final Date: Sat Jan 06 08:35:21 UTC 2029
            SubjectDN: CN=scm@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=1
           Public Key: RSA Public Key [31:69:fa:e8:9d:a5:ba:51:c2:87:51:63:bf:8b:3d:41:e3:34:75:a9],[56:66:d1:a4]
        modulus: 968fe5198291b495303e05a1059b29c510c1905f28fa16e8295ebd694c358b97272d14dfaebdd55538a7c29c4d03f98ab7ed9b4b464768f9527f3497ce4681b4a617789cb191ef48d8fff65b4c8c5ba7d1d35c4c8152c10dbb3ee25058dd9310a16cf49cd5fae14e1c2a34ea1db577939fe1fb7e84f2f12bfb3b00993e6d8328c01d28599bfea67c2dfdf75c36aa077112ff3b41b0f4da67a483c9d23ff4890912892e81b77ef6780b1366f9148a5c8bb8a3231cd0c91e4146d4544c1dbfc9424263c0c3bbd96e1a9ecbf40875af6dc434f4e17bf247403a4f7c6817ff353f1995079feac79e0a915e8305f371b688599a38e85180750ce22a3b11b310c194a9
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 3f122d0c7094b4f320c4db1da2647af3b5c28b63
                       5b232c9e976e456e1537a8d2c165d8d6d7107e65
                       33cce40861cbeacbbca5a72b03fe732649cc9887
                       d525de412d7c907d59d3c2f977ab2b4fffbf7e56
                       94cb41a10b1d13cfdd6f00b6845e4c9a3ea53f6a
                       7b7abcc2442e7f8546d481d27b3859289a74159a
                       8ca16de7c9472802ddcef3a80c37670dd2c131da
                       3300625567537355384ed3119c383a16a969a5d3
                       5255920b6704bdf840015a41ae2df0640cc4c653
                       4dd4de2e91d40752f15d500ece0c9e07c49dc96e
                       c6ad42e7e9fa3ec4bb185561c29af2be03bd7e1c
                       1f894f1709258d26e0adf9933b988dbe59987248
                       e733bc75050146e5577de99f7177869e
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [7] IMPLICIT 
        DER Octet String[4] 


2023-11-29 08:35:36,843 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-29 08:35:36,846 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-29 08:35:36,903 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
2023-11-29 08:35:36,912 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-11-29 08:35:36,915 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
2023-11-29 08:35:36,916 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-11-29 08:35:36,917 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
2023-11-29 08:35:36,917 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
2023-11-29 08:35:36,917 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2023-11-29 08:35:36,918 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
2023-11-29 08:35:36,920 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-29 08:35:36,923 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2023-11-29 08:35:36,929 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-11-29 08:35:36,937 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2023-11-29 08:35:36,939 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-11-29 08:35:36,940 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-11-29 08:35:37,205 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
2023-11-29 08:35:37,207 [main] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-11-29 08:35:37,207 [main] INFO server.RaftServerConfigKeys: raft.server.close.threshold = 60s (default)
2023-11-29 08:35:37,207 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-11-29 08:35:37,210 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-11-29 08:35:37,211 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
2023-11-29 08:35:37,211 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
2023-11-29 08:35:37,213 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServer: aa263ec8-044c-47b5-a9b4-ebebfa162b60: found a subdirectory /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998
2023-11-29 08:35:37,236 [main] INFO server.RaftServer: aa263ec8-044c-47b5-a9b4-ebebfa162b60: addNew group-6E05370E2998:[] returns group-6E05370E2998:java.util.concurrent.CompletableFuture@1fb30e5d[Not completed]
2023-11-29 08:35:37,257 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60: new RaftServerImpl for group-6E05370E2998:[] with SCMStateMachine:uninitialized
2023-11-29 08:35:37,258 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-11-29 08:35:37,259 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
2023-11-29 08:35:37,259 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
2023-11-29 08:35:37,259 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
2023-11-29 08:35:37,259 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-11-29 08:35:37,260 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.member.majority-add = false (default)
2023-11-29 08:35:37,260 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2023-11-29 08:35:37,266 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-11-29 08:35:37,272 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
2023-11-29 08:35:37,275 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
2023-11-29 08:35:37,278 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
2023-11-29 08:35:37,278 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100ms (default)
2023-11-29 08:35:37,282 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
2023-11-29 08:35:37,283 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.read-after-write-consistent.write-index-cache.expiry-time = 60s (default)
2023-11-29 08:35:37,379 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2023-11-29 08:35:37,382 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-11-29 08:35:37,382 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
2023-11-29 08:35:37,382 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
2023-11-29 08:35:37,383 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
2023-11-29 08:35:37,383 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
2023-11-29 08:35:37,385 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
2023-11-29 08:35:37,385 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
2023-11-29 08:35:37,385 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
2023-11-29 08:35:37,442 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
2023-11-29 08:35:37,493 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
2023-11-29 08:35:37,494 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
2023-11-29 08:35:37,502 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
2023-11-29 08:35:37,505 [main] INFO ha.SequenceIdGenerator: upgrade CertificateId to 2
2023-11-29 08:35:37,508 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
2023-11-29 08:35:37,585 [main] WARN server.ServerUtils: ozone.scm.stale.node.interval value = 30000 is smaller than min = 90000 based on the key value of hdds.heartbeat.interval, reset to the min value 90000.
2023-11-29 08:35:37,585 [main] WARN server.ServerUtils: ozone.scm.stale.node.interval value = 30000 is smaller than min = 90000 based on the key value of hdds.heartbeat.interval, reset to the min value 90000.
2023-11-29 08:35:37,585 [main] WARN server.ServerUtils: ozone.scm.dead.node.interval value = 45000 is smaller than min = 180000 based on the key value of ozone.scm.stale.node.interval, reset to the min value 180000.
2023-11-29 08:35:37,604 [main] INFO node.SCMNodeManager: Entering startup safe mode.
2023-11-29 08:35:37,626 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
2023-11-29 08:35:37,630 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2023-11-29 08:35:37,641 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
2023-11-29 08:35:37,665 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
2023-11-29 08:35:37,665 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2023-11-29 08:35:37,672 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
2023-11-29 08:35:37,672 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
2023-11-29 08:35:37,675 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
2023-11-29 08:35:37,676 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
2023-11-29 08:35:37,683 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
2023-11-29 08:35:37,683 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
2023-11-29 08:35:37,718 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
2023-11-29 08:35:37,719 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
2023-11-29 08:35:37,744 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
2023-11-29 08:35:37,839 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
2023-11-29 08:35:37,840 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
2023-11-29 08:35:37,843 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:35:37,858 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
2023-11-29 08:35:37,863 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:37,865 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
2023-11-29 08:35:38,044 [main] INFO security.SecretKeyManagerService: Scheduling rotation checker with interval PT10M
2023-11-29 08:35:38,045 [main] INFO ha.SCMServiceManager: Registering service SecretKeyManagerService.
2023-11-29 08:35:38,067 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
2023-11-29 08:35:38,070 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
2023-11-29 08:35:38,071 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 2 on primary SCM
2023-11-29 08:35:38,082 [main] INFO server.SCMCertStore: Scm certificate 2 for CN=scm-sub@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=2 is stored
2023-11-29 08:35:38,082 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
2023-11-29 08:35:38,084 [main] INFO server.SCMCertStore: Scm certificate 1 for CN=scm@scm,OU=aa263ec8-044c-47b5-a9b4-ebebfa162b60,O=CID-5a8c5d5f-0863-4bca-8e33-6e05370e2998,SERIALNUMBER=1 is stored
2023-11-29 08:35:38,089 [main] INFO ha.SequenceIdGenerator: upgrade CertificateId to 2
2023-11-29 08:35:38,109 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-11-29 08:35:38,143 [main] INFO ipc.Server: Listener at 0.0.0.0:9961
2023-11-29 08:35:38,146 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
2023-11-29 08:35:38,209 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [testuser, recon, om, scm]
2023-11-29 08:35:38,566 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2023-11-29 08:35:38,573 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-11-29 08:35:38,573 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
2023-11-29 08:35:38,573 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
2023-11-29 08:35:38,608 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2023-11-29 08:35:38,613 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-11-29 08:35:38,613 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
2023-11-29 08:35:38,614 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
2023-11-29 08:35:38,644 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
2023-11-29 08:35:38,654 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-11-29 08:35:38,655 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
2023-11-29 08:35:38,655 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
2023-11-29 08:35:38,822 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
2023-11-29 08:35:38,825 [main] INFO server.StorageContainerManager: 
Container Balancer status:
Key                            Value
Running                        false
Container Balancer Configuration values:
Key                                                Value
Threshold                                          10
Max Datanodes to Involve per Iteration(percent)    20
Max Size to Move per Iteration                     500GB
Max Size Entering Target per Iteration             26GB
Max Size Leaving Source per Iteration              26GB
Number of Iterations                               10
Time Limit for Single Container's Movement         65min
Time Limit for Single Container's Replication      50min
Interval between each Iteration                    70min
Whether to Enable Network Topology                 false
Whether to Trigger Refresh Datanode Usage Info     false
Container IDs to Exclude from Balancing            None
Datanodes Specified to be Balanced                 None
Datanodes Excluded from Balancing                  None

2023-11-29 08:35:38,825 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
2023-11-29 08:35:38,835 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
2023-11-29 08:35:38,842 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
2023-11-29 08:35:38,848 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
2023-11-29 08:35:38,849 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
2023-11-29 08:35:38,849 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2023-11-29 08:35:38,860 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998/in_use.lock acquired by nodename 8@scm
2023-11-29 08:35:38,864 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=aa263ec8-044c-47b5-a9b4-ebebfa162b60} from /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998/current/raft-meta
2023-11-29 08:35:38,887 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: set configuration 0: peers:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894]|listeners:[], old=null
2023-11-29 08:35:38,895 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
2023-11-29 08:35:38,904 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
2023-11-29 08:35:38,904 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-29 08:35:38,905 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-11-29 08:35:38,906 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
2023-11-29 08:35:38,910 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-11-29 08:35:38,915 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
2023-11-29 08:35:38,915 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-11-29 08:35:38,915 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-29 08:35:38,919 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO util.AwaitToRun: Thread[aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-cacheEviction-AwaitToRun,5,main] started
2023-11-29 08:35:38,923 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998
2023-11-29 08:35:38,924 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2023-11-29 08:35:38,924 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
2023-11-29 08:35:38,926 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2023-11-29 08:35:38,927 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
2023-11-29 08:35:38,927 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
2023-11-29 08:35:38,928 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
2023-11-29 08:35:38,928 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-11-29 08:35:38,929 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-11-29 08:35:38,931 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
2023-11-29 08:35:38,931 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-11-29 08:35:38,933 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
2023-11-29 08:35:38,934 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
2023-11-29 08:35:38,934 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
2023-11-29 08:35:38,966 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: set configuration 0: peers:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894]|listeners:[], old=null
2023-11-29 08:35:38,967 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998/current/log_inprogress_0
2023-11-29 08:35:38,969 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO segmented.SegmentedRaftLogWorker: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-11-29 08:35:39,012 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: start as a follower, conf=0: peers:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894]|listeners:[], old=null
2023-11-29 08:35:39,012 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: changes role from      null to FOLLOWER at term 1 for startAsFollower
2023-11-29 08:35:39,013 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO impl.RoleInfo: aa263ec8-044c-47b5-a9b4-ebebfa162b60: start aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState
2023-11-29 08:35:39,014 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
2023-11-29 08:35:39,015 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-11-29 08:35:39,015 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6E05370E2998,id=aa263ec8-044c-47b5-a9b4-ebebfa162b60
2023-11-29 08:35:39,017 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.trigger-when-stop.enabled = true (default)
2023-11-29 08:35:39,017 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-11-29 08:35:39,017 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
2023-11-29 08:35:39,018 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
2023-11-29 08:35:39,018 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
2023-11-29 08:35:39,021 [main] INFO server.RaftServer: aa263ec8-044c-47b5-a9b4-ebebfa162b60: start RPC server
2023-11-29 08:35:39,051 [main] INFO server.GrpcService: aa263ec8-044c-47b5-a9b4-ebebfa162b60: GrpcService started, listening on 9894
2023-11-29 08:35:39,053 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-aa263ec8-044c-47b5-a9b4-ebebfa162b60: Started
2023-11-29 08:35:39,060 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894]
2023-11-29 08:35:39,061 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
2023-11-29 08:35:39,063 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
2023-11-29 08:35:39,063 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
2023-11-29 08:35:39,063 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
2023-11-29 08:35:39,140 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2023-11-29 08:35:39,155 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-11-29 08:35:39,155 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
2023-11-29 08:35:39,222 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
2023-11-29 08:35:39,222 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-11-29 08:35:39,223 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
2023-11-29 08:35:39,237 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
2023-11-29 08:35:39,238 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
2023-11-29 08:35:39,239 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-11-29 08:35:39,243 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
2023-11-29 08:35:39,259 [main] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
2023-11-29 08:35:39,260 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-11-29 08:35:39,260 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
2023-11-29 08:35:39,260 [main] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
2023-11-29 08:35:39,474 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
2023-11-29 08:35:39,474 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
2023-11-29 08:35:39,475 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
2023-11-29 08:35:39,647 [main] INFO util.log: Logging initialized @5574ms to org.eclipse.jetty.util.log.Slf4jLog
2023-11-29 08:35:39,749 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:34033 / 172.19.0.4:34033
2023-11-29 08:35:39,752 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:35091 / 172.19.0.13:35091
2023-11-29 08:35:39,787 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-11-29 08:35:39,807 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-11-29 08:35:39,872 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:35447 / 172.19.0.12:35447
2023-11-29 08:35:39,895 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:43139 / 172.19.0.2:43139
2023-11-29 08:35:39,895 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:33401 / 172.19.0.8:33401
2023-11-29 08:35:39,939 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-11-29 08:35:39,941 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-11-29 08:35:39,945 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-11-29 08:35:39,946 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:44561 / 172.19.0.9:44561
2023-11-29 08:35:39,963 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-11-29 08:35:39,999 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
2023-11-29 08:35:40,010 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-11-29 08:35:40,025 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
2023-11-29 08:35:40,025 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
2023-11-29 08:35:40,025 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
2023-11-29 08:35:40,028 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
2023-11-29 08:35:40,040 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_4.ozonesecure_default:35091 / 172.19.0.13:35091
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:40,041 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_5.ozonesecure_default:35447 / 172.19.0.12:35447
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:40,091 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#6 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_recon_1.ozonesecure_default:43139 / 172.19.0.2:43139
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:40,101 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#6 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_1.ozonesecure_default:33401 / 172.19.0.8:33401
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:40,102 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#6 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_2.ozonesecure_default:44561 / 172.19.0.9:44561
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:40,131 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
2023-11-29 08:35:40,133 [main] INFO http.HttpServer2: Jetty bound to port 9876
2023-11-29 08:35:40,134 [main] INFO server.Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 11.0.19+7-LTS
2023-11-29 08:35:40,244 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:44145 / 172.19.0.10:44145
2023-11-29 08:35:40,259 [main] INFO server.session: DefaultSessionIdManager workerName=node0
2023-11-29 08:35:40,259 [main] INFO server.session: No SessionScavenger set, using defaults
2023-11-29 08:35:40,268 [main] INFO server.session: node0 Scavenging every 600000ms
2023-11-29 08:35:40,270 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-11-29 08:35:40,271 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_3.ozonesecure_default:44145 / 172.19.0.10:44145
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:40,303 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/scm.keytab, for principal HTTP/scm@EXAMPLE.COM
2023-11-29 08:35:40,320 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6a022b0e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
2023-11-29 08:35:40,321 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2e4c4373{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-11-29 08:35:40,462 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/scm.keytab, for principal HTTP/scm@EXAMPLE.COM
2023-11-29 08:35:40,472 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3e84c698{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-10479696088398161928/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
2023-11-29 08:35:40,484 [main] INFO server.AbstractConnector: Started ServerConnector@74355746{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
2023-11-29 08:35:40,484 [main] INFO server.Server: Started @6411ms
2023-11-29 08:35:40,486 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
2023-11-29 08:35:40,486 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
2023-11-29 08:35:40,512 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
2023-11-29 08:35:40,619 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from scm:38309 / 172.19.0.6:38309
2023-11-29 08:35:40,638 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-11-29 08:35:40,639 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#0 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from scm:38309 / 172.19.0.6:38309
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:42,067 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_4.ozonesecure_default:35091 / 172.19.0.13:35091
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:42,120 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_2.ozonesecure_default:44561 / 172.19.0.9:44561
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:42,132 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_1.ozonesecure_default:33401 / 172.19.0.8:33401
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:42,133 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#7 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_recon_1.ozonesecure_default:43139 / 172.19.0.2:43139
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:42,136 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_5.ozonesecure_default:35447 / 172.19.0.12:35447
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:42,303 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_3.ozonesecure_default:44145 / 172.19.0.10:44145
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:42,648 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#1 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from scm:38309 / 172.19.0.6:38309
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:42,655 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-scm/sub-ca-refreshCACertificates] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
, while invoking $Proxy15.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.6:9961 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
2023-11-29 08:35:42,848 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:35:43,957 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from scm:43317 / 172.19.0.6:43317
2023-11-29 08:35:43,975 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:35:44,072 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#9 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_4.ozonesecure_default:35091 / 172.19.0.13:35091
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:44,125 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_2.ozonesecure_default:44561 / 172.19.0.9:44561
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:44,130 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO impl.FollowerState: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5116816348ns, electionTimeout:5113ms
2023-11-29 08:35:44,130 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO impl.RoleInfo: aa263ec8-044c-47b5-a9b4-ebebfa162b60: shutdown aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState
2023-11-29 08:35:44,131 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
2023-11-29 08:35:44,133 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
2023-11-29 08:35:44,133 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-FollowerState] INFO impl.RoleInfo: aa263ec8-044c-47b5-a9b4-ebebfa162b60: start aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1
2023-11-29 08:35:44,138 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_1.ozonesecure_default:33401 / 172.19.0.8:33401
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:44,139 [IPC Server handler 1 on default port 9961] INFO ipc.Server: IPC Server handler 1 on default port 9961, call Call#0 Retry#9 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_datanode_5.ozonesecure_default:35447 / 172.19.0.12:35447
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:44,147 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#8 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from ozonesecure_recon_1.ozonesecure_default:43139 / 172.19.0.2:43139
org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:aa263ec8-044c-47b5-a9b4-ebebfa162b60 is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:110)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:244)
	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:90)
	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:18732)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
2023-11-29 08:35:44,148 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO impl.LeaderElection: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894]|listeners:[], old=null
2023-11-29 08:35:44,151 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO impl.LeaderElection: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
2023-11-29 08:35:44,158 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO impl.LeaderElection: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894]|listeners:[], old=null
2023-11-29 08:35:44,159 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO impl.LeaderElection: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1 ELECTION round 0: result PASSED (term=2)
2023-11-29 08:35:44,159 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO impl.RoleInfo: aa263ec8-044c-47b5-a9b4-ebebfa162b60: shutdown aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1
2023-11-29 08:35:44,159 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
2023-11-29 08:35:44,165 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
2023-11-29 08:35:44,169 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-11-29 08:35:44,169 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
2023-11-29 08:35:44,173 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
2023-11-29 08:35:44,174 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
2023-11-29 08:35:44,174 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
2023-11-29 08:35:44,181 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.enabled = false (default)
2023-11-29 08:35:44,182 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.timeout.ratio = 0.9 (default)
2023-11-29 08:35:44,182 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2023-11-29 08:35:44,182 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2023-11-29 08:35:44,182 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-11-29 08:35:44,184 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO impl.RoleInfo: aa263ec8-044c-47b5-a9b4-ebebfa162b60: start aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderStateImpl
2023-11-29 08:35:44,184 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: set firstElectionSinceStartup to false for becomeLeader
2023-11-29 08:35:44,184 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
2023-11-29 08:35:44,184 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
2023-11-29 08:35:44,189 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: change Leader from null to aa263ec8-044c-47b5-a9b4-ebebfa162b60 at term 2 for becomeLeader, leader elected after 6912ms
2023-11-29 08:35:44,194 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
2023-11-29 08:35:44,197 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998/current/log_inprogress_0 to /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998/current/log_0-0
2023-11-29 08:35:44,198 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-LeaderElection1] INFO server.RaftServer$Division: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998: set configuration 1: peers:[aa263ec8-044c-47b5-a9b4-ebebfa162b60|scm:9894]|listeners:[], old=null
2023-11-29 08:35:44,212 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/5a8c5d5f-0863-4bca-8e33-6e05370e2998/current/log_inprogress_1
2023-11-29 08:35:44,218 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO server.RaftServer$Division: leader is ready since appliedIndex == 1 >= startIndex == 1
2023-11-29 08:35:44,219 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
2023-11-29 08:35:44,219 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
2023-11-29 08:35:44,220 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: Initializing SecretKeys.
2023-11-29 08:35:44,221 [SecretKeyManagerService] INFO symmetric.SecretKeyManager: No valid key has been loaded. A new key is generated: SecretKey(id = cb0c00a2-9311-4ddf-8b92-fdf6088efdd0, creation at: 2023-11-29T08:35:44.220702Z, expire at: 2023-12-06T08:35:44.220702Z)
2023-11-29 08:35:44,224 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:44,224 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
2023-11-29 08:35:44,225 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
2023-11-29 08:35:44,225 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
2023-11-29 08:35:44,226 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
2023-11-29 08:35:44,227 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
2023-11-29 08:35:44,278 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Updating keys with [SecretKey(id = cb0c00a2-9311-4ddf-8b92-fdf6088efdd0, creation at: 2023-11-29T08:35:44.220Z, expire at: 2023-12-06T08:35:44.220Z)]
2023-11-29 08:35:44,279 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO symmetric.SecretKeyStateImpl: Current key updated SecretKey(id = cb0c00a2-9311-4ddf-8b92-fdf6088efdd0, creation at: 2023-11-29T08:35:44.220Z, expire at: 2023-12-06T08:35:44.220Z)
2023-11-29 08:35:44,315 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO symmetric.LocalSecretKeyStore: Saved [SecretKey(id = cb0c00a2-9311-4ddf-8b92-fdf6088efdd0, creation at: 2023-11-29T08:35:44.220Z, expire at: 2023-12-06T08:35:44.220Z)] to file /data/metadata/scm/keys/secret_keys.json
2023-11-29 08:35:44,316 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:44,317 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
2023-11-29 08:35:44,317 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
2023-11-29 08:35:44,336 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: cc7a6c85-4ef7-435a-ad17-19227fb0f721
2023-11-29 08:35:44,350 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for CertificateId, expected lastId is 0, actual lastId is 2.
2023-11-29 08:35:44,354 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:44,359 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:44,359 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 2 to 3.
2023-11-29 08:35:44,399 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-29 08:35:44,399 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-29 08:35:44,427 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-29 08:35:44,742 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:44,759 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-29 08:35:44,760 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-29 08:35:44,761 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-29 08:35:44,761 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-29 08:35:44,762 [aa263ec8-044c-47b5-a9b4-ebebfa162b60-scm/sub-ca-refreshCACertificates] INFO client.SCMCertificateClient: CA certificates are not changed.
2023-11-29 08:35:44,808 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-29 08:35:44,808 [IPC Server handler 0 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-29 08:35:44,933 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:38439 / 172.19.0.10:38439
2023-11-29 08:35:44,937 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:35:46,076 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: 14edb126-374b-48c1-9512-07b6e7b411d0
2023-11-29 08:35:46,081 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:46,081 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 3 to 4.
2023-11-29 08:35:46,087 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-29 08:35:46,089 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-29 08:35:46,109 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-29 08:35:46,134 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: 18573edc-19c5-4d66-af25-cb4aed99fc20
2023-11-29 08:35:46,303 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:46,398 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: 85e0215a-4c76-4245-8b91-bf1607887fe5
2023-11-29 08:35:46,406 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:46,407 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 4 to 5.
2023-11-29 08:35:46,411 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-29 08:35:46,428 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-29 08:35:46,460 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-29 08:35:46,694 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:46,772 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:46,773 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 5 to 6.
2023-11-29 08:35:46,777 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-29 08:35:46,789 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-29 08:35:46,809 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-29 08:35:46,892 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:46,942 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: 1ca5126a-7d7d-4d7f-a939-29db03d1fab7
2023-11-29 08:35:46,953 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:46,953 [IPC Server handler 1 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 6 to 7.
2023-11-29 08:35:46,965 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for RECON recon, UUID: e804acb4-479c-41b3-8603-57391c7cb8db
2023-11-29 08:35:46,981 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-29 08:35:46,986 [IPC Server handler 1 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-29 08:35:47,056 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-29 08:35:47,228 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:47,288 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:41649 / 172.19.0.4:41649
2023-11-29 08:35:47,308 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-11-29 08:35:47,310 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:47,311 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 7 to 8.
2023-11-29 08:35:47,315 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-29 08:35:47,316 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-29 08:35:47,321 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-29 08:35:47,406 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:47,504 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-29 08:35:47,504 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-29 08:35:47,505 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-29 08:35:47,505 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-29 08:35:47,506 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-29 08:35:47,506 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-29 08:35:47,552 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om, UUID: d85a08bc-605e-4f7a-9bbf-2366d6347100
2023-11-29 08:35:47,575 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:47,588 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 8 to 9.
2023-11-29 08:35:47,626 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-29 08:35:47,637 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-29 08:35:47,701 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-29 08:35:47,852 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:47,859 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:35:47,910 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-29 08:35:47,910 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-29 08:35:47,911 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-29 08:35:47,911 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-29 08:35:48,047 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-29 08:35:48,047 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-29 08:35:48,442 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:45299 / 172.19.0.13:45299
2023-11-29 08:35:48,455 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:35:48,529 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:34015 / 172.19.0.9:34015
2023-11-29 08:35:48,536 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:35:48,625 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:44713 / 172.19.0.12:44713
2023-11-29 08:35:48,635 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:35:48,789 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:42493 / 172.19.0.8:42493
2023-11-29 08:35:48,818 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:35:52,187 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:45209 / 172.19.0.10:45209
2023-11-29 08:35:52,195 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:35:52,859 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:35:54,398 [IPC Server handler 21 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/cc7a6c85-4ef7-435a-ad17-19227fb0f721
2023-11-29 08:35:54,416 [IPC Server handler 21 on default port 9861] INFO node.SCMNodeManager: Registered datanode: cc7a6c85-4ef7-435a-ad17-19227fb0f721{ip: 172.19.0.10, host: ozonesecure_datanode_3.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 3, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-11-29 08:35:54,428 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-11-29 08:35:54,429 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 5 required.
2023-11-29 08:35:54,435 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8ba86d71-04a5-42c3-a114-5ea889abb5f0 to datanode:cc7a6c85-4ef7-435a-ad17-19227fb0f721
2023-11-29 08:35:54,562 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:54,596 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 8ba86d71-04a5-42c3-a114-5ea889abb5f0, Nodes: cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:35:54.434529Z[UTC]]
2023-11-29 08:35:57,859 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:35:58,207 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:35:58,209 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=8ba86d71-04a5-42c3-a114-5ea889abb5f0
2023-11-29 08:35:58,220 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:02,345 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:45909 / 172.19.0.4:45909
2023-11-29 08:36:02,377 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-11-29 08:36:02,860 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:36:03,050 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:39219 / 172.19.0.13:39219
2023-11-29 08:36:03,088 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:36:03,105 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:03,204 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:34391 / 172.19.0.8:34391
2023-11-29 08:36:03,243 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:36:03,306 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:40947 / 172.19.0.9:40947
2023-11-29 08:36:03,351 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:36:03,850 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:34459 / 172.19.0.12:34459
2023-11-29 08:36:03,897 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:36:05,057 [IPC Server handler 45 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/14edb126-374b-48c1-9512-07b6e7b411d0
2023-11-29 08:36:05,058 [IPC Server handler 45 on default port 9861] INFO node.SCMNodeManager: Registered datanode: 14edb126-374b-48c1-9512-07b6e7b411d0{ip: 172.19.0.13, host: ozonesecure_datanode_4.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 4, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-11-29 08:36:05,059 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-11-29 08:36:05,061 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 5 required.
2023-11-29 08:36:05,061 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d117c241-66cb-46b8-b36f-060fb8f1d309 to datanode:14edb126-374b-48c1-9512-07b6e7b411d0
2023-11-29 08:36:05,064 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:36:05,065 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: d117c241-66cb-46b8-b36f-060fb8f1d309, Nodes: 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:36:05.061487Z[UTC]]
2023-11-29 08:36:05,205 [IPC Server handler 53 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1ca5126a-7d7d-4d7f-a939-29db03d1fab7
2023-11-29 08:36:05,205 [IPC Server handler 53 on default port 9861] INFO node.SCMNodeManager: Registered datanode: 1ca5126a-7d7d-4d7f-a939-29db03d1fab7{ip: 172.19.0.8, host: ozonesecure_datanode_1.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 7, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-11-29 08:36:05,206 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-11-29 08:36:05,212 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 5 required.
2023-11-29 08:36:05,217 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9fa09465-7d42-40c5-a907-575e36ea36c3 to datanode:1ca5126a-7d7d-4d7f-a939-29db03d1fab7
2023-11-29 08:36:05,221 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:36:05,222 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 9fa09465-7d42-40c5-a907-575e36ea36c3, Nodes: 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:36:05.217068Z[UTC]]
2023-11-29 08:36:05,283 [IPC Server handler 44 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/18573edc-19c5-4d66-af25-cb4aed99fc20
2023-11-29 08:36:05,283 [IPC Server handler 44 on default port 9861] INFO node.SCMNodeManager: Registered datanode: 18573edc-19c5-4d66-af25-cb4aed99fc20{ip: 172.19.0.9, host: ozonesecure_datanode_2.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 5, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-11-29 08:36:05,284 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-11-29 08:36:05,285 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=491e2d46-0f5c-4018-93f4-700e47d0f063 to datanode:18573edc-19c5-4d66-af25-cb4aed99fc20
2023-11-29 08:36:05,285 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 4 DataNodes registered, 5 required.
2023-11-29 08:36:05,290 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:36:05,291 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 491e2d46-0f5c-4018-93f4-700e47d0f063, Nodes: 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:36:05.285413Z[UTC]]
2023-11-29 08:36:05,911 [IPC Server handler 45 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/85e0215a-4c76-4245-8b91-bf1607887fe5
2023-11-29 08:36:05,912 [IPC Server handler 45 on default port 9861] INFO node.SCMNodeManager: Registered datanode: 85e0215a-4c76-4245-8b91-bf1607887fe5{ip: 172.19.0.12, host: ozonesecure_datanode_5.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 6, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-11-29 08:36:05,912 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 5 DataNodes registered, 5 required.
2023-11-29 08:36:05,912 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
2023-11-29 08:36:05,912 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
2023-11-29 08:36:05,912 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
2023-11-29 08:36:05,912 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-11-29 08:36:05,913 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-11-29 08:36:05,913 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=4bd0b1e0-0583-4e69-8e4f-18de078b2460 to datanode:85e0215a-4c76-4245-8b91-bf1607887fe5
2023-11-29 08:36:05,916 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:36:05,917 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 4bd0b1e0-0583-4e69-8e4f-18de078b2460, Nodes: 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:36:05.913520Z[UTC]]
2023-11-29 08:36:05,921 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c839b0ad-1f9a-442f-a553-d3dc7ca170e4 to datanode:cc7a6c85-4ef7-435a-ad17-19227fb0f721
2023-11-29 08:36:05,921 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c839b0ad-1f9a-442f-a553-d3dc7ca170e4 to datanode:1ca5126a-7d7d-4d7f-a939-29db03d1fab7
2023-11-29 08:36:05,921 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c839b0ad-1f9a-442f-a553-d3dc7ca170e4 to datanode:18573edc-19c5-4d66-af25-cb4aed99fc20
2023-11-29 08:36:05,925 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:36:05,926 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: c839b0ad-1f9a-442f-a553-d3dc7ca170e4, Nodes: cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:36:05.921663Z[UTC]]
2023-11-29 08:36:06,816 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:39515 / 172.19.0.4:39515
2023-11-29 08:36:06,829 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolOm
2023-11-29 08:36:06,910 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:42099 / 172.19.0.2:42099
2023-11-29 08:36:06,916 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:36:07,860 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:36:08,390 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:36:08,404 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=d117c241-66cb-46b8-b36f-060fb8f1d309
2023-11-29 08:36:08,404 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:08,570 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:36:08,573 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=9fa09465-7d42-40c5-a907-575e36ea36c3
2023-11-29 08:36:08,573 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:08,629 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:36:08,630 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=491e2d46-0f5c-4018-93f4-700e47d0f063
2023-11-29 08:36:08,630 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:08,738 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:08,823 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:09,293 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
2023-11-29 08:36:09,293 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=4bd0b1e0-0583-4e69-8e4f-18de078b2460
2023-11-29 08:36:09,294 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:09,895 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:12,860 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:36:13,770 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:13,794 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:13,868 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:14,448 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:15,263 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-11-29 08:36:15,266 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
2023-11-29 08:36:15,266 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=c839b0ad-1f9a-442f-a553-d3dc7ca170e4
2023-11-29 08:36:15,268 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
2023-11-29 08:36:15,268 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
2023-11-29 08:36:15,268 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
2023-11-29 08:36:15,268 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
2023-11-29 08:36:15,268 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
2023-11-29 08:36:15,268 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
2023-11-29 08:36:15,268 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
2023-11-29 08:36:15,268 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
2023-11-29 08:36:15,273 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
2023-11-29 08:36:15,273 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
2023-11-29 08:36:17,860 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:36:22,861 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:36:27,861 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:36:32,861 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:36:35,495 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:34407 / 172.19.0.4:34407
2023-11-29 08:36:35,502 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-11-29 08:36:35,513 [IPC Server handler 0 on default port 9863] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12), 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9), 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10)]. isPolicySatisfied: true.
2023-11-29 08:36:35,522 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
2023-11-29 08:36:35,533 [IPC Server handler 0 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: eae65e74-85ad-4b98-99dd-7ba6697041ec, Nodes: 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:36:35.513488Z[UTC]]
2023-11-29 08:36:35,535 [aa263ec8-044c-47b5-a9b4-ebebfa162b60@group-6E05370E2998-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
2023-11-29 08:36:35,537 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
2023-11-29 08:36:36,921 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:38387 / 172.19.0.10:38387
2023-11-29 08:36:36,924 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:36:37,053 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:43179 / 172.19.0.10:43179
2023-11-29 08:36:37,092 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:38157 / 172.19.0.12:38157
2023-11-29 08:36:37,098 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:36:37,099 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:36:37,203 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:38765 / 172.19.0.13:38765
2023-11-29 08:36:37,247 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:36:37,324 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:37537 / 172.19.0.12:37537
2023-11-29 08:36:37,360 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:36:37,375 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:33873 / 172.19.0.13:33873
2023-11-29 08:36:37,392 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:36:37,861 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:36:37,874 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:38371 / 172.19.0.9:38371
2023-11-29 08:36:37,897 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:34057 / 172.19.0.8:34057
2023-11-29 08:36:37,901 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:36:37,911 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:36:38,080 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:42979 / 172.19.0.8:42979
2023-11-29 08:36:38,109 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:42227 / 172.19.0.9:42227
2023-11-29 08:36:38,140 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:36:38,173 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:39173 / 172.19.0.2:39173
2023-11-29 08:36:38,193 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:36:38,213 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:36:38,861 [IPC Server handler 48 on default port 9863] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10), 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9), 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)]. isPolicySatisfied: true.
2023-11-29 08:36:38,870 [IPC Server handler 48 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: 92c34565-a825-4b90-bc39-d6b5c93d0c86, Nodes: cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:36:38.862119Z[UTC]]
2023-11-29 08:36:39,684 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:40439 / 172.19.0.4:40439
2023-11-29 08:36:39,688 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:36:41,262 [IPC Server handler 28 on default port 9863] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9), 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12), 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10)]. isPolicySatisfied: true.
2023-11-29 08:36:41,269 [IPC Server handler 28 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: a58aa67b-2252-4159-a629-cb289acc086e, Nodes: 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:36:41.263507Z[UTC]]
2023-11-29 08:36:42,521 [IPC Server handler 0 on default port 9863] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10), 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9), 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12), 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)]. isPolicySatisfied: true.
2023-11-29 08:36:42,529 [IPC Server handler 0 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: e2b7f39d-6249-4a5c-94fa-70ffde895a82, Nodes: cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10)18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:36:42.522739Z[UTC]]
2023-11-29 08:36:42,761 [IPC Server handler 48 on default port 9863] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9), 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8), cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10), 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)]. isPolicySatisfied: true.
2023-11-29 08:36:42,766 [IPC Server handler 48 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: 3d6c627a-77f1-4bff-9c80-67c20c639ef2, Nodes: 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:36:42.761655Z[UTC]]
2023-11-29 08:36:42,861 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 30000ms after safemode exit
2023-11-29 08:36:44,722 [IPC Server handler 28 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:36:44,963 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:36:45,815 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:36:47,865 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 5 containers.
2023-11-29 08:36:52,866 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2023-11-29 08:36:57,866 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 5 containers.
2023-11-29 08:37:02,867 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 5 containers.
2023-11-29 08:37:06,882 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:43357 / 172.19.0.4:43357
2023-11-29 08:37:06,886 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-11-29 08:37:06,916 [IPC Server handler 12 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
2023-11-29 08:37:07,528 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:45127 / 172.19.0.2:45127
2023-11-29 08:37:07,544 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:37:07,868 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2023-11-29 08:37:11,772 [IPC Server handler 12 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:12,868 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 5 containers.
2023-11-29 08:37:12,876 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:41259 / 172.19.0.13:41259
2023-11-29 08:37:12,884 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:46725 / 172.19.0.12:46725
2023-11-29 08:37:12,887 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:12,901 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:12,904 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:34645 / 172.19.0.9:34645
2023-11-29 08:37:12,912 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:12,958 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:34465 / 172.19.0.10:34465
2023-11-29 08:37:12,959 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:37867 / 172.19.0.8:37867
2023-11-29 08:37:12,968 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:12,972 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:17,263 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:38931 / 172.19.0.4:38931
2023-11-29 08:37:17,274 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:37:17,277 [IPC Server handler 58 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:17,861 [IPC Server handler 71 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:17,869 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 5 containers.
2023-11-29 08:37:18,424 [IPC Server handler 37 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:22,870 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2023-11-29 08:37:26,296 [IPC Server handler 7 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:26,896 [IPC Server handler 58 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:27,444 [IPC Server handler 81 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:27,870 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 5 containers.
2023-11-29 08:37:32,871 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2023-11-29 08:37:35,429 [IPC Server handler 81 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:35,430 [IPC Server handler 53 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:35,431 [IPC Server handler 22 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:37,872 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2023-11-29 08:37:37,962 [IPC Server handler 7 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:39,146 [IPC Server handler 67 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:41,107 [IPC Server handler 67 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:42,026 [IPC Server handler 23 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:42,872 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 5 containers.
2023-11-29 08:37:42,887 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:35203 / 172.19.0.12:35203
2023-11-29 08:37:42,888 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:46691 / 172.19.0.2:46691
2023-11-29 08:37:42,891 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:34895 / 172.19.0.9:34895
2023-11-29 08:37:42,903 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:42,907 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:34419 / 172.19.0.13:34419
2023-11-29 08:37:42,908 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:42,911 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:37:42,912 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:42,951 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:38637 / 172.19.0.8:38637
2023-11-29 08:37:42,951 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:40709 / 172.19.0.10:40709
2023-11-29 08:37:42,958 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:42,964 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:43,168 [IPC Server handler 64 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:47,400 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:47,854 [IPC Server handler 58 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:47,873 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
2023-11-29 08:37:48,314 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:49,162 [IPC Server handler 79 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:49,716 [IPC Server handler 12 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:51,083 [IPC Server handler 79 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:52,873 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 5 containers.
2023-11-29 08:37:54,883 [IPC Server handler 89 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:54,887 [IPC Server handler 7 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:55,106 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:40059 / 172.19.0.10:40059
2023-11-29 08:37:55,154 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:46679 / 172.19.0.9:46679
2023-11-29 08:37:55,158 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:55,183 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:55,185 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:45227 / 172.19.0.8:45227
2023-11-29 08:37:55,186 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:41795 / 172.19.0.2:41795
2023-11-29 08:37:55,208 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:37:55,225 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:37:55,276 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:40441 / 172.19.0.4:40441
2023-11-29 08:37:55,292 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:37:55,295 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:56,223 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:56,227 [IPC Server handler 36 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:56,299 [IPC Server handler 17 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:37:57,874 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:38:02,783 [IPC Server handler 58 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:02,787 [IPC Server handler 89 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:02,875 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:38:02,905 [IPC Server handler 67 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:07,875 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:38:12,860 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:40843 / 172.19.0.12:40843
2023-11-29 08:38:12,867 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:35655 / 172.19.0.13:35655
2023-11-29 08:38:12,879 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:38:12,879 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:38:12,882 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:38:17,879 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:38:22,880 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:38:25,117 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:42355 / 172.19.0.10:42355
2023-11-29 08:38:25,155 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:36017 / 172.19.0.9:36017
2023-11-29 08:38:25,188 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:37915 / 172.19.0.8:37915
2023-11-29 08:38:25,190 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:38:25,190 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:38:25,190 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:34445 / 172.19.0.2:34445
2023-11-29 08:38:25,203 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:38:25,206 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:38:27,881 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:38:29,882 [IPC Server handler 67 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:30,378 [IPC Server handler 63 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:30,847 [IPC Server handler 23 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:31,314 [IPC Server handler 88 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:31,783 [IPC Server handler 58 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:32,224 [IPC Server handler 36 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:32,689 [IPC Server handler 12 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:32,881 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:38:33,160 [IPC Server handler 36 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:33,622 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:34,090 [IPC Server handler 36 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:34,568 [IPC Server handler 28 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:35,027 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:35,489 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:37,882 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:38:42,862 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:32901 / 172.19.0.13:32901
2023-11-29 08:38:42,871 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:38:42,872 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:45075 / 172.19.0.12:45075
2023-11-29 08:38:42,876 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:38:42,883 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:38:42,896 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:43391 / 172.19.0.2:43391
2023-11-29 08:38:42,904 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:38:45,102 [IPC Server handler 44 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:38:47,883 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:38:47,901 [IPC Server handler 79 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:49,358 [IPC Server handler 65 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:51,148 [IPC Server handler 17 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:51,148 [IPC Server handler 88 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.11
2023-11-29 08:38:52,884 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:38:55,102 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:46513 / 172.19.0.10:46513
2023-11-29 08:38:55,110 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:38:55,166 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:45739 / 172.19.0.9:45739
2023-11-29 08:38:55,166 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:43153 / 172.19.0.8:43153
2023-11-29 08:38:55,172 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:38:55,176 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:38:57,884 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:39:02,885 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:39:06,880 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:36705 / 172.19.0.4:36705
2023-11-29 08:39:06,883 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-11-29 08:39:07,885 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:39:12,856 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:45055 / 172.19.0.12:45055
2023-11-29 08:39:12,867 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:39:12,875 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:37265 / 172.19.0.13:37265
2023-11-29 08:39:12,880 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:39:12,886 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:39:17,887 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:39:22,887 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:39:25,115 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:36509 / 172.19.0.10:36509
2023-11-29 08:39:25,122 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:39:25,168 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:35685 / 172.19.0.9:35685
2023-11-29 08:39:25,170 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:46681 / 172.19.0.8:46681
2023-11-29 08:39:25,177 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:39:25,183 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:39:25,195 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:36227 / 172.19.0.2:36227
2023-11-29 08:39:25,210 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:39:27,888 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:39:32,888 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:39:36,943 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:46251 / 172.19.0.4:46251
2023-11-29 08:39:36,946 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-11-29 08:39:37,889 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:39:42,327 [IPC Server handler 40 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:39:42,897 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:39:42,911 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:43365 / 172.19.0.2:43365
2023-11-29 08:39:42,913 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:38961 / 172.19.0.13:38961
2023-11-29 08:39:42,922 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:45697 / 172.19.0.12:45697
2023-11-29 08:39:42,926 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:39:42,927 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:39:42,939 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:39:47,446 [IPC Server handler 22 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:39:47,898 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:39:52,899 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:39:55,116 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:38295 / 172.19.0.10:38295
2023-11-29 08:39:55,129 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:39:55,154 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:42399 / 172.19.0.2:42399
2023-11-29 08:39:55,156 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:46515 / 172.19.0.8:46515
2023-11-29 08:39:55,167 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:39:55,172 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:39:55,183 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:41571 / 172.19.0.9:41571
2023-11-29 08:39:55,208 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:39:57,899 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:40:00,635 [IPC Server handler 12 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:40:02,900 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:40:05,937 [IPC Server handler 64 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:40:07,901 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:40:12,852 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:35783 / 172.19.0.13:35783
2023-11-29 08:40:12,864 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:38969 / 172.19.0.12:38969
2023-11-29 08:40:12,874 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:40:12,887 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:40:12,902 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:40:17,903 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:40:19,376 [IPC Server handler 16 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:40:22,903 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:40:24,656 [IPC Server handler 71 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:40:25,106 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:45903 / 172.19.0.10:45903
2023-11-29 08:40:25,114 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:40:25,144 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:34027 / 172.19.0.8:34027
2023-11-29 08:40:25,151 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:35407 / 172.19.0.9:35407
2023-11-29 08:40:25,155 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:40:25,159 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:40:27,904 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:40:32,906 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:40:37,908 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:40:40,874 [IPC Server handler 67 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:40:42,856 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:36757 / 172.19.0.12:36757
2023-11-29 08:40:42,872 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:36717 / 172.19.0.13:36717
2023-11-29 08:40:42,876 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:40:42,895 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:40:42,912 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:40:42,919 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:45931 / 172.19.0.2:45931
2023-11-29 08:40:42,933 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:40:47,235 [IPC Server handler 14 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:40:47,913 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:40:52,914 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:40:55,103 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:43419 / 172.19.0.10:43419
2023-11-29 08:40:55,148 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:34519 / 172.19.0.8:34519
2023-11-29 08:40:55,165 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:36377 / 172.19.0.9:36377
2023-11-29 08:40:55,167 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:40:55,181 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:40:55,187 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:39779 / 172.19.0.2:39779
2023-11-29 08:40:55,194 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:40:55,199 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:40:57,914 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:41:00,710 [IPC Server handler 89 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:41:02,915 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:41:05,840 [IPC Server handler 67 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:41:07,916 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:41:12,882 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:39539 / 172.19.0.13:39539
2023-11-29 08:41:12,895 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:33027 / 172.19.0.12:33027
2023-11-29 08:41:12,899 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:41:12,910 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:41:12,918 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:41:17,919 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:41:18,951 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:41:22,919 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:41:24,114 [IPC Server handler 36 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:41:25,107 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:41597 / 172.19.0.10:41597
2023-11-29 08:41:25,115 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:41:25,148 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:44197 / 172.19.0.8:44197
2023-11-29 08:41:25,165 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:38035 / 172.19.0.2:38035
2023-11-29 08:41:25,172 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:41:25,175 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:43893 / 172.19.0.9:43893
2023-11-29 08:41:25,179 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:41:25,194 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:41:27,920 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:41:32,920 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:41:37,341 [IPC Server handler 83 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:41:37,617 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:41127 / 172.19.0.2:41127
2023-11-29 08:41:37,631 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:41:37,921 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:41:42,553 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:41:42,848 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:45785 / 172.19.0.12:45785
2023-11-29 08:41:42,857 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:35303 / 172.19.0.13:35303
2023-11-29 08:41:42,865 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:41:42,867 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:41:42,924 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:41:47,924 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:41:52,925 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:41:55,101 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:40427 / 172.19.0.10:40427
2023-11-29 08:41:55,132 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:45303 / 172.19.0.8:45303
2023-11-29 08:41:55,147 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:46079 / 172.19.0.9:46079
2023-11-29 08:41:55,147 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:41:55,149 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:41:55,155 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:41:56,337 [IPC Server handler 83 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:41:57,925 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:42:01,632 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:42:02,926 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:42:07,926 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:42:12,883 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:41985 / 172.19.0.13:41985
2023-11-29 08:42:12,885 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:42007 / 172.19.0.12:42007
2023-11-29 08:42:12,886 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:42:12,892 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:42:12,927 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:42:17,927 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:42:22,928 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:42:25,108 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:36503 / 172.19.0.10:36503
2023-11-29 08:42:25,115 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:42:25,131 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:40897 / 172.19.0.8:40897
2023-11-29 08:42:25,143 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:42391 / 172.19.0.9:42391
2023-11-29 08:42:25,160 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:42:25,162 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:42:27,929 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:42:32,929 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:42:35,236 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:37911 / 172.19.0.4:37911
2023-11-29 08:42:35,249 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-11-29 08:42:36,578 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:46353 / 172.19.0.4:46353
2023-11-29 08:42:36,579 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:42:37,930 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:42:42,861 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:33055 / 172.19.0.13:33055
2023-11-29 08:42:42,863 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:42:42,930 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:42:47,931 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:42:50,425 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:41183 / 172.19.0.4:41183
2023-11-29 08:42:50,429 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-11-29 08:42:50,429 [IPC Server handler 75 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:42:52,931 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:42:55,102 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:35611 / 172.19.0.10:35611
2023-11-29 08:42:55,106 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:42:55,129 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:36801 / 172.19.0.8:36801
2023-11-29 08:42:55,136 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:33027 / 172.19.0.9:33027
2023-11-29 08:42:55,142 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:42:55,143 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:42:57,931 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:43:02,932 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:43:07,932 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:43:12,853 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:37599 / 172.19.0.13:37599
2023-11-29 08:43:12,860 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:43:12,932 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:43:17,933 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:43:22,933 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:43:25,104 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:46297 / 172.19.0.10:46297
2023-11-29 08:43:25,109 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:43:25,138 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:46341 / 172.19.0.8:46341
2023-11-29 08:43:25,154 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:43:25,154 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:34749 / 172.19.0.9:34749
2023-11-29 08:43:25,159 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:43:27,933 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:43:32,934 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:43:37,934 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:43:39,685 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:39411 / 172.19.0.4:39411
2023-11-29 08:43:39,687 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:43:39,694 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:43127 / 172.19.0.4:43127
2023-11-29 08:43:39,695 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-11-29 08:43:39,696 [IPC Server handler 58 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:43:42,735 [IPC Server handler 7 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:43:42,863 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:37065 / 172.19.0.13:37065
2023-11-29 08:43:42,872 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:43:42,934 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:43:43,648 [EventQueue-StaleNodeForStaleNodeHandler] INFO node.StaleNodeHandler: Datanode 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) moved to stale state. Finalizing its pipelines [PipelineID=4bd0b1e0-0583-4e69-8e4f-18de078b2460, PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2]
2023-11-29 08:43:43,651 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 4bd0b1e0-0583-4e69-8e4f-18de078b2460, Nodes: 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:85e0215a-4c76-4245-8b91-bf1607887fe5, CreationTimestamp2023-11-29T08:36:05.913Z[UTC]] moved to CLOSED state
2023-11-29 08:43:43,658 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Container #1 closed for pipeline=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec
2023-11-29 08:43:43,659 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1, current state: CLOSING
2023-11-29 08:43:43,669 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: eae65e74-85ad-4b98-99dd-7ba6697041ec, Nodes: 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10), ReplicationConfig: EC{rs-3-2-1024k}, State:OPEN, leaderId:, CreationTimestamp2023-11-29T08:36:35.513Z[UTC]] moved to CLOSED state
2023-11-29 08:43:43,673 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Container #4 closed for pipeline=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82
2023-11-29 08:43:43,677 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e2b7f39d-6249-4a5c-94fa-70ffde895a82, Nodes: cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10)18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), ReplicationConfig: EC{rs-3-2-1024k}, State:OPEN, leaderId:, CreationTimestamp2023-11-29T08:36:42.522Z[UTC]] moved to CLOSED state
2023-11-29 08:43:43,680 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Container #2 closed for pipeline=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86
2023-11-29 08:43:43,681 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 92c34565-a825-4b90-bc39-d6b5c93d0c86, Nodes: cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12), ReplicationConfig: EC{rs-3-2-1024k}, State:OPEN, leaderId:, CreationTimestamp2023-11-29T08:36:38.862Z[UTC]] moved to CLOSED state
2023-11-29 08:43:43,684 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Container #3 closed for pipeline=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e
2023-11-29 08:43:43,686 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #4, current state: CLOSING
2023-11-29 08:43:43,688 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2, current state: CLOSING
2023-11-29 08:43:43,688 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #3, current state: CLOSING
2023-11-29 08:43:43,688 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a58aa67b-2252-4159-a629-cb289acc086e, Nodes: 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10), ReplicationConfig: EC{rs-3-2-1024k}, State:OPEN, leaderId:, CreationTimestamp2023-11-29T08:36:41.263Z[UTC]] moved to CLOSED state
2023-11-29 08:43:43,690 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Container #5 closed for pipeline=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2
2023-11-29 08:43:43,691 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3d6c627a-77f1-4bff-9c80-67c20c639ef2, Nodes: 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12), ReplicationConfig: EC{rs-3-2-1024k}, State:OPEN, leaderId:, CreationTimestamp2023-11-29T08:36:42.761Z[UTC]] moved to CLOSED state
2023-11-29 08:43:43,699 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #5, current state: CLOSING
2023-11-29 08:43:44,621 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:41867 / 172.19.0.2:41867
2023-11-29 08:43:44,628 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:43:47,157 [IPC Server handler 88 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:43:47,935 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.657964Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1701247997935 and scm deadline 1701248027935
2023-11-29 08:43:47,936 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.657964Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701247997936 and scm deadline 1701248027936
2023-11-29 08:43:47,936 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.657964Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) with datanode deadline 1701247997936 and scm deadline 1701248027936
2023-11-29 08:43:47,936 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.657964Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701247997936 and scm deadline 1701248027936
2023-11-29 08:43:47,936 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.657964Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) with datanode deadline 1701247997936 and scm deadline 1701248027936
2023-11-29 08:43:47,937 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.679428Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) with datanode deadline 1701247997937 and scm deadline 1701248027937
2023-11-29 08:43:47,937 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.679428Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701247997937 and scm deadline 1701248027937
2023-11-29 08:43:47,937 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.679428Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1701247997937 and scm deadline 1701248027937
2023-11-29 08:43:47,937 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.679428Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701247997937 and scm deadline 1701248027937
2023-11-29 08:43:47,938 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.679428Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) with datanode deadline 1701247997938 and scm deadline 1701248027938
2023-11-29 08:43:47,938 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.683885Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1701247997938 and scm deadline 1701248027938
2023-11-29 08:43:47,938 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.683885Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701247997938 and scm deadline 1701248027938
2023-11-29 08:43:47,938 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.683885Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) with datanode deadline 1701247997938 and scm deadline 1701248027938
2023-11-29 08:43:47,939 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.683885Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701247997939 and scm deadline 1701248027939
2023-11-29 08:43:47,939 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.683885Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) with datanode deadline 1701247997939 and scm deadline 1701248027939
2023-11-29 08:43:47,939 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.672591Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) with datanode deadline 1701247997939 and scm deadline 1701248027939
2023-11-29 08:43:47,939 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.672591Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1701247997939 and scm deadline 1701248027939
2023-11-29 08:43:47,939 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.672591Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) with datanode deadline 1701247997939 and scm deadline 1701248027939
2023-11-29 08:43:47,940 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.672591Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701247997940 and scm deadline 1701248027940
2023-11-29 08:43:47,940 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.672591Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701247997940 and scm deadline 1701248027940
2023-11-29 08:43:47,940 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.690034Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701247997940 and scm deadline 1701248027940
2023-11-29 08:43:47,940 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.690034Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701247997940 and scm deadline 1701248027940
2023-11-29 08:43:47,940 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.690034Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) with datanode deadline 1701247997940 and scm deadline 1701248027940
2023-11-29 08:43:47,941 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.690034Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) with datanode deadline 1701247997941 and scm deadline 1701248027941
2023-11-29 08:43:47,941 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.690034Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1701247997941 and scm deadline 1701248027941
2023-11-29 08:43:47,941 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 7 milliseconds for processing 6 containers.
2023-11-29 08:43:52,419 [IPC Server handler 63 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:43:52,942 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.657964Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1701248002942 and scm deadline 1701248032942
2023-11-29 08:43:52,942 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.657964Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248002942 and scm deadline 1701248032942
2023-11-29 08:43:52,942 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.657964Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) with datanode deadline 1701248002942 and scm deadline 1701248032942
2023-11-29 08:43:52,942 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.657964Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248002942 and scm deadline 1701248032942
2023-11-29 08:43:52,942 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.657964Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) with datanode deadline 1701248002942 and scm deadline 1701248032942
2023-11-29 08:43:52,943 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.679428Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) with datanode deadline 1701248002943 and scm deadline 1701248032943
2023-11-29 08:43:52,943 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.679428Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248002943 and scm deadline 1701248032943
2023-11-29 08:43:52,943 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.679428Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1701248002943 and scm deadline 1701248032943
2023-11-29 08:43:52,943 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.679428Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248002943 and scm deadline 1701248032943
2023-11-29 08:43:52,943 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.679428Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) with datanode deadline 1701248002943 and scm deadline 1701248032943
2023-11-29 08:43:52,944 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.683885Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1701248002944 and scm deadline 1701248032944
2023-11-29 08:43:52,944 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.683885Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248002944 and scm deadline 1701248032944
2023-11-29 08:43:52,944 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.683885Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) with datanode deadline 1701248002944 and scm deadline 1701248032944
2023-11-29 08:43:52,944 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.683885Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248002944 and scm deadline 1701248032944
2023-11-29 08:43:52,945 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.683885Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) with datanode deadline 1701248002945 and scm deadline 1701248032945
2023-11-29 08:43:52,945 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.672591Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) with datanode deadline 1701248002945 and scm deadline 1701248032945
2023-11-29 08:43:52,945 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.672591Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1701248002945 and scm deadline 1701248032945
2023-11-29 08:43:52,946 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.672591Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) with datanode deadline 1701248002946 and scm deadline 1701248032946
2023-11-29 08:43:52,946 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.672591Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248002946 and scm deadline 1701248032946
2023-11-29 08:43:52,948 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.672591Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248002948 and scm deadline 1701248032948
2023-11-29 08:43:52,948 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.690034Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248002948 and scm deadline 1701248032948
2023-11-29 08:43:52,948 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.690034Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248002948 and scm deadline 1701248032948
2023-11-29 08:43:52,948 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.690034Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) with datanode deadline 1701248002948 and scm deadline 1701248032948
2023-11-29 08:43:52,949 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.690034Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) with datanode deadline 1701248002949 and scm deadline 1701248032949
2023-11-29 08:43:52,949 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSING, stateEnterTime=2023-11-29T08:43:43.690034Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) with datanode deadline 1701248002949 and scm deadline 1701248032949
2023-11-29 08:43:52,950 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 9 milliseconds for processing 6 containers.
2023-11-29 08:43:55,101 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:33843 / 172.19.0.10:33843
2023-11-29 08:43:55,107 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:43:55,137 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:45179 / 172.19.0.8:45179
2023-11-29 08:43:55,152 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:43:55,159 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:43733 / 172.19.0.9:43733
2023-11-29 08:43:55,168 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:43:56,116 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #3 to CLOSED state, datanode cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) reported CLOSED replica with index 5.
2023-11-29 08:43:56,118 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) reported CLOSED replica with index 1.
2023-11-29 08:43:56,143 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) reported CLOSED replica with index 5.
2023-11-29 08:43:56,162 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #4 to CLOSED state, datanode cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) reported CLOSED replica with index 1.
2023-11-29 08:43:56,209 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #5 to CLOSED state, datanode 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) reported CLOSED replica with index 1.
2023-11-29 08:43:56,822 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:34475 / 172.19.0.4:34475
2023-11-29 08:43:56,825 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:43:56,828 [IPC Server handler 67 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:43:57,951 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248007950 and scm deadline 1701248037950
2023-11-29 08:43:57,951 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248007951 and scm deadline 1701248037951
2023-11-29 08:43:57,955 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248007955 and scm deadline 1701248037955
2023-11-29 08:43:57,955 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248007955 and scm deadline 1701248037955
2023-11-29 08:43:57,956 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248007956 and scm deadline 1701248037956
2023-11-29 08:43:57,956 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248007956 and scm deadline 1701248037956
2023-11-29 08:43:57,956 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248007956 and scm deadline 1701248037956
2023-11-29 08:43:57,957 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248007956 and scm deadline 1701248037956
2023-11-29 08:43:57,957 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248007957 and scm deadline 1701248037957
2023-11-29 08:43:57,957 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248007957 and scm deadline 1701248037957
2023-11-29 08:43:57,957 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 7 milliseconds for processing 6 containers.
2023-11-29 08:43:59,886 [IPC Server handler 64 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:02,958 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248012958 and scm deadline 1701248042958
2023-11-29 08:44:02,959 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248012959 and scm deadline 1701248042959
2023-11-29 08:44:02,959 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248012959 and scm deadline 1701248042959
2023-11-29 08:44:02,959 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248012959 and scm deadline 1701248042959
2023-11-29 08:44:02,960 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:02,962 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248012962 and scm deadline 1701248042962
2023-11-29 08:44:02,962 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248012962 and scm deadline 1701248042962
2023-11-29 08:44:02,963 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248012963 and scm deadline 1701248042963
2023-11-29 08:44:02,963 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248012963 and scm deadline 1701248042963
2023-11-29 08:44:02,963 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248012963 and scm deadline 1701248042963
2023-11-29 08:44:02,963 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248012963 and scm deadline 1701248042963
2023-11-29 08:44:02,963 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 5 milliseconds for processing 6 containers.
2023-11-29 08:44:07,284 [IPC Server handler 40 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:07,964 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248017964 and scm deadline 1701248047964
2023-11-29 08:44:07,965 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248017965 and scm deadline 1701248047965
2023-11-29 08:44:07,965 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248017965 and scm deadline 1701248047965
2023-11-29 08:44:07,965 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248017965 and scm deadline 1701248047965
2023-11-29 08:44:07,966 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248017966 and scm deadline 1701248047966
2023-11-29 08:44:07,966 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248017966 and scm deadline 1701248047966
2023-11-29 08:44:07,966 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248017966 and scm deadline 1701248047966
2023-11-29 08:44:07,966 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248017966 and scm deadline 1701248047966
2023-11-29 08:44:07,966 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248017966 and scm deadline 1701248047966
2023-11-29 08:44:07,967 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248017967 and scm deadline 1701248047967
2023-11-29 08:44:07,967 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 6 containers.
2023-11-29 08:44:12,879 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:40475 / 172.19.0.13:40475
2023-11-29 08:44:12,889 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:44:12,967 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248022967 and scm deadline 1701248052967
2023-11-29 08:44:12,968 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248022968 and scm deadline 1701248052968
2023-11-29 08:44:12,968 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248022968 and scm deadline 1701248052968
2023-11-29 08:44:12,968 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248022968 and scm deadline 1701248052968
2023-11-29 08:44:12,969 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248022969 and scm deadline 1701248052969
2023-11-29 08:44:12,969 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248022969 and scm deadline 1701248052969
2023-11-29 08:44:12,969 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248022969 and scm deadline 1701248052969
2023-11-29 08:44:12,969 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248022969 and scm deadline 1701248052969
2023-11-29 08:44:12,970 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248022970 and scm deadline 1701248052970
2023-11-29 08:44:12,970 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) with datanode deadline 1701248022970 and scm deadline 1701248052970
2023-11-29 08:44:12,970 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 6 containers.
2023-11-29 08:44:13,554 [IPC Server handler 48 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:17,971 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248027971 and scm deadline 1701248057971
2023-11-29 08:44:17,971 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248027971 and scm deadline 1701248057971
2023-11-29 08:44:17,972 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248027972 and scm deadline 1701248057972
2023-11-29 08:44:17,972 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248027972 and scm deadline 1701248057972
2023-11-29 08:44:17,972 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248027972 and scm deadline 1701248057972
2023-11-29 08:44:17,972 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2023-11-29 08:44:22,973 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248032973 and scm deadline 1701248062973
2023-11-29 08:44:22,973 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248032973 and scm deadline 1701248062973
2023-11-29 08:44:22,974 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248032974 and scm deadline 1701248062974
2023-11-29 08:44:22,974 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248032974 and scm deadline 1701248062974
2023-11-29 08:44:22,975 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248032974 and scm deadline 1701248062974
2023-11-29 08:44:22,975 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 6 containers.
2023-11-29 08:44:24,842 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:43817 / 172.19.0.4:43817
2023-11-29 08:44:24,851 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
2023-11-29 08:44:24,851 [IPC Server handler 23 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:26,171 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:36489 / 172.19.0.10:36489
2023-11-29 08:44:26,173 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:44:26,215 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:33637 / 172.19.0.9:33637
2023-11-29 08:44:26,227 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:44:26,233 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:40539 / 172.19.0.8:40539
2023-11-29 08:44:26,242 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:44:27,975 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248037975 and scm deadline 1701248067975
2023-11-29 08:44:27,976 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248037976 and scm deadline 1701248067976
2023-11-29 08:44:27,976 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248037976 and scm deadline 1701248067976
2023-11-29 08:44:27,976 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248037976 and scm deadline 1701248067976
2023-11-29 08:44:27,976 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248037976 and scm deadline 1701248067976
2023-11-29 08:44:27,977 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2023-11-29 08:44:28,888 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:45297 / 172.19.0.4:45297
2023-11-29 08:44:28,889 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:44:28,891 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:31,954 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:32,977 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248042977 and scm deadline 1701248072977
2023-11-29 08:44:32,977 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248042977 and scm deadline 1701248072977
2023-11-29 08:44:32,978 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248042978 and scm deadline 1701248072978
2023-11-29 08:44:32,978 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248042978 and scm deadline 1701248072978
2023-11-29 08:44:32,978 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248042978 and scm deadline 1701248072978
2023-11-29 08:44:32,978 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:44:35,024 [IPC Server handler 44 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:37,979 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248047979 and scm deadline 1701248077979
2023-11-29 08:44:37,980 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248047980 and scm deadline 1701248077980
2023-11-29 08:44:37,980 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248047980 and scm deadline 1701248077980
2023-11-29 08:44:37,981 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248047981 and scm deadline 1701248077981
2023-11-29 08:44:37,983 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248047982 and scm deadline 1701248077982
2023-11-29 08:44:37,983 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 5 milliseconds for processing 6 containers.
2023-11-29 08:44:39,268 [IPC Server handler 40 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:42,984 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248052984 and scm deadline 1701248082984
2023-11-29 08:44:42,984 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248052984 and scm deadline 1701248082984
2023-11-29 08:44:42,985 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248052985 and scm deadline 1701248082985
2023-11-29 08:44:42,985 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248052985 and scm deadline 1701248082985
2023-11-29 08:44:42,985 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248052985 and scm deadline 1701248082985
2023-11-29 08:44:42,985 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2023-11-29 08:44:44,474 [IPC Server handler 53 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:47,986 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248057986 and scm deadline 1701248087986
2023-11-29 08:44:47,986 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248057986 and scm deadline 1701248087986
2023-11-29 08:44:47,987 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248057987 and scm deadline 1701248087987
2023-11-29 08:44:47,987 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248057987 and scm deadline 1701248087987
2023-11-29 08:44:47,987 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248057987 and scm deadline 1701248087987
2023-11-29 08:44:47,987 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2023-11-29 08:44:48,853 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_om_1.ozonesecure_default:44593 / 172.19.0.4:44593
2023-11-29 08:44:48,855 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:44:48,857 [IPC Server handler 79 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:51,918 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:52,988 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248062988 and scm deadline 1701248092988
2023-11-29 08:44:52,988 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248062988 and scm deadline 1701248092988
2023-11-29 08:44:52,988 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248062988 and scm deadline 1701248092988
2023-11-29 08:44:52,989 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248062989 and scm deadline 1701248092989
2023-11-29 08:44:52,989 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248062989 and scm deadline 1701248092989
2023-11-29 08:44:52,989 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2023-11-29 08:44:54,991 [IPC Server handler 44 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:44:56,176 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:32875 / 172.19.0.10:32875
2023-11-29 08:44:56,200 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:44:56,244 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:33359 / 172.19.0.9:33359
2023-11-29 08:44:56,253 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:46283 / 172.19.0.8:46283
2023-11-29 08:44:56,264 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:44:56,275 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:44:57,990 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248067990 and scm deadline 1701248097990
2023-11-29 08:44:57,990 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248067990 and scm deadline 1701248097990
2023-11-29 08:44:57,991 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248067991 and scm deadline 1701248097991
2023-11-29 08:44:57,991 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248067991 and scm deadline 1701248097991
2023-11-29 08:44:57,991 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248067991 and scm deadline 1701248097991
2023-11-29 08:44:57,992 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2023-11-29 08:44:59,316 [IPC Server handler 83 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:45:02,992 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248072992 and scm deadline 1701248102992
2023-11-29 08:45:02,993 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248072993 and scm deadline 1701248102993
2023-11-29 08:45:02,993 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248072993 and scm deadline 1701248102993
2023-11-29 08:45:02,994 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248072994 and scm deadline 1701248102994
2023-11-29 08:45:02,994 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248072994 and scm deadline 1701248102994
2023-11-29 08:45:02,994 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2023-11-29 08:45:05,692 [IPC Server handler 58 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
2023-11-29 08:45:07,995 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248077995 and scm deadline 1701248107995
2023-11-29 08:45:07,995 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248077995 and scm deadline 1701248107995
2023-11-29 08:45:07,996 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248077996 and scm deadline 1701248107996
2023-11-29 08:45:07,996 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248077996 and scm deadline 1701248107996
2023-11-29 08:45:07,996 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248077996 and scm deadline 1701248107996
2023-11-29 08:45:07,996 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:45:12,997 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, force: true] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248082997 and scm deadline 1701248112997
2023-11-29 08:45:12,998 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, force: true] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248082998 and scm deadline 1701248112998
2023-11-29 08:45:12,998 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 3, pipelineID: PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, force: true] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248082998 and scm deadline 1701248112998
2023-11-29 08:45:12,998 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 4, pipelineID: PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, force: true] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248082998 and scm deadline 1701248112998
2023-11-29 08:45:12,999 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 5, pipelineID: PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, force: true] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12) with datanode deadline 1701248082999 and scm deadline 1701248112999
2023-11-29 08:45:12,999 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2023-11-29 08:45:13,656 [EventQueue-DeadNodeForDeadNodeHandler] INFO node.DeadNodeHandler: A dead datanode is detected. 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)
2023-11-29 08:45:13,659 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=4bd0b1e0-0583-4e69-8e4f-18de078b2460 close command to datanode 85e0215a-4c76-4245-8b91-bf1607887fe5
2023-11-29 08:45:13,669 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 4bd0b1e0-0583-4e69-8e4f-18de078b2460, Nodes: 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:85e0215a-4c76-4245-8b91-bf1607887fe5, CreationTimestamp2023-11-29T08:36:05.913Z[UTC]] removed.
2023-11-29 08:45:13,684 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: eae65e74-85ad-4b98-99dd-7ba6697041ec, Nodes: 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2023-11-29T08:36:35.513Z[UTC]] removed.
2023-11-29 08:45:13,687 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e2b7f39d-6249-4a5c-94fa-70ffde895a82, Nodes: cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10)18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2023-11-29T08:36:42.522Z[UTC]] removed.
2023-11-29 08:45:13,694 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 92c34565-a825-4b90-bc39-d6b5c93d0c86, Nodes: cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2023-11-29T08:36:38.862Z[UTC]] removed.
2023-11-29 08:45:13,705 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a58aa67b-2252-4159-a629-cb289acc086e, Nodes: 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2023-11-29T08:36:41.263Z[UTC]] removed.
2023-11-29 08:45:13,707 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3d6c627a-77f1-4bff-9c80-67c20c639ef2, Nodes: 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9)1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8)cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2023-11-29T08:36:42.761Z[UTC]] removed.
2023-11-29 08:45:13,710 [EventQueue-DeadNodeForDeadNodeHandler] INFO node.DeadNodeHandler: Clearing command queue of size 96 for DN 85e0215a-4c76-4245-8b91-bf1607887fe5(ozonesecure_datanode_5.ozonesecure_default/172.19.0.12)
2023-11-29 08:45:13,710 [EventQueue-DeadNodeForDeadNodeHandler] INFO net.NetworkTopologyImpl: Removed a node: /default-rack/85e0215a-4c76-4245-8b91-bf1607887fe5
2023-11-29 08:45:14,620 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:38499 / 172.19.0.2:38499
2023-11-29 08:45:14,631 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:45:16,925 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:33743 / 172.19.0.13:33743
2023-11-29 08:45:16,962 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-11-29 08:45:16,962 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: a7933ea8-2a7b-4502-a59f-1d9a107921c7
2023-11-29 08:45:16,966 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 9 to 10.
2023-11-29 08:45:16,971 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-29 08:45:16,971 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-29 08:45:16,978 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-29 08:45:17,218 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-29 08:45:17,218 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-29 08:45:17,312 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:43689 / 172.19.0.12:43689
2023-11-29 08:45:17,335 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
2023-11-29 08:45:17,336 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn dn, UUID: 70894f80-2c29-44a0-9c28-03984a644300
2023-11-29 08:45:17,341 [IPC Server handler 0 on default port 9961] INFO ha.SequenceIdGenerator: Allocate a batch for CertificateId, change lastId from 10 to 11.
2023-11-29 08:45:17,349 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions in CSR: 2.5.29.15, 2.5.29.17
2023-11-29 08:45:17,350 [IPC Server handler 0 on default port 9961] INFO authority.DefaultApprover: Extensions to add to the certificate if they present in CSR: 2.5.29.17, 2.5.29.19, 1.3.6.1.5.5.7.1.12, 2.5.29.35, 2.5.29.15, 2.5.29.37
2023-11-29 08:45:17,359 [IPC Server handler 0 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2023-11-29 08:45:17,580 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 0 Root CA certificates
2023-11-29 08:45:17,580 [IPC Server handler 1 on default port 9961] INFO client.SCMCertificateClient: scm/sub-ca has 1 CA certificates
2023-11-29 08:45:17,661 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:43243 / 172.19.0.13:43243
2023-11-29 08:45:17,686 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:45:17,918 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:33019 / 172.19.0.12:33019
2023-11-29 08:45:17,954 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:45:18,003 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 6 containers.
2023-11-29 08:45:20,607 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:42805 / 172.19.0.6:42805
2023-11-29 08:45:20,671 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:45:22,937 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:100)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:777)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2023-11-29 08:45:22,938 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:100)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:777)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2023-11-29 08:45:22,938 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:100)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:777)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2023-11-29 08:45:22,939 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:100)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:777)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2023-11-29 08:45:22,939 [UnderReplicatedProcessor] ERROR replication.UnhealthyReplicationProcessor: Error processing Health result of class: class org.apache.hadoop.hdds.scm.container.replication.ContainerHealthResult$UnderReplicatedHealthResult for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault}
org.apache.hadoop.hdds.scm.exceptions.SCMException: Placement Policy: class org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter did not return any nodes. Number of required Nodes 1, Datasize Required: 1073741824
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManagerUtil.getTargetDatanodes(ReplicationManagerUtil.java:100)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.getTargetDatanodes(ECUnderReplicationHandler.java:412)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processMissingIndexes(ECUnderReplicationHandler.java:305)
	at org.apache.hadoop.hdds.scm.container.replication.ECUnderReplicationHandler.processAndSendCommands(ECUnderReplicationHandler.java:161)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processUnderReplicatedContainer(ReplicationManager.java:777)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:60)
	at org.apache.hadoop.hdds.scm.container.replication.UnderReplicatedProcessor.sendDatanodeCommands(UnderReplicatedProcessor.java:29)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processContainer(UnhealthyReplicationProcessor.java:156)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.processAll(UnhealthyReplicationProcessor.java:116)
	at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:165)
	at java.base/java.lang.Thread.run(Thread.java:829)
2023-11-29 08:45:22,939 [UnderReplicatedProcessor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {}, failed processing 5, deferred due to load 0
2023-11-29 08:45:23,004 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:45:24,646 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:44177 / 172.19.0.13:44177
2023-11-29 08:45:24,668 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:45:24,943 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:32969 / 172.19.0.12:32969
2023-11-29 08:45:24,952 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:45:26,172 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:40207 / 172.19.0.10:40207
2023-11-29 08:45:26,175 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:45:26,214 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:42161 / 172.19.0.9:42161
2023-11-29 08:45:26,224 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:45:26,236 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:43911 / 172.19.0.8:43911
2023-11-29 08:45:26,239 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:45:26,638 [IPC Server handler 45 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a7933ea8-2a7b-4502-a59f-1d9a107921c7
2023-11-29 08:45:26,639 [IPC Server handler 45 on default port 9861] INFO node.SCMNodeManager: Registered datanode: a7933ea8-2a7b-4502-a59f-1d9a107921c7{ip: 172.19.0.13, host: ozonesecure_datanode_5.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 10, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-11-29 08:45:26,639 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-11-29 08:45:26,640 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=f5c35e47-491b-49af-8e04-70479deae1b1 to datanode:a7933ea8-2a7b-4502-a59f-1d9a107921c7
2023-11-29 08:45:26,643 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: f5c35e47-491b-49af-8e04-70479deae1b1, Nodes: a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:45:26.640176Z[UTC]]
2023-11-29 08:45:26,925 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/70894f80-2c29-44a0-9c28-03984a644300
2023-11-29 08:45:26,925 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered datanode: 70894f80-2c29-44a0-9c28-03984a644300{ip: 172.19.0.12, host: ozonesecure_datanode_4.ozonesecure_default, ports: [HTTP=9882, CLIENT_RPC=19864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, RATIS_DATASTREAM=9855, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 11, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-11-29 08:45:26,926 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
2023-11-29 08:45:26,926 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c3ad7445-9dc4-4d94-b54f-3c31eb3d1d7d to datanode:70894f80-2c29-44a0-9c28-03984a644300
2023-11-29 08:45:26,928 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: c3ad7445-9dc4-4d94-b54f-3c31eb3d1d7d, Nodes: 70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:45:26.926588Z[UTC]]
2023-11-29 08:45:26,929 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=356cedd4-db62-4d9e-bb55-659eb39e6829 to datanode:a7933ea8-2a7b-4502-a59f-1d9a107921c7
2023-11-29 08:45:26,929 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=356cedd4-db62-4d9e-bb55-659eb39e6829 to datanode:70894f80-2c29-44a0-9c28-03984a644300
2023-11-29 08:45:26,929 [RatisPipelineUtilsThread-0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=356cedd4-db62-4d9e-bb55-659eb39e6829 to datanode:14edb126-374b-48c1-9512-07b6e7b411d0
2023-11-29 08:45:26,930 [RatisPipelineUtilsThread-0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 356cedd4-db62-4d9e-bb55-659eb39e6829, Nodes: a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:45:26.929223Z[UTC]]
2023-11-29 08:45:27,940 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)]. isPolicySatisfied: true.
2023-11-29 08:45:27,942 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode 70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12) and command replicateContainerCommand. Assuming zero
2023-11-29 08:45:27,942 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode 70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12) and command reconstructECContainersCommand. Assuming zero
2023-11-29 08:45:27,942 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 1, replicationConfig: EC{rs-3-2-1024k}, sources: [18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) replicaIndex: 2, 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 3, 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) replicaIndex: 4, cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) replicaIndex: 5], targets: [70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)], missingIndexes: [1]] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to 70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12) with datanode deadline 1701248097942 and scm deadline 1701248127942
2023-11-29 08:45:27,943 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)]. isPolicySatisfied: true.
2023-11-29 08:45:27,943 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13) and command replicateContainerCommand. Assuming zero
2023-11-29 08:45:27,943 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13) and command reconstructECContainersCommand. Assuming zero
2023-11-29 08:45:27,944 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 5, replicationConfig: EC{rs-3-2-1024k}, sources: [18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) replicaIndex: 1, 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 2, cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) replicaIndex: 3, 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) replicaIndex: 4], targets: [a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)], missingIndexes: [5]] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13) with datanode deadline 1701248097943 and scm deadline 1701248127943
2023-11-29 08:45:27,944 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)]. isPolicySatisfied: true.
2023-11-29 08:45:27,944 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13) and command replicateContainerCommand. Assuming zero
2023-11-29 08:45:27,944 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13) and command reconstructECContainersCommand. Assuming zero
2023-11-29 08:45:27,944 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 4, replicationConfig: EC{rs-3-2-1024k}, sources: [cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) replicaIndex: 1, 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) replicaIndex: 2, 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 3, 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) replicaIndex: 5], targets: [a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)], missingIndexes: [4]] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13) with datanode deadline 1701248097944 and scm deadline 1701248127944
2023-11-29 08:45:27,944 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)]. isPolicySatisfied: true.
2023-11-29 08:45:27,944 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13) and command replicateContainerCommand. Assuming zero
2023-11-29 08:45:27,944 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13) and command reconstructECContainersCommand. Assuming zero
2023-11-29 08:45:27,944 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 3, replicationConfig: EC{rs-3-2-1024k}, sources: [18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) replicaIndex: 1, 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 2, 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) replicaIndex: 4, cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) replicaIndex: 5], targets: [a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)], missingIndexes: [3]] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13) with datanode deadline 1701248097944 and scm deadline 1701248127944
2023-11-29 08:45:27,945 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)]. isPolicySatisfied: true.
2023-11-29 08:45:27,945 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode 70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12) and command replicateContainerCommand. Assuming zero
2023-11-29 08:45:27,945 [UnderReplicatedProcessor] WARN node.SCMNodeManager: No command count information for datanode 70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12) and command reconstructECContainersCommand. Assuming zero
2023-11-29 08:45:27,945 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 2, replicationConfig: EC{rs-3-2-1024k}, sources: [cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) replicaIndex: 1, 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 2, 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) replicaIndex: 3, 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) replicaIndex: 4], targets: [70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)], missingIndexes: [5]] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to 70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12) with datanode deadline 1701248097945 and scm deadline 1701248127945
2023-11-29 08:45:27,945 [UnderReplicatedProcessor] INFO replication.UnhealthyReplicationProcessor: Processed 5 containers with health state counts {UNDER_REPLICATED=5}, failed processing 0, deferred due to load 0
2023-11-29 08:45:28,005 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:45:29,782 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=f5c35e47-491b-49af-8e04-70479deae1b1
2023-11-29 08:45:30,045 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:43725 / 172.19.0.2:43725
2023-11-29 08:45:30,048 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:45:30,208 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=c3ad7445-9dc4-4d94-b54f-3c31eb3d1d7d
2023-11-29 08:45:30,888 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:44737 / 172.19.0.13:44737
2023-11-29 08:45:30,907 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:45:33,006 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:45:34,615 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:34875 / 172.19.0.6:34875
2023-11-29 08:45:34,627 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:45:38,007 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:45:43,007 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:45:44,599 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:39831 / 172.19.0.2:39831
2023-11-29 08:45:44,600 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:45:46,662 [EventQueue-StaleNodeForStaleNodeHandler] INFO node.StaleNodeHandler: Datanode 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13) moved to stale state. Finalizing its pipelines [PipelineID=356cedd4-db62-4d9e-bb55-659eb39e6829, PipelineID=d117c241-66cb-46b8-b36f-060fb8f1d309]
2023-11-29 08:45:46,666 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 356cedd4-db62-4d9e-bb55-659eb39e6829, Nodes: a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-11-29T08:45:26.929Z[UTC]] moved to CLOSED state
2023-11-29 08:45:46,669 [EventQueue-StaleNodeForStaleNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d117c241-66cb-46b8-b36f-060fb8f1d309, Nodes: 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:14edb126-374b-48c1-9512-07b6e7b411d0, CreationTimestamp2023-11-29T08:36:05.061Z[UTC]] moved to CLOSED state
2023-11-29 08:45:48,008 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:45:48,094 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:40715 / 172.19.0.6:40715
2023-11-29 08:45:48,105 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:45:51,159 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:43781 / 172.19.0.12:43781
2023-11-29 08:45:51,193 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:45:53,009 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:45:56,167 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:46783 / 172.19.0.10:46783
2023-11-29 08:45:56,170 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:45:56,224 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:40803 / 172.19.0.9:40803
2023-11-29 08:45:56,234 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:45:56,242 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:46631 / 172.19.0.8:46631
2023-11-29 08:45:56,249 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:45:58,010 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:46:01,425 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:39037 / 172.19.0.6:39037
2023-11-29 08:46:01,441 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:46:03,011 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:46:05,108 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:37419 / 172.19.0.13:37419
2023-11-29 08:46:05,113 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:46:08,012 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:46:13,012 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:46:14,666 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:34551 / 172.19.0.6:34551
2023-11-29 08:46:14,675 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:46:18,013 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:46:21,155 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:38081 / 172.19.0.12:38081
2023-11-29 08:46:21,157 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:46:23,014 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:46:26,171 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:33479 / 172.19.0.10:33479
2023-11-29 08:46:26,179 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:46:26,236 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:46553 / 172.19.0.9:46553
2023-11-29 08:46:26,247 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:46:26,247 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:45597 / 172.19.0.8:45597
2023-11-29 08:46:26,273 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:46:27,978 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:35027 / 172.19.0.6:35027
2023-11-29 08:46:27,989 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:46:28,015 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:46:33,016 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:46:35,111 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:36791 / 172.19.0.13:36791
2023-11-29 08:46:35,128 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:46:37,698 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:36981 / 172.19.0.2:36981
2023-11-29 08:46:37,700 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:46:38,017 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:46:41,381 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:40221 / 172.19.0.6:40221
2023-11-29 08:46:41,392 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:46:43,017 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:46:48,018 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:46:51,153 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:33745 / 172.19.0.12:33745
2023-11-29 08:46:51,163 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:46:53,019 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:46:54,670 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:34697 / 172.19.0.6:34697
2023-11-29 08:46:54,684 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:46:56,172 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:35981 / 172.19.0.10:35981
2023-11-29 08:46:56,178 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:46:56,214 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:39137 / 172.19.0.9:39137
2023-11-29 08:46:56,218 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:46:56,232 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:46841 / 172.19.0.8:46841
2023-11-29 08:46:56,248 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:46:58,020 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:47:03,020 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:47:05,125 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:38761 / 172.19.0.13:38761
2023-11-29 08:47:05,128 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:47:07,837 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:39631 / 172.19.0.6:39631
2023-11-29 08:47:07,860 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:47:08,021 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:47:13,022 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:47:14,605 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_recon_1.ozonesecure_default:39531 / 172.19.0.2:39531
2023-11-29 08:47:14,606 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:47:16,666 [EventQueue-DeadNodeForDeadNodeHandler] INFO node.DeadNodeHandler: A dead datanode is detected. 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)
2023-11-29 08:47:16,666 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=356cedd4-db62-4d9e-bb55-659eb39e6829 close command to datanode a7933ea8-2a7b-4502-a59f-1d9a107921c7
2023-11-29 08:47:16,666 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=356cedd4-db62-4d9e-bb55-659eb39e6829 close command to datanode 70894f80-2c29-44a0-9c28-03984a644300
2023-11-29 08:47:16,666 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=356cedd4-db62-4d9e-bb55-659eb39e6829 close command to datanode 14edb126-374b-48c1-9512-07b6e7b411d0
2023-11-29 08:47:16,669 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 356cedd4-db62-4d9e-bb55-659eb39e6829, Nodes: a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:70894f80-2c29-44a0-9c28-03984a644300, CreationTimestamp2023-11-29T08:45:26.929Z[UTC]] removed.
2023-11-29 08:47:16,669 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=d117c241-66cb-46b8-b36f-060fb8f1d309 close command to datanode 14edb126-374b-48c1-9512-07b6e7b411d0
2023-11-29 08:47:16,670 [EventQueue-DeadNodeForDeadNodeHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d117c241-66cb-46b8-b36f-060fb8f1d309, Nodes: 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:14edb126-374b-48c1-9512-07b6e7b411d0, CreationTimestamp2023-11-29T08:36:05.061Z[UTC]] removed.
2023-11-29 08:47:16,670 [EventQueue-DeadNodeForDeadNodeHandler] INFO node.DeadNodeHandler: Clearing command queue of size 3 for DN 14edb126-374b-48c1-9512-07b6e7b411d0(ozonesecure_datanode_4.ozonesecure_default/172.19.0.13)
2023-11-29 08:47:16,670 [EventQueue-DeadNodeForDeadNodeHandler] INFO net.NetworkTopologyImpl: Removed a node: /default-rack/14edb126-374b-48c1-9512-07b6e7b411d0
2023-11-29 08:47:18,023 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:47:21,056 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:34433 / 172.19.0.6:34433
2023-11-29 08:47:21,071 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:47:21,154 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:37857 / 172.19.0.12:37857
2023-11-29 08:47:21,160 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:47:21,162 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=356cedd4-db62-4d9e-bb55-659eb39e6829 is not found
2023-11-29 08:47:22,951 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)]. isPolicySatisfied: true.
2023-11-29 08:47:22,951 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 1, replicationConfig: EC{rs-3-2-1024k}, sources: [18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) replicaIndex: 2, 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 3, cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) replicaIndex: 5], targets: [a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)], missingIndexes: [4]] for container ContainerInfo{id=#1, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.147469Z, pipelineID=PipelineID=eae65e74-85ad-4b98-99dd-7ba6697041ec, owner=omServiceIdDefault} to a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13) with datanode deadline 1701248212951 and scm deadline 1701248242951
2023-11-29 08:47:22,952 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)]. isPolicySatisfied: true.
2023-11-29 08:47:22,952 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 5, replicationConfig: EC{rs-3-2-1024k}, sources: [18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) replicaIndex: 1, 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 2, cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) replicaIndex: 3], targets: [70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)], missingIndexes: [4]] for container ContainerInfo{id=#5, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.212023Z, pipelineID=PipelineID=3d6c627a-77f1-4bff-9c80-67c20c639ef2, owner=omServiceIdDefault} to 70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12) with datanode deadline 1701248212952 and scm deadline 1701248242952
2023-11-29 08:47:22,952 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)]. isPolicySatisfied: true.
2023-11-29 08:47:22,952 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 4, replicationConfig: EC{rs-3-2-1024k}, sources: [cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) replicaIndex: 1, 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) replicaIndex: 2, 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 3], targets: [70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)], missingIndexes: [5]] for container ContainerInfo{id=#4, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.164117Z, pipelineID=PipelineID=e2b7f39d-6249-4a5c-94fa-70ffde895a82, owner=omServiceIdDefault} to 70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12) with datanode deadline 1701248212952 and scm deadline 1701248242952
2023-11-29 08:47:22,952 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)]. isPolicySatisfied: true.
2023-11-29 08:47:22,952 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 3, replicationConfig: EC{rs-3-2-1024k}, sources: [18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) replicaIndex: 1, 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 2, cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) replicaIndex: 5], targets: [70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12)], missingIndexes: [4]] for container ContainerInfo{id=#3, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.117976Z, pipelineID=PipelineID=a58aa67b-2252-4159-a629-cb289acc086e, owner=omServiceIdDefault} to 70894f80-2c29-44a0-9c28-03984a644300(ozonesecure_datanode_4.ozonesecure_default/172.19.0.12) with datanode deadline 1701248212952 and scm deadline 1701248242952
2023-11-29 08:47:22,953 [UnderReplicatedProcessor] INFO algorithms.SCMContainerPlacementRackScatter: Chosen nodes: [a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)]. isPolicySatisfied: true.
2023-11-29 08:47:22,953 [UnderReplicatedProcessor] INFO replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 2, replicationConfig: EC{rs-3-2-1024k}, sources: [cc7a6c85-4ef7-435a-ad17-19227fb0f721(ozonesecure_datanode_3.ozonesecure_default/172.19.0.10) replicaIndex: 1, 1ca5126a-7d7d-4d7f-a939-29db03d1fab7(ozonesecure_datanode_1.ozonesecure_default/172.19.0.8) replicaIndex: 2, 18573edc-19c5-4d66-af25-cb4aed99fc20(ozonesecure_datanode_2.ozonesecure_default/172.19.0.9) replicaIndex: 3], targets: [a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13)], missingIndexes: [4]] for container ContainerInfo{id=#2, state=CLOSED, stateEnterTime=2023-11-29T08:43:56.119796Z, pipelineID=PipelineID=92c34565-a825-4b90-bc39-d6b5c93d0c86, owner=omServiceIdDefault} to a7933ea8-2a7b-4502-a59f-1d9a107921c7(ozonesecure_datanode_5.ozonesecure_default/172.19.0.13) with datanode deadline 1701248212953 and scm deadline 1701248242953
2023-11-29 08:47:22,953 [UnderReplicatedProcessor] INFO replication.UnhealthyReplicationProcessor: Processed 5 containers with health state counts {UNDER_REPLICATED=5}, failed processing 0, deferred due to load 0
2023-11-29 08:47:23,024 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:47:26,170 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:33371 / 172.19.0.10:33371
2023-11-29 08:47:26,180 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:47:26,216 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:41399 / 172.19.0.9:41399
2023-11-29 08:47:26,224 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:47:26,234 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:34807 / 172.19.0.8:34807
2023-11-29 08:47:26,244 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:47:28,025 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:47:33,026 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:47:34,337 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:44731 / 172.19.0.6:44731
2023-11-29 08:47:34,349 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:47:35,109 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:34093 / 172.19.0.13:34093
2023-11-29 08:47:35,115 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:47:38,031 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:47:43,031 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:47:47,529 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:36251 / 172.19.0.6:36251
2023-11-29 08:47:47,538 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:47:48,032 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:47:51,152 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:43895 / 172.19.0.12:43895
2023-11-29 08:47:51,155 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:47:52,291 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:38283 / 172.19.0.12:38283
2023-11-29 08:47:52,300 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SecretKeyProtocolDatanode
2023-11-29 08:47:53,033 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:47:56,168 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:41379 / 172.19.0.10:41379
2023-11-29 08:47:56,171 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:47:56,216 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:33627 / 172.19.0.9:33627
2023-11-29 08:47:56,219 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:47:56,236 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:35575 / 172.19.0.8:35575
2023-11-29 08:47:56,238 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:47:58,034 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:48:00,725 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:36569 / 172.19.0.6:36569
2023-11-29 08:48:00,740 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:48:03,034 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:48:08,035 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:48:08,099 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:35081 / 172.19.0.13:35081
2023-11-29 08:48:08,104 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:48:13,036 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:48:13,907 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:46511 / 172.19.0.6:46511
2023-11-29 08:48:13,924 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:48:18,038 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:48:23,039 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:48:25,338 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:37921 / 172.19.0.12:37921
2023-11-29 08:48:25,341 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:48:26,176 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:38681 / 172.19.0.10:38681
2023-11-29 08:48:26,189 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:48:26,252 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:45565 / 172.19.0.9:45565
2023-11-29 08:48:26,284 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:48:26,303 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:39997 / 172.19.0.8:39997
2023-11-29 08:48:26,314 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:48:27,084 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:39499 / 172.19.0.6:39499
2023-11-29 08:48:27,095 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:48:28,039 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:48:33,040 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:48:38,041 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:48:38,110 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:34637 / 172.19.0.13:34637
2023-11-29 08:48:38,116 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:48:40,274 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:44539 / 172.19.0.6:44539
2023-11-29 08:48:40,284 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:48:43,041 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:48:48,042 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:48:53,043 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:48:53,350 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:43189 / 172.19.0.6:43189
2023-11-29 08:48:53,362 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:48:55,321 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:33797 / 172.19.0.12:33797
2023-11-29 08:48:55,330 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:48:56,167 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:33131 / 172.19.0.10:33131
2023-11-29 08:48:56,172 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:48:56,214 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:36125 / 172.19.0.9:36125
2023-11-29 08:48:56,218 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:48:56,236 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:40615 / 172.19.0.8:40615
2023-11-29 08:48:56,257 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:48:58,043 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:49:03,044 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:49:06,468 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:34331 / 172.19.0.6:34331
2023-11-29 08:49:06,488 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:49:08,044 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:49:08,106 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:40269 / 172.19.0.13:40269
2023-11-29 08:49:08,112 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:49:13,045 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:49:18,046 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:49:19,623 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:33151 / 172.19.0.6:33151
2023-11-29 08:49:19,636 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:49:23,046 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:49:25,338 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:38729 / 172.19.0.12:38729
2023-11-29 08:49:25,341 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:49:26,171 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:39267 / 172.19.0.10:39267
2023-11-29 08:49:26,179 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:49:26,216 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:40079 / 172.19.0.9:40079
2023-11-29 08:49:26,225 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:49:26,236 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:46227 / 172.19.0.8:46227
2023-11-29 08:49:26,240 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:49:28,047 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:49:32,978 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:39419 / 172.19.0.6:39419
2023-11-29 08:49:32,992 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:49:33,047 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:49:38,048 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:49:38,100 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:34673 / 172.19.0.13:34673
2023-11-29 08:49:38,104 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:49:43,048 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:49:46,106 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:37489 / 172.19.0.6:37489
2023-11-29 08:49:46,118 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:49:48,049 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:49:53,049 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:49:55,319 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_4.ozonesecure_default:45245 / 172.19.0.12:45245
2023-11-29 08:49:55,322 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:49:56,169 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_3.ozonesecure_default:41151 / 172.19.0.10:41151
2023-11-29 08:49:56,182 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:49:56,216 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_2.ozonesecure_default:46495 / 172.19.0.9:46495
2023-11-29 08:49:56,234 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:49:56,243 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_1.ozonesecure_default:45609 / 172.19.0.8:45609
2023-11-29 08:49:56,249 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:49:58,050 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:49:59,115 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:36661 / 172.19.0.6:36661
2023-11-29 08:49:59,130 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:50:03,051 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:50:08,051 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-11-29 08:50:08,102 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from ozonesecure_datanode_5.ozonesecure_default:37517 / 172.19.0.13:37517
2023-11-29 08:50:08,107 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
2023-11-29 08:50:12,420 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from scm:35597 / 172.19.0.6:35597
2023-11-29 08:50:12,430 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
2023-11-29 08:50:13,052 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-11-29 08:50:18,052 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 6 containers.
