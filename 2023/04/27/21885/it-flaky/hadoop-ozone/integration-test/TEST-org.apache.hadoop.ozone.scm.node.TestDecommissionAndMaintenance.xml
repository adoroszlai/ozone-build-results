<?xml version="1.0" encoding="UTF-8"?>
<testsuite xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="https://maven.apache.org/surefire/maven-surefire-plugin/xsd/surefire-test-report-3.0.xsd" version="3.0" name="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="220.507" tests="7" errors="2" skipped="0" failures="0">
  <properties>
    <property name="awt.toolkit" value="sun.awt.X11.XToolkit"/>
    <property name="file.encoding.pkg" value="sun.io"/>
    <property name="java.specification.version" value="1.8"/>
    <property name="sun.cpu.isalist" value=""/>
    <property name="sun.jnu.encoding" value="UTF-8"/>
    <property name="java.class.path" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/classes:/home/runner/.m2/repository/org/apache/ozone/ozone-common/1.4.0-SNAPSHOT/ozone-common-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/io/grpc/grpc-netty/1.51.1/grpc-netty-1.51.1.jar:/home/runner/.m2/repository/io/grpc/grpc-core/1.51.1/grpc-core-1.51.1.jar:/home/runner/.m2/repository/com/google/android/annotations/4.1.1.4/annotations-4.1.1.4.jar:/home/runner/.m2/repository/org/codehaus/mojo/animal-sniffer-annotations/1.21/animal-sniffer-annotations-1.21.jar:/home/runner/.m2/repository/com/google/errorprone/error_prone_annotations/2.2.0/error_prone_annotations-2.2.0.jar:/home/runner/.m2/repository/io/perfmark/perfmark-api/0.25.0/perfmark-api-0.25.0.jar:/home/runner/.m2/repository/io/netty/netty-codec-http2/4.1.86.Final/netty-codec-http2-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-common/4.1.86.Final/netty-common-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-buffer/4.1.86.Final/netty-buffer-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-http/4.1.86.Final/netty-codec-http-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-handler-proxy/4.1.86.Final/netty-handler-proxy-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-socks/4.1.86.Final/netty-codec-socks-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.54.Final/netty-tcnative-boringssl-static-2.0.54.Final.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-classes/2.0.54.Final/netty-tcnative-classes-2.0.54.Final.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.54.Final/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.54.Final/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.54.Final/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.54.Final/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.54.Final/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/home/runner/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-common/1.4.0-SNAPSHOT/hdds-common-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-client/1.4.0-SNAPSHOT/hdds-client-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-interface-client/1.4.0-SNAPSHOT/ozone-interface-client-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.3.5/hadoop-hdfs-client-3.3.5.jar:/home/runner/.m2/repository/com/squareup/okhttp3/okhttp/4.9.3/okhttp-4.9.3.jar:/home/runner/.m2/repository/com/squareup/okio/okio/2.8.0/okio-2.8.0.jar:/home/runner/.m2/repository/org/jetbrains/kotlin/kotlin-stdlib-common/1.6.21/kotlin-stdlib-common-1.6.21.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-test-utils/1.4.0-SNAPSHOT/hdds-test-utils-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/google/guava/guava/31.1-jre/guava-31.1-jre.jar:/home/runner/.m2/repository/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.jar:/home/runner/.m2/repository/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/runner/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/runner/.m2/repository/org/checkerframework/checker-qual/3.12.0/checker-qual-3.12.0.jar:/home/runner/.m2/repository/com/google/j2objc/j2objc-annotations/1.3/j2objc-annotations-1.3.jar:/home/runner/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/home/runner/.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/home/runner/.m2/repository/ch/qos/reload4j/reload4j/1.2.22/reload4j-1.2.22.jar:/home/runner/.m2/repository/org/slf4j/slf4j-api/1.7.36/slf4j-api-1.7.36.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-scm/1.4.0-SNAPSHOT/hdds-server-scm-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-hadoop-dependency-server/1.4.0-SNAPSHOT/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.3.5/hadoop-hdfs-3.3.5.jar:/home/runner/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-framework/1.4.0-SNAPSHOT/hdds-server-framework-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/io/dropwizard/metrics/metrics-core/3.2.4/metrics-core-3.2.4.jar:/home/runner/.m2/repository/org/apache/commons/commons-text/1.4/commons-text-1.4.jar:/home/runner/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.67/bcprov-jdk15on-1.67.jar:/home/runner/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-framework/1.4.0-SNAPSHOT/hdds-server-framework-1.4.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-interface-server/1.4.0-SNAPSHOT/hdds-interface-server-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-interface-admin/1.4.0-SNAPSHOT/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-managed-rocksdb/1.4.0-SNAPSHOT/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/rocksdb/rocksdbjni/7.7.3/rocksdbjni-7.7.3.jar:/home/runner/.m2/repository/org/slf4j/slf4j-reload4j/1.7.36/slf4j-reload4j-1.7.36.jar:/home/runner/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/runner/.m2/repository/org/apache/logging/log4j/log4j-core/2.17.1/log4j-core-2.17.1.jar:/home/runner/.m2/repository/com/lmax/disruptor/3.4.2/disruptor-3.4.2.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-util/9.4.51.v20230217/jetty-util-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-server/9.4.51.v20230217/jetty-server-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-http/9.4.51.v20230217/jetty-http-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-io/9.4.51.v20230217/jetty-io-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-servlet/9.4.51.v20230217/jetty-servlet-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-security/9.4.51.v20230217/jetty-security-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.4.51.v20230217/jetty-util-ajax-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-webapp/9.4.51.v20230217/jetty-webapp-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-xml/9.4.51.v20230217/jetty-xml-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-server/2.4.2-8b8bdda-SNAPSHOT/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-thirdparty-misc/1.0.3/ratis-thirdparty-misc-1.0.3.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-proto/2.4.2-8b8bdda-SNAPSHOT/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-common/2.4.2-8b8bdda-SNAPSHOT/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-client/2.4.2-8b8bdda-SNAPSHOT/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-server-api/2.4.2-8b8bdda-SNAPSHOT/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-metrics/2.4.2-8b8bdda-SNAPSHOT/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/io/prometheus/simpleclient_dropwizard/0.7.0/simpleclient_dropwizard-0.7.0.jar:/home/runner/.m2/repository/io/prometheus/simpleclient/0.7.0/simpleclient-0.7.0.jar:/home/runner/.m2/repository/io/prometheus/simpleclient_common/0.7.0/simpleclient_common-0.7.0.jar:/home/runner/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.13.4/jackson-datatype-jsr310-2.13.4.jar:/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.13.4/jackson-core-2.13.4.jar:/home/runner/.m2/repository/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar:/home/runner/.m2/repository/org/apache/ozone/rocksdb-checkpoint-differ/1.4.0-SNAPSHOT/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/awaitility/awaitility/4.2.0/awaitility-4.2.0.jar:/home/runner/.m2/repository/org/hamcrest/hamcrest/2.1/hamcrest-2.1.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-manager/1.4.0-SNAPSHOT/ozone-manager-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/aspectj/aspectjrt/1.9.7/aspectjrt-1.9.7.jar:/home/runner/.m2/repository/org/aspectj/aspectjweaver/1.9.7/aspectjweaver-1.9.7.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-interface-client/1.4.0-SNAPSHOT/hdds-interface-client-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/thirdparty/hadoop-shaded-protobuf_3_7/1.1.1/hadoop-shaded-protobuf_3_7-1.1.1.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-interface-storage/1.4.0-SNAPSHOT/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-intg/2.3.0/ranger-intg-2.3.0.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugins-common/2.3.0/ranger-plugins-common-2.3.0.jar:/home/runner/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugins-cred/2.3.0/ranger-plugins-cred-2.3.0.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugins-audit/2.3.0/ranger-plugins-audit-2.3.0.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-client/9.4.51.v20230217/jetty-client-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpmime/4.5.6/httpmime-4.5.6.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpcore-nio/4.4.13/httpcore-nio-4.4.13.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpasyncclient/4.1.3/httpasyncclient-4.1.3.jar:/home/runner/.m2/repository/com/carrotsearch/hppc/0.8.0/hppc-0.8.0.jar:/home/runner/.m2/repository/org/apache/orc/orc-core/1.5.8/orc-core-1.5.8.jar:/home/runner/.m2/repository/net/java/dev/jna/jna/5.2.0/jna-5.2.0.jar:/home/runner/.m2/repository/net/java/dev/jna/jna-platform/5.2.0/jna-platform-5.2.0.jar:/home/runner/.m2/repository/com/kstruct/gethostname4j/0.0.2/gethostname4j-0.0.2.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugin-classloader/2.3.0/ranger-plugin-classloader-2.3.0.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-rocks-native/1.4.0-SNAPSHOT/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-minikdc/3.3.5/hadoop-minikdc-3.3.5.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-s3gateway/1.4.0-SNAPSHOT/ozone-s3gateway-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar:/home/runner/.m2/repository/org/jboss/weld/servlet/weld-servlet-shaded/3.1.9.Final/weld-servlet-shaded-3.1.9.Final.jar:/home/runner/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.34/jersey-container-servlet-core-2.34.jar:/home/runner/.m2/repository/org/glassfish/hk2/external/jakarta.inject/2.6.1/jakarta.inject-2.6.1.jar:/home/runner/.m2/repository/org/glassfish/jersey/core/jersey-common/2.34/jersey-common-2.34.jar:/home/runner/.m2/repository/jakarta/ws/rs/jakarta.ws.rs-api/2.1.6/jakarta.ws.rs-api-2.1.6.jar:/home/runner/.m2/repository/org/glassfish/jersey/ext/cdi/jersey-cdi1x/2.34/jersey-cdi1x-2.34.jar:/home/runner/.m2/repository/org/glassfish/jersey/inject/jersey-hk2/2.34/jersey-hk2-2.34.jar:/home/runner/.m2/repository/org/glassfish/hk2/hk2-locator/2.6.1/hk2-locator-2.6.1.jar:/home/runner/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.34/jersey-media-jaxb-2.34.jar:/home/runner/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.3/osgi-resource-locator-1.0.3.jar:/home/runner/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0/hk2-api-2.5.0.jar:/home/runner/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0/hk2-utils-2.5.0.jar:/home/runner/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0/aopalliance-repackaged-2.5.0.jar:/home/runner/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-xml/2.13.4/jackson-dataformat-xml-2.13.4.jar:/home/runner/.m2/repository/org/codehaus/woodstox/stax2-api/4.2.1/stax2-api-4.2.1.jar:/home/runner/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.4.0/woodstox-core-5.4.0.jar:/home/runner/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.13.4/jackson-module-jaxb-annotations-2.13.4.jar:/home/runner/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.3/jakarta.xml.bind-api-2.3.3.jar:/home/runner/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.2/jakarta.activation-api-1.2.2.jar:/home/runner/.m2/repository/javax/enterprise/cdi-api/2.0/cdi-api-2.0.jar:/home/runner/.m2/repository/javax/el/javax.el-api/3.0.0/javax.el-api-3.0.0.jar:/home/runner/.m2/repository/javax/interceptor/javax.interceptor-api/1.2/javax.interceptor-api-1.2.jar:/home/runner/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/runner/.m2/repository/javax/xml/bind/jaxb-api/2.3.0/jaxb-api-2.3.0.jar:/home/runner/.m2/repository/org/glassfish/jaxb/jaxb-runtime/2.3.0.1/jaxb-runtime-2.3.0.1.jar:/home/runner/.m2/repository/org/glassfish/jaxb/jaxb-core/2.3.0.1/jaxb-core-2.3.0.1.jar:/home/runner/.m2/repository/org/glassfish/jaxb/txw2/2.3.0.1/txw2-2.3.0.1.jar:/home/runner/.m2/repository/com/sun/istack/istack-commons-runtime/3.0.5/istack-commons-runtime-3.0.5.jar:/home/runner/.m2/repository/org/jvnet/staxex/stax-ex/1.7.8/stax-ex-1.7.8.jar:/home/runner/.m2/repository/com/sun/xml/fastinfoset/FastInfoset/1.2.13/FastInfoset-1.2.13.jar:/home/runner/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/runner/.m2/repository/io/grpc/grpc-protobuf/1.51.1/grpc-protobuf-1.51.1.jar:/home/runner/.m2/repository/io/grpc/grpc-api/1.51.1/grpc-api-1.51.1.jar:/home/runner/.m2/repository/io/grpc/grpc-context/1.51.1/grpc-context-1.51.1.jar:/home/runner/.m2/repository/com/google/api/grpc/proto-google-common-protos/2.9.0/proto-google-common-protos-2.9.0.jar:/home/runner/.m2/repository/io/grpc/grpc-protobuf-lite/1.51.1/grpc-protobuf-lite-1.51.1.jar:/home/runner/.m2/repository/io/grpc/grpc-stub/1.51.1/grpc-stub-1.51.1.jar:/home/runner/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-resolver/4.1.86.Final/netty-resolver-4.1.86.Final.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-csi/1.4.0-SNAPSHOT/ozone-csi-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/google/protobuf/protobuf-java-util/3.19.6/protobuf-java-util-3.19.6.jar:/home/runner/.m2/repository/com/google/code/gson/gson/2.9.0/gson-2.9.0.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-config/1.4.0-SNAPSHOT/hdds-config-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-epoll/4.1.86.Final/netty-transport-native-epoll-4.1.86.Final-linux-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-transport-classes-epoll/4.1.86.Final/netty-transport-classes-epoll-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.86.Final/netty-transport-native-unix-common-4.1.86.Final.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-recon/1.4.0-SNAPSHOT/ozone-recon-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-reconcodegen/1.4.0-SNAPSHOT/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/google/inject/guice/5.1.0/guice-5.1.0.jar:/home/runner/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/runner/.m2/repository/com/google/inject/extensions/guice-assistedinject/5.1.0/guice-assistedinject-5.1.0.jar:/home/runner/.m2/repository/com/google/inject/extensions/guice-servlet/5.1.0/guice-servlet-5.1.0.jar:/home/runner/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.34/jersey-container-servlet-2.34.jar:/home/runner/.m2/repository/org/glassfish/hk2/guice-bridge/2.5.0/guice-bridge-2.5.0.jar:/home/runner/.m2/repository/org/glassfish/jersey/core/jersey-server/2.34/jersey-server-2.34.jar:/home/runner/.m2/repository/org/glassfish/jersey/core/jersey-client/2.34/jersey-client-2.34.jar:/home/runner/.m2/repository/jakarta/annotation/jakarta.annotation-api/1.3.5/jakarta.annotation-api-1.3.5.jar:/home/runner/.m2/repository/jakarta/validation/jakarta.validation-api/2.0.2/jakarta.validation-api-2.0.2.jar:/home/runner/.m2/repository/org/glassfish/jersey/media/jersey-media-json-jackson/2.34/jersey-media-json-jackson-2.34.jar:/home/runner/.m2/repository/org/glassfish/jersey/ext/jersey-entity-filtering/2.34/jersey-entity-filtering-2.34.jar:/home/runner/.m2/repository/org/jooq/jooq/3.11.10/jooq-3.11.10.jar:/home/runner/.m2/repository/org/jooq/jooq-meta/3.11.10/jooq-meta-3.11.10.jar:/home/runner/.m2/repository/org/jooq/jooq-codegen/3.11.10/jooq-codegen-3.11.10.jar:/home/runner/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/home/runner/.m2/repository/org/apache/derby/derby/10.14.2.0/derby-10.14.2.0.jar:/home/runner/.m2/repository/org/xerial/sqlite-jdbc/3.25.2/sqlite-jdbc-3.25.2.jar:/home/runner/.m2/repository/org/springframework/spring-jdbc/5.3.26/spring-jdbc-5.3.26.jar:/home/runner/.m2/repository/org/springframework/spring-beans/5.3.26/spring-beans-5.3.26.jar:/home/runner/.m2/repository/org/springframework/spring-core/5.3.26/spring-core-5.3.26.jar:/home/runner/.m2/repository/org/springframework/spring-tx/5.3.26/spring-tx-5.3.26.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-client/1.4.0-SNAPSHOT/ozone-client-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-erasurecode/1.4.0-SNAPSHOT/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-filesystem/1.4.0-SNAPSHOT/ozone-filesystem-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-filesystem-common/1.4.0-SNAPSHOT/ozone-filesystem-common-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-tools/1.4.0-SNAPSHOT/ozone-tools-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/amazonaws/aws-java-sdk-core/1.12.261/aws-java-sdk-core-1.12.261.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/runner/.m2/repository/software/amazon/ion/ion-java/1.0.2/ion-java-1.0.2.jar:/home/runner/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.13.4/jackson-dataformat-cbor-2.13.4.jar:/home/runner/.m2/repository/joda-time/joda-time/2.10.6/joda-time-2.10.6.jar:/home/runner/.m2/repository/com/amazonaws/aws-java-sdk-s3/1.12.261/aws-java-sdk-s3-1.12.261.jar:/home/runner/.m2/repository/com/amazonaws/aws-java-sdk-kms/1.12.261/aws-java-sdk-kms-1.12.261.jar:/home/runner/.m2/repository/com/amazonaws/jmespath-java/1.12.261/jmespath-java-1.12.261.jar:/home/runner/.m2/repository/org/kohsuke/metainf-services/metainf-services/1.8/metainf-services-1.8.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-tools/1.4.0-SNAPSHOT/hdds-tools-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-tools/2.4.2-8b8bdda-SNAPSHOT/ratis-tools-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/runner/.m2/repository/org/apache/commons/commons-lang3/3.7/commons-lang3-3.7.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-manager/1.4.0-SNAPSHOT/ozone-manager-1.4.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-common/1.4.0-SNAPSHOT/hdds-common-1.4.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-hadoop-dependency-client/1.4.0-SNAPSHOT/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/info/picocli/picocli/4.6.1/picocli-4.6.1.jar:/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.13.4/jackson-annotations-2.13.4.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-annotation-processing/1.4.0-SNAPSHOT/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-netty/2.4.2-8b8bdda-SNAPSHOT/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-grpc/2.4.2-8b8bdda-SNAPSHOT/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/logging/log4j/log4j-api/2.17.1/log4j-api-2.17.1.jar:/home/runner/.m2/repository/org/apache/commons/commons-pool2/2.6.0/commons-pool2-2.6.0.jar:/home/runner/.m2/repository/org/bouncycastle/bcpkix-jdk15on/1.67/bcpkix-jdk15on-1.67.jar:/home/runner/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/runner/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/runner/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/runner/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-client/1.6.0/jaeger-client-1.6.0.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-thrift/1.6.0/jaeger-thrift-1.6.0.jar:/home/runner/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-core/1.6.0/jaeger-core-1.6.0.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-tracerresolver/1.6.0/jaeger-tracerresolver-1.6.0.jar:/home/runner/.m2/repository/io/opentracing/contrib/opentracing-tracerresolver/0.1.8/opentracing-tracerresolver-0.1.8.jar:/home/runner/.m2/repository/org/jetbrains/kotlin/kotlin-stdlib/1.6.21/kotlin-stdlib-1.6.21.jar:/home/runner/.m2/repository/org/jetbrains/annotations/13.0/annotations-13.0.jar:/home/runner/.m2/repository/io/opentracing/opentracing-util/0.33.0/opentracing-util-0.33.0.jar:/home/runner/.m2/repository/io/opentracing/opentracing-api/0.33.0/opentracing-api-0.33.0.jar:/home/runner/.m2/repository/io/opentracing/opentracing-noop/0.33.0/opentracing-noop-0.33.0.jar:/home/runner/.m2/repository/org/yaml/snakeyaml/2.0/snakeyaml-2.0.jar:/home/runner/.m2/repository/junit/junit/4.13.1/junit-4.13.1.jar:/home/runner/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.8.2/junit-jupiter-api-5.8.2.jar:/home/runner/.m2/repository/org/opentest4j/opentest4j/1.2.0/opentest4j-1.2.0.jar:/home/runner/.m2/repository/org/junit/platform/junit-platform-commons/1.8.2/junit-platform-commons-1.8.2.jar:/home/runner/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-params/5.8.2/junit-jupiter-params-5.8.2.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-migrationsupport/5.8.2/junit-jupiter-migrationsupport-5.8.2.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-engine/5.8.2/junit-jupiter-engine-5.8.2.jar:/home/runner/.m2/repository/org/junit/platform/junit-platform-engine/1.8.2/junit-platform-engine-1.8.2.jar:/home/runner/.m2/repository/org/junit/vintage/junit-vintage-engine/5.8.2/junit-vintage-engine-5.8.2.jar:/home/runner/.m2/repository/org/junit/platform/junit-platform-launcher/1.8.2/junit-platform-launcher-1.8.2.jar:/home/runner/.m2/repository/org/mockito/mockito-core/2.28.2/mockito-core-2.28.2.jar:/home/runner/.m2/repository/net/bytebuddy/byte-buddy/1.9.10/byte-buddy-1.9.10.jar:/home/runner/.m2/repository/net/bytebuddy/byte-buddy-agent/1.9.10/byte-buddy-agent-1.9.10.jar:/home/runner/.m2/repository/org/objenesis/objenesis/1.0/objenesis-1.0.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-kms/3.3.5/hadoop-kms-3.3.5.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-auth/3.3.5/hadoop-auth-3.3.5.jar:/home/runner/.m2/repository/com/nimbusds/nimbus-jose-jwt/9.8.1/nimbus-jose-jwt-9.8.1.jar:/home/runner/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/runner/.m2/repository/net/minidev/json-smart/2.4.7/json-smart-2.4.7.jar:/home/runner/.m2/repository/net/minidev/accessors-smart/2.4.7/accessors-smart-2.4.7.jar:/home/runner/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/runner/.m2/repository/org/apache/zookeeper/zookeeper/3.5.6/zookeeper-3.5.6.jar:/home/runner/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.6/zookeeper-jute-3.5.6.jar:/home/runner/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/runner/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/runner/.m2/repository/org/apache/hadoop/thirdparty/hadoop-shaded-guava/1.1.1/hadoop-shaded-guava-1.1.1.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/runner/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/runner/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-common/3.3.5/hadoop-common-3.3.5.jar:/home/runner/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/runner/.m2/repository/commons-net/commons-net/3.9.0/commons-net-3.9.0.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/runner/.m2/repository/com/github/pjfanning/jersey-json/1.20/jersey-json-1.20.jar:/home/runner/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/runner/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/runner/.m2/repository/com/google/re2j/re2j/1.1/re2j-1.1.jar:/home/runner/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/runner/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/runner/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/runner/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/runner/.m2/repository/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar:/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.13.4.2/jackson-databind-2.13.4.2.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-kms/3.3.5/hadoop-kms-3.3.5-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-scm/1.4.0-SNAPSHOT/hdds-server-scm-1.4.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/com/github/luben/zstd-jni/1.5.2-5/zstd-jni-1.5.2-5.jar:/home/runner/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/runner/.m2/repository/io/netty/netty-codec/4.1.86.Final/netty-codec-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-handler/4.1.86.Final/netty-handler-4.1.86.Final.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-hadoop-dependency-test/1.4.0-SNAPSHOT/hdds-hadoop-dependency-test-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-common/3.3.5/hadoop-common-3.3.5-tests.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.3.5/hadoop-hdfs-3.3.5-tests.jar:/home/runner/.m2/repository/org/assertj/assertj-core/3.12.2/assertj-core-3.12.2.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-distcp/3.3.5/hadoop-distcp-3.3.5.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.3.5/hadoop-mapreduce-client-jobclient-3.3.5.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.3.5/hadoop-mapreduce-client-common-3.3.5.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.3.5/hadoop-yarn-common-3.3.5.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.3.5/hadoop-yarn-api-3.3.5.jar:/home/runner/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/runner/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.13.4/jackson-jaxrs-json-provider-2.13.4.jar:/home/runner/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.13.4/jackson-jaxrs-base-2.13.4.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.3.5/hadoop-yarn-client-3.3.5.jar:/home/runner/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.4.48.v20220622/websocket-client-9.4.48.v20220622.jar:/home/runner/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.4.48.v20220622/websocket-common-9.4.48.v20220622.jar:/home/runner/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.4.48.v20220622/websocket-api-9.4.48.v20220622.jar:/home/runner/.m2/repository/org/jline/jline/3.9.0/jline-3.9.0.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.3.5/hadoop-mapreduce-client-core-3.3.5.jar:/home/runner/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/home/runner/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-annotations/3.3.5/hadoop-annotations-3.3.5.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/../lib/tools.jar:/home/runner/.m2/repository/io/netty/netty/3.10.6.Final/netty-3.10.6.Final.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-distcp/3.3.5/hadoop-distcp-3.3.5-tests.jar:/home/runner/.m2/repository/org/slf4j/jul-to-slf4j/1.7.36/jul-to-slf4j-1.7.36.jar:"/>
    <property name="java.vm.vendor" value="Temurin"/>
    <property name="sun.arch.data.model" value="64"/>
    <property name="test.build.dir" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir"/>
    <property name="test.cache.data" value=""/>
    <property name="java.vendor.url" value="https://adoptium.net/"/>
    <property name="user.timezone" value="Etc/UTC"/>
    <property name="java.vm.specification.version" value="1.8"/>
    <property name="os.name" value="Linux"/>
    <property name="test.build.data" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir"/>
    <property name="sun.java.launcher" value="SUN_STANDARD"/>
    <property name="sun.boot.library.path" value="/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/amd64"/>
    <property name="sun.java.command" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/surefire/surefirebooter4428481175257605732.jar /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/surefire 2023-04-27T05-47-33_641-jvmRun1 surefire4432204830257200842tmp surefire_212132445452104327701tmp"/>
    <property name="surefire.test.class.path" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/classes:/home/runner/.m2/repository/org/apache/ozone/ozone-common/1.4.0-SNAPSHOT/ozone-common-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/io/grpc/grpc-netty/1.51.1/grpc-netty-1.51.1.jar:/home/runner/.m2/repository/io/grpc/grpc-core/1.51.1/grpc-core-1.51.1.jar:/home/runner/.m2/repository/com/google/android/annotations/4.1.1.4/annotations-4.1.1.4.jar:/home/runner/.m2/repository/org/codehaus/mojo/animal-sniffer-annotations/1.21/animal-sniffer-annotations-1.21.jar:/home/runner/.m2/repository/com/google/errorprone/error_prone_annotations/2.2.0/error_prone_annotations-2.2.0.jar:/home/runner/.m2/repository/io/perfmark/perfmark-api/0.25.0/perfmark-api-0.25.0.jar:/home/runner/.m2/repository/io/netty/netty-codec-http2/4.1.86.Final/netty-codec-http2-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-common/4.1.86.Final/netty-common-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-buffer/4.1.86.Final/netty-buffer-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-http/4.1.86.Final/netty-codec-http-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-handler-proxy/4.1.86.Final/netty-handler-proxy-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-socks/4.1.86.Final/netty-codec-socks-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.54.Final/netty-tcnative-boringssl-static-2.0.54.Final.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-classes/2.0.54.Final/netty-tcnative-classes-2.0.54.Final.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.54.Final/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.54.Final/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.54.Final/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.54.Final/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.54.Final/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/home/runner/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-common/1.4.0-SNAPSHOT/hdds-common-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-client/1.4.0-SNAPSHOT/hdds-client-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-interface-client/1.4.0-SNAPSHOT/ozone-interface-client-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.3.5/hadoop-hdfs-client-3.3.5.jar:/home/runner/.m2/repository/com/squareup/okhttp3/okhttp/4.9.3/okhttp-4.9.3.jar:/home/runner/.m2/repository/com/squareup/okio/okio/2.8.0/okio-2.8.0.jar:/home/runner/.m2/repository/org/jetbrains/kotlin/kotlin-stdlib-common/1.6.21/kotlin-stdlib-common-1.6.21.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-test-utils/1.4.0-SNAPSHOT/hdds-test-utils-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/google/guava/guava/31.1-jre/guava-31.1-jre.jar:/home/runner/.m2/repository/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.jar:/home/runner/.m2/repository/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/runner/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/runner/.m2/repository/org/checkerframework/checker-qual/3.12.0/checker-qual-3.12.0.jar:/home/runner/.m2/repository/com/google/j2objc/j2objc-annotations/1.3/j2objc-annotations-1.3.jar:/home/runner/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/home/runner/.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/home/runner/.m2/repository/ch/qos/reload4j/reload4j/1.2.22/reload4j-1.2.22.jar:/home/runner/.m2/repository/org/slf4j/slf4j-api/1.7.36/slf4j-api-1.7.36.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-scm/1.4.0-SNAPSHOT/hdds-server-scm-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-hadoop-dependency-server/1.4.0-SNAPSHOT/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.3.5/hadoop-hdfs-3.3.5.jar:/home/runner/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-framework/1.4.0-SNAPSHOT/hdds-server-framework-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/io/dropwizard/metrics/metrics-core/3.2.4/metrics-core-3.2.4.jar:/home/runner/.m2/repository/org/apache/commons/commons-text/1.4/commons-text-1.4.jar:/home/runner/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.67/bcprov-jdk15on-1.67.jar:/home/runner/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-framework/1.4.0-SNAPSHOT/hdds-server-framework-1.4.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-interface-server/1.4.0-SNAPSHOT/hdds-interface-server-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-interface-admin/1.4.0-SNAPSHOT/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-managed-rocksdb/1.4.0-SNAPSHOT/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/rocksdb/rocksdbjni/7.7.3/rocksdbjni-7.7.3.jar:/home/runner/.m2/repository/org/slf4j/slf4j-reload4j/1.7.36/slf4j-reload4j-1.7.36.jar:/home/runner/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/runner/.m2/repository/org/apache/logging/log4j/log4j-core/2.17.1/log4j-core-2.17.1.jar:/home/runner/.m2/repository/com/lmax/disruptor/3.4.2/disruptor-3.4.2.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-util/9.4.51.v20230217/jetty-util-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-server/9.4.51.v20230217/jetty-server-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-http/9.4.51.v20230217/jetty-http-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-io/9.4.51.v20230217/jetty-io-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-servlet/9.4.51.v20230217/jetty-servlet-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-security/9.4.51.v20230217/jetty-security-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.4.51.v20230217/jetty-util-ajax-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-webapp/9.4.51.v20230217/jetty-webapp-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-xml/9.4.51.v20230217/jetty-xml-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-server/2.4.2-8b8bdda-SNAPSHOT/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-thirdparty-misc/1.0.3/ratis-thirdparty-misc-1.0.3.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-proto/2.4.2-8b8bdda-SNAPSHOT/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-common/2.4.2-8b8bdda-SNAPSHOT/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-client/2.4.2-8b8bdda-SNAPSHOT/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-server-api/2.4.2-8b8bdda-SNAPSHOT/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-metrics/2.4.2-8b8bdda-SNAPSHOT/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/io/prometheus/simpleclient_dropwizard/0.7.0/simpleclient_dropwizard-0.7.0.jar:/home/runner/.m2/repository/io/prometheus/simpleclient/0.7.0/simpleclient-0.7.0.jar:/home/runner/.m2/repository/io/prometheus/simpleclient_common/0.7.0/simpleclient_common-0.7.0.jar:/home/runner/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.13.4/jackson-datatype-jsr310-2.13.4.jar:/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.13.4/jackson-core-2.13.4.jar:/home/runner/.m2/repository/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar:/home/runner/.m2/repository/org/apache/ozone/rocksdb-checkpoint-differ/1.4.0-SNAPSHOT/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/awaitility/awaitility/4.2.0/awaitility-4.2.0.jar:/home/runner/.m2/repository/org/hamcrest/hamcrest/2.1/hamcrest-2.1.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-manager/1.4.0-SNAPSHOT/ozone-manager-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/aspectj/aspectjrt/1.9.7/aspectjrt-1.9.7.jar:/home/runner/.m2/repository/org/aspectj/aspectjweaver/1.9.7/aspectjweaver-1.9.7.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-interface-client/1.4.0-SNAPSHOT/hdds-interface-client-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/thirdparty/hadoop-shaded-protobuf_3_7/1.1.1/hadoop-shaded-protobuf_3_7-1.1.1.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-interface-storage/1.4.0-SNAPSHOT/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-intg/2.3.0/ranger-intg-2.3.0.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugins-common/2.3.0/ranger-plugins-common-2.3.0.jar:/home/runner/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugins-cred/2.3.0/ranger-plugins-cred-2.3.0.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugins-audit/2.3.0/ranger-plugins-audit-2.3.0.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-client/9.4.51.v20230217/jetty-client-9.4.51.v20230217.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpmime/4.5.6/httpmime-4.5.6.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpcore-nio/4.4.13/httpcore-nio-4.4.13.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpasyncclient/4.1.3/httpasyncclient-4.1.3.jar:/home/runner/.m2/repository/com/carrotsearch/hppc/0.8.0/hppc-0.8.0.jar:/home/runner/.m2/repository/org/apache/orc/orc-core/1.5.8/orc-core-1.5.8.jar:/home/runner/.m2/repository/net/java/dev/jna/jna/5.2.0/jna-5.2.0.jar:/home/runner/.m2/repository/net/java/dev/jna/jna-platform/5.2.0/jna-platform-5.2.0.jar:/home/runner/.m2/repository/com/kstruct/gethostname4j/0.0.2/gethostname4j-0.0.2.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugin-classloader/2.3.0/ranger-plugin-classloader-2.3.0.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-rocks-native/1.4.0-SNAPSHOT/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-minikdc/3.3.5/hadoop-minikdc-3.3.5.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-s3gateway/1.4.0-SNAPSHOT/ozone-s3gateway-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar:/home/runner/.m2/repository/org/jboss/weld/servlet/weld-servlet-shaded/3.1.9.Final/weld-servlet-shaded-3.1.9.Final.jar:/home/runner/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.34/jersey-container-servlet-core-2.34.jar:/home/runner/.m2/repository/org/glassfish/hk2/external/jakarta.inject/2.6.1/jakarta.inject-2.6.1.jar:/home/runner/.m2/repository/org/glassfish/jersey/core/jersey-common/2.34/jersey-common-2.34.jar:/home/runner/.m2/repository/jakarta/ws/rs/jakarta.ws.rs-api/2.1.6/jakarta.ws.rs-api-2.1.6.jar:/home/runner/.m2/repository/org/glassfish/jersey/ext/cdi/jersey-cdi1x/2.34/jersey-cdi1x-2.34.jar:/home/runner/.m2/repository/org/glassfish/jersey/inject/jersey-hk2/2.34/jersey-hk2-2.34.jar:/home/runner/.m2/repository/org/glassfish/hk2/hk2-locator/2.6.1/hk2-locator-2.6.1.jar:/home/runner/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.34/jersey-media-jaxb-2.34.jar:/home/runner/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.3/osgi-resource-locator-1.0.3.jar:/home/runner/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0/hk2-api-2.5.0.jar:/home/runner/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0/hk2-utils-2.5.0.jar:/home/runner/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0/aopalliance-repackaged-2.5.0.jar:/home/runner/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-xml/2.13.4/jackson-dataformat-xml-2.13.4.jar:/home/runner/.m2/repository/org/codehaus/woodstox/stax2-api/4.2.1/stax2-api-4.2.1.jar:/home/runner/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.4.0/woodstox-core-5.4.0.jar:/home/runner/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.13.4/jackson-module-jaxb-annotations-2.13.4.jar:/home/runner/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.3/jakarta.xml.bind-api-2.3.3.jar:/home/runner/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.2/jakarta.activation-api-1.2.2.jar:/home/runner/.m2/repository/javax/enterprise/cdi-api/2.0/cdi-api-2.0.jar:/home/runner/.m2/repository/javax/el/javax.el-api/3.0.0/javax.el-api-3.0.0.jar:/home/runner/.m2/repository/javax/interceptor/javax.interceptor-api/1.2/javax.interceptor-api-1.2.jar:/home/runner/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/runner/.m2/repository/javax/xml/bind/jaxb-api/2.3.0/jaxb-api-2.3.0.jar:/home/runner/.m2/repository/org/glassfish/jaxb/jaxb-runtime/2.3.0.1/jaxb-runtime-2.3.0.1.jar:/home/runner/.m2/repository/org/glassfish/jaxb/jaxb-core/2.3.0.1/jaxb-core-2.3.0.1.jar:/home/runner/.m2/repository/org/glassfish/jaxb/txw2/2.3.0.1/txw2-2.3.0.1.jar:/home/runner/.m2/repository/com/sun/istack/istack-commons-runtime/3.0.5/istack-commons-runtime-3.0.5.jar:/home/runner/.m2/repository/org/jvnet/staxex/stax-ex/1.7.8/stax-ex-1.7.8.jar:/home/runner/.m2/repository/com/sun/xml/fastinfoset/FastInfoset/1.2.13/FastInfoset-1.2.13.jar:/home/runner/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/runner/.m2/repository/io/grpc/grpc-protobuf/1.51.1/grpc-protobuf-1.51.1.jar:/home/runner/.m2/repository/io/grpc/grpc-api/1.51.1/grpc-api-1.51.1.jar:/home/runner/.m2/repository/io/grpc/grpc-context/1.51.1/grpc-context-1.51.1.jar:/home/runner/.m2/repository/com/google/api/grpc/proto-google-common-protos/2.9.0/proto-google-common-protos-2.9.0.jar:/home/runner/.m2/repository/io/grpc/grpc-protobuf-lite/1.51.1/grpc-protobuf-lite-1.51.1.jar:/home/runner/.m2/repository/io/grpc/grpc-stub/1.51.1/grpc-stub-1.51.1.jar:/home/runner/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-resolver/4.1.86.Final/netty-resolver-4.1.86.Final.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-csi/1.4.0-SNAPSHOT/ozone-csi-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/google/protobuf/protobuf-java-util/3.19.6/protobuf-java-util-3.19.6.jar:/home/runner/.m2/repository/com/google/code/gson/gson/2.9.0/gson-2.9.0.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-config/1.4.0-SNAPSHOT/hdds-config-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-epoll/4.1.86.Final/netty-transport-native-epoll-4.1.86.Final-linux-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-transport-classes-epoll/4.1.86.Final/netty-transport-classes-epoll-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.86.Final/netty-transport-native-unix-common-4.1.86.Final.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-recon/1.4.0-SNAPSHOT/ozone-recon-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-reconcodegen/1.4.0-SNAPSHOT/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/google/inject/guice/5.1.0/guice-5.1.0.jar:/home/runner/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/runner/.m2/repository/com/google/inject/extensions/guice-assistedinject/5.1.0/guice-assistedinject-5.1.0.jar:/home/runner/.m2/repository/com/google/inject/extensions/guice-servlet/5.1.0/guice-servlet-5.1.0.jar:/home/runner/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.34/jersey-container-servlet-2.34.jar:/home/runner/.m2/repository/org/glassfish/hk2/guice-bridge/2.5.0/guice-bridge-2.5.0.jar:/home/runner/.m2/repository/org/glassfish/jersey/core/jersey-server/2.34/jersey-server-2.34.jar:/home/runner/.m2/repository/org/glassfish/jersey/core/jersey-client/2.34/jersey-client-2.34.jar:/home/runner/.m2/repository/jakarta/annotation/jakarta.annotation-api/1.3.5/jakarta.annotation-api-1.3.5.jar:/home/runner/.m2/repository/jakarta/validation/jakarta.validation-api/2.0.2/jakarta.validation-api-2.0.2.jar:/home/runner/.m2/repository/org/glassfish/jersey/media/jersey-media-json-jackson/2.34/jersey-media-json-jackson-2.34.jar:/home/runner/.m2/repository/org/glassfish/jersey/ext/jersey-entity-filtering/2.34/jersey-entity-filtering-2.34.jar:/home/runner/.m2/repository/org/jooq/jooq/3.11.10/jooq-3.11.10.jar:/home/runner/.m2/repository/org/jooq/jooq-meta/3.11.10/jooq-meta-3.11.10.jar:/home/runner/.m2/repository/org/jooq/jooq-codegen/3.11.10/jooq-codegen-3.11.10.jar:/home/runner/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/home/runner/.m2/repository/org/apache/derby/derby/10.14.2.0/derby-10.14.2.0.jar:/home/runner/.m2/repository/org/xerial/sqlite-jdbc/3.25.2/sqlite-jdbc-3.25.2.jar:/home/runner/.m2/repository/org/springframework/spring-jdbc/5.3.26/spring-jdbc-5.3.26.jar:/home/runner/.m2/repository/org/springframework/spring-beans/5.3.26/spring-beans-5.3.26.jar:/home/runner/.m2/repository/org/springframework/spring-core/5.3.26/spring-core-5.3.26.jar:/home/runner/.m2/repository/org/springframework/spring-tx/5.3.26/spring-tx-5.3.26.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-client/1.4.0-SNAPSHOT/ozone-client-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-erasurecode/1.4.0-SNAPSHOT/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-filesystem/1.4.0-SNAPSHOT/ozone-filesystem-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-filesystem-common/1.4.0-SNAPSHOT/ozone-filesystem-common-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-tools/1.4.0-SNAPSHOT/ozone-tools-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/amazonaws/aws-java-sdk-core/1.12.261/aws-java-sdk-core-1.12.261.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/runner/.m2/repository/software/amazon/ion/ion-java/1.0.2/ion-java-1.0.2.jar:/home/runner/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.13.4/jackson-dataformat-cbor-2.13.4.jar:/home/runner/.m2/repository/joda-time/joda-time/2.10.6/joda-time-2.10.6.jar:/home/runner/.m2/repository/com/amazonaws/aws-java-sdk-s3/1.12.261/aws-java-sdk-s3-1.12.261.jar:/home/runner/.m2/repository/com/amazonaws/aws-java-sdk-kms/1.12.261/aws-java-sdk-kms-1.12.261.jar:/home/runner/.m2/repository/com/amazonaws/jmespath-java/1.12.261/jmespath-java-1.12.261.jar:/home/runner/.m2/repository/org/kohsuke/metainf-services/metainf-services/1.8/metainf-services-1.8.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-tools/1.4.0-SNAPSHOT/hdds-tools-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-tools/2.4.2-8b8bdda-SNAPSHOT/ratis-tools-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/runner/.m2/repository/org/apache/commons/commons-lang3/3.7/commons-lang3-3.7.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-manager/1.4.0-SNAPSHOT/ozone-manager-1.4.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-common/1.4.0-SNAPSHOT/hdds-common-1.4.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-hadoop-dependency-client/1.4.0-SNAPSHOT/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/info/picocli/picocli/4.6.1/picocli-4.6.1.jar:/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.13.4/jackson-annotations-2.13.4.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-annotation-processing/1.4.0-SNAPSHOT/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-netty/2.4.2-8b8bdda-SNAPSHOT/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-grpc/2.4.2-8b8bdda-SNAPSHOT/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/logging/log4j/log4j-api/2.17.1/log4j-api-2.17.1.jar:/home/runner/.m2/repository/org/apache/commons/commons-pool2/2.6.0/commons-pool2-2.6.0.jar:/home/runner/.m2/repository/org/bouncycastle/bcpkix-jdk15on/1.67/bcpkix-jdk15on-1.67.jar:/home/runner/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/runner/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/runner/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/runner/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-client/1.6.0/jaeger-client-1.6.0.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-thrift/1.6.0/jaeger-thrift-1.6.0.jar:/home/runner/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-core/1.6.0/jaeger-core-1.6.0.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-tracerresolver/1.6.0/jaeger-tracerresolver-1.6.0.jar:/home/runner/.m2/repository/io/opentracing/contrib/opentracing-tracerresolver/0.1.8/opentracing-tracerresolver-0.1.8.jar:/home/runner/.m2/repository/org/jetbrains/kotlin/kotlin-stdlib/1.6.21/kotlin-stdlib-1.6.21.jar:/home/runner/.m2/repository/org/jetbrains/annotations/13.0/annotations-13.0.jar:/home/runner/.m2/repository/io/opentracing/opentracing-util/0.33.0/opentracing-util-0.33.0.jar:/home/runner/.m2/repository/io/opentracing/opentracing-api/0.33.0/opentracing-api-0.33.0.jar:/home/runner/.m2/repository/io/opentracing/opentracing-noop/0.33.0/opentracing-noop-0.33.0.jar:/home/runner/.m2/repository/org/yaml/snakeyaml/2.0/snakeyaml-2.0.jar:/home/runner/.m2/repository/junit/junit/4.13.1/junit-4.13.1.jar:/home/runner/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.8.2/junit-jupiter-api-5.8.2.jar:/home/runner/.m2/repository/org/opentest4j/opentest4j/1.2.0/opentest4j-1.2.0.jar:/home/runner/.m2/repository/org/junit/platform/junit-platform-commons/1.8.2/junit-platform-commons-1.8.2.jar:/home/runner/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-params/5.8.2/junit-jupiter-params-5.8.2.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-migrationsupport/5.8.2/junit-jupiter-migrationsupport-5.8.2.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-engine/5.8.2/junit-jupiter-engine-5.8.2.jar:/home/runner/.m2/repository/org/junit/platform/junit-platform-engine/1.8.2/junit-platform-engine-1.8.2.jar:/home/runner/.m2/repository/org/junit/vintage/junit-vintage-engine/5.8.2/junit-vintage-engine-5.8.2.jar:/home/runner/.m2/repository/org/junit/platform/junit-platform-launcher/1.8.2/junit-platform-launcher-1.8.2.jar:/home/runner/.m2/repository/org/mockito/mockito-core/2.28.2/mockito-core-2.28.2.jar:/home/runner/.m2/repository/net/bytebuddy/byte-buddy/1.9.10/byte-buddy-1.9.10.jar:/home/runner/.m2/repository/net/bytebuddy/byte-buddy-agent/1.9.10/byte-buddy-agent-1.9.10.jar:/home/runner/.m2/repository/org/objenesis/objenesis/1.0/objenesis-1.0.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-kms/3.3.5/hadoop-kms-3.3.5.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-auth/3.3.5/hadoop-auth-3.3.5.jar:/home/runner/.m2/repository/com/nimbusds/nimbus-jose-jwt/9.8.1/nimbus-jose-jwt-9.8.1.jar:/home/runner/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/runner/.m2/repository/net/minidev/json-smart/2.4.7/json-smart-2.4.7.jar:/home/runner/.m2/repository/net/minidev/accessors-smart/2.4.7/accessors-smart-2.4.7.jar:/home/runner/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/runner/.m2/repository/org/apache/zookeeper/zookeeper/3.5.6/zookeeper-3.5.6.jar:/home/runner/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.6/zookeeper-jute-3.5.6.jar:/home/runner/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/runner/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/runner/.m2/repository/org/apache/hadoop/thirdparty/hadoop-shaded-guava/1.1.1/hadoop-shaded-guava-1.1.1.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/runner/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/runner/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-common/3.3.5/hadoop-common-3.3.5.jar:/home/runner/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/runner/.m2/repository/commons-net/commons-net/3.9.0/commons-net-3.9.0.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/runner/.m2/repository/com/github/pjfanning/jersey-json/1.20/jersey-json-1.20.jar:/home/runner/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/runner/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/runner/.m2/repository/com/google/re2j/re2j/1.1/re2j-1.1.jar:/home/runner/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/runner/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/runner/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/runner/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/runner/.m2/repository/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar:/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.13.4.2/jackson-databind-2.13.4.2.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-kms/3.3.5/hadoop-kms-3.3.5-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-scm/1.4.0-SNAPSHOT/hdds-server-scm-1.4.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/com/github/luben/zstd-jni/1.5.2-5/zstd-jni-1.5.2-5.jar:/home/runner/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/runner/.m2/repository/io/netty/netty-codec/4.1.86.Final/netty-codec-4.1.86.Final.jar:/home/runner/.m2/repository/io/netty/netty-handler/4.1.86.Final/netty-handler-4.1.86.Final.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-hadoop-dependency-test/1.4.0-SNAPSHOT/hdds-hadoop-dependency-test-1.4.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-common/3.3.5/hadoop-common-3.3.5-tests.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.3.5/hadoop-hdfs-3.3.5-tests.jar:/home/runner/.m2/repository/org/assertj/assertj-core/3.12.2/assertj-core-3.12.2.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-distcp/3.3.5/hadoop-distcp-3.3.5.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.3.5/hadoop-mapreduce-client-jobclient-3.3.5.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.3.5/hadoop-mapreduce-client-common-3.3.5.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.3.5/hadoop-yarn-common-3.3.5.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.3.5/hadoop-yarn-api-3.3.5.jar:/home/runner/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/runner/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.13.4/jackson-jaxrs-json-provider-2.13.4.jar:/home/runner/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.13.4/jackson-jaxrs-base-2.13.4.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.3.5/hadoop-yarn-client-3.3.5.jar:/home/runner/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.4.48.v20220622/websocket-client-9.4.48.v20220622.jar:/home/runner/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.4.48.v20220622/websocket-common-9.4.48.v20220622.jar:/home/runner/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.4.48.v20220622/websocket-api-9.4.48.v20220622.jar:/home/runner/.m2/repository/org/jline/jline/3.9.0/jline-3.9.0.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.3.5/hadoop-mapreduce-client-core-3.3.5.jar:/home/runner/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/home/runner/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-annotations/3.3.5/hadoop-annotations-3.3.5.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/../lib/tools.jar:/home/runner/.m2/repository/io/netty/netty/3.10.6.Final/netty-3.10.6.Final.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-distcp/3.3.5/hadoop-distcp-3.3.5-tests.jar:/home/runner/.m2/repository/org/slf4j/jul-to-slf4j/1.7.36/jul-to-slf4j-1.7.36.jar:"/>
    <property name="sun.cpu.endian" value="little"/>
    <property name="user.home" value="/home/runner"/>
    <property name="user.language" value="en"/>
    <property name="java.specification.vendor" value="Oracle Corporation"/>
    <property name="java.home" value="/usr/lib/jvm/temurin-8-jdk-amd64/jre"/>
    <property name="java.security.krb5.conf" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/krb5.conf"/>
    <property name="file.separator" value="/"/>
    <property name="basedir" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test"/>
    <property name="line.separator" value="&#10;"/>
    <property name="java.vm.specification.vendor" value="Oracle Corporation"/>
    <property name="java.specification.name" value="Java Platform API Specification"/>
    <property name="skip.installnpx" value="true"/>
    <property name="java.awt.graphicsenv" value="sun.awt.X11GraphicsEnvironment"/>
    <property name="surefire.fork.timeout" value="3600"/>
    <property name="surefire.real.class.path" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/surefire/surefirebooter4428481175257605732.jar:/home/runner/.m2/repository/org/jacoco/org.jacoco.agent/0.8.5/org.jacoco.agent-0.8.5-runtime.jar"/>
    <property name="sun.boot.class.path" value="/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/classes"/>
    <property name="hadoop.log.dir" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log"/>
    <property name="sun.management.compiler" value="HotSpot 64-Bit Tiered Compilers"/>
    <property name="java.runtime.version" value="1.8.0_362-b09"/>
    <property name="skip.npx" value="true"/>
    <property name="user.name" value="runner"/>
    <property name="java.net.preferIPv4Stack" value="true"/>
    <property name="path.separator" value=":"/>
    <property name="java.security.egd" value="file:///dev/urandom"/>
    <property name="os.version" value="5.15.0-1036-azure"/>
    <property name="java.endorsed.dirs" value="/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/endorsed"/>
    <property name="java.runtime.name" value="OpenJDK Runtime Environment"/>
    <property name="file.encoding" value="UTF-8"/>
    <property name="java.vm.name" value="OpenJDK 64-Bit Server VM"/>
    <property name="test.build.webapps" value=""/>
    <property name="localRepository" value="/home/runner/.m2/repository"/>
    <property name="jetty.git.hash" value="b45c405e4544384de066f814ed42ae3dceacdd49"/>
    <property name="java.vendor.url.bug" value="https://github.com/adoptium/adoptium-support/issues"/>
    <property name="require.test.libhadoop" value=""/>
    <property name="java.io.tmpdir" value="/tmp"/>
    <property name="java.version" value="1.8.0_362"/>
    <property name="surefire.rerunFailingTestsCount" value="5"/>
    <property name="user.dir" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test"/>
    <property name="os.arch" value="amd64"/>
    <property name="test.build.classes" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes"/>
    <property name="java.vm.specification.name" value="Java Virtual Machine Specification"/>
    <property name="java.awt.printerjob" value="sun.print.PSPrinterJob"/>
    <property name="sun.os.patch.level" value="unknown"/>
    <property name="org.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads" value="false"/>
    <property name="hadoop.tmp.dir" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/tmp"/>
    <property name="java.library.path" value="/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib"/>
    <property name="java.vendor" value="Temurin"/>
    <property name="java.vm.info" value="mixed mode"/>
    <property name="java.vm.version" value="25.362-b09"/>
    <property name="java.specification.maintenance.version" value="4"/>
    <property name="sun.io.unicode.encoding" value="UnicodeLittle"/>
    <property name="java.ext.dirs" value="/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/ext:/usr/java/packages/lib/ext"/>
    <property name="java.class.version" value="52.0"/>
  </properties>
  <testcase name="testNodeWithOpenPipelineCanBeDecommissionedAndRecommissioned" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="55.833"/>
  <testcase name="testContainerIsReplicatedWhenAllNodesGotoMaintenance" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="37.3"/>
  <testcase name="testMaintenanceEndsAutomaticallyAtTimeout" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="39.244"/>
  <testcase name="testSingleNodeWithOpenPipelineCanGotoMaintenance" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="6.737">
    <error type="org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException"><![CDATA[org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 not found
	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:158)
	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerImpl.getPipeline(PipelineStateManagerImpl.java:139)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeLocal(SCMHAInvocationHandler.java:87)
	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:72)
	at com.sun.proxy.$Proxy26.getPipeline(Unknown Source)
	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.getPipeline(PipelineManagerImpl.java:274)
	at org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance.testSingleNodeWithOpenPipelineCanGotoMaintenance(TestDecommissionAndMaintenance.java:325)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
]]></error>
    <system-out><![CDATA[2023-04-27 06:30:07,658 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:shutdown(456)) - Shutting down the Mini Ozone Cluster
2023-04-27 06:30:07,665 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stop(474)) - Stopping the Mini Ozone Cluster
2023-04-27 06:30:07,665 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopOM(560)) - Stopping the OzoneManager
2023-04-27 06:30:07,665 [Mini-Cluster-Provider-Reap] INFO  om.OzoneManager (OzoneManager.java:stop(2165)) - om1[localhost:0]: Stopping Ozone Manager
2023-04-27 06:30:07,667 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3501)) - Stopping server on 46623
2023-04-27 06:30:07,670 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1430)) - Stopping IPC Server listener on 0
2023-04-27 06:30:07,674 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1563)) - Stopping IPC Server Responder
2023-04-27 06:30:07,674 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - om1: close
2023-04-27 06:30:07,676 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - om1: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:07,677 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - om1: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:07,677 [om1-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - om1@group-C5BA1605619E: shutdown
2023-04-27 06:30:07,677 [om1-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - om1: shutdown om1@group-C5BA1605619E-LeaderStateImpl
2023-04-27 06:30:07,677 [om1-impl-thread2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - om1@group-C5BA1605619E-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:07,688 [om1-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - om1@group-C5BA1605619E-StateMachineUpdater: set stopIndex = 86
2023-04-27 06:30:07,688 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:takeSnapshot(479)) - Current Snapshot Index (t:1, i:86)
2023-04-27 06:30:07,690 [Listener at 127.0.0.1/36727] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(148)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:07,695 [Listener at 127.0.0.1/36727] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(148)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:07,696 [Listener at 127.0.0.1/36727] INFO  ha.SCMHANodeDetails (SCMHANodeDetails.java:loadSCMHAConfig(209)) - ServiceID for StorageContainerManager is null
2023-04-27 06:30:07,696 [Listener at 127.0.0.1/36727] INFO  ha.SCMHANodeDetails (SCMHANodeDetails.java:loadSCMHAConfig(214)) - ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2023-04-27 06:30:07,696 [Listener at 127.0.0.1/36727] WARN  utils.HAUtils (HAUtils.java:getMetaDir(350)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:07,696 [Listener at 127.0.0.1/36727] WARN  db.DBStoreBuilder (DBStoreBuilder.java:applyDBDefinition(182)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:07,739 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - om1@group-C5BA1605619E-StateMachineUpdater: Took a snapshot at index 86
2023-04-27 06:30:07,739 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - om1@group-C5BA1605619E-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 86
2023-04-27 06:30:07,739 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:close(533)) - StateMachine has shutdown. Shutdown OzoneManager if not already shutdown.
2023-04-27 06:30:07,739 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:stopDaemon(540)) - Stopping OMDoubleBuffer flush thread
2023-04-27 06:30:07,739 [OMDoubleBufferFlushThread] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:canFlush(625)) - OMDoubleBuffer flush thread OMDoubleBufferFlushThread is interrupted and will exit.
2023-04-27 06:30:07,741 [om1-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - om1@group-C5BA1605619E: closes. applyIndex: 86
2023-04-27 06:30:07,741 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:07,742 [om1-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker close()
2023-04-27 06:30:07,745 [JvmPauseMonitor16] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-om1: Stopped
2023-04-27 06:30:07,745 [Mini-Cluster-Provider-Reap] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:close(533)) - StateMachine has shutdown. Shutdown OzoneManager if not already shutdown.
2023-04-27 06:30:07,745 [Mini-Cluster-Provider-Reap] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:stopDaemon(549)) - OMDoubleBuffer flush thread is not running.
2023-04-27 06:30:07,745 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service KeyDeletingService
2023-04-27 06:30:07,745 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service KeyDeletingService
2023-04-27 06:30:07,746 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service OpenKeyCleanupService
2023-04-27 06:30:07,746 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SstFilteringService
2023-04-27 06:30:07,746 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SnapshotDeletingService
2023-04-27 06:30:07,751 [main] INFO  rpc.RpcClient (RpcClient.java:createVolume(476)) - Creating Volume: vol1, with user15074 as owner and space quota set to -1 bytes, counts quota set to -1
2023-04-27 06:30:07,754 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@177c3cad{ozoneManager,/,null,STOPPED}{file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/ozoneManager}
2023-04-27 06:30:07,757 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@9903dbe{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:07,757 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:07,757 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@32f09c68{static,/static,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/static,STOPPED}
2023-04-27 06:30:07,757 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@1255e96d{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:07,765 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SnapshotDiffCleanupService
2023-04-27 06:30:07,766 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopDatanodes(537)) - Stopping the HddsDatanodes
2023-04-27 06:30:07,769 [Mini-Cluster-Provider-Reap] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:07,769 [OM StateMachine ApplyTransaction Thread - 0] INFO  volume.OMVolumeCreateRequest (OMVolumeCreateRequest.java:validateAndUpdateCache(195)) - created volume:vol1 for user:user15074
2023-04-27 06:30:07,770 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39: close
2023-04-27 06:30:07,770 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5: shutdown
2023-04-27 06:30:07,771 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-13E93A7B1EF5,id=2ebc1c73-026e-4922-9f91-c27c5284ff39
2023-04-27 06:30:07,771 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39: shutdown 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5-LeaderStateImpl
2023-04-27 06:30:07,771 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:07,771 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:07,771 [2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-13E93A7B1EF5: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-4/data/ratis/4d91dd73-517d-4b94-a25d-13e93a7b1ef5/sm/snapshot.1_0
2023-04-27 06:30:07,772 [2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-13E93A7B1EF5: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-4/data/ratis/4d91dd73-517d-4b94-a25d-13e93a7b1ef5/sm/snapshot.1_0 took: 1 ms
2023-04-27 06:30:07,773 [2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:07,773 [2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:07,773 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5: closes. applyIndex: 0
2023-04-27 06:30:07,775 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:07,786 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread3] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259: shutdown
2023-04-27 06:30:07,786 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread3] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-238259678259,id=2ebc1c73-026e-4922-9f91-c27c5284ff39
2023-04-27 06:30:07,787 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread3] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39: shutdown 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-FollowerState
2023-04-27 06:30:07,787 [2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-238259678259: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-4/data/ratis/0013f2b1-5ac3-4f2d-a0a1-238259678259/sm/snapshot.1_0
2023-04-27 06:30:07,787 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread3] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:07,788 [2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-238259678259: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-4/data/ratis/0013f2b1-5ac3-4f2d-a0a1-238259678259/sm/snapshot.1_0 took: 1 ms
2023-04-27 06:30:07,788 [2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:07,788 [2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:07,788 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread3] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259: closes. applyIndex: 0
2023-04-27 06:30:07,790 [2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-FollowerState was interrupted
2023-04-27 06:30:07,790 [2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:07,790 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-238259678259-SegmentedRaftLogWorker close()
2023-04-27 06:30:07,792 [grpc-default-executor-3] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: RST_STREAM closed stream. HTTP/2 error code: CANCEL
2023-04-27 06:30:07,792 [grpc-default-executor-3] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=2094, lastRpcResponseTime=2093) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:07,793 [grpc-default-executor-6] WARN  server.GrpcServerProtocolService (LogUtils.java:warn(122)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39: installSnapshot onError, lastRequest: 45e99655-b061-40f7-b011-1b1883fa2319->2ebc1c73-026e-4922-9f91-c27c5284ff39#1-t1,previous=(t:0, i:0),leaderCommit=0,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "d34b33f9-e64c-4718-893f-5b1a944a0d93"
address: "10.1.0.33:42891"
dataStreamAddress: "10.1.0.33:37831"
clientAddress: "10.1.0.33:42891"
adminAddress: "10.1.0.33:42891"
startupRole: FOLLOWER
,id: "45e99655-b061-40f7-b011-1b1883fa2319"
address: "10.1.0.33:34159"
priority: 1
dataStreamAddress: "10.1.0.33:46485"
clientAddress: "10.1.0.33:34159"
adminAddress: "10.1.0.33:34159"
startupRole: FOLLOWER
,id: "2ebc1c73-026e-4922-9f91-c27c5284ff39"
address: "10.1.0.33:40783"
dataStreamAddress: "10.1.0.33:41415"
clientAddress: "10.1.0.33:40783"
adminAddress: "10.1.0.33:40783"
startupRole: FOLLOWER
, old:): org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: client cancelled
2023-04-27 06:30:07,793 [grpc-default-executor-1] WARN  server.GrpcServerProtocolService (LogUtils.java:warn(122)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39: installSnapshot onError, lastRequest: null: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: client cancelled
2023-04-27 06:30:07,793 [grpc-default-executor-4] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: RST_STREAM closed stream. HTTP/2 error code: CANCEL
2023-04-27 06:30:07,793 [grpc-default-executor-4] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=2095, lastRpcResponseTime=2094) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:07,793 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:07,794 [2ebc1c73-026e-4922-9f91-c27c5284ff39-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x0183127e, L:/0:0:0:0:0:0:0:0:41415] CLOSE
2023-04-27 06:30:07,794 [2ebc1c73-026e-4922-9f91-c27c5284ff39-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x0183127e, L:/0:0:0:0:0:0:0:0:41415] INACTIVE
2023-04-27 06:30:07,794 [2ebc1c73-026e-4922-9f91-c27c5284ff39-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x0183127e, L:/0:0:0:0:0:0:0:0:41415] UNREGISTERED
2023-04-27 06:30:07,799 [2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:07,799 [2ebc1c73-026e-4922-9f91-c27c5284ff39-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39@group-13E93A7B1EF5-SegmentedRaftLogWorker close()
2023-04-27 06:30:07,818 [JvmPauseMonitor21] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-2ebc1c73-026e-4922-9f91-c27c5284ff39: Stopped
2023-04-27 06:30:07,819 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:07,820 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761: close
2023-04-27 06:30:07,827 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D: shutdown
2023-04-27 06:30:07,827 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-88AC0215203D,id=9c3df6cb-795c-4fc9-beaf-ef1567f7c761
2023-04-27 06:30:07,827 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761: shutdown 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-FollowerState
2023-04-27 06:30:07,828 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:07,829 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread3] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49: shutdown
2023-04-27 06:30:07,829 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread3] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-273EDE928D49,id=9c3df6cb-795c-4fc9-beaf-ef1567f7c761
2023-04-27 06:30:07,829 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread3] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761: shutdown 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49-LeaderStateImpl
2023-04-27 06:30:07,829 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread3] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:07,830 [grpc-default-executor-3] WARN  server.GrpcServerProtocolService (LogUtils.java:warn(122)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761: installSnapshot onError, lastRequest: 1ef0e7b0-3474-409b-a89e-735ba5bcb335->9c3df6cb-795c-4fc9-beaf-ef1567f7c761#427-t1,previous=(t:1, i:42),leaderCommit=42,initializing? true,entries: size=1, first=(t:1, i:43), METADATAENTRY(c:41): org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: client cancelled
2023-04-27 06:30:07,830 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:07,830 [grpc-default-executor-4] WARN  server.GrpcServerProtocolService (LogUtils.java:warn(122)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761: installSnapshot onError, lastRequest: null: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: client cancelled
2023-04-27 06:30:07,831 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: RST_STREAM closed stream. HTTP/2 error code: CANCEL
2023-04-27 06:30:07,831 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x6ccebec3, L:/0:0:0:0:0:0:0:0:43811] CLOSE
2023-04-27 06:30:07,831 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x6ccebec3, L:/0:0:0:0:0:0:0:0:43811] INACTIVE
2023-04-27 06:30:07,831 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x6ccebec3, L:/0:0:0:0:0:0:0:0:43811] UNREGISTERED
2023-04-27 06:30:07,832 [grpc-default-executor-8] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: RST_STREAM closed stream. HTTP/2 error code: CANCEL
2023-04-27 06:30:07,832 [grpc-default-executor-8] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761: nextIndex: updateUnconditionally 44 -> 43
2023-04-27 06:30:07,832 [grpc-default-executor-1] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761: nextIndex: updateUnconditionally 43 -> 42
2023-04-27 06:30:07,832 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread3] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:07,833 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-273EDE928D49: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-1/data/ratis/35bc3d66-4dd0-4aa2-bb62-273ede928d49/sm/snapshot.1_0
2023-04-27 06:30:07,834 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-273EDE928D49: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-1/data/ratis/35bc3d66-4dd0-4aa2-bb62-273ede928d49/sm/snapshot.1_0 took: 1 ms
2023-04-27 06:30:07,834 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:07,834 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:07,835 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread3] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49: closes. applyIndex: 0
2023-04-27 06:30:07,846 [main] INFO  rpc.RpcClient (RpcClient.java:createBucket(697)) - Creating Bucket: vol1/bucket1, with bucket layout LEGACY, runner as owner, Versioning false, Storage Type set to DISK and Encryption set to false, Replication Type set to server-side default replication type, Namespace Quota set to -1, Space Quota set to -1 
2023-04-27 06:30:07,857 [grpc-default-executor-8] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:07,858 [grpc-default-executor-8] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761: nextIndex: updateUnconditionally 43 -> 42
2023-04-27 06:30:07,859 [OM StateMachine ApplyTransaction Thread - 0] INFO  bucket.OMBucketCreateRequest (OMBucketCreateRequest.java:validateAndUpdateCache(270)) - created bucket: bucket1 of layout LEGACY in volume: vol1
2023-04-27 06:30:07,860 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:07,860 [grpc-default-executor-1] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761: nextIndex: updateUnconditionally 42 -> 41
2023-04-27 06:30:07,882 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:07,882 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-273EDE928D49-SegmentedRaftLogWorker close()
2023-04-27 06:30:07,886 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-StateMachineUpdater: set stopIndex = 43
2023-04-27 06:30:07,886 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-FollowerState was interrupted
2023-04-27 06:30:07,887 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-88AC0215203D: Taking a snapshot at:(t:1, i:43) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-1/data/ratis/0b06232b-259f-4de5-84c5-88ac0215203d/sm/snapshot.1_43
2023-04-27 06:30:07,889 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-88AC0215203D: Finished taking a snapshot at:(t:1, i:43) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-1/data/ratis/0b06232b-259f-4de5-84c5-88ac0215203d/sm/snapshot.1_43 took: 2 ms
2023-04-27 06:30:07,889 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-StateMachineUpdater: Took a snapshot at index 43
2023-04-27 06:30:07,889 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 43
2023-04-27 06:30:07,892 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D: closes. applyIndex: 43
2023-04-27 06:30:07,893 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:07,893 [9c3df6cb-795c-4fc9-beaf-ef1567f7c761-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761@group-88AC0215203D-SegmentedRaftLogWorker close()
2023-04-27 06:30:07,907 [JvmPauseMonitor18] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-9c3df6cb-795c-4fc9-beaf-ef1567f7c761: Stopped
2023-04-27 06:30:07,913 [IPC Server handler 1 on default port 37175] INFO  ha.SequenceIdGenerator (SequenceIdGenerator.java:getNextId(128)) - Allocate a batch for containerId, change lastId from 0 to 1000.
2023-04-27 06:30:07,914 [IPC Server handler 1 on default port 37175] WARN  ha.SequenceIdGenerator (SequenceIdGenerator.java:allocateBatch(237)) - Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
2023-04-27 06:30:07,914 [IPC Server handler 1 on default port 37175] INFO  ha.SequenceIdGenerator (SequenceIdGenerator.java:getNextId(128)) - Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
2023-04-27 06:30:07,998 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:08,042 [Listener at 127.0.0.1/36727] INFO  net.NodeSchemaLoader (NodeSchemaLoader.java:loadSchemaFromFile(129)) - Loading schema from [jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-common/1.4.0-SNAPSHOT/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
2023-04-27 06:30:08,042 [Listener at 127.0.0.1/36727] INFO  net.NodeSchemaLoader (NodeSchemaLoader.java:loadSchema(176)) - Loading network topology layer schema file
2023-04-27 06:30:08,050 [Listener at 127.0.0.1/36727] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:08,050 [Listener at 127.0.0.1/36727] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:08,050 [Listener at 127.0.0.1/36727] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:08,054 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:08,119 [grpc-default-executor-6] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:08,119 [grpc-default-executor-8] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:08,127 [grpc-default-executor-6] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761: nextIndex: updateUnconditionally 41 -> 40
2023-04-27 06:30:08,129 [grpc-default-executor-8] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761: nextIndex: updateUnconditionally 40 -> 39
2023-04-27 06:30:08,130 [grpc-default-executor-6] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:08,131 [grpc-default-executor-6] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761: nextIndex: updateUnconditionally 40 -> 39
2023-04-27 06:30:08,132 [grpc-default-executor-8] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:08,132 [grpc-default-executor-8] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761: nextIndex: updateUnconditionally 39 -> 38
2023-04-27 06:30:08,191 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 195 milliseconds for processing 1 containers.
2023-04-27 06:30:08,208 [grpc-default-executor-8] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:08,209 [grpc-default-executor-8] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1, lastRpcResponseTime=2510) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:08,224 [grpc-default-executor-6] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:08,224 [grpc-default-executor-6] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=17, lastRpcResponseTime=2526) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:08,362 [Listener at 127.0.0.1/36727] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 307 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:08,365 [Listener at 127.0.0.1/36727] INFO  ha.SequenceIdGenerator (SequenceIdGenerator.java:upgradeToSequenceId(349)) - upgrade localId to 111677748019200000
2023-04-27 06:30:08,365 [Listener at 127.0.0.1/36727] INFO  ha.SequenceIdGenerator (SequenceIdGenerator.java:upgradeToSequenceId(359)) - upgrade delTxnId to 0
2023-04-27 06:30:08,365 [Listener at 127.0.0.1/36727] INFO  ha.SequenceIdGenerator (SequenceIdGenerator.java:upgradeToSequenceId(376)) - upgrade containerId to 0
2023-04-27 06:30:08,365 [Listener at 127.0.0.1/36727] INFO  ha.SequenceIdGenerator (SequenceIdGenerator.java:<init>(220)) - Init the HA SequenceIdGenerator.
2023-04-27 06:30:08,389 [Listener at 127.0.0.1/36727] INFO  node.SCMNodeManager (SCMNodeManager.java:<init>(156)) - Entering startup safe mode.
2023-04-27 06:30:08,389 [Listener at 127.0.0.1/36727] INFO  algorithms.ContainerPlacementPolicyFactory (ContainerPlacementPolicyFactory.java:getPolicyInternal(86)) - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
2023-04-27 06:30:08,390 [Listener at 127.0.0.1/36727] INFO  algorithms.ContainerPlacementPolicyFactory (ContainerPlacementPolicyFactory.java:getPolicyInternal(86)) - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2023-04-27 06:30:08,390 [Listener at 127.0.0.1/36727] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:initialize(78)) - No pipeline exists in current db
2023-04-27 06:30:08,390 [Listener at 127.0.0.1/36727] INFO  algorithms.LeaderChoosePolicyFactory (LeaderChoosePolicyFactory.java:getPolicy(57)) - Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
2023-04-27 06:30:08,390 [Listener at 127.0.0.1/36727] INFO  algorithms.ContainerPlacementPolicyFactory (ContainerPlacementPolicyFactory.java:getPolicyInternal(86)) - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2023-04-27 06:30:08,390 [Listener at 127.0.0.1/36727] INFO  ha.SCMServiceManager (SCMServiceManager.java:register(42)) - Registering service BackgroundPipelineCreator.
2023-04-27 06:30:08,390 [Listener at 127.0.0.1/36727] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:start(124)) - Starting RatisPipelineUtilsThread.
2023-04-27 06:30:08,391 [Listener at 127.0.0.1/36727] INFO  BackgroundPipelineScrubber (BackgroundSCMService.java:start(68)) - Starting BackgroundPipelineScrubber Service.
2023-04-27 06:30:08,391 [Listener at 127.0.0.1/36727] INFO  ha.SCMServiceManager (SCMServiceManager.java:register(42)) - Registering service BackgroundPipelineScrubber.
2023-04-27 06:30:08,392 [Listener at 127.0.0.1/36727] INFO  ExpiredContainerReplicaOpScrubber (BackgroundSCMService.java:start(68)) - Starting ExpiredContainerReplicaOpScrubber Service.
2023-04-27 06:30:08,392 [Listener at 127.0.0.1/36727] INFO  ha.SCMServiceManager (SCMServiceManager.java:register(42)) - Registering service ExpiredContainerReplicaOpScrubber.
2023-04-27 06:30:08,393 [Listener at 127.0.0.1/36727] INFO  algorithms.PipelineChoosePolicyFactory (PipelineChoosePolicyFactory.java:createPipelineChoosePolicyFromClass(73)) - Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
2023-04-27 06:30:08,393 [Listener at 127.0.0.1/36727] INFO  ha.SCMServiceManager (SCMServiceManager.java:register(42)) - Registering service SCMBlockDeletingService.
2023-04-27 06:30:08,395 [Listener at 127.0.0.1/36727] INFO  replication.ReplicationManager (ReplicationManager.java:start(288)) - Starting Replication Monitor Thread.
2023-04-27 06:30:08,397 [Listener at 127.0.0.1/36727] INFO  ha.SCMServiceManager (SCMServiceManager.java:register(42)) - Registering service ReplicationManager.
2023-04-27 06:30:08,397 [Listener at 127.0.0.1/36727] INFO  safemode.ContainerSafeModeRule (ContainerSafeModeRule.java:<init>(89)) - containers with one replica threshold count 0
2023-04-27 06:30:08,397 [Listener at 127.0.0.1/36727] INFO  safemode.HealthyPipelineSafeModeRule (HealthyPipelineSafeModeRule.java:initializeRule(169)) - Total pipeline count is 0, healthy pipeline threshold count is 1
2023-04-27 06:30:08,397 [Listener at 127.0.0.1/36727] INFO  safemode.OneReplicaPipelineSafeModeRule (OneReplicaPipelineSafeModeRule.java:initializeRule(180)) - Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
2023-04-27 06:30:08,406 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:08,412 [Listener at 127.0.0.1/36727] INFO  server.StorageContainerManager (StorageContainerManager.java:<init>(403)) - SCM start with adminUsers: [runner]
2023-04-27 06:30:08,412 [Listener at 127.0.0.1/36727] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(90)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-04-27 06:30:08,413 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1319)) - Starting Socket Reader #1 for port 0
2023-04-27 06:30:08,414 [Listener at 0.0.0.0/38893] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(90)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-04-27 06:30:08,414 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1319)) - Starting Socket Reader #1 for port 0
2023-04-27 06:30:08,415 [Listener at 0.0.0.0/46021] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(90)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-04-27 06:30:08,415 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1319)) - Starting Socket Reader #1 for port 0
2023-04-27 06:30:08,470 [Listener at 0.0.0.0/44011] INFO  ha.SCMServiceManager (SCMServiceManager.java:register(42)) - Registering service ContainerBalancer.
2023-04-27 06:30:08,471 [Listener at 0.0.0.0/44011] INFO  server.StorageContainerManager (StorageContainerManager.java:<init>(420)) - 
Container Balancer status:
Key                            Value
Running                        false
Container Balancer Configuration values:
Key                                                Value
Threshold                                          10
Max Datanodes to Involve per Iteration(percent)    20
Max Size to Move per Iteration                     500GB
Max Size Entering Target per Iteration             26GB
Max Size Leaving Source per Iteration              26GB

2023-04-27 06:30:08,471 [Listener at 0.0.0.0/44011] INFO  ha.SCMContext (SCMContext.java:updateSafeModeStatus(228)) - Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
2023-04-27 06:30:08,471 [Listener at 0.0.0.0/44011] INFO  server.StorageContainerManager (StorageContainerManager.java:start(1477)) - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:44011
2023-04-27 06:30:08,473 [Listener at 0.0.0.0/44011] WARN  impl.MetricsConfig (MetricsConfig.java:loadFirst(136)) - Cannot locate configuration: tried hadoop-metrics2-storagecontainermanager.properties,hadoop-metrics2.properties
2023-04-27 06:30:08,474 [Listener at 0.0.0.0/44011] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(378)) - Scheduled Metric snapshot period at 10 second(s).
2023-04-27 06:30:08,474 [Listener at 0.0.0.0/44011] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - StorageContainerManager metrics system started
2023-04-27 06:30:08,502 [Listener at 0.0.0.0/44011] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:start(204)) - Sink prometheus started
2023-04-27 06:30:08,502 [Listener at 0.0.0.0/44011] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:registerSink(305)) - Registered sink prometheus
2023-04-27 06:30:08,545 [Listener at 0.0.0.0/44011] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:start(197)) - RPC server for Client  is listening at /0.0.0.0:44011
2023-04-27 06:30:08,549 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1558)) - IPC Server Responder: starting
2023-04-27 06:30:08,552 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1398)) - IPC Server listener on 0: starting
2023-04-27 06:30:08,561 [Listener at 0.0.0.0/44011] INFO  server.StorageContainerManager (StorageContainerManager.java:start(1491)) - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:46021
2023-04-27 06:30:08,561 [Listener at 0.0.0.0/44011] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:start(152)) - RPC server for Block Protocol is listening at /0.0.0.0:46021
2023-04-27 06:30:08,566 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1558)) - IPC Server Responder: starting
2023-04-27 06:30:08,570 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1398)) - IPC Server listener on 0: starting
2023-04-27 06:30:08,639 [Listener at 0.0.0.0/44011] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:start(193)) - ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:38893
2023-04-27 06:30:08,642 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1558)) - IPC Server Responder: starting
2023-04-27 06:30:08,645 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1398)) - IPC Server listener on 0: starting
2023-04-27 06:30:08,752 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4f74f9e9] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:08,753 [Listener at 0.0.0.0/44011] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for scm at: http://0.0.0.0:0
2023-04-27 06:30:08,755 [Listener at 0.0.0.0/44011] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:08,756 [Listener at 0.0.0.0/44011] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:08,761 [Listener at 0.0.0.0/44011] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.scm is not defined
2023-04-27 06:30:08,762 [Listener at 0.0.0.0/44011] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:08,762 [Listener at 0.0.0.0/44011] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
2023-04-27 06:30:08,762 [Listener at 0.0.0.0/44011] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:08,762 [Listener at 0.0.0.0/44011] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:08,763 [Listener at 0.0.0.0/44011] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of scm uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/ozone-meta/webserver
2023-04-27 06:30:08,763 [Listener at 0.0.0.0/44011] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 37539
2023-04-27 06:30:08,763 [Listener at 0.0.0.0/44011] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:08,806 [Listener at 0.0.0.0/44011] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:08,806 [Listener at 0.0.0.0/44011] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:08,806 [Listener at 0.0.0.0/44011] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 660000ms
2023-04-27 06:30:08,810 [Listener at 0.0.0.0/44011] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@660b264{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:08,810 [Listener at 0.0.0.0/44011] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@16c57528{static,/static,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/static,AVAILABLE}
2023-04-27 06:30:08,825 [Listener at 0.0.0.0/44011] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@50fd15f9{scm,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/scm/,AVAILABLE}{file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/scm}
2023-04-27 06:30:08,847 [Listener at 0.0.0.0/44011] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@1de2227e{HTTP/1.1, (http/1.1)}{0.0.0.0:37539}
2023-04-27 06:30:08,847 [Listener at 0.0.0.0/44011] INFO  server.Server (Server.java:doStart(415)) - Started @135295ms
2023-04-27 06:30:08,847 [Listener at 0.0.0.0/44011] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:08,848 [Listener at 0.0.0.0/44011] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of scm listening at http://0.0.0.0:37539
2023-04-27 06:30:08,849 [Listener at 0.0.0.0/44011] WARN  server.ServerUtils (ServerUtils.java:getDBPath(225)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:08,852 [Listener at 0.0.0.0/44011] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:loadOMHAConfig(115)) - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
2023-04-27 06:30:08,852 [Listener at 0.0.0.0/44011] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:loadOMHAConfig(226)) - Configuration does not have ozone.om.address set. Falling back to the default OM address /127.0.0.1:0
2023-04-27 06:30:08,852 [Listener at 0.0.0.0/44011] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:getOMNodeDetailsForNonHA(254)) - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
2023-04-27 06:30:08,852 [Listener at 0.0.0.0/44011] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:getOMNodeDetailsForNonHA(261)) - OM Node ID is not set. Setting it to the default ID: om1
2023-04-27 06:30:08,852 [Listener at 0.0.0.0/44011] WARN  server.ServerUtils (ServerUtils.java:getDBPath(225)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:08,855 [Listener at 0.0.0.0/44011] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = HSYNC (version = 4), software layout = HSYNC (version = 4)
2023-04-27 06:30:08,999 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:09,054 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:09,191 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:09,220 [Listener at 0.0.0.0/44011] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 360 ms to scan 2 urls, producing 172 keys and 478 values [using 2 cores]
2023-04-27 06:30:09,220 [Listener at 0.0.0.0/44011] INFO  upgrade.OMLayoutVersionManager (OMLayoutVersionManager.java:lambda$0(115)) - Skipping Upgrade Action MockOmUpgradeAction since it has been finalized.
2023-04-27 06:30:09,221 [Listener at 0.0.0.0/44011] WARN  server.ServerUtils (ServerUtils.java:getDBPath(225)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:09,221 [Listener at 0.0.0.0/44011] INFO  proxy.SCMBlockLocationFailoverProxyProvider (SCMBlockLocationFailoverProxyProvider.java:<init>(114)) - Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=/0.0.0.0:46021]
2023-04-27 06:30:09,222 [Listener at 0.0.0.0/44011] INFO  proxy.SCMBlockLocationFailoverProxyProvider (SCMBlockLocationFailoverProxyProvider.java:<init>(114)) - Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=/0.0.0.0:46021]
2023-04-27 06:30:09,376 [Listener at 0.0.0.0/44011] INFO  om.OzoneManager (OzoneManager.java:<init>(634)) - OM start with adminUsers: [runner]
2023-04-27 06:30:09,377 [Listener at 0.0.0.0/44011] WARN  server.ServerUtils (ServerUtils.java:getDBPath(225)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:09,377 [Listener at 0.0.0.0/44011] INFO  codec.OmKeyInfoCodec (OmKeyInfoCodec.java:<init>(49)) - OmKeyInfoCodec ignorePipeline = true
2023-04-27 06:30:09,377 [Listener at 0.0.0.0/44011] INFO  codec.RepeatedOmKeyInfoCodec (RepeatedOmKeyInfoCodec.java:<init>(41)) - RepeatedOmKeyInfoCodec ignorePipeline = true
2023-04-27 06:30:09,379 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,380 [grpc-default-executor-5] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761: nextIndex: updateUnconditionally 38 -> 37
2023-04-27 06:30:09,380 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,380 [grpc-default-executor-1] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761: nextIndex: updateUnconditionally 37 -> 36
2023-04-27 06:30:09,390 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,390 [grpc-default-executor-6] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,390 [grpc-default-executor-1] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761: nextIndex: updateUnconditionally 37 -> 36
2023-04-27 06:30:09,392 [grpc-default-executor-6] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761: nextIndex: updateUnconditionally 36 -> 35
2023-04-27 06:30:09,406 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:09,461 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,462 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,462 [grpc-default-executor-1] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1255, lastRpcResponseTime=3763) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,463 [grpc-default-executor-5] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1255, lastRpcResponseTime=3764) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,465 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,465 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,465 [grpc-default-executor-5] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1258, lastRpcResponseTime=3766) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,466 [grpc-default-executor-1] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1258, lastRpcResponseTime=3767) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,467 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,467 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,468 [grpc-default-executor-1] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1260, lastRpcResponseTime=3769) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,468 [grpc-default-executor-5] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1260, lastRpcResponseTime=3769) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,469 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,469 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,470 [grpc-default-executor-5] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1262, lastRpcResponseTime=3771) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,470 [grpc-default-executor-1] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1262, lastRpcResponseTime=3771) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,471 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,471 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,472 [grpc-default-executor-1] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1264, lastRpcResponseTime=3773) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,472 [grpc-default-executor-5] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1265, lastRpcResponseTime=3773) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,473 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,474 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,474 [grpc-default-executor-5] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1267, lastRpcResponseTime=3775) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,475 [grpc-default-executor-1] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1267, lastRpcResponseTime=3776) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,476 [grpc-default-executor-1] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,476 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,476 [grpc-default-executor-1] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1269, lastRpcResponseTime=3777) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,476 [grpc-default-executor-5] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1269, lastRpcResponseTime=3778) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,477 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,478 [grpc-default-executor-5] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1270, lastRpcResponseTime=3779) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,504 [grpc-default-executor-6] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,505 [grpc-default-executor-6] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=27, lastRpcResponseTime=3806) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,506 [grpc-default-executor-4] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,506 [grpc-default-executor-4] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=28, lastRpcResponseTime=3807) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,507 [grpc-default-executor-6] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:09,507 [grpc-default-executor-6] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=29, lastRpcResponseTime=3808) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:09,854 [ContainerOp-13baae5c-b58f-4d4f-a606-8969f0a26ac7-1] INFO  keyvalue.KeyValueHandler (ContainerUtils.java:logAndReturnError(91)) - Operation: PutBlock , Trace ID:  , Message: java.lang.NullPointerException , Result: CONTAINER_INTERNAL_ERROR , StorageContainerException Occurred.
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: java.lang.NullPointerException
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:235)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:320)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.lambda$dispatch$0(HddsDispatcher.java:171)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:170)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:439)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:449)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$submitTask$8(ContainerStateMachine.java:834)
	at org.apache.ratis.util.TaskQueue.lambda$submit$0(TaskQueue.java:121)
	at org.apache.ratis.util.LogUtils.runAndLog(LogUtils.java:38)
	at org.apache.ratis.util.LogUtils$1.run(LogUtils.java:79)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.persistPutBlock(BlockManagerImpl.java:154)
	at org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.putBlock(BlockManagerImpl.java:103)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handlePutBlock(KeyValueHandler.java:556)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:261)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:232)
	... 15 more
2023-04-27 06:30:09,867 [ContainerOp-13baae5c-b58f-4d4f-a606-8969f0a26ac7-1] WARN  keyvalue.KeyValueContainer (KeyValueContainer.java:markContainerUnhealthy(348)) - Moving container /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-1/data-0/containers/hdds/96a9b0ac-a466-4378-aad7-6952c9070703/current/containerDir0/3 to state UNHEALTHY from state:OPEN
2023-04-27 06:30:09,868 [ContainerOp-13baae5c-b58f-4d4f-a606-8969f0a26ac7-1] INFO  impl.HddsDispatcher (HddsDispatcher.java:dispatchRequest(361)) - Marked Container UNHEALTHY, ContainerID: 3
2023-04-27 06:30:09,870 [FixedThreadPoolWithAffinityExecutor-8-0] WARN  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(259)) - Container #3 is in OPEN state, but the datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) reports an UNHEALTHY replica.
2023-04-27 06:30:09,873 [Mini-Cluster-Provider-Reap] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-4/data-0/containers/hdds/d25cf8bb-08a8-4312-8236-5e4a36901885/DS-3544052f-fd8c-43f9-9cfc-a6286320ba25/container.db for volume DS-3544052f-fd8c-43f9-9cfc-a6286320ba25
2023-04-27 06:30:09,873 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:09,874 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:09,912 [Mini-Cluster-Provider-Reap] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
06:30:09.891 [ContainerOp-13baae5c-b58f-4d4f-a606-8969f0a26ac7-1] ERROR DNAudit - user=null | ip=null | op=PUT_BLOCK {blockData=[blockId=conID: 3 locID: 111677748019200019 bcsId: 0, size=19]} | ret=FAILURE
java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:389) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.lambda$dispatch$0(HddsDispatcher.java:171) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87) ~[hdds-server-framework-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:170) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:439) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:449) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$submitTask$8(ContainerStateMachine.java:834) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.ratis.util.TaskQueue.lambda$submit$0(TaskQueue.java:121) ~[ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:2.4.2-8b8bdda-SNAPSHOT]
	at org.apache.ratis.util.LogUtils.runAndLog(LogUtils.java:38) [ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:2.4.2-8b8bdda-SNAPSHOT]
	at org.apache.ratis.util.LogUtils$1.run(LogUtils.java:79) [ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:2.4.2-8b8bdda-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_362]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_362]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_362]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_362]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_362]
2023-04-27 06:30:09,979 [ContainerOp-13baae5c-b58f-4d4f-a606-8969f0a26ac7-1] ERROR ratis.ContainerStateMachine (ContainerStateMachine.java:lambda$applyTransaction$10(927)) - gid group-8969F0A26AC7 : ApplyTransaction failed. cmd PutBlock logIndex 42 msg : java.lang.NullPointerException Container Result: CONTAINER_INTERNAL_ERROR
2023-04-27 06:30:09,985 [ForkJoinPool.commonPool-worker-0] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-1/data-0/containers/hdds/d25cf8bb-08a8-4312-8236-5e4a36901885/DS-806b4f26-300e-48c1-a135-8cf7131e9113/container.db for volume DS-806b4f26-300e-48c1-a135-8cf7131e9113
2023-04-27 06:30:09,985 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:09,997 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:09,999 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:09,999 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
2023-04-27 06:30:10,005 [IPC Server handler 13 on default port 37175] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 37ee7bc9-b834-49c7-bf6a-23376a986332, Nodes: 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:09.998Z[Etc/UTC]].
2023-04-27 06:30:10,006 [IPC Server handler 13 on default port 37175] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 37ee7bc9-b834-49c7-bf6a-23376a986332, Nodes: 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:09.998Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:10,036 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@554937c9{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:10,040 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@417cdf2e{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:10,042 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:10,051 [ContainerOp-13baae5c-b58f-4d4f-a606-8969f0a26ac7-1] ERROR ratis.XceiverServerRatis (XceiverServerRatis.java:triggerPipelineClose(719)) - pipeline Action CLOSE on pipeline PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7.Reason : Ratis Transaction failure in datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083 with role FOLLOWER .Triggering pipeline close action.
2023-04-27 06:30:10,055 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:10,053 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@7b7ae505{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:10,068 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@27c6c33{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:10,071 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.PipelineActionHandler (PipelineActionHandler.java:processPipelineAction(83)) - Received pipeline action CLOSE for PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 from datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083. Reason : Ratis Transaction failure in datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083 with role FOLLOWER .Triggering pipeline close action.
2023-04-27 06:30:10,074 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #1 closed for pipeline=PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7
2023-04-27 06:30:10,090 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #3 closed for pipeline=PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7
2023-04-27 06:30:10,092 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #4 closed for pipeline=PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7
2023-04-27 06:30:10,092 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 13baae5c-b58f-4d4f-a606-8969f0a26ac7, Nodes: a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:2b022b8d-8bf4-4c13-9ad2-4e7e18903189, CreationTimestamp2023-04-27T06:28:59.846Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:10,092 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 close command to datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083
2023-04-27 06:30:10,092 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 close command to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d
2023-04-27 06:30:10,092 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 close command to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189
2023-04-27 06:30:10,093 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 13baae5c-b58f-4d4f-a606-8969f0a26ac7, Nodes: a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:2b022b8d-8bf4-4c13-9ad2-4e7e18903189, CreationTimestamp2023-04-27T06:28:59.846Z[Etc/UTC]] removed.
2023-04-27 06:30:10,104 [Mini-Cluster-Provider-Reap] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:10,106 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - d34b33f9-e64c-4718-893f-5b1a944a0d93: close
2023-04-27 06:30:10,124 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15: shutdown
2023-04-27 06:30:10,125 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-A34746412A15,id=d34b33f9-e64c-4718-893f-5b1a944a0d93
2023-04-27 06:30:10,125 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - d34b33f9-e64c-4718-893f-5b1a944a0d93: shutdown d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15-LeaderStateImpl
2023-04-27 06:30:10,127 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:10,129 [d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-A34746412A15: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-6/data/ratis/3ee33afa-e5d6-4fca-b2f7-a34746412a15/sm/snapshot.1_0
2023-04-27 06:30:10,130 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:10,130 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@4aa09a6c{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:10,131 [d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-A34746412A15: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-6/data/ratis/3ee33afa-e5d6-4fca-b2f7-a34746412a15/sm/snapshot.1_0 took: 1 ms
2023-04-27 06:30:10,132 [d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:10,132 [d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:10,132 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15: closes. applyIndex: 0
2023-04-27 06:30:10,133 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@78fd61e5{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:10,133 [ForkJoinPool.commonPool-worker-0] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:10,147 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - d34b33f9-e64c-4718-893f-5b1a944a0d93: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:10,149 [grpc-default-executor-8] WARN  server.GrpcServerProtocolService (LogUtils.java:warn(122)) - d34b33f9-e64c-4718-893f-5b1a944a0d93: installSnapshot onError, lastRequest: 45e99655-b061-40f7-b011-1b1883fa2319->d34b33f9-e64c-4718-893f-5b1a944a0d93#1-t1,previous=(t:0, i:0),leaderCommit=0,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "d34b33f9-e64c-4718-893f-5b1a944a0d93"
address: "10.1.0.33:42891"
dataStreamAddress: "10.1.0.33:37831"
clientAddress: "10.1.0.33:42891"
adminAddress: "10.1.0.33:42891"
startupRole: FOLLOWER
,id: "45e99655-b061-40f7-b011-1b1883fa2319"
address: "10.1.0.33:34159"
priority: 1
dataStreamAddress: "10.1.0.33:46485"
clientAddress: "10.1.0.33:34159"
adminAddress: "10.1.0.33:34159"
startupRole: FOLLOWER
,id: "2ebc1c73-026e-4922-9f91-c27c5284ff39"
address: "10.1.0.33:40783"
dataStreamAddress: "10.1.0.33:41415"
clientAddress: "10.1.0.33:40783"
adminAddress: "10.1.0.33:40783"
startupRole: FOLLOWER
, old:): org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: client cancelled
2023-04-27 06:30:10,149 [grpc-default-executor-4] WARN  server.GrpcServerProtocolService (LogUtils.java:warn(122)) - d34b33f9-e64c-4718-893f-5b1a944a0d93: installSnapshot onError, lastRequest: null: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: client cancelled
2023-04-27 06:30:10,149 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - d34b33f9-e64c-4718-893f-5b1a944a0d93: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:10,150 [d34b33f9-e64c-4718-893f-5b1a944a0d93-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x6bdba8a6, L:/0:0:0:0:0:0:0:0:37831] CLOSE
2023-04-27 06:30:10,150 [d34b33f9-e64c-4718-893f-5b1a944a0d93-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x6bdba8a6, L:/0:0:0:0:0:0:0:0:37831] INACTIVE
2023-04-27 06:30:10,150 [d34b33f9-e64c-4718-893f-5b1a944a0d93-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x6bdba8a6, L:/0:0:0:0:0:0:0:0:37831] UNREGISTERED
2023-04-27 06:30:10,150 [grpc-default-executor-8] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->d34b33f9-e64c-4718-893f-5b1a944a0d93-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: RST_STREAM closed stream. HTTP/2 error code: CANCEL
2023-04-27 06:30:10,150 [grpc-default-executor-8] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->d34b33f9-e64c-4718-893f-5b1a944a0d93-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->d34b33f9-e64c-4718-893f-5b1a944a0d93(c0,m0,n1, attendVote=true, lastRpcSendTime=1943, lastRpcResponseTime=1922) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:10,151 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread3] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259: shutdown
2023-04-27 06:30:10,151 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread3] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-238259678259,id=d34b33f9-e64c-4718-893f-5b1a944a0d93
2023-04-27 06:30:10,151 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread3] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - d34b33f9-e64c-4718-893f-5b1a944a0d93: shutdown d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-FollowerState
2023-04-27 06:30:10,181 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@4745fa22{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:10,181 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@67bda9d6{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:10,181 [d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:10,198 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #3, current state: CLOSING
2023-04-27 06:30:10,199 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #1, current state: CLOSING
2023-04-27 06:30:10,199 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #3, current state: CLOSING
2023-04-27 06:30:10,199 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #4, current state: CLOSING
2023-04-27 06:30:10,199 [grpc-default-executor-8] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->d34b33f9-e64c-4718-893f-5b1a944a0d93-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: RST_STREAM closed stream. HTTP/2 error code: CANCEL
2023-04-27 06:30:10,202 [grpc-default-executor-8] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->d34b33f9-e64c-4718-893f-5b1a944a0d93-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->d34b33f9-e64c-4718-893f-5b1a944a0d93(c0,m0,n1, attendVote=true, lastRpcSendTime=1995, lastRpcResponseTime=1973) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:10,212 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:10,212 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33).
2023-04-27 06:30:10,212 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33).
2023-04-27 06:30:10,212 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33).
2023-04-27 06:30:10,212 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:10,213 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33).
2023-04-27 06:30:10,213 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:10,213 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33).
2023-04-27 06:30:10,217 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-A34746412A15-SegmentedRaftLogWorker close()
2023-04-27 06:30:10,219 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread3] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:10,220 [d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-FollowerState was interrupted
2023-04-27 06:30:10,220 [d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-238259678259: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-6/data/ratis/0013f2b1-5ac3-4f2d-a0a1-238259678259/sm/snapshot.1_0
2023-04-27 06:30:10,221 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 9 milliseconds for processing 7 containers.
2023-04-27 06:30:10,222 [d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-238259678259: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-6/data/ratis/0013f2b1-5ac3-4f2d-a0a1-238259678259/sm/snapshot.1_0 took: 3 ms
2023-04-27 06:30:10,223 [d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:10,223 [d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:10,231 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread3] INFO  server.RaftServer$Division (ServerState.java:close(466)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259: closes. applyIndex: 0
2023-04-27 06:30:10,233 [Listener at 0.0.0.0/44011] INFO  om.OzoneManager (OzoneManager.java:instantiateServices(764)) - S3 Multi-Tenancy is disabled
2023-04-27 06:30:10,235 [Listener at 0.0.0.0/44011] WARN  server.ServerUtils (ServerUtils.java:getDBPath(225)) - ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:10,239 [d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:10,239 [d34b33f9-e64c-4718-893f-5b1a944a0d93-impl-thread3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - d34b33f9-e64c-4718-893f-5b1a944a0d93@group-238259678259-SegmentedRaftLogWorker close()
2023-04-27 06:30:10,272 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:10,274 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 is not found
2023-04-27 06:30:10,289 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335: close
2023-04-27 06:30:10,290 [JvmPauseMonitor24] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-d34b33f9-e64c-4718-893f-5b1a944a0d93: Stopped
2023-04-27 06:30:10,311 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D: shutdown
2023-04-27 06:30:10,311 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-88AC0215203D,id=1ef0e7b0-3474-409b-a89e-735ba5bcb335
2023-04-27 06:30:10,312 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335: shutdown 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D-LeaderStateImpl
2023-04-27 06:30:10,314 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:10,315 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f Close channels
2023-04-27 06:30:10,315 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread3] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B: shutdown
2023-04-27 06:30:10,315 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread3] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-1E3407FA9A4B,id=1ef0e7b0-3474-409b-a89e-735ba5bcb335
2023-04-27 06:30:10,315 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread3] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335: shutdown 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B-LeaderStateImpl
2023-04-27 06:30:10,316 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread3] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:10,317 [1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-1E3407FA9A4B: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-2/data/ratis/38a72eae-e492-49a9-a27b-1e3407fa9a4b/sm/snapshot.1_0
2023-04-27 06:30:10,317 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread3] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:10,318 [1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-1E3407FA9A4B: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-2/data/ratis/38a72eae-e492-49a9-a27b-1e3407fa9a4b/sm/snapshot.1_0 took: 1 ms
2023-04-27 06:30:10,318 [1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:10,318 [1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:10,320 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:10,323 [grpc-default-executor-6] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f: Completed APPEND_ENTRIES, lastRequest: 1ef0e7b0-3474-409b-a89e-735ba5bcb335->f83db7c9-4a26-48b7-8329-a6a26bc6bc0f#429-t1,previous=(t:1, i:42),leaderCommit=42,initializing? true,entries: size=1, first=(t:1, i:43), METADATAENTRY(c:41)
2023-04-27 06:30:10,323 [1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-GrpcLogAppender-LogAppenderDaemon] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(200)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->9c3df6cb-795c-4fc9-beaf-ef1567f7c761-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2023-04-27 06:30:10,324 [grpc-default-executor-5] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f: Completed APPEND_ENTRIES, lastRequest: null
2023-04-27 06:30:10,332 [1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-88AC0215203D: Taking a snapshot at:(t:1, i:43) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-2/data/ratis/0b06232b-259f-4de5-84c5-88ac0215203d/sm/snapshot.1_43
2023-04-27 06:30:10,332 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D-StateMachineUpdater: set stopIndex = 43
2023-04-27 06:30:10,334 [1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-88AC0215203D: Finished taking a snapshot at:(t:1, i:43) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-2/data/ratis/0b06232b-259f-4de5-84c5-88ac0215203d/sm/snapshot.1_43 took: 2 ms
2023-04-27 06:30:10,334 [1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D-StateMachineUpdater: Took a snapshot at index 43
2023-04-27 06:30:10,334 [1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 43
2023-04-27 06:30:10,335 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D: closes. applyIndex: 43
2023-04-27 06:30:10,335 [1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:10,335 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D-SegmentedRaftLogWorker close()
2023-04-27 06:30:10,337 [grpc-default-executor-8] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:10,340 [grpc-default-executor-3] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:10,342 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761 Close channels
2023-04-27 06:30:10,343 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:10,343 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread3] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B: closes. applyIndex: 0
2023-04-27 06:30:10,348 [grpc-default-executor-8] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(137)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-GrpcLogAppender: Failed to getClient for f83db7c9-4a26-48b7-8329-a6a26bc6bc0f
org.apache.ratis.protocol.exceptions.AlreadyClosedException: 1ef0e7b0-3474-409b-a89e-735ba5bcb335 is already CLOSED
	at org.apache.ratis.util.PeerProxyMap$PeerAndProxy.getProxy(PeerProxyMap.java:61)
	at org.apache.ratis.util.PeerProxyMap.getProxy(PeerProxyMap.java:115)
	at org.apache.ratis.grpc.server.GrpcLogAppender.getClient(GrpcLogAppender.java:116)
	at org.apache.ratis.grpc.server.GrpcLogAppender.resetClient(GrpcLogAppender.java:121)
	at org.apache.ratis.grpc.server.GrpcLogAppender.access$500(GrpcLogAppender.java:58)
	at org.apache.ratis.grpc.server.GrpcLogAppender$AppendLogResponseHandler.onCompleted(GrpcLogAppender.java:416)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:485)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:562)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:743)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:722)
	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:10,348 [grpc-default-executor-3] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(137)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-88AC0215203D->f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-GrpcLogAppender: Failed to getClient for f83db7c9-4a26-48b7-8329-a6a26bc6bc0f
org.apache.ratis.protocol.exceptions.AlreadyClosedException: 1ef0e7b0-3474-409b-a89e-735ba5bcb335 is already CLOSED
	at org.apache.ratis.util.PeerProxyMap$PeerAndProxy.getProxy(PeerProxyMap.java:61)
	at org.apache.ratis.util.PeerProxyMap.getProxy(PeerProxyMap.java:115)
	at org.apache.ratis.grpc.server.GrpcLogAppender.getClient(GrpcLogAppender.java:116)
	at org.apache.ratis.grpc.server.GrpcLogAppender.resetClient(GrpcLogAppender.java:121)
	at org.apache.ratis.grpc.server.GrpcLogAppender.access$500(GrpcLogAppender.java:58)
	at org.apache.ratis.grpc.server.GrpcLogAppender$AppendLogResponseHandler.onCompleted(GrpcLogAppender.java:416)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:485)
	at org.apache.ratis.thirdparty.io.grpc.internal.DelayedClientCall$DelayedListener$3.run(DelayedClientCall.java:468)
	at org.apache.ratis.thirdparty.io.grpc.internal.DelayedClientCall$DelayedListener.delayOrExecute(DelayedClientCall.java:432)
	at org.apache.ratis.thirdparty.io.grpc.internal.DelayedClientCall$DelayedListener.onClose(DelayedClientCall.java:465)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:562)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:743)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:722)
	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:10,361 [1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:10,361 [Listener at 0.0.0.0/44011] INFO  om.OzoneManager (OzoneManager.java:addS3GVolumeToDB(4263)) - Created Volume s3v With Owner runner required for S3Gateway operations.
2023-04-27 06:30:10,361 [Listener at 0.0.0.0/44011] WARN  server.ServerUtils (ServerUtils.java:getDefaultRatisDirectory(237)) - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
2023-04-27 06:30:10,361 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-impl-thread3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335@group-1E3407FA9A4B-SegmentedRaftLogWorker close()
2023-04-27 06:30:10,361 [Listener at 0.0.0.0/44011] WARN  utils.OzoneManagerRatisUtils (OzoneManagerRatisUtils.java:getOMRatisSnapshotDirectory(459)) - ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
2023-04-27 06:30:10,362 [Listener at 0.0.0.0/44011] WARN  server.ServerUtils (ServerUtils.java:getDefaultRatisDirectory(237)) - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
2023-04-27 06:30:10,362 [Listener at 0.0.0.0/44011] INFO  ratis.OzoneManagerRatisServer (OzoneManagerRatisServer.java:<init>(163)) - Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: localhost:37081
2023-04-27 06:30:10,363 [Listener at 0.0.0.0/44011] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:loadSnapshotInfoFromDB(670)) - LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
2023-04-27 06:30:10,363 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xc6c98920, L:/0:0:0:0:0:0:0:0:45649] CLOSE
2023-04-27 06:30:10,363 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xc6c98920, L:/0:0:0:0:0:0:0:0:45649] INACTIVE
2023-04-27 06:30:10,363 [1ef0e7b0-3474-409b-a89e-735ba5bcb335-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xc6c98920, L:/0:0:0:0:0:0:0:0:45649] UNREGISTERED
2023-04-27 06:30:10,368 [Listener at 0.0.0.0/44011] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:10,368 [Listener at 0.0.0.0/44011] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:10,368 [Listener at 0.0.0.0/44011] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.port = 37081 (fallback to raft.grpc.server.port)
2023-04-27 06:30:10,368 [Listener at 0.0.0.0/44011] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:10,368 [Listener at 0.0.0.0/44011] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.port = 37081 (fallback to raft.grpc.server.port)
2023-04-27 06:30:10,368 [Listener at 0.0.0.0/44011] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:10,368 [Listener at 0.0.0.0/44011] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 37081 (custom)
2023-04-27 06:30:10,369 [Listener at 0.0.0.0/44011] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 33554432 (custom)
2023-04-27 06:30:10,370 [Listener at 0.0.0.0/44011] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:10,370 [Listener at 0.0.0.0/44011] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2023-04-27 06:30:10,370 [Listener at 0.0.0.0/44011] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 3000ms (default)
2023-04-27 06:30:10,370 [Listener at 0.0.0.0/44011] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:10,370 [Listener at 0.0.0.0/44011] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:10,370 [Listener at 0.0.0.0/44011] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:10,373 [Listener at 0.0.0.0/44011] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = DISABLED (default)
2023-04-27 06:30:10,373 [Listener at 0.0.0.0/44011] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:10,373 [Listener at 0.0.0.0/44011] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:10,373 [Listener at 0.0.0.0/44011] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 120s (custom)
2023-04-27 06:30:10,373 [Listener at 0.0.0.0/44011] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:10,373 [Listener at 0.0.0.0/44011] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/ozone-meta/ratis] (custom)
2023-04-27 06:30:10,406 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:10,414 [Listener at 0.0.0.0/44011] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - om1: addNew group-C5BA1605619E:[om1|rpc:localhost:37081|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@38a3386b[Not completed]
2023-04-27 06:30:10,414 [Listener at 0.0.0.0/44011] INFO  om.OzoneManager (OzoneManager.java:initializeRatisServer(2106)) - OzoneManager Ratis server initialized at port 37081
2023-04-27 06:30:10,414 [Listener at 0.0.0.0/44011] INFO  om.OzoneManager (OzoneManager.java:getRpcServer(1153)) - Creating RPC Server
2023-04-27 06:30:10,416 [pool-1955-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:localhost:37081|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
2023-04-27 06:30:10,416 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 1s (custom)
2023-04-27 06:30:10,416 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 1200ms (custom)
2023-04-27 06:30:10,416 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:10,416 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 120s (custom)
2023-04-27 06:30:10,416 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:10,416 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:10,416 [pool-1955-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:localhost:37081|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:10,417 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/ozone-meta/ratis] (custom)
2023-04-27 06:30:10,418 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:10,418 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:10,418 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 120s (custom)
2023-04-27 06:30:10,418 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 300s (custom)
2023-04-27 06:30:10,418 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:10,422 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:10,422 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:10,422 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:10,423 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:10,423 [pool-1955-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:10,441 [JvmPauseMonitor19] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-1ef0e7b0-3474-409b-a89e-735ba5bcb335: Stopped
2023-04-27 06:30:10,580 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33) moved to stale state. Finalizing its pipelines [PipelineID=0b06232b-259f-4de5-84c5-88ac0215203d, PipelineID=35bc3d66-4dd0-4aa2-bb62-273ede928d49]
2023-04-27 06:30:10,581 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #3 closed for pipeline=PipelineID=0b06232b-259f-4de5-84c5-88ac0215203d
2023-04-27 06:30:10,581 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #5 closed for pipeline=PipelineID=0b06232b-259f-4de5-84c5-88ac0215203d
2023-04-27 06:30:10,581 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #6 closed for pipeline=PipelineID=0b06232b-259f-4de5-84c5-88ac0215203d
2023-04-27 06:30:10,582 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 0b06232b-259f-4de5-84c5-88ac0215203d, Nodes: 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33)f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33)9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:1ef0e7b0-3474-409b-a89e-735ba5bcb335, CreationTimestamp2023-04-27T06:28:41.730Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:10,582 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 35bc3d66-4dd0-4aa2-bb62-273ede928d49, Nodes: 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:9c3df6cb-795c-4fc9-beaf-ef1567f7c761, CreationTimestamp2023-04-27T06:28:41.299Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:10,582 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #3, current state: CLOSING
2023-04-27 06:30:10,582 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #5, current state: CLOSING
2023-04-27 06:30:10,582 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #6, current state: CLOSING
2023-04-27 06:30:10,665 [IPC Server handler 7 on default port 37175] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: fc22a943-ba2e-46ef-a412-943475173222, Nodes: d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:10.665Z[Etc/UTC]].
2023-04-27 06:30:10,666 [IPC Server handler 7 on default port 37175] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: fc22a943-ba2e-46ef-a412-943475173222, Nodes: d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:10.665Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:10,680 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode 2ebc1c73-026e-4922-9f91-c27c5284ff39(fv-az260-775/10.1.0.33) moved to stale state. Finalizing its pipelines [PipelineID=4d91dd73-517d-4b94-a25d-13e93a7b1ef5, PipelineID=0013f2b1-5ac3-4f2d-a0a1-238259678259]
2023-04-27 06:30:10,680 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 4d91dd73-517d-4b94-a25d-13e93a7b1ef5, Nodes: 2ebc1c73-026e-4922-9f91-c27c5284ff39(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:2ebc1c73-026e-4922-9f91-c27c5284ff39, CreationTimestamp2023-04-27T06:28:42.672Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:10,681 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 0013f2b1-5ac3-4f2d-a0a1-238259678259, Nodes: 45e99655-b061-40f7-b011-1b1883fa2319(fv-az260-775/10.1.0.33)d34b33f9-e64c-4718-893f-5b1a944a0d93(fv-az260-775/10.1.0.33)2ebc1c73-026e-4922-9f91-c27c5284ff39(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:45e99655-b061-40f7-b011-1b1883fa2319, CreationTimestamp2023-04-27T06:29:36.723Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:11,409 [grpc-default-executor-3] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->d34b33f9-e64c-4718-893f-5b1a944a0d93-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:11,409 [grpc-default-executor-3] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->d34b33f9-e64c-4718-893f-5b1a944a0d93-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->d34b33f9-e64c-4718-893f-5b1a944a0d93(c0,m0,n1, attendVote=true, lastRpcSendTime=4, lastRpcResponseTime=3181) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:11,410 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:11,410 [grpc-default-executor-8] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->d34b33f9-e64c-4718-893f-5b1a944a0d93-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:11,410 [grpc-default-executor-8] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->d34b33f9-e64c-4718-893f-5b1a944a0d93-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->d34b33f9-e64c-4718-893f-5b1a944a0d93(c0,m0,n1, attendVote=true, lastRpcSendTime=5, lastRpcResponseTime=3181) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:11,426 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,426 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,426 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,427 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,427 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,427 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,427 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,427 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,427 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 8 containers.
2023-04-27 06:30:11,428 [JvmPauseMonitor36] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-ef6aab41-a1eb-41b3-a4a5-2458878a1611: Detected pause in JVM or host machine (eg GC): pause of approximately 285048359ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,428 [JvmPauseMonitor40] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b: Detected pause in JVM or host machine (eg GC): pause of approximately 215439493ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,429 [JvmPauseMonitor33] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-om1: Detected pause in JVM or host machine (eg GC): pause of approximately 330291905ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,429 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:11,433 [JvmPauseMonitor25] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-om1: Detected pause in JVM or host machine (eg GC): pause of approximately 411933120ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,433 [JvmPauseMonitor32] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-8ee12707-a777-4421-9344-714c0bb69310: Detected pause in JVM or host machine (eg GC): pause of approximately 411385291ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,433 [JvmPauseMonitor22] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-45e99655-b061-40f7-b011-1b1883fa2319: Detected pause in JVM or host machine (eg GC): pause of approximately 412007425ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,434 [JvmPauseMonitor28] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-2b022b8d-8bf4-4c13-9ad2-4e7e18903189: Detected pause in JVM or host machine (eg GC): pause of approximately 413319895ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,434 [JvmPauseMonitor31] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-63017b46-a8f7-4bfc-aab4-ba20a30a5f58: Detected pause in JVM or host machine (eg GC): pause of approximately 413566909ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,434 [JvmPauseMonitor26] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-220eace7-05d2-4cc3-8ea2-3a8f6657333d: Detected pause in JVM or host machine (eg GC): pause of approximately 413693716ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,434 [JvmPauseMonitor17] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-f83db7c9-4a26-48b7-8329-a6a26bc6bc0f: Detected pause in JVM or host machine (eg GC): pause of approximately 415203998ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,435 [JvmPauseMonitor29] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-d129cee7-7074-498b-a1c7-3e6cb07a0899: Detected pause in JVM or host machine (eg GC): pause of approximately 420616590ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,435 [JvmPauseMonitor30] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-0e67dfdf-fb52-47b7-aad6-2fa818fce5ce: Detected pause in JVM or host machine (eg GC): pause of approximately 420756298ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,435 [JvmPauseMonitor27] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-a05fa3a3-4f42-4e00-9e5b-4e78c9e14083: Detected pause in JVM or host machine (eg GC): pause of approximately 428111995ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33).
2023-04-27 06:30:11,436 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:11,436 [JvmPauseMonitor37] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-a1850722-adbb-4bac-9148-47228c85758b: Detected pause in JVM or host machine (eg GC): pause of approximately 490365163ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,439 [JvmPauseMonitor38] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-7a6d3cb5-ca5e-45e5-99fe-a8e18444e416: Detected pause in JVM or host machine (eg GC): pause of approximately 583354592ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,440 [JvmPauseMonitor35] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-dbe8ec56-fdf9-4402-aab9-993d7a20391d: Detected pause in JVM or host machine (eg GC): pause of approximately 590696889ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,440 [JvmPauseMonitor39] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5: Detected pause in JVM or host machine (eg GC): pause of approximately 590800194ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,440 [JvmPauseMonitor34] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-70ce52ad-a2f4-4f2c-b96d-cd309621d39c: Detected pause in JVM or host machine (eg GC): pause of approximately 590895300ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=540ms
GC pool 'PS Scavenge' had collection(s): count=1 time=139ms
2023-04-27 06:30:11,443 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 is not found
2023-04-27 06:30:11,443 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 is not found
2023-04-27 06:30:11,446 [d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-FollowerState] WARN  impl.FollowerState (FollowerState.java:run(130)) - Unexpected long sleep: sleep 5105ms but took extra 709124745ns (> threshold = 300ms)
2023-04-27 06:30:11,452 [grpc-default-executor-8] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:11,453 [grpc-default-executor-8] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=0, lastRpcResponseTime=5754) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:11,453 [grpc-default-executor-3] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:11,453 [grpc-default-executor-3] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(131)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39-GrpcLogAppender: Leader has not got in touch with Follower 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259->2ebc1c73-026e-4922-9f91-c27c5284ff39(c0,m0,n1, attendVote=true, lastRpcSendTime=1, lastRpcResponseTime=5754) yet, just keep nextIndex unchanged and retry.
2023-04-27 06:30:11,589 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 is not found
2023-04-27 06:30:11,603 [IPC Server handler 7 on default port 37175] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: e68537c7-dd5e-4934-bd5b-0f78194036e1, Nodes: d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:11.602Z[Etc/UTC]].
2023-04-27 06:30:11,604 [IPC Server handler 7 on default port 37175] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: e68537c7-dd5e-4934-bd5b-0f78194036e1, Nodes: d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:11.602Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:11,740 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 is not found
2023-04-27 06:30:11,869 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:remove(107)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083: remove  FOLLOWER a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7:t1, leader=2b022b8d-8bf4-4c13-9ad2-4e7e18903189, voted=2b022b8d-8bf4-4c13-9ad2-4e7e18903189, raftlog=Memoized:a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-SegmentedRaftLog:OPENED:c46, conf=0: peers:[2b022b8d-8bf4-4c13-9ad2-4e7e18903189|rpc:10.1.0.33:37665|dataStream:10.1.0.33:43489|priority:1|startupRole:FOLLOWER, a05fa3a3-4f42-4e00-9e5b-4e78c9e14083|rpc:10.1.0.33:41393|dataStream:10.1.0.33:35141|priority:0|startupRole:FOLLOWER, 220eace7-05d2-4cc3-8ea2-3a8f6657333d|rpc:10.1.0.33:35487|dataStream:10.1.0.33:40337|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
2023-04-27 06:30:11,869 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7: shutdown
2023-04-27 06:30:11,869 [Command processor thread] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8969F0A26AC7,id=a05fa3a3-4f42-4e00-9e5b-4e78c9e14083
2023-04-27 06:30:11,869 [Command processor thread] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083: shutdown a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-FollowerState
2023-04-27 06:30:11,870 [Command processor thread] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-StateMachineUpdater: set stopIndex = 46
2023-04-27 06:30:11,870 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-FollowerState was interrupted
2023-04-27 06:30:11,870 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-StateMachineUpdater] ERROR ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(324)) - Failed to take snapshot  for group-8969F0A26AC7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:41)
2023-04-27 06:30:11,870 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-StateMachineUpdater] ERROR impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(282)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-StateMachineUpdater: Failed to take snapshot
org.apache.ratis.protocol.exceptions.StateMachineException: Failed to take snapshot  for group-8969F0A26AC7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:41)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:323)
	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:270)
	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:262)
	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:183)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:11,870 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-StateMachineUpdater] ERROR ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(324)) - Failed to take snapshot  for group-8969F0A26AC7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:41)
2023-04-27 06:30:11,871 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-StateMachineUpdater] ERROR impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(282)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-StateMachineUpdater: Failed to take snapshot
org.apache.ratis.protocol.exceptions.StateMachineException: Failed to take snapshot  for group-8969F0A26AC7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:41)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:323)
	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:270)
	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:262)
	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:186)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:11,871 [Command processor thread] INFO  server.RaftServer$Division (ServerState.java:close(466)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7: closes. applyIndex: 41
2023-04-27 06:30:11,871 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:11,872 [Command processor thread] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7-SegmentedRaftLogWorker close()
2023-04-27 06:30:11,878 [IPC Server handler 14 on default port 37175] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 3541dabf-3351-48cd-a257-9e51dd0a6317, Nodes: a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33)63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:11.878Z[Etc/UTC]].
2023-04-27 06:30:11,887 [IPC Server handler 14 on default port 37175] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 3541dabf-3351-48cd-a257-9e51dd0a6317, Nodes: a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33)63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:11.878Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:11,897 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 38.
2023-04-27 06:30:11,897 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 38.
2023-04-27 06:30:11,902 [FixedThreadPoolWithAffinityExecutor-8-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(285)) - Moving container #1 to QUASI_CLOSED state, datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) reported QUASI_CLOSED replica.
2023-04-27 06:30:11,908 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 46.
2023-04-27 06:30:11,908 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 46.
2023-04-27 06:30:11,910 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:remove(107)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189: remove    LEADER 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7:t1, leader=2b022b8d-8bf4-4c13-9ad2-4e7e18903189, voted=2b022b8d-8bf4-4c13-9ad2-4e7e18903189, raftlog=Memoized:2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-SegmentedRaftLog:OPENED:c48, conf=0: peers:[2b022b8d-8bf4-4c13-9ad2-4e7e18903189|rpc:10.1.0.33:37665|dataStream:10.1.0.33:43489|priority:1|startupRole:FOLLOWER, a05fa3a3-4f42-4e00-9e5b-4e78c9e14083|rpc:10.1.0.33:41393|dataStream:10.1.0.33:35141|priority:0|startupRole:FOLLOWER, 220eace7-05d2-4cc3-8ea2-3a8f6657333d|rpc:10.1.0.33:35487|dataStream:10.1.0.33:40337|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
2023-04-27 06:30:11,910 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7: shutdown
2023-04-27 06:30:11,910 [Command processor thread] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8969F0A26AC7,id=2b022b8d-8bf4-4c13-9ad2-4e7e18903189
2023-04-27 06:30:11,911 [Command processor thread] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189: shutdown 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-LeaderStateImpl
2023-04-27 06:30:11,911 [Command processor thread] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:11,911 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7->a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-GrpcLogAppender-LogAppenderDaemon] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(200)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7->a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2023-04-27 06:30:11,912 [grpc-default-executor-8] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083: Completed APPEND_ENTRIES, lastRequest: 2b022b8d-8bf4-4c13-9ad2-4e7e18903189->a05fa3a3-4f42-4e00-9e5b-4e78c9e14083#279-t1,previous=(t:1, i:47),leaderCommit=46,initializing? true,entries: size=1, first=(t:1, i:48), METADATAENTRY(c:46)
2023-04-27 06:30:11,911 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7->220eace7-05d2-4cc3-8ea2-3a8f6657333d-GrpcLogAppender-LogAppenderDaemon] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(200)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7->220eace7-05d2-4cc3-8ea2-3a8f6657333d-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2023-04-27 06:30:11,912 [grpc-default-executor-5] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083: Completed APPEND_ENTRIES, lastRequest: null
2023-04-27 06:30:11,913 [grpc-default-executor-3] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d: Completed APPEND_ENTRIES, lastRequest: 2b022b8d-8bf4-4c13-9ad2-4e7e18903189->220eace7-05d2-4cc3-8ea2-3a8f6657333d#270-t1,previous=(t:1, i:47),leaderCommit=46,initializing? true,entries: size=1, first=(t:1, i:48), METADATAENTRY(c:46)
2023-04-27 06:30:11,913 [FixedThreadPoolWithAffinityExecutor-8-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(285)) - Moving container #4 to QUASI_CLOSED state, datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) reported QUASI_CLOSED replica.
2023-04-27 06:30:11,913 [grpc-default-executor-5] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d: Completed APPEND_ENTRIES, lastRequest: null
2023-04-27 06:30:11,913 [grpc-default-executor-6] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7->a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:11,913 [grpc-default-executor-6] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7->a05fa3a3-4f42-4e00-9e5b-4e78c9e14083: nextIndex: updateUnconditionally 49 -> 48
2023-04-27 06:30:11,914 [grpc-default-executor-3] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7->a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:11,914 [grpc-default-executor-3] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7->a05fa3a3-4f42-4e00-9e5b-4e78c9e14083: nextIndex: updateUnconditionally 48 -> 47
2023-04-27 06:30:11,914 [grpc-default-executor-3] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7->220eace7-05d2-4cc3-8ea2-3a8f6657333d-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:11,914 [grpc-default-executor-6] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7->220eace7-05d2-4cc3-8ea2-3a8f6657333d-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:11,914 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:groupRemove(428)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-8969F0A26AC7: Succeed to remove RaftStorageDirectory Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-1/data/ratis/13baae5c-b58f-4d4f-a606-8969f0a26ac7
2023-04-27 06:30:11,914 [grpc-default-executor-3] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7->220eace7-05d2-4cc3-8ea2-3a8f6657333d: nextIndex: updateUnconditionally 49 -> 48
2023-04-27 06:30:11,922 [grpc-default-executor-6] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7->220eace7-05d2-4cc3-8ea2-3a8f6657333d: nextIndex: updateUnconditionally 48 -> 47
2023-04-27 06:30:11,915 [Command processor thread] INFO  commandhandler.ClosePipelineCommandHandler (ClosePipelineCommandHandler.java:handle(78)) - Close Pipeline PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 command on datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083.
2023-04-27 06:30:11,925 [Command processor thread] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-StateMachineUpdater: set stopIndex = 48
2023-04-27 06:30:11,929 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-8969F0A26AC7: Taking a snapshot at:(t:1, i:48) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-2/data/ratis/13baae5c-b58f-4d4f-a606-8969f0a26ac7/sm/snapshot.1_48
2023-04-27 06:30:11,933 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-8969F0A26AC7: Finished taking a snapshot at:(t:1, i:48) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-2/data/ratis/13baae5c-b58f-4d4f-a606-8969f0a26ac7/sm/snapshot.1_48 took: 4 ms
2023-04-27 06:30:11,933 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-StateMachineUpdater: Took a snapshot at index 48
2023-04-27 06:30:11,933 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 48
2023-04-27 06:30:11,934 [Command processor thread] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7: closes. applyIndex: 48
2023-04-27 06:30:11,934 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:11,934 [Command processor thread] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7-SegmentedRaftLogWorker close()
2023-04-27 06:30:11,946 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 38.
2023-04-27 06:30:11,946 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 38.
2023-04-27 06:30:11,952 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 42.
2023-04-27 06:30:11,952 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 42.
2023-04-27 06:30:11,965 [FixedThreadPoolWithAffinityExecutor-9-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(285)) - Moving container #3 to QUASI_CLOSED state, datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) reported QUASI_CLOSED replica.
2023-04-27 06:30:11,974 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 46.
2023-04-27 06:30:11,975 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 46.
2023-04-27 06:30:11,983 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:groupRemove(428)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-8969F0A26AC7: Succeed to remove RaftStorageDirectory Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-2/data/ratis/13baae5c-b58f-4d4f-a606-8969f0a26ac7
2023-04-27 06:30:11,983 [Command processor thread] INFO  commandhandler.ClosePipelineCommandHandler (ClosePipelineCommandHandler.java:handle(78)) - Close Pipeline PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 command on datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189.
2023-04-27 06:30:12,204 [IPC Server handler 5 on default port 37175] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 7a77590e-48e7-4a17-b391-b680b8534a24, Nodes: 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:12.203Z[Etc/UTC]].
2023-04-27 06:30:12,204 [IPC Server handler 5 on default port 37175] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 7a77590e-48e7-4a17-b391-b680b8534a24, Nodes: 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:12.203Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:12,410 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:12,427 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:forceCloseContainer(1080)) - Force closing container #1 with BCSID 38, which is in QUASI_CLOSED state.
2023-04-27 06:30:12,427 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,428 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,428 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,428 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #3 is under replicated. Expected replica count is 3, but found 2.
2023-04-27 06:30:12,428 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=3, replicaIndex=0, sourceNodes=[2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)], priority=NORMAL to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)
2023-04-27 06:30:12,428 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:forceCloseContainer(1080)) - Force closing container #4 with BCSID 46, which is in QUASI_CLOSED state.
2023-04-27 06:30:12,428 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,428 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,429 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 2 milliseconds for processing 11 containers.
2023-04-27 06:30:12,429 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:12,430 [Mini-Cluster-Provider-Reap] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-6/data-0/containers/hdds/d25cf8bb-08a8-4312-8236-5e4a36901885/DS-0c504a97-027e-4245-a13e-7da89b7d0328/container.db for volume DS-0c504a97-027e-4245-a13e-7da89b7d0328
2023-04-27 06:30:12,430 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:12,434 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:12,435 [Mini-Cluster-Provider-Reap] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
2023-04-27 06:30:12,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33).
2023-04-27 06:30:12,437 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:12,446 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 is not found
2023-04-27 06:30:12,473 [Listener at 0.0.0.0/44011] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 2057 ms to scan 19 urls, producing 68 keys and 5104 values [using 2 cores]
2023-04-27 06:30:12,474 [Listener at 0.0.0.0/44011] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(90)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-04-27 06:30:12,478 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1319)) - Starting Socket Reader #1 for port 0
2023-04-27 06:30:12,493 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@602ea58a{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:12,494 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@3d427060{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:12,494 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:12,532 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@3bdca761{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:12,533 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@621645ff{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:12,550 [ForkJoinPool.commonPool-worker-0] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-2/data-0/containers/hdds/d25cf8bb-08a8-4312-8236-5e4a36901885/DS-14c45caa-1749-46a9-99ea-456b3e03f553/container.db for volume DS-14c45caa-1749-46a9-99ea-456b3e03f553
2023-04-27 06:30:12,550 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:12,551 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:12,554 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
2023-04-27 06:30:12,590 [Listener at 127.0.0.1/43855] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - OzoneManager metrics system started (again)
2023-04-27 06:30:12,608 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@5046f370{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:12,609 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@6e0cf67e{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:12,610 [ForkJoinPool.commonPool-worker-0] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:12,611 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@654246d{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:12,612 [Mini-Cluster-Provider-Reap] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:12,629 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@5d3e17cc{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:12,634 [Listener at 127.0.0.1/43855] INFO  om.OzoneManager (OzoneManager.java:start(1574)) - OzoneManager RPC server is listening at localhost/127.0.0.1:43855
2023-04-27 06:30:12,634 [Listener at 127.0.0.1/43855] INFO  ratis.OzoneManagerRatisServer (OzoneManagerRatisServer.java:start(558)) - Starting OzoneManagerRatisServer om1 at port 37081
2023-04-27 06:30:12,644 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - 45e99655-b061-40f7-b011-1b1883fa2319: close
2023-04-27 06:30:12,647 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259: shutdown
2023-04-27 06:30:12,648 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-238259678259,id=45e99655-b061-40f7-b011-1b1883fa2319
2023-04-27 06:30:12,648 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 45e99655-b061-40f7-b011-1b1883fa2319: shutdown 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259-LeaderStateImpl
2023-04-27 06:30:12,649 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - 45e99655-b061-40f7-b011-1b1883fa2319: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:12,659 [om1-impl-thread1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/ozone-meta/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
2023-04-27 06:30:12,660 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread3] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52: shutdown
2023-04-27 06:30:12,660 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread3] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-6B3DED7F7F52,id=45e99655-b061-40f7-b011-1b1883fa2319
2023-04-27 06:30:12,660 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread3] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 45e99655-b061-40f7-b011-1b1883fa2319: shutdown 45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52-LeaderStateImpl
2023-04-27 06:30:12,663 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:12,663 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:12,664 [45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-238259678259: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-5/data/ratis/0013f2b1-5ac3-4f2d-a0a1-238259678259/sm/snapshot.1_0
2023-04-27 06:30:12,665 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread3] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:12,665 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread3] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:12,665 [45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-6B3DED7F7F52: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-5/data/ratis/3de14205-b00c-47f4-b302-6b3ded7f7f52/sm/snapshot.1_0
2023-04-27 06:30:12,667 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:12,667 [om1-impl-thread1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/ozone-meta/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:12,668 [45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-238259678259: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-5/data/ratis/0013f2b1-5ac3-4f2d-a0a1-238259678259/sm/snapshot.1_0 took: 4 ms
2023-04-27 06:30:12,668 [45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-6B3DED7F7F52: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-5/data/ratis/3de14205-b00c-47f4-b302-6b3ded7f7f52/sm/snapshot.1_0 took: 3 ms
2023-04-27 06:30:12,668 [45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:12,668 [45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:12,668 [45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:12,668 [45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:12,668 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f: close
2023-04-27 06:30:12,669 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread3] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52: closes. applyIndex: 0
2023-04-27 06:30:12,669 [45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:12,669 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259: closes. applyIndex: 0
2023-04-27 06:30:12,669 [Mini-Cluster-Provider-Reap] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - d34b33f9-e64c-4718-893f-5b1a944a0d93 Close channels
2023-04-27 06:30:12,670 [Mini-Cluster-Provider-Reap] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - 447b6ccf-579c-4325-a2f6-0d45e052ea27 Close channels
2023-04-27 06:30:12,670 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-6B3DED7F7F52-SegmentedRaftLogWorker close()
2023-04-27 06:30:12,670 [Mini-Cluster-Provider-Reap] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - 2ebc1c73-026e-4922-9f91-c27c5284ff39 Close channels
2023-04-27 06:30:12,670 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - 45e99655-b061-40f7-b011-1b1883fa2319: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:12,671 [om1-impl-thread1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/ozone-meta/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
2023-04-27 06:30:12,671 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:12,671 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:12,672 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:12,672 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:12,672 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:12,673 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 4194304 (custom)
2023-04-27 06:30:12,674 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:12,674 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:12,674 [om1-impl-thread1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/ozone-meta/ratis/bf265839-605b-3f16-9796-c5ba1605619e
2023-04-27 06:30:12,674 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2023-04-27 06:30:12,674 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 4096 (default)
2023-04-27 06:30:12,674 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 4194304 (custom)
2023-04-27 06:30:12,675 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 4194304 (custom)
2023-04-27 06:30:12,675 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:12,675 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D: shutdown
2023-04-27 06:30:12,675 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:12,675 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:12,675 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:12,675 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-88AC0215203D,id=f83db7c9-4a26-48b7-8329-a6a26bc6bc0f
2023-04-27 06:30:12,677 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f: shutdown f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-FollowerState
2023-04-27 06:30:12,677 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-StateMachineUpdater: set stopIndex = 43
2023-04-27 06:30:12,677 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-FollowerState was interrupted
2023-04-27 06:30:12,677 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-88AC0215203D: Taking a snapshot at:(t:1, i:43) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-0/data/ratis/0b06232b-259f-4de5-84c5-88ac0215203d/sm/snapshot.1_43
2023-04-27 06:30:12,676 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread3] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C: shutdown
2023-04-27 06:30:12,679 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread3] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-1BD70155463C,id=f83db7c9-4a26-48b7-8329-a6a26bc6bc0f
2023-04-27 06:30:12,679 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread3] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f: shutdown f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C-LeaderStateImpl
2023-04-27 06:30:12,679 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread3] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:12,676 [45e99655-b061-40f7-b011-1b1883fa2319-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xe0fd7bfd, L:/0:0:0:0:0:0:0:0:46485] CLOSE
2023-04-27 06:30:12,680 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-1BD70155463C: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-0/data/ratis/4ef9d8d5-0e5c-4292-add3-1bd70155463c/sm/snapshot.1_0
2023-04-27 06:30:12,680 [45e99655-b061-40f7-b011-1b1883fa2319-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xe0fd7bfd, L:/0:0:0:0:0:0:0:0:46485] INACTIVE
2023-04-27 06:30:12,680 [45e99655-b061-40f7-b011-1b1883fa2319-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xe0fd7bfd, L:/0:0:0:0:0:0:0:0:46485] UNREGISTERED
2023-04-27 06:30:12,680 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-88AC0215203D: Finished taking a snapshot at:(t:1, i:43) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-0/data/ratis/0b06232b-259f-4de5-84c5-88ac0215203d/sm/snapshot.1_43 took: 4 ms
2023-04-27 06:30:12,676 [45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:12,681 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 64KB (=65536) (default)
2023-04-27 06:30:12,681 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread3] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:12,675 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:12,681 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-StateMachineUpdater: Took a snapshot at index 43
2023-04-27 06:30:12,681 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 43
2023-04-27 06:30:12,681 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:12,681 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - 9c3df6cb-795c-4fc9-beaf-ef1567f7c761 Close channels
2023-04-27 06:30:12,682 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - 1ef0e7b0-3474-409b-a89e-735ba5bcb335 Close channels
2023-04-27 06:30:12,682 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:12,681 [45e99655-b061-40f7-b011-1b1883fa2319-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 45e99655-b061-40f7-b011-1b1883fa2319@group-238259678259-SegmentedRaftLogWorker close()
2023-04-27 06:30:12,682 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xeeb5b782, L:/0:0:0:0:0:0:0:0:34373] CLOSE
2023-04-27 06:30:12,682 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-1BD70155463C: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-0/data/ratis/4ef9d8d5-0e5c-4292-add3-1bd70155463c/sm/snapshot.1_0 took: 2 ms
2023-04-27 06:30:12,682 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xeeb5b782, L:/0:0:0:0:0:0:0:0:34373] INACTIVE
2023-04-27 06:30:12,682 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xeeb5b782, L:/0:0:0:0:0:0:0:0:34373] UNREGISTERED
2023-04-27 06:30:12,682 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:12,683 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:12,683 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread3] INFO  server.RaftServer$Division (ServerState.java:close(466)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C: closes. applyIndex: 0
2023-04-27 06:30:12,698 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:12,698 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:12,698 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = false (default)
2023-04-27 06:30:12,698 [om1-impl-thread1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:12,699 [om1-impl-thread1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:12,703 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D: closes. applyIndex: 43
2023-04-27 06:30:12,704 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:12,704 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-88AC0215203D-SegmentedRaftLogWorker close()
2023-04-27 06:30:12,709 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:12,710 [f83db7c9-4a26-48b7-8329-a6a26bc6bc0f-impl-thread3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - f83db7c9-4a26-48b7-8329-a6a26bc6bc0f@group-1BD70155463C-SegmentedRaftLogWorker close()
2023-04-27 06:30:12,709 [om1-impl-thread1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:localhost:37081|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:12,710 [om1-impl-thread1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:12,728 [JvmPauseMonitor22] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-45e99655-b061-40f7-b011-1b1883fa2319: Stopped
2023-04-27 06:30:12,734 [om1-impl-thread1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - om1: start om1@group-C5BA1605619E-FollowerState
2023-04-27 06:30:12,736 [om1-impl-thread1] ERROR util.JmxRegister (JmxRegister.java:tryRegister(40)) - Failed to register JMX Bean with name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
javax.management.InstanceAlreadyExistsException: Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.ratis.util.JmxRegister.tryRegister(JmxRegister.java:38)
	at org.apache.ratis.util.JmxRegister.register(JmxRegister.java:56)
	at org.apache.ratis.server.impl.RaftServerImpl.registerMBean(RaftServerImpl.java:353)
	at org.apache.ratis.server.impl.RaftServerImpl.start(RaftServerImpl.java:344)
	at org.apache.ratis.util.ConcurrentUtils.accept(ConcurrentUtils.java:173)
	at org.apache.ratis.util.ConcurrentUtils.lambda$null$3(ConcurrentUtils.java:165)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:12,736 [om1-impl-thread1] ERROR util.JmxRegister (JmxRegister.java:tryRegister(40)) - Failed to register JMX Bean with name Ratis:service=RaftServer,group=group-C5BA1605619E,id="om1"
javax.management.InstanceAlreadyExistsException: Ratis:service=RaftServer,group=group-C5BA1605619E,id="om1"
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.ratis.util.JmxRegister.tryRegister(JmxRegister.java:38)
	at org.apache.ratis.util.JmxRegister.register(JmxRegister.java:56)
	at org.apache.ratis.server.impl.RaftServerImpl.registerMBean(RaftServerImpl.java:353)
	at org.apache.ratis.server.impl.RaftServerImpl.start(RaftServerImpl.java:344)
	at org.apache.ratis.util.ConcurrentUtils.accept(ConcurrentUtils.java:173)
	at org.apache.ratis.util.ConcurrentUtils.lambda$null$3(ConcurrentUtils.java:165)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:12,737 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:12,737 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 400000 (default)
2023-04-27 06:30:12,737 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = -1 (default)
2023-04-27 06:30:12,737 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = true (custom)
2023-04-27 06:30:12,737 [om1@group-C5BA1605619E-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 1s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:12,738 [om1@group-C5BA1605619E-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 1200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:12,744 [Listener at 127.0.0.1/43855] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - om1: start RPC server
2023-04-27 06:30:12,745 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - om1: GrpcService started, listening on 37081
2023-04-27 06:30:12,746 [Listener at 127.0.0.1/43855] INFO  om.OzoneManager (OzoneManager.java:start(1590)) - Version File has different layout version (4) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
2023-04-27 06:30:12,746 [JvmPauseMonitor41] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-om1: Started
2023-04-27 06:30:12,762 [JvmPauseMonitor17] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-f83db7c9-4a26-48b7-8329-a6a26bc6bc0f: Stopped
2023-04-27 06:30:12,763 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for ozoneManager at: http://0.0.0.0:0
2023-04-27 06:30:12,763 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:12,764 [Listener at 127.0.0.1/43855] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:12,765 [Listener at 127.0.0.1/43855] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.ozoneManager is not defined
2023-04-27 06:30:12,765 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:12,766 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
2023-04-27 06:30:12,766 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:12,766 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:12,766 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of ozoneManager uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/ozone-meta/webserver
2023-04-27 06:30:12,766 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 39267
2023-04-27 06:30:12,766 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:12,790 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:12,791 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:12,791 [Listener at 127.0.0.1/43855] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 600000ms
2023-04-27 06:30:12,791 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@54907028{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:12,792 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@533595f0{static,/static,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/static,AVAILABLE}
2023-04-27 06:30:12,797 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@72065a81{ozoneManager,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/ozoneManager/,AVAILABLE}{file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/ozoneManager}
2023-04-27 06:30:12,804 [Listener at 127.0.0.1/43855] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@50459c16{HTTP/1.1, (http/1.1)}{0.0.0.0:39267}
2023-04-27 06:30:12,804 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(415)) - Started @139252ms
2023-04-27 06:30:12,804 [Listener at 127.0.0.1/43855] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:12,806 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of ozoneManager listening at http://0.0.0.0:39267
2023-04-27 06:30:12,806 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1558)) - IPC Server Responder: starting
2023-04-27 06:30:12,807 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1398)) - IPC Server listener on 0: starting
2023-04-27 06:30:12,823 [Listener at 127.0.0.1/43855] INFO  om.OzoneManager (OzoneManager.java:startTrashEmptier(2050)) - Trash Interval set to 0. Files deleted won't move to trash
2023-04-27 06:30:12,842 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@14a5c9b3] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:12,874 [Listener at 127.0.0.1/43855] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:12,874 [Listener at 127.0.0.1/43855] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:12,874 [Listener at 127.0.0.1/43855] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:12,894 [Listener at 127.0.0.1/43855] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:12,957 [Listener at 127.0.0.1/43855] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:13,062 [Listener at 127.0.0.1/43855] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 104 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:13,070 [Listener at 127.0.0.1/43855] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:13,076 [Listener at 127.0.0.1/43855] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:13,076 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:13,076 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data-0/containers/hdds
2023-04-27 06:30:13,085 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data-0/containers/hdds
2023-04-27 06:30:13,088 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode d34b33f9-e64c-4718-893f-5b1a944a0d93(fv-az260-775/10.1.0.33) moved to stale state. Finalizing its pipelines [PipelineID=3ee33afa-e5d6-4fca-b2f7-a34746412a15, PipelineID=0013f2b1-5ac3-4f2d-a0a1-238259678259]
2023-04-27 06:30:13,089 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 3ee33afa-e5d6-4fca-b2f7-a34746412a15, Nodes: d34b33f9-e64c-4718-893f-5b1a944a0d93(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d34b33f9-e64c-4718-893f-5b1a944a0d93, CreationTimestamp2023-04-27T06:28:44.122Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:13,106 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis to VolumeSet
2023-04-27 06:30:13,106 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis
2023-04-27 06:30:13,106 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis
2023-04-27 06:30:13,127 [Thread-2657] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data-0/containers/hdds
2023-04-27 06:30:13,127 [Listener at 127.0.0.1/43855] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:13,131 [Listener at 127.0.0.1/43855] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:13,131 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:13,131 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:13,131 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:13,131 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:13,132 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:13,132 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:13,132 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:13,132 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:13,133 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:13,133 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:13,133 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:13,133 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:13,133 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:13,134 [Listener at 127.0.0.1/43855] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:13,135 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:13,135 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:13,135 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:13,135 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:13,135 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:13,135 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:13,136 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:13,136 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:13,137 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:13,137 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:13,137 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:13,137 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:13,137 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:13,138 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:13,138 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis] (custom)
2023-04-27 06:30:13,138 [2cb742ca-d762-4bc5-b311-f495a87c6b6b-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x26680437] REGISTERED
2023-04-27 06:30:13,139 [2cb742ca-d762-4bc5-b311-f495a87c6b6b-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x26680437] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:13,139 [2cb742ca-d762-4bc5-b311-f495a87c6b6b-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x26680437, L:/0:0:0:0:0:0:0:0:42543] ACTIVE
2023-04-27 06:30:13,142 [Listener at 127.0.0.1/43855] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:13,152 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:13,153 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:13,154 [Listener at 127.0.0.1/43855] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:13,155 [Listener at 127.0.0.1/43855] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:13,156 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:13,156 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:13,156 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:13,156 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:13,157 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/meta/webserver
2023-04-27 06:30:13,157 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 35319
2023-04-27 06:30:13,157 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:13,162 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:13,162 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:13,163 [Listener at 127.0.0.1/43855] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 600000ms
2023-04-27 06:30:13,164 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@39f7aea5{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:13,164 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@4dccbe98{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:13,306 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33) moved to stale state. Finalizing its pipelines [PipelineID=0b06232b-259f-4de5-84c5-88ac0215203d, PipelineID=38a72eae-e492-49a9-a27b-1e3407fa9a4b]
2023-04-27 06:30:13,306 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 38a72eae-e492-49a9-a27b-1e3407fa9a4b, Nodes: 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:1ef0e7b0-3474-409b-a89e-735ba5bcb335, CreationTimestamp2023-04-27T06:28:41.729Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:13,410 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:13,429 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:13,429 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:forceCloseContainer(1080)) - Force closing container #1 with BCSID 38, which is in QUASI_CLOSED state.
2023-04-27 06:30:13,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:forceCloseContainer(1080)) - Force closing container #4 with BCSID 46, which is in QUASI_CLOSED state.
2023-04-27 06:30:13,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,430 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 11 containers.
2023-04-27 06:30:13,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,438 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,438 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,438 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,438 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,438 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,443 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,443 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,443 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33).
2023-04-27 06:30:13,443 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 6 milliseconds for processing 6 containers.
2023-04-27 06:30:13,445 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:remove(107)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d: remove  FOLLOWER 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7:t1, leader=2b022b8d-8bf4-4c13-9ad2-4e7e18903189, voted=2b022b8d-8bf4-4c13-9ad2-4e7e18903189, raftlog=Memoized:220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-SegmentedRaftLog:OPENED:c46, conf=0: peers:[2b022b8d-8bf4-4c13-9ad2-4e7e18903189|rpc:10.1.0.33:37665|dataStream:10.1.0.33:43489|priority:1|startupRole:FOLLOWER, a05fa3a3-4f42-4e00-9e5b-4e78c9e14083|rpc:10.1.0.33:41393|dataStream:10.1.0.33:35141|priority:0|startupRole:FOLLOWER, 220eace7-05d2-4cc3-8ea2-3a8f6657333d|rpc:10.1.0.33:35487|dataStream:10.1.0.33:40337|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
2023-04-27 06:30:13,446 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7: shutdown
2023-04-27 06:30:13,446 [Command processor thread] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8969F0A26AC7,id=220eace7-05d2-4cc3-8ea2-3a8f6657333d
2023-04-27 06:30:13,446 [Command processor thread] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d: shutdown 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-FollowerState
2023-04-27 06:30:13,446 [Command processor thread] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-StateMachineUpdater: set stopIndex = 46
2023-04-27 06:30:13,446 [220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-FollowerState was interrupted
2023-04-27 06:30:13,447 [220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-8969F0A26AC7: Taking a snapshot at:(t:1, i:46) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-0/data/ratis/13baae5c-b58f-4d4f-a606-8969f0a26ac7/sm/snapshot.1_46
2023-04-27 06:30:13,451 [220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-8969F0A26AC7: Finished taking a snapshot at:(t:1, i:46) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-0/data/ratis/13baae5c-b58f-4d4f-a606-8969f0a26ac7/sm/snapshot.1_46 took: 5 ms
2023-04-27 06:30:13,452 [220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-StateMachineUpdater: Took a snapshot at index 46
2023-04-27 06:30:13,452 [220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 46
2023-04-27 06:30:13,452 [Command processor thread] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7: closes. applyIndex: 46
2023-04-27 06:30:13,455 [220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:13,456 [Command processor thread] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7-SegmentedRaftLogWorker close()
2023-04-27 06:30:13,521 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 38.
2023-04-27 06:30:13,521 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 38.
2023-04-27 06:30:13,529 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 42.
2023-04-27 06:30:13,529 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 42.
2023-04-27 06:30:13,537 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 46.
2023-04-27 06:30:13,538 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 46.
2023-04-27 06:30:13,548 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:groupRemove(428)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-8969F0A26AC7: Succeed to remove RaftStorageDirectory Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-0/data/ratis/13baae5c-b58f-4d4f-a606-8969f0a26ac7
2023-04-27 06:30:13,548 [Command processor thread] INFO  commandhandler.ClosePipelineCommandHandler (ClosePipelineCommandHandler.java:handle(78)) - Close Pipeline PipelineID=13baae5c-b58f-4d4f-a606-8969f0a26ac7 command on datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d.
2023-04-27 06:30:13,607 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(83)) - A dead datanode is detected. 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33)
2023-04-27 06:30:13,608 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=0b06232b-259f-4de5-84c5-88ac0215203d close command to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335
2023-04-27 06:30:13,608 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=0b06232b-259f-4de5-84c5-88ac0215203d close command to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f
2023-04-27 06:30:13,608 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=0b06232b-259f-4de5-84c5-88ac0215203d close command to datanode 9c3df6cb-795c-4fc9-beaf-ef1567f7c761
2023-04-27 06:30:13,608 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 0b06232b-259f-4de5-84c5-88ac0215203d, Nodes: 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33)f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33)9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:1ef0e7b0-3474-409b-a89e-735ba5bcb335, CreationTimestamp2023-04-27T06:28:41.730Z[Etc/UTC]] removed.
2023-04-27 06:30:13,608 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=35bc3d66-4dd0-4aa2-bb62-273ede928d49 close command to datanode 9c3df6cb-795c-4fc9-beaf-ef1567f7c761
2023-04-27 06:30:13,609 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 35bc3d66-4dd0-4aa2-bb62-273ede928d49, Nodes: 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:9c3df6cb-795c-4fc9-beaf-ef1567f7c761, CreationTimestamp2023-04-27T06:28:41.299Z[Etc/UTC]] removed.
2023-04-27 06:30:13,609 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(96)) - Clearing command queue of size 14 for DN 9c3df6cb-795c-4fc9-beaf-ef1567f7c761(fv-az260-775/10.1.0.33)
2023-04-27 06:30:13,609 [EventQueue-DeadNodeForDeadNodeHandler] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:remove(190)) - Removed a node: /default-rack/9c3df6cb-795c-4fc9-beaf-ef1567f7c761
2023-04-27 06:30:13,712 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(83)) - A dead datanode is detected. 2ebc1c73-026e-4922-9f91-c27c5284ff39(fv-az260-775/10.1.0.33)
2023-04-27 06:30:13,712 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=4d91dd73-517d-4b94-a25d-13e93a7b1ef5 close command to datanode 2ebc1c73-026e-4922-9f91-c27c5284ff39
2023-04-27 06:30:13,712 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 4d91dd73-517d-4b94-a25d-13e93a7b1ef5, Nodes: 2ebc1c73-026e-4922-9f91-c27c5284ff39(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:2ebc1c73-026e-4922-9f91-c27c5284ff39, CreationTimestamp2023-04-27T06:28:42.672Z[Etc/UTC]] removed.
2023-04-27 06:30:13,712 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=0013f2b1-5ac3-4f2d-a0a1-238259678259 close command to datanode 45e99655-b061-40f7-b011-1b1883fa2319
2023-04-27 06:30:13,712 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=0013f2b1-5ac3-4f2d-a0a1-238259678259 close command to datanode d34b33f9-e64c-4718-893f-5b1a944a0d93
2023-04-27 06:30:13,712 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=0013f2b1-5ac3-4f2d-a0a1-238259678259 close command to datanode 2ebc1c73-026e-4922-9f91-c27c5284ff39
2023-04-27 06:30:13,712 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 0013f2b1-5ac3-4f2d-a0a1-238259678259, Nodes: 45e99655-b061-40f7-b011-1b1883fa2319(fv-az260-775/10.1.0.33)d34b33f9-e64c-4718-893f-5b1a944a0d93(fv-az260-775/10.1.0.33)2ebc1c73-026e-4922-9f91-c27c5284ff39(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:45e99655-b061-40f7-b011-1b1883fa2319, CreationTimestamp2023-04-27T06:29:36.723Z[Etc/UTC]] removed.
2023-04-27 06:30:13,713 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(96)) - Clearing command queue of size 2 for DN 2ebc1c73-026e-4922-9f91-c27c5284ff39(fv-az260-775/10.1.0.33)
2023-04-27 06:30:13,713 [EventQueue-DeadNodeForDeadNodeHandler] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:remove(190)) - Removed a node: /default-rack/2ebc1c73-026e-4922-9f91-c27c5284ff39
2023-04-27 06:30:13,725 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(73)) - Starting replication of container 3 from [2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)] using NO_COMPRESSION
2023-04-27 06:30:13,742 [grpc-default-executor-3] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(62)) - Streaming container data (3) to other datanode with compression NO_COMPRESSION
2023-04-27 06:30:13,755 [grpc-default-executor-3] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(111)) - Sent 12800 bytes for container 3
2023-04-27 06:30:13,756 [grpc-default-executor-3] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(218)) - Container 3 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-4/data-0/containers/tmp/container-copy/container-3.tar
2023-04-27 06:30:13,759 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(88)) - Container 3 is downloaded with size 12800, starting to import.
2023-04-27 06:30:13,782 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(95)) - Container 3 is replicated successfully
2023-04-27 06:30:13,782 [ContainerReplicationThread-0] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(220)) - Successful DONE replicateContainerCommand: containerId=3, replicaIndex=0, sourceNodes=[2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)], priority=NORMAL, transferred 12800 bytes
2023-04-27 06:30:13,810 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@314e0578{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/meta/webserver/jetty-0_0_0_0-35319-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-8352418181421248839/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:13,815 [Listener at 127.0.0.1/43855] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@20b346aa{HTTP/1.1, (http/1.1)}{0.0.0.0:35319}
2023-04-27 06:30:13,815 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(415)) - Started @140263ms
2023-04-27 06:30:13,816 [Listener at 127.0.0.1/43855] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:13,818 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:35319
2023-04-27 06:30:13,822 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:13,822 [Listener at 127.0.0.1/43855] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:13,822 [Listener at 127.0.0.1/43855] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:13,823 [Listener at 127.0.0.1/43855] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:13,828 [om1@group-C5BA1605619E-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:1094183777ns, electionTimeout:1090ms
2023-04-27 06:30:13,828 [om1@group-C5BA1605619E-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - om1: shutdown om1@group-C5BA1605619E-FollowerState
2023-04-27 06:30:13,828 [om1@group-C5BA1605619E-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:13,844 [om1@group-C5BA1605619E-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:13,844 [om1@group-C5BA1605619E-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - om1: start om1@group-C5BA1605619E-LeaderElection73
2023-04-27 06:30:13,845 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7c9baaa2] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:13,852 [Listener at 127.0.0.1/43855] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:13,856 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/meta/datanode.id
2023-04-27 06:30:13,859 [om1@group-C5BA1605619E-LeaderElection73] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - om1@group-C5BA1605619E-LeaderElection73 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:localhost:37081|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:13,859 [om1@group-C5BA1605619E-LeaderElection73] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - om1@group-C5BA1605619E-LeaderElection73 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:13,861 [om1@group-C5BA1605619E-LeaderElection73] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - om1@group-C5BA1605619E-LeaderElection73 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:localhost:37081|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:13,861 [om1@group-C5BA1605619E-LeaderElection73] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - om1@group-C5BA1605619E-LeaderElection73 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:13,861 [om1@group-C5BA1605619E-LeaderElection73] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - om1: shutdown om1@group-C5BA1605619E-LeaderElection73
2023-04-27 06:30:13,861 [om1@group-C5BA1605619E-LeaderElection73] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:13,862 [om1@group-C5BA1605619E-LeaderElection73] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 3443ms
2023-04-27 06:30:13,862 [om1@group-C5BA1605619E-LeaderElection73] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:13,862 [om1@group-C5BA1605619E-LeaderElection73] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 4096 (default)
2023-04-27 06:30:13,862 [om1@group-C5BA1605619E-LeaderElection73] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 64MB (=67108864) (default)
2023-04-27 06:30:13,862 [om1@group-C5BA1605619E-LeaderElection73] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 10s (default)
2023-04-27 06:30:13,862 [om1@group-C5BA1605619E-LeaderElection73] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:13,862 [om1@group-C5BA1605619E-LeaderElection73] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:13,862 [om1@group-C5BA1605619E-LeaderElection73] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 4096 (default)
2023-04-27 06:30:13,863 [om1@group-C5BA1605619E-LeaderElection73] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:13,863 [om1@group-C5BA1605619E-LeaderElection73] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - om1: start om1@group-C5BA1605619E-LeaderStateImpl
2023-04-27 06:30:13,863 [om1@group-C5BA1605619E-LeaderElection73] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:13,883 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/ozone-meta/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
2023-04-27 06:30:13,883 [om1@group-C5BA1605619E-LeaderElection73] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:localhost:37081|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:13,898 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:notifyConfigurationChanged(201)) - Received Configuration change notification from Ratis. New Peer list:
[id: "om1"
address: "localhost:37081"
startupRole: FOLLOWER
]
2023-04-27 06:30:13,914 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 38.
2023-04-27 06:30:13,915 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 38.
2023-04-27 06:30:13,918 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 1 is closed with bcsId 38.
2023-04-27 06:30:13,918 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 46.
2023-04-27 06:30:13,919 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 46.
2023-04-27 06:30:13,920 [FixedThreadPoolWithAffinityExecutor-8-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(318)) - Moving container #1 to CLOSED state, datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) reported CLOSED replica.
2023-04-27 06:30:13,923 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 4 is closed with bcsId 46.
2023-04-27 06:30:13,925 [FixedThreadPoolWithAffinityExecutor-8-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(318)) - Moving container #4 to CLOSED state, datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) reported CLOSED replica.
2023-04-27 06:30:13,927 [Listener at 127.0.0.1/43855] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:14,027 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 38.
2023-04-27 06:30:14,027 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 38.
2023-04-27 06:30:14,035 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 1 is closed with bcsId 38.
2023-04-27 06:30:14,045 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 46.
2023-04-27 06:30:14,045 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 46.
2023-04-27 06:30:14,048 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 4 is closed with bcsId 46.
2023-04-27 06:30:14,053 [Listener at 127.0.0.1/43855] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 124 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:14,056 [Listener at 127.0.0.1/43855] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:14,062 [Listener at 127.0.0.1/43855] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:14,062 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:14,063 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data-0/containers/hdds
2023-04-27 06:30:14,063 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data-0/containers/hdds
2023-04-27 06:30:14,081 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis to VolumeSet
2023-04-27 06:30:14,081 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis
2023-04-27 06:30:14,081 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis
2023-04-27 06:30:14,131 [Thread-2673] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data-0/containers/hdds
2023-04-27 06:30:14,135 [Listener at 127.0.0.1/43855] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:14,137 [Listener at 127.0.0.1/43855] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:14,137 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:14,137 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:14,137 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:14,137 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:14,137 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:14,137 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:14,137 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:14,138 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:14,138 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:14,138 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:14,138 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:14,138 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:14,138 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:14,139 [Listener at 127.0.0.1/43855] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:14,140 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:14,140 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:14,140 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:14,140 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:14,140 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:14,140 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:14,140 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:14,141 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:14,141 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:14,141 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:14,145 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:14,145 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:14,145 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:14,145 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:14,145 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis] (custom)
2023-04-27 06:30:14,146 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xfe2cff8d] REGISTERED
2023-04-27 06:30:14,146 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xfe2cff8d] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:14,146 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xfe2cff8d, L:/0:0:0:0:0:0:0:0:43189] ACTIVE
2023-04-27 06:30:14,148 [Listener at 127.0.0.1/43855] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:14,156 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:14,156 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:14,157 [Listener at 127.0.0.1/43855] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:14,159 [Listener at 127.0.0.1/43855] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:14,161 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:14,161 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:14,162 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:14,162 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:14,162 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/meta/webserver
2023-04-27 06:30:14,162 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 41531
2023-04-27 06:30:14,162 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:14,169 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:14,170 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:14,170 [Listener at 127.0.0.1/43855] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 660000ms
2023-04-27 06:30:14,172 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@49d36bab{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:14,173 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@31ee86a5{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:14,411 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
]]></system-out>
  </testcase>
  <testcase name="testSCMHandlesRestartForMaintenanceNode" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="32.005">
    <error type="java.util.concurrent.TimeoutException"><![CDATA[java.util.concurrent.TimeoutException: 
Timed out waiting for condition. Thread diagnostics:
Timestamp: 2023-04-27 06:30:46,211

"735446df-424c-4d38-a683-bd4ef5c8b9e6-server-thread3" daemon prio=5 tid=5965 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5203 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DataNode DiskChecker thread 0" daemon prio=5 tid=5576 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=5214 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5816 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=3895 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 1 on default port 46383" daemon prio=5 tid=5435 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 17 on default port 43297" daemon prio=5 tid=5431 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-4"  prio=5 tid=4903 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=3584 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 14 on default port 32931" daemon prio=5 tid=5540 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"FixedThreadPoolWithAffinityExecutor-8-0" daemon prio=5 tid=4304 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1736155493-5697" daemon prio=5 tid=5697 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=5598 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1189028704-5147-acceptor-0@3738108d-ServerConnector@774a1373{HTTP/1.1, (http/1.1)}{0.0.0.0:39005}" daemon prio=3 tid=5147 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"e5e627c9-cddd-4fde-83a7-403d4c1ca45c-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=4753 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 6 on default port 32931" daemon prio=5 tid=5532 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 4 on default port 46383" daemon prio=5 tid=5438 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"pool-2555-thread-1" daemon prio=5 tid=5656 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5260 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1728646390-5638" daemon prio=5 tid=5638 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=3852 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"SCM Heartbeat Processing Thread - 0" daemon prio=5 tid=3347 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"10617bfc-881b-4e3d-8722-e7f81aaf7e30-server-thread2" daemon prio=5 tid=5945 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=5264 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#2" daemon prio=5 tid=5865 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=4805 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@4f74f9e9" daemon prio=5 tid=4370 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"Socket Reader #1 for port 0"  prio=5 tid=3384 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1342)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1321)
"IPC Server handler 16 on default port 44467" daemon prio=5 tid=3455 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"pool-2046-thread-1"  prio=5 tid=5274 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 19 on default port 32931" daemon prio=5 tid=5545 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-0-0" daemon prio=5 tid=3897 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Over Replicated Processor" daemon prio=5 tid=5385 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:140)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=5091 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Socket Reader #1 for port 0"  prio=5 tid=5388 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1342)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1321)
"IPC Server handler 18 on default port 43297" daemon prio=5 tid=5432 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-SegmentedRaftLogWorker"  prio=5 tid=5909 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1827130166-5520" daemon prio=5 tid=5520 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"OpenKeyCleanupService#0" daemon prio=5 tid=4613 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"3804776e-bffa-42a7-addb-662dc325de80-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=5035 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=5673 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-SegmentedRaftLogWorker"  prio=5 tid=5309 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5@group-F129EA28C032-SegmentedRaftLogWorker"  prio=5 tid=4016 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"a1850722-adbb-4bac-9148-47228c85758b-client-thread1" daemon prio=5 tid=4815 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-SegmentedRaftLogWorker"  prio=5 tid=5905 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp974801434-3769" daemon prio=5 tid=3769 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=4128 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Under Replicated Processor" daemon prio=5 tid=4277 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:140)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=5750 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#2" daemon prio=5 tid=5851 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 7 on default port 43297" daemon prio=5 tid=5421 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1214830352-5070" daemon prio=5 tid=5070 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 6 on default port 36727" daemon prio=5 tid=3548 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode State Machine Task Thread - 0"  prio=5 tid=3891 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-29610c5d-1"  prio=5 tid=3607 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-StateMachineUpdater" daemon prio=5 tid=5875 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 6 on default port 46021" daemon prio=5 tid=4334 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"2cb742ca-d762-4bc5-b311-f495a87c6b6b-server-thread3" daemon prio=5 tid=5374 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FollowerState" daemon prio=5 tid=5317 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:128)
"qtp1197505558-4671" daemon prio=5 tid=4671 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 4 on default port 42409" daemon prio=5 tid=3463 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"2cb742ca-d762-4bc5-b311-f495a87c6b6b-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=4661 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-9-0" daemon prio=5 tid=4305 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-4"  prio=5 tid=4902 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=5846 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=5132 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"7a6d3cb5-ca5e-45e5-99fe-a8e18444e416@group-63EA1A3276A4-SegmentedRaftLogWorker"  prio=5 tid=4027 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1254266408-4855" daemon prio=5 tid=4855 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"qtp974801434-3765" daemon prio=5 tid=3765 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"7a6d3cb5-ca5e-45e5-99fe-a8e18444e416@group-F129EA28C032-FollowerState" daemon prio=5 tid=4092 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:128)
"qtp679454742-3834-acceptor-0@6ffac691-ServerConnector@46b69de1{HTTP/1.1, (http/1.1)}{0.0.0.0:46721}" daemon prio=3 tid=3834 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 3 on default port 46383" daemon prio=5 tid=5437 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-StateMachineUpdater" daemon prio=5 tid=5898 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"pool-2136-thread-1"  prio=5 tid=5287 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=3935 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-9-0" daemon prio=5 tid=3397 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=5185 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=5182 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=3632 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=2181 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 5 on default port 43297" daemon prio=5 tid=5419 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@6a1d119f" daemon prio=5 tid=5689 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=3814 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 7 on default port 32931" daemon prio=5 tid=5533 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=3904 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-1301a7a2-1"  prio=5 tid=5699 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 12 on default port 46383" daemon prio=5 tid=5446 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode State Machine Task Thread - 0"  prio=5 tid=3589 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=4728 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"prometheus" daemon prio=5 tid=5413 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at org.apache.hadoop.metrics2.impl.SinkQueue.waitForData(SinkQueue.java:114)
        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:83)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:135)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:89)
"qtp434207794-5109" daemon prio=5 tid=5109 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"BackgroundPipelineScrubberThread" daemon prio=5 tid=3354 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.ha.BackgroundSCMService.run(BackgroundSCMService.java:110)
        at org.apache.hadoop.hdds.scm.ha.BackgroundSCMService$$Lambda$412/2019944060.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=5796 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=5744 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"OpenKeyCleanupService#0" daemon prio=5 tid=5513 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule" daemon prio=5 tid=5175 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@6cd1f569" daemon prio=5 tid=5546 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-5-0" daemon prio=5 tid=3393 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor33" daemon prio=5 tid=3525 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"grpc-default-executor-6" daemon prio=5 tid=1369 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Over Replicated Processor" daemon prio=5 tid=4278 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:140)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 12 on default port 43297" daemon prio=5 tid=5426 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"EventQueue-NodeReportForNodeReportHandler" daemon prio=5 tid=5248 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=5648 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=5094 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"pool-2496-thread-1"  prio=5 tid=5871 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 10 on default port 36727" daemon prio=5 tid=3552 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-2"  prio=5 tid=4851 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"grpc-default-boss-ELG-1-1" daemon prio=5 tid=146 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-2-0" daemon prio=5 tid=5404 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Socket Reader #1 for port 0"  prio=5 tid=4287 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1342)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1321)
"ChunkReader-ELG-0" daemon prio=5 tid=3778 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-SegmentedRaftLogWorker"  prio=5 tid=5276 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-SegmentedRaftLogWorker"  prio=5 tid=5914 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"om1@group-C5BA1605619E-LeaderStateImpl" daemon prio=5 tid=5631 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-StateMachineUpdater" daemon prio=5 tid=5882 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-6-0" daemon prio=5 tid=3394 runnable
java.lang.Thread.State: RUNNABLE
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=4793 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-2"  prio=5 tid=4854 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1581909030-3490" daemon prio=5 tid=3490 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=5751 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD->735446df-424c-4d38-a683-bd4ef5c8b9e6-GrpcLogAppender-LogAppenderDaemon" daemon prio=5 tid=5960 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.grpc.server.GrpcLogAppender.mayWait(GrpcLogAppender.java:198)
        at org.apache.ratis.grpc.server.GrpcLogAppender.run(GrpcLogAppender.java:148)
        at org.apache.ratis.server.leader.LogAppenderDaemon.run(LogAppenderDaemon.java:78)
        at org.apache.ratis.server.leader.LogAppenderDaemon$$Lambda$1130/481315216.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 3 on default port 44467" daemon prio=5 tid=3442 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 1 on default port 44011" daemon prio=5 tid=4309 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ContainerReplicationThread-0" daemon prio=5 tid=5478 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.PriorityBlockingQueue.take(PriorityBlockingQueue.java:549)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp434207794-5110" daemon prio=5 tid=5110 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=4809 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-StateMachineUpdater" daemon prio=5 tid=5342 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:42409 - 0 "  prio=5 tid=3896 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 1 on default port 35697" daemon prio=5 tid=5455 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"SnapshotDiffCleanupService#0" daemon prio=5 tid=3511 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"timer6" daemon prio=5 tid=574 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"ChunkWriter-0-0" daemon prio=5 tid=3941 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 13 on default port 35697" daemon prio=5 tid=5467 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"pool-2262-thread-1"  prio=5 tid=5327 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=5862 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"a1850722-adbb-4bac-9148-47228c85758b@group-880F14DAB24A-SegmentedRaftLogWorker"  prio=5 tid=4009 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 0 on default port 46021" daemon prio=5 tid=4328 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1691761470-4374" daemon prio=5 tid=4374 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"SCMBlockDeletingService#0" daemon prio=5 tid=5474 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor48" daemon prio=5 tid=5263 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 13 on default port 44011" daemon prio=5 tid=4321 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@611f0f5d" daemon prio=5 tid=3636 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-SegmentedRaftLogWorker"  prio=5 tid=5331 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=5054 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1010451749-3654" daemon prio=5 tid=3654 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-4-0" daemon prio=5 tid=3392 runnable
java.lang.Thread.State: RUNNABLE
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"surefire-forkedjvm-command-thread" daemon prio=5 tid=10 runnable
java.lang.Thread.State: RUNNABLE
        at java.io.FileInputStream.readBytes(Native Method)
        at java.io.FileInputStream.read(FileInputStream.java:255)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
        at org.apache.maven.surefire.api.util.internal.Channels$3.readImpl(Channels.java:214)
        at org.apache.maven.surefire.api.util.internal.AbstractNoninterruptibleReadableChannel.read(AbstractNoninterruptibleReadableChannel.java:54)
        at org.apache.maven.surefire.booter.spi.LegacyMasterProcessChannelDecoder.decode(LegacyMasterProcessChannelDecoder.java:80)
        at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:343)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 12 on default port 36727" daemon prio=5 tid=3554 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp899835961-3538" daemon prio=5 tid=3538 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-StateMachineUpdater" daemon prio=5 tid=5904 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"pool-991-thread-3"  prio=5 tid=5224 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1613-thread-1" daemon prio=5 tid=3641 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1910932943-5557-acceptor-0@748714d6-ServerConnector@2c9d021c{HTTP/1.1, (http/1.1)}{0.0.0.0:43203}" daemon prio=3 tid=5557 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=5647 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 6 on default port 35697" daemon prio=5 tid=5460 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233->2cb742ca-d762-4bc5-b311-f495a87c6b6b-GrpcLogAppender-LogAppenderDaemon" daemon prio=5 tid=5368 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.grpc.server.GrpcLogAppender.mayWait(GrpcLogAppender.java:198)
        at org.apache.ratis.grpc.server.GrpcLogAppender.run(GrpcLogAppender.java:148)
        at org.apache.ratis.server.leader.LogAppenderDaemon.run(LogAppenderDaemon.java:78)
        at org.apache.ratis.server.leader.LogAppenderDaemon$$Lambda$1130/481315216.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1254266408-4856-acceptor-0@e1fa68c-ServerConnector@30d9ab2a{HTTP/1.1, (http/1.1)}{0.0.0.0:46545}" daemon prio=3 tid=4856 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 7 on default port 43855" daemon prio=5 tid=4634 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1117823686-3874-acceptor-0@e934dec-ServerConnector@fad5b2e{HTTP/1.1, (http/1.1)}{0.0.0.0:44841}" daemon prio=3 tid=3874 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=3321 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DataNode DiskChecker thread 0" daemon prio=5 tid=5717 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"efdbc657-ca6b-4fea-9dc1-5411634f3e98-impl-thread1"  prio=5 tid=5103 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"dbe8ec56-fdf9-4402-aab9-993d7a20391d-server-thread2" daemon prio=5 tid=5325 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-SegmentedRaftLogWorker"  prio=5 tid=5118 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=4720 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 11 on default port 35697" daemon prio=5 tid=5465 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 10 on default port 44011" daemon prio=5 tid=4318 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-0-0" daemon prio=5 tid=5838 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1561-thread-1"  prio=5 tid=3514 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 8 on default port 36727" daemon prio=5 tid=3550 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-2-0" daemon prio=5 tid=5723 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1726-thread-1" daemon prio=5 tid=3828 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-77cc6307-1"  prio=5 tid=5670 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=5079 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=3637 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=5758 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=4990 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Over Replicated Processor" daemon prio=5 tid=3363 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:140)
        at java.lang.Thread.run(Thread.java:750)
"KeyDeletingService#0" daemon prio=5 tid=3528 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5257 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 17 on default port 38893" daemon prio=5 tid=4365 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"KeyDeletingService#0" daemon prio=5 tid=4611 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 4 on default port 32931" daemon prio=5 tid=5530 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1016845872-5039" daemon prio=5 tid=5039 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=5183 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2577-thread-1" daemon prio=5 tid=5685 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderStateImpl" daemon prio=5 tid=5643 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=5746 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"a1850722-adbb-4bac-9148-47228c85758b@group-F129EA28C032->7a6d3cb5-ca5e-45e5-99fe-a8e18444e416-GrpcLogAppender-LogAppenderDaemon" daemon prio=5 tid=4096 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.grpc.server.GrpcLogAppender.mayWait(GrpcLogAppender.java:198)
        at org.apache.ratis.grpc.server.GrpcLogAppender.run(GrpcLogAppender.java:148)
        at org.apache.ratis.server.leader.LogAppenderDaemon.run(LogAppenderDaemon.java:78)
        at org.apache.ratis.server.leader.LogAppenderDaemon$$Lambda$1130/481315216.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=3809 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 13 on default port 42409" daemon prio=5 tid=3472 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-SegmentedRaftLogWorker"  prio=5 tid=5340 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"10617bfc-881b-4e3d-8722-e7f81aaf7e30-server-thread1" daemon prio=5 tid=5944 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1010451749-3649-acceptor-0@463b1310-ServerConnector@6a1cfbfd{HTTP/1.1, (http/1.1)}{0.0.0.0:37109}" daemon prio=3 tid=3649 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 15 on default port 46021" daemon prio=5 tid=4343 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=5053 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b-server-thread1" daemon prio=5 tid=5322 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 5 on default port 43855" daemon prio=5 tid=4632 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@14a5c9b3" daemon prio=5 tid=4647 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-StateMachineUpdater" daemon prio=5 tid=5893 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-2399665D7ECA-SegmentedRaftLogWorker"  prio=5 tid=3996 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#2" daemon prio=5 tid=5869 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=3583 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 13 on default port 36727" daemon prio=5 tid=3555 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@5dbb2c9d" daemon prio=5 tid=3832 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"timer2" daemon prio=5 tid=548 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"BlockDeletingService#0" daemon prio=5 tid=5233 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-0"  prio=5 tid=4818 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp96343072-3691" daemon prio=5 tid=3691 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"pool-1531-thread-1"  prio=5 tid=5028 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"f88c54b2-8776-4954-8dfb-f8bea6a862a6-impl-thread1"  prio=5 tid=5142 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderStateImpl" daemon prio=5 tid=5949 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-5"  prio=5 tid=4909 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp56975137-4760" daemon prio=5 tid=4760 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@7fbd522c" daemon prio=5 tid=3665 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=5005 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderStateImpl" daemon prio=5 tid=5378 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"DataNode DiskChecker thread 0" daemon prio=5 tid=5684 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2208-thread-1"  prio=5 tid=5282 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=3661 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-impl-thread1"  prio=5 tid=4848 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-StartAdminOnNodeForStartDatanodeAdminHandler" daemon prio=5 tid=5087 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=5708 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D->dbe8ec56-fdf9-4402-aab9-993d7a20391d-GrpcLogAppender-LogAppenderDaemon" daemon prio=5 tid=5321 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.grpc.server.GrpcLogAppender.mayWait(GrpcLogAppender.java:198)
        at org.apache.ratis.grpc.server.GrpcLogAppender.run(GrpcLogAppender.java:148)
        at org.apache.ratis.server.leader.LogAppenderDaemon.run(LogAppenderDaemon.java:78)
        at org.apache.ratis.server.leader.LogAppenderDaemon$$Lambda$1130/481315216.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=5202 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1711-thread-1"  prio=5 tid=3762 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FollowerState" daemon prio=5 tid=5939 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:128)
"735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-StateMachineUpdater" daemon prio=5 tid=5920 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=3942 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1233436001-5479" daemon prio=5 tid=5479 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 13 on default port 44467" daemon prio=5 tid=3452 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@60d3e5ba" daemon prio=5 tid=5660 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server listener on 0" daemon prio=5 tid=3366 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1404)
"IPC Server handler 4 on default port 44467" daemon prio=5 tid=3443 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server listener on 0" daemon prio=5 tid=3374 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1404)
"EndpointStateMachine task thread for /0.0.0.0:35697 - 0 "  prio=5 tid=5712 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=5235 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD->ec732e1e-81da-4c3b-ad71-f2fe790a57c7-GrpcLogAppender-LogAppenderDaemon" daemon prio=5 tid=5961 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.grpc.server.GrpcLogAppender.mayWait(GrpcLogAppender.java:198)
        at org.apache.ratis.grpc.server.GrpcLogAppender.run(GrpcLogAppender.java:148)
        at org.apache.ratis.server.leader.LogAppenderDaemon.run(LogAppenderDaemon.java:78)
        at org.apache.ratis.server.leader.LogAppenderDaemon$$Lambda$1130/481315216.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule" daemon prio=5 tid=3911 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=5195 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule" daemon prio=5 tid=5831 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"efdbc657-ca6b-4fea-9dc1-5411634f3e98-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=5102 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=5229 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=5676 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-PipelineReportForPipelineReportHandler" daemon prio=5 tid=5177 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server Responder" daemon prio=5 tid=5395 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1578)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1561)
"pool-2526-thread-1"  prio=5 tid=5582 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 18 on default port 33597" daemon prio=5 tid=3437 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Session-HouseKeeper-28f9d5ca-1"  prio=5 tid=3655 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=3806 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1955-thread-1"  prio=5 tid=4496 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=5205 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1691761470-4375" daemon prio=5 tid=4375 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 10 on default port 43855" daemon prio=5 tid=4637 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-2399665D7ECA-StateMachineUpdater" daemon prio=5 tid=3998 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 3 on default port 46021" daemon prio=5 tid=4331 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Socket Reader #1 for port 0"  prio=5 tid=3375 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1342)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1321)
"SCMBlockDeletingService#0" daemon prio=5 tid=4368 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp96343072-3690" daemon prio=5 tid=3690 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 12 on default port 33597" daemon prio=5 tid=3431 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"OMDoubleBufferFlushThread" daemon prio=5 tid=4488 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:614)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.flushTransactions(OzoneManagerDoubleBuffer.java:258)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer$$Lambda$552/1824377039.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"pool-1576-thread-1"  prio=5 tid=3970 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=3886 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=5566 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1728646390-5634-acceptor-0@8d559f2-ServerConnector@3251c6ea{HTTP/1.1, (http/1.1)}{0.0.0.0:45569}" daemon prio=3 tid=5634 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"Hadoop-Metrics-Updater-0" daemon prio=5 tid=4290 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5860 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp698964380-5735-acceptor-0@5c4d2599-ServerConnector@56ebc398{HTTP/1.1, (http/1.1)}{0.0.0.0:36927}" daemon prio=3 tid=5735 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:35697 - 0 "  prio=5 tid=5837 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"735446df-424c-4d38-a683-bd4ef5c8b9e6-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=5687 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"Hadoop-Metrics-Updater-0" daemon prio=5 tid=5401 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server Responder" daemon prio=5 tid=4580 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1578)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1561)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5849 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"RatisPipelineUtilsThread - 0"  prio=5 tid=5380 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.pipeline.BackgroundPipelineCreator.run(BackgroundPipelineCreator.java:176)
        at org.apache.hadoop.hdds.scm.pipeline.BackgroundPipelineCreator$$Lambda$409/1972512903.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=3810 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp96343072-3688" daemon prio=5 tid=3688 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=5623 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=3830 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=3633 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-StateMachineUpdater" daemon prio=5 tid=5878 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"pool-2277-thread-1" daemon prio=5 tid=5060 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=3802 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp434207794-5113" daemon prio=5 tid=5113 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=3888 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=5230 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 4 on default port 44011" daemon prio=5 tid=4312 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 7 on default port 35697" daemon prio=5 tid=5461 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"fc27ded4-a200-4b47-a78c-bc930dadcc21-impl-thread1"  prio=5 tid=5773 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 1 on default port 32931" daemon prio=5 tid=5527 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"DataNode DiskChecker thread 0" daemon prio=5 tid=4744 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1254266408-4860" daemon prio=5 tid=4860 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState" daemon prio=5 tid=5941 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:128)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=5184 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@53fc788" daemon prio=5 tid=5774 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=5050 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=5674 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5762 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor40" daemon prio=5 tid=3958 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=5820 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1606-thread-1"  prio=5 tid=3598 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 4 on default port 43297" daemon prio=5 tid=5418 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"BlockDeletingService#1" daemon prio=5 tid=5085 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-05CCBA5F75D2-LeaderStateImpl" daemon prio=5 tid=4083 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=4814 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule" daemon prio=5 tid=5833 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"RatisPipelineUtilsThread - 0"  prio=5 tid=4273 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.pipeline.BackgroundPipelineCreator.run(BackgroundPipelineCreator.java:176)
        at org.apache.hadoop.hdds.scm.pipeline.BackgroundPipelineCreator$$Lambda$409/1972512903.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 11 on default port 36727" daemon prio=5 tid=3553 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 6 on default port 44011" daemon prio=5 tid=4314 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"EventQueue-PipelineReportForPipelineReportHandler" daemon prio=5 tid=5834 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=5606 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 11 on default port 42409" daemon prio=5 tid=3470 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Socket Reader #1 for port 0"  prio=5 tid=5502 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1342)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1321)
"qtp1691761470-4377" daemon prio=5 tid=4377 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-SegmentedRaftLogWorker"  prio=5 tid=5891 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-7"  prio=5 tid=4951 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-StateMachineUpdater" daemon prio=5 tid=5124 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=3963 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"dbe8ec56-fdf9-4402-aab9-993d7a20391d-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=3595 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"a1850722-adbb-4bac-9148-47228c85758b@group-880F14DAB24A-LeaderStateImpl" daemon prio=5 tid=4089 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=5020 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=3850 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-3-0" daemon prio=5 tid=5405 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule" daemon prio=5 tid=5176 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 8 on default port 35697" daemon prio=5 tid=5462 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=3961 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5854 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 1 on default port 42409" daemon prio=5 tid=3460 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkReader-ELG-0" daemon prio=5 tid=3959 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"qtp1581909030-3489" daemon prio=5 tid=3489 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 14 on default port 43855" daemon prio=5 tid=4641 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 18 on default port 44467" daemon prio=5 tid=3457 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ReplicationMonitor" daemon prio=5 tid=3361 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:971)
        at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager$$Lambda$422/684577835.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"BackgroundPipelineScrubberThread" daemon prio=5 tid=5381 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.ha.BackgroundSCMService.run(BackgroundSCMService.java:110)
        at org.apache.hadoop.hdds.scm.ha.BackgroundSCMService$$Lambda$412/2019944060.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=3659 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server idle connection scanner for port 42409" daemon prio=5 tid=3368 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"JvmPauseMonitor44" daemon prio=5 tid=5194 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-3"  prio=5 tid=4866 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=4743 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 18 on default port 42409" daemon prio=5 tid=3477 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 2 on default port 32931" daemon prio=5 tid=5528 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"BlockDeletingService#1" daemon prio=5 tid=5829 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"OpenKeyCleanupService#0" daemon prio=5 tid=3529 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1197505558-4668" daemon prio=5 tid=4668 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor36" daemon prio=5 tid=3849 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 14 on default port 36727" daemon prio=5 tid=3556 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=2208 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-62a33165-1"  prio=5 tid=5047 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5086 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-6"  prio=5 tid=4948 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 16 on default port 38893" daemon prio=5 tid=4364 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1736155493-5693" daemon prio=5 tid=5693 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=3859 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#2" daemon prio=5 tid=5899 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerReplicationThread-0" daemon prio=5 tid=5356 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.PriorityBlockingQueue.take(PriorityBlockingQueue.java:549)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1691761470-4373-acceptor-0@13308c31-ServerConnector@1de2227e{HTTP/1.1, (http/1.1)}{0.0.0.0:37539}" daemon prio=3 tid=4373 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"qtp1581909030-3487" daemon prio=5 tid=3487 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ExpiredContainerReplicaOpScrubberThread" daemon prio=5 tid=5382 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.ha.BackgroundSCMService.run(BackgroundSCMService.java:110)
        at org.apache.hadoop.hdds.scm.ha.BackgroundSCMService$$Lambda$412/2019944060.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"FullTableCache Cleanup Thread - 0" daemon prio=5 tid=4790 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@25149f35" daemon prio=5 tid=3562 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"SCM Heartbeat Processing Thread - 0" daemon prio=5 tid=4272 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=5265 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 6 on default port 44467" daemon prio=5 tid=3445 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 19 on default port 43855" daemon prio=5 tid=4646 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-0-0" daemon prio=5 tid=5853 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:35697 - 0 "  prio=5 tid=5819 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Under Replicated Processor" daemon prio=5 tid=5384 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:140)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=5022 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp96343072-3693" daemon prio=5 tid=3693 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=3858 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1827130166-5524" daemon prio=5 tid=5524 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"pool-2481-thread-1"  prio=5 tid=5498 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2456-thread-1"  prio=5 tid=5492 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server idle connection scanner for port 33597" daemon prio=5 tid=3385 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"pool-1569-thread-1" daemon prio=5 tid=3566 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=708 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"efdbc657-ca6b-4fea-9dc1-5411634f3e98-server-thread1" daemon prio=5 tid=5617 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler" daemon prio=5 tid=5016 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server listener on 0" daemon prio=5 tid=4286 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1404)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=5785 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp698964380-5737" daemon prio=5 tid=5737 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 19 on default port 43297" daemon prio=5 tid=5433 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-server-thread2" daemon prio=5 tid=5373 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:42409 - 0 "  prio=5 tid=3844 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@6847f790" daemon prio=5 tid=3588 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"pool-2599-thread-1" daemon prio=5 tid=5718 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=5021 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DataNode DiskChecker thread 0" daemon prio=5 tid=5032 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=5128 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp592685883-5780" daemon prio=5 tid=5780 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=5843 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-FollowerState" daemon prio=5 tid=5366 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:128)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=5089 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1910932943-5558" daemon prio=5 tid=5558 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server Responder" daemon prio=5 tid=3386 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1578)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1561)
"ChunkWriter-0-0" daemon prio=5 tid=5002 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp679454742-3839" daemon prio=5 tid=3839 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 13 on default port 32931" daemon prio=5 tid=5539 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=5062 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 3 on default port 35697" daemon prio=5 tid=5457 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"EndpointStateMachine task thread for /0.0.0.0:38893 - 0 "  prio=5 tid=5213 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1736155493-5696" daemon prio=5 tid=5696 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 2 on default port 38893" daemon prio=5 tid=4350 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"FixedThreadPoolWithAffinityExecutor-1-0" daemon prio=5 tid=5403 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-9"  prio=5 tid=4960 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-DatanodeCommandForSCMNodeManager"  prio=5 tid=5835 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor38" daemon prio=5 tid=3930 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=3591 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ec732e1e-81da-4c3b-ad71-f2fe790a57c7-server-thread3" daemon prio=5 tid=5966 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 18 on default port 38893" daemon prio=5 tid=4366 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-SegmentedRaftLogWorker"  prio=5 tid=5876 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp974801434-3768" daemon prio=5 tid=3768 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ContainerReplicationThread-0" daemon prio=5 tid=5269 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.PriorityBlockingQueue.take(PriorityBlockingQueue.java:549)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 4 on default port 35697" daemon prio=5 tid=5458 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1189028704-5152" daemon prio=5 tid=5152 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1189028704-5148" daemon prio=5 tid=5148 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 13 on default port 46383" daemon prio=5 tid=5447 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@4cbcc31f" daemon prio=5 tid=3597 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=5010 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-DatanodeCommandForSCMNodeManager"  prio=5 tid=5174 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=728 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=5786 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-NewNodeForNewNodeHandler" daemon prio=5 tid=5830 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5228 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-7"  prio=5 tid=4950 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2618-thread-1"  prio=5 tid=5733 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 14 on default port 43297" daemon prio=5 tid=5428 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 13 on default port 43855" daemon prio=5 tid=4640 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"pool-1752-thread-1" daemon prio=5 tid=3867 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1887242307-5586" daemon prio=5 tid=5586 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 5 on default port 46021" daemon prio=5 tid=4333 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"7a6d3cb5-ca5e-45e5-99fe-a8e18444e416-server-thread2" daemon prio=5 tid=4100 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2012-thread-1"  prio=5 tid=4616 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"OMDoubleBufferFlushThread" daemon prio=5 tid=3512 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:614)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.flushTransactions(OzoneManagerDoubleBuffer.java:258)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer$$Lambda$552/1824377039.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-NodeReportForNodeReportHandler" daemon prio=5 tid=3965 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=4252 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp56975137-4764" daemon prio=5 tid=4764 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=5575 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 8 on default port 32931" daemon prio=5 tid=5534 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderStateImpl" daemon prio=5 tid=5975 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=5126 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=3885 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-05CCBA5F75D2-StateMachineUpdater" daemon prio=5 tid=4003 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=5192 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=3733 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp592685883-5783" daemon prio=5 tid=5783 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=5196 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 9 on default port 32931" daemon prio=5 tid=5535 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=5127 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=3950 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-FollowerState" daemon prio=5 tid=5957 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:128)
"DataNode DiskChecker thread 0" daemon prio=5 tid=5624 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1189028704-5146" daemon prio=5 tid=5146 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"OMDoubleBufferFlushThread" daemon prio=5 tid=5496 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:614)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.flushTransactions(OzoneManagerDoubleBuffer.java:258)
        at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer$$Lambda$552/1824377039.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=5227 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=5814 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=5199 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Socket Reader #1 for port 0"  prio=5 tid=3518 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1342)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1321)
"IPC Server handler 16 on default port 35697" daemon prio=5 tid=5470 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"BackgroundPipelineScrubberThread" daemon prio=5 tid=4274 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.ha.BackgroundSCMService.run(BackgroundSCMService.java:110)
        at org.apache.hadoop.hdds.scm.ha.BackgroundSCMService$$Lambda$412/2019944060.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=4655 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-StateMachineUpdater" daemon prio=5 tid=5333 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=4722 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ec732e1e-81da-4c3b-ad71-f2fe790a57c7-server-thread1" daemon prio=5 tid=5964 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Parameter Sending Thread for 0.0.0.0/0.0.0.0:38893" daemon prio=5 tid=4993 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferQueue.awaitFulfill(SynchronousQueue.java:764)
        at java.util.concurrent.SynchronousQueue$TransferQueue.transfer(SynchronousQueue.java:695)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at org.apache.hadoop.ipc.Client$Connection$RpcRequestSender.run(Client.java:1122)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 11 on default port 32931" daemon prio=5 tid=5537 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"BlockDeletingService#1" daemon prio=5 tid=5730 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-OpenPipelineForHealthyPipelineSafeModeRule" daemon prio=5 tid=3972 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 6 on default port 46383" daemon prio=5 tid=5440 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"SnapshotDeletingService#0" daemon prio=5 tid=5515 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5722 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=5222 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DataNode DiskChecker thread 0" daemon prio=5 tid=5549 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-3"  prio=5 tid=4852 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Hadoop-Metrics-Updater-0" daemon prio=5 tid=3373 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 13 on default port 43297" daemon prio=5 tid=5427 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"JvmPauseMonitor42" daemon prio=5 tid=5008 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Hadoop-Metrics-Updater-0" daemon prio=5 tid=3521 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=5844 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=5048 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 2 on default port 46021" daemon prio=5 tid=4330 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"2cb742ca-d762-4bc5-b311-f495a87c6b6b-server-thread1" daemon prio=5 tid=5370 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-44dd5315-1"  prio=5 tid=5487 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-6-0" daemon prio=5 tid=5408 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=5259 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=5855 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=3957 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1703-thread-1"  prio=5 tid=4020 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=5747 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 1 on default port 43297" daemon prio=5 tid=5415 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"StaleRecoveringContainerScrubbingService#2" daemon prio=5 tid=5890 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5003 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1197505558-4667-acceptor-0@39cb6b4b-ServerConnector@20b346aa{HTTP/1.1, (http/1.1)}{0.0.0.0:35319}" daemon prio=3 tid=4667 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"qtp974801434-3771" daemon prio=5 tid=3771 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=3869 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=5096 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp592685883-5781" daemon prio=5 tid=5781 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-362684FCDA51-StateMachineUpdater" daemon prio=5 tid=4040 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=4794 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server Responder" daemon prio=5 tid=4284 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1578)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1561)
"IPC Server handler 16 on default port 43297" daemon prio=5 tid=5430 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=3586 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp56975137-4759" daemon prio=5 tid=4759 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp698964380-5734" daemon prio=5 tid=5734 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"qtp1581909030-3486" daemon prio=5 tid=3486 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Client (815625593) connection to 0.0.0.0/0.0.0.0:35697 from runner" daemon prio=5 tid=5713 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:1043)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1094)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@2dac9329" daemon prio=5 tid=5104 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@7200c794" daemon prio=5 tid=5605 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=3668 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=5216 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=3889 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1429887399-5662" daemon prio=5 tid=5662 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"Hadoop-Metrics-Updater-0" daemon prio=5 tid=5396 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-693-thread-2"  prio=5 tid=3893 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=3168 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1736155493-5692-acceptor-0@49234a5-ServerConnector@1a933422{HTTP/1.1, (http/1.1)}{0.0.0.0:36753}" daemon prio=3 tid=5692 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=5604 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server listener on 0" daemon prio=5 tid=5397 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1404)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=5567 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"SnapshotDeletingService#0" daemon prio=5 tid=4615 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"main"  prio=5 tid=1 runnable
java.lang.Thread.State: RUNNABLE
        at java.lang.Thread.dumpThreads(Native Method)
        at java.lang.Thread.getAllStackTraces(Thread.java:1615)
        at org.apache.ozone.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:93)
        at org.apache.ozone.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:79)
        at org.apache.ozone.test.GenericTestUtils.waitFor(GenericTestUtils.java:231)
        at org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance.waitForDnToReachPersistedOpState(TestDecommissionAndMaintenance.java:718)
        at org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance.testSCMHandlesRestartForMaintenanceNode(TestDecommissionAndMaintenance.java:561)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
        at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
        at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
        at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
        at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor$$Lambda$164/2089016471.apply(Unknown Source)
        at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
        at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall$$Lambda$165/1610702581.apply(Unknown Source)
        at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
        at org.junit.jupiter.engine.execution.ExecutableInvoker$$Lambda$322/2052435819.apply(Unknown Source)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
        at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
        at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor$$Lambda$1251/1677960357.execute(Unknown Source)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask$$Lambda$264/1158258131.execute(Unknown Source)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask$$Lambda$263/1665197552.invoke(Unknown Source)
        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask$$Lambda$262/1262854901.execute(Unknown Source)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService$$Lambda$268/538185145.accept(Unknown Source)
        at java.util.ArrayList.forEach(ArrayList.java:1259)
        at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask$$Lambda$264/1158258131.execute(Unknown Source)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask$$Lambda$263/1665197552.invoke(Unknown Source)
        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask$$Lambda$262/1262854901.execute(Unknown Source)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService$$Lambda$268/538185145.accept(Unknown Source)
        at java.util.ArrayList.forEach(ArrayList.java:1259)
        at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask$$Lambda$264/1158258131.execute(Unknown Source)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask$$Lambda$263/1665197552.invoke(Unknown Source)
        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask$$Lambda$262/1262854901.execute(Unknown Source)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
        at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
        at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator$$Lambda$220/1714113641.accept(Unknown Source)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
        at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
        at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
        at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
        at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
        at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
"Session-HouseKeeper-8f8d44c-1"  prio=5 tid=5742 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=5768 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=5093 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FullTableCache Cleanup Thread - 0" daemon prio=5 tid=2176 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 12 on default port 35697" daemon prio=5 tid=5466 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Command processor thread" daemon prio=5 tid=3861 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=5791 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"pool-2570-thread-1"  prio=5 tid=5661 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp679454742-3838" daemon prio=5 tid=3838 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 15 on default port 38893" daemon prio=5 tid=4363 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp56975137-4762" daemon prio=5 tid=4762 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5755 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=5754 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Timer-4"  prio=5 tid=3526 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"IPC Server handler 16 on default port 44011" daemon prio=5 tid=4324 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"FixedThreadPoolWithAffinityExecutor-8-0" daemon prio=5 tid=5410 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2504-thread-1"  prio=5 tid=5555 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-DatanodeCommandForSCMNodeManager"  prio=5 tid=3909 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1736155493-5694" daemon prio=5 tid=5694 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 14 on default port 35697" daemon prio=5 tid=5468 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode State Machine Task Thread - 0"  prio=5 tid=5652 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-ContainerActionsForContainerActionsHandler" daemon prio=5 tid=4844 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderStateImpl" daemon prio=5 tid=5701 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"IPC Server idle connection scanner for port 32931" daemon prio=5 tid=5503 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"Datanode State Machine Task Thread - 1"  prio=5 tid=3743 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-4-0" daemon prio=5 tid=4300 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"7a6d3cb5-ca5e-45e5-99fe-a8e18444e416@group-F129EA28C032-StateMachineUpdater" daemon prio=5 tid=4023 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"DataNode DiskChecker thread 0" daemon prio=5 tid=5099 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 18 on default port 32931" daemon prio=5 tid=5544 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-SegmentedRaftLogWorker"  prio=5 tid=5918 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@780fad76" daemon prio=5 tid=5554 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 2 on default port 43855" daemon prio=5 tid=4629 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"BlockDeletingService#0" daemon prio=5 tid=5859 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-SegmentedRaftLogWorker"  prio=5 tid=5902 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1254266408-4862" daemon prio=5 tid=4862 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server Responder" daemon prio=5 tid=5390 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1578)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1561)
"pool-2636-thread-1"  prio=5 tid=5924 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-5e58a3c-1"  prio=5 tid=5784 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor35" daemon prio=5 tid=3817 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"pool-1531-thread-2"  prio=5 tid=5029 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1910932943-5563" daemon prio=5 tid=5563 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Listener at 127.0.0.1/32931"  prio=5 tid=14 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ArrayBlockingQueue.put(ArrayBlockingQueue.java:353)
        at org.apache.hadoop.ozone.MiniOzoneClusterProvider.lambda$createClusters$1(MiniOzoneClusterProvider.java:237)
        at org.apache.hadoop.ozone.MiniOzoneClusterProvider$$Lambda$342/1240843015.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1117823686-3875" daemon prio=5 tid=3875 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-NodeReportForNodeReportHandler" daemon prio=5 tid=5868 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp679454742-3837" daemon prio=5 tid=3837 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1373787253-4617" daemon prio=5 tid=4617 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5815 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=5190 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"dbe8ec56-fdf9-4402-aab9-993d7a20391d-server-thread1" daemon prio=5 tid=5323 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 2 on default port 33597" daemon prio=5 tid=3421 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"pool-1564-thread-1"  prio=5 tid=3532 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp899835961-3536" daemon prio=5 tid=3536 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=5702 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=3934 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"SnapshotDiffCleanupService#0" daemon prio=5 tid=4485 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"process reaper" daemon prio=10 tid=12 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=3804 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor50" daemon prio=5 tid=5726 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=5678 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5215 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5212 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-0"  prio=5 tid=4817 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=5646 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=3851 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DatanodeAdminManager-0" daemon prio=5 tid=3364 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server Responder" daemon prio=5 tid=5504 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1578)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1561)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=3857 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=5134 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-991-thread-1"  prio=5 tid=4508 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=3564 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D->7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b-GrpcLogAppender-LogAppenderDaemon" daemon prio=5 tid=5320 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.grpc.server.GrpcLogAppender.mayWait(GrpcLogAppender.java:198)
        at org.apache.ratis.grpc.server.GrpcLogAppender.run(GrpcLogAppender.java:148)
        at org.apache.ratis.server.leader.LogAppenderDaemon.run(LogAppenderDaemon.java:78)
        at org.apache.ratis.server.leader.LogAppenderDaemon$$Lambda$1130/481315216.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"FullTableCache Cleanup Thread - 0" daemon prio=5 tid=704 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:38893 - 0 "  prio=5 tid=5076 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1214830352-5069" daemon prio=5 tid=5069 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 8 on default port 38893" daemon prio=5 tid=4356 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Parameter Sending Thread for 0.0.0.0/0.0.0.0:35697" daemon prio=5 tid=5714 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferQueue.awaitFulfill(SynchronousQueue.java:764)
        at java.util.concurrent.SynchronousQueue$TransferQueue.transfer(SynchronousQueue.java:695)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at org.apache.hadoop.ipc.Client$Connection$RpcRequestSender.run(Client.java:1122)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=3846 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"2cb742ca-d762-4bc5-b311-f495a87c6b6b-server-thread2" daemon prio=5 tid=5371 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor37" daemon prio=5 tid=3901 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=3352 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=5798 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 5 on default port 38893" daemon prio=5 tid=4353 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp2011203216-3601" daemon prio=5 tid=3601 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 6 on default port 43297" daemon prio=5 tid=5420 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server idle connection scanner for port 38893" daemon prio=5 tid=4283 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"pool-1647-thread-1"  prio=5 tid=4006 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp974801434-3766" daemon prio=5 tid=3766 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#2" daemon prio=5 tid=5895 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"om1@group-C5BA1605619E-LeaderStateImpl" daemon prio=5 tid=4729 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5@group-ED275D8DA029-LeaderStateImpl" daemon prio=5 tid=4105 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"JvmPauseMonitor49" daemon prio=5 tid=5509 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-9"  prio=5 tid=4958 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Lease Manager-LeaseManager#LeaseMonitor" daemon prio=5 tid=4369 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ozone.lease.LeaseManager$LeaseMonitor.run(LeaseManager.java:269)
        at java.lang.Thread.run(Thread.java:750)
"pool-2484-thread-1"  prio=5 tid=5516 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp434207794-5112" daemon prio=5 tid=5112 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=3826 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"timer0" daemon prio=5 tid=576 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"Periodic HDDS volume checker" daemon prio=5 tid=5136 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp698964380-5740" daemon prio=5 tid=5740 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1827130166-5518-acceptor-0@407663d-ServerConnector@590cf1eb{HTTP/1.1, (http/1.1)}{0.0.0.0:35641}" daemon prio=3 tid=5518 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"qtp1016845872-5044" daemon prio=5 tid=5044 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 18 on default port 35697" daemon prio=5 tid=5472 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Session-HouseKeeper-21790ca3-1"  prio=5 tid=5564 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1699819437-3576" daemon prio=5 tid=3576 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 11 on default port 43855" daemon prio=5 tid=4638 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 19 on default port 44467" daemon prio=5 tid=3458 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-1-0" daemon prio=5 tid=5078 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=3629 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server listener on 0" daemon prio=5 tid=5392 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1404)
"JvmPauseMonitor56" daemon prio=5 tid=5857 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=3860 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=5019 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=3816 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=3946 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 17 on default port 36727" daemon prio=5 tid=3559 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Periodic HDDS volume checker" daemon prio=5 tid=3745 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-FollowerState" daemon prio=5 tid=5365 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:128)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=707 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2->efdbc657-ca6b-4fea-9dc1-5411634f3e98-GrpcLogAppender-LogAppenderDaemon" daemon prio=5 tid=5616 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.grpc.server.GrpcLogAppender.mayWait(GrpcLogAppender.java:198)
        at org.apache.ratis.grpc.server.GrpcLogAppender.run(GrpcLogAppender.java:148)
        at org.apache.ratis.server.leader.LogAppenderDaemon.run(LogAppenderDaemon.java:78)
        at org.apache.ratis.server.leader.LogAppenderDaemon$$Lambda$1130/481315216.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1233436001-5483" daemon prio=5 tid=5483 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"pool-1635-thread-1" daemon prio=5 tid=3670 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=5743 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"a1850722-adbb-4bac-9148-47228c85758b@group-880F14DAB24A-StateMachineUpdater" daemon prio=5 tid=4011 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=3781 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"om1-client-thread1" daemon prio=5 tid=4787 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp974801434-3763" daemon prio=5 tid=3763 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 9 on default port 36727" daemon prio=5 tid=3551 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-SegmentedRaftLogWorker"  prio=5 tid=5873 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-SegmentedRaftLogWorker"  prio=5 tid=5293 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@48fd57f0" daemon prio=5 tid=5143 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-StateMachineUpdater" daemon prio=5 tid=5120 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"qtp1016845872-5040-acceptor-0@3df1a037-ServerConnector@48fc2c4c{HTTP/1.1, (http/1.1)}{0.0.0.0:41095}" daemon prio=3 tid=5040 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=5082 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@4f284ed9" daemon prio=5 tid=3481 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"qtp1233436001-5486" daemon prio=5 tid=5486 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server listener on 0" daemon prio=5 tid=3383 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1404)
"ChunkReader-ELG-0" daemon prio=5 tid=3902 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=3813 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp679454742-3840" daemon prio=5 tid=3840 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"grpc-default-executor-7" daemon prio=5 tid=1370 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=4721 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"timer1" daemon prio=5 tid=549 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"qtp592685883-5776" daemon prio=5 tid=5776 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 12 on default port 38893" daemon prio=5 tid=4360 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"7a6d3cb5-ca5e-45e5-99fe-a8e18444e416@group-63EA1A3276A4-StateMachineUpdater" daemon prio=5 tid=4029 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 9 on default port 33597" daemon prio=5 tid=3428 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"735446df-424c-4d38-a683-bd4ef5c8b9e6-server-thread2" daemon prio=5 tid=5963 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1214830352-5071" daemon prio=5 tid=5071 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp698964380-5738" daemon prio=5 tid=5738 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@6d6fbf9d" daemon prio=5 tid=5133 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-OpenPipelineForHealthyPipelineSafeModeRule" daemon prio=5 tid=5275 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 16 on default port 46021" daemon prio=5 tid=4344 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 9 on default port 38893" daemon prio=5 tid=4357 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1016845872-5045" daemon prio=5 tid=5045 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1910932943-5560" daemon prio=5 tid=5560 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor41" daemon prio=5 tid=4609 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-3a1962f5-1"  prio=5 tid=5525 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=5207 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=3167 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5191 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 17 on default port 35697" daemon prio=5 tid=5471 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"pool-1767-thread-1"  prio=5 tid=3872 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-StateMachineUpdater" daemon prio=5 tid=5281 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=5092 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DataNode DiskChecker thread 0" daemon prio=5 tid=4656 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp592685883-5777-acceptor-0@615fbec4-ServerConnector@78ff36ff{HTTP/1.1, (http/1.1)}{0.0.0.0:40463}" daemon prio=3 tid=5777 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"qtp96343072-3687-acceptor-0@70dce03e-ServerConnector@7ee6d7{HTTP/1.1, (http/1.1)}{0.0.0.0:40491}" daemon prio=3 tid=3687 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"ec732e1e-81da-4c3b-ad71-f2fe790a57c7-server-thread2" daemon prio=5 tid=5967 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DataNode DiskChecker thread 0" daemon prio=5 tid=5059 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=3949 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=3631 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1254266408-4858" daemon prio=5 tid=4858 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=3730 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 7 on default port 46383" daemon prio=5 tid=5441 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=5675 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"2cb742ca-d762-4bc5-b311-f495a87c6b6b-impl-thread1"  prio=5 tid=4662 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=5023 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@695005b4" daemon prio=5 tid=3645 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"pool-2562-thread-1"  prio=5 tid=5901 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1373787253-4624" daemon prio=5 tid=4624 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Timer-5"  prio=5 tid=4610 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"qtp1429887399-5669" daemon prio=5 tid=5669 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=3887 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1117823686-3877" daemon prio=5 tid=3877 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"pool-2625-thread-1" daemon prio=5 tid=5760 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2303-thread-1" daemon prio=5 tid=5100 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5847 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor45" daemon prio=5 tid=5206 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1010451749-3650" daemon prio=5 tid=3650 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=5179 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b-server-thread2" daemon prio=5 tid=5324 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-OpenPipelineForHealthyPipelineSafeModeRule" daemon prio=5 tid=5872 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DataNode DiskChecker thread 0" daemon prio=5 tid=5137 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DatanodeAdminManager-0" daemon prio=5 tid=5386 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5267 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Hadoop-Metrics-Updater-0" daemon prio=5 tid=4581 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 14 on default port 42409" daemon prio=5 tid=3473 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState" daemon prio=5 tid=5612 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:128)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@4c7a3781" daemon prio=5 tid=3683 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 19 on default port 46021" daemon prio=5 tid=4347 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-8"  prio=5 tid=4954 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1887242307-5584-acceptor-0@3da1dc20-ServerConnector@704a231{HTTP/1.1, (http/1.1)}{0.0.0.0:46477}" daemon prio=3 tid=5584 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-DatanodeCommandQueueUpdatedForDatanodeCommandCountUpdatedHandler" daemon prio=5 tid=5867 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp974801434-3764-acceptor-0@57b72136-ServerConnector@44a77f88{HTTP/1.1, (http/1.1)}{0.0.0.0:35519}" daemon prio=3 tid=3764 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-FollowerState" daemon prio=5 tid=5958 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:128)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=5601 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 12 on default port 43855" daemon prio=5 tid=4639 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp56975137-4757" daemon prio=5 tid=4757 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 13 on default port 33597" daemon prio=5 tid=3432 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"FixedThreadPoolWithAffinityExecutor-0-0" daemon prio=5 tid=5402 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2217-thread-1"  prio=5 tid=4850 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderStateImpl" daemon prio=5 tid=5319 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"IPC Server handler 3 on default port 36727" daemon prio=5 tid=3545 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=3729 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1620-thread-1"  prio=5 tid=3991 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1233436001-5482" daemon prio=5 tid=5482 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"pool-2255-thread-1" daemon prio=5 tid=5033 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DataNode DiskChecker thread 0" daemon prio=5 tid=5759 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2292-thread-1"  prio=5 tid=5065 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Lease Manager-LeaseManager#LeaseMonitor" daemon prio=5 tid=5475 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ozone.lease.LeaseManager$LeaseMonitor.run(LeaseManager.java:269)
        at java.lang.Thread.run(Thread.java:750)
"qtp96343072-3692" daemon prio=5 tid=3692 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=5129 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5821 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"b85c0d2e-fc3b-4312-9b63-e609b6e2c228-impl-thread1"  prio=5 tid=5553 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2270-thread-1"  prio=5 tid=5038 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-5"  prio=5 tid=4906 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=4811 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"SstFilteringService#0" daemon prio=5 tid=3530 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=3928 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1910932943-5559" daemon prio=5 tid=5559 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"b85c0d2e-fc3b-4312-9b63-e609b6e2c228-server-thread2" daemon prio=5 tid=5947 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5@group-ED275D8DA029-StateMachineUpdater" daemon prio=5 tid=4034 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-d31818-1"  prio=5 tid=3772 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 12 on default port 42409" daemon prio=5 tid=3471 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=5052 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1016845872-5042" daemon prio=5 tid=5042 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-1fed978d-1"  prio=5 tid=3842 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"3804776e-bffa-42a7-addb-662dc325de80-impl-thread1"  prio=5 tid=5036 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=4847 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"Socket Reader #1 for port 0"  prio=5 tid=4578 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1342)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1321)
"EventQueue-CloseContainerForCloseContainerEventHandler" daemon prio=5 tid=4846 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=3820 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-1-0" daemon prio=5 tid=4297 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=3775 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1827130166-5517" daemon prio=5 tid=5517 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"qtp1691761470-4376" daemon prio=5 tid=4376 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 9 on default port 44467" daemon prio=5 tid=3448 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@6489d4ef" daemon prio=5 tid=4812 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@74f0e5fb" daemon prio=5 tid=5055 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=3903 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server idle connection scanner for port 43297" daemon prio=5 tid=5399 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"Socket Reader #1 for port 0"  prio=5 tid=4282 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1342)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1321)
"ChunkWriter-3-0" daemon prio=5 tid=3776 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5200 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=5051 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1694-thread-1" daemon prio=5 tid=3747 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DataNode DiskChecker thread 0" daemon prio=5 tid=5655 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5197 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 9 on default port 42409" daemon prio=5 tid=3468 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1189028704-5153" daemon prio=5 tid=5153 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"a1850722-adbb-4bac-9148-47228c85758b@group-F129EA28C032->b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5-GrpcLogAppender-LogAppenderDaemon" daemon prio=5 tid=4095 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.grpc.server.GrpcLogAppender.mayWait(GrpcLogAppender.java:198)
        at org.apache.ratis.grpc.server.GrpcLogAppender.run(GrpcLogAppender.java:148)
        at org.apache.ratis.server.leader.LogAppenderDaemon.run(LogAppenderDaemon.java:78)
        at org.apache.ratis.server.leader.LogAppenderDaemon$$Lambda$1130/481315216.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=3812 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=3320 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-6"  prio=5 tid=4947 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor53" daemon prio=5 tid=5812 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-SegmentedRaftLogWorker"  prio=5 tid=5122 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 7 on default port 42409" daemon prio=5 tid=3466 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp899835961-3534-acceptor-0@7612e051-ServerConnector@485b54be{HTTP/1.1, (http/1.1)}{0.0.0.0:46561}" daemon prio=3 tid=3534 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=3931 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"qtp1016845872-5046" daemon prio=5 tid=5046 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1373787253-4621" daemon prio=5 tid=4621 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5839 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 0 on default port 32931" daemon prio=5 tid=5526 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"pool-2335-thread-1"  prio=5 tid=5352 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=5644 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 19 on default port 38893" daemon prio=5 tid=4367 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"7a6d3cb5-ca5e-45e5-99fe-a8e18444e416@group-63EA1A3276A4-LeaderStateImpl" daemon prio=5 tid=4103 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"ChunkWriter-3-0" daemon prio=5 tid=5823 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1699819437-3577" daemon prio=5 tid=3577 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:35697 - 0 "  prio=5 tid=5807 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"7a6d3cb5-ca5e-45e5-99fe-a8e18444e416-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=3759 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=5217 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server listener on 0" daemon prio=5 tid=3517 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1404)
"qtp1197505558-4669" daemon prio=5 tid=4669 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 6 on default port 42409" daemon prio=5 tid=3465 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 10 on default port 42409" daemon prio=5 tid=3469 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderStateImpl" daemon prio=5 tid=5969 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"ChunkReader-ELG-0" daemon prio=5 tid=5825 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5818 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp698964380-5736" daemon prio=5 tid=5736 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp679454742-3841" daemon prio=5 tid=3841 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-7-0" daemon prio=5 tid=5409 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"timer4" daemon prio=5 tid=554 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5223 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=5083 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 8 on default port 46021" daemon prio=5 tid=4336 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkReader-ELG-0" daemon prio=5 tid=5813 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=4813 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 4 on default port 38893" daemon prio=5 tid=4352 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-SegmentedRaftLogWorker"  prio=5 tid=5288 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 9 on default port 35697" daemon prio=5 tid=5463 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5828 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5075 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1892-thread-1"  prio=5 tid=4371 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=5204 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 18 on default port 46021" daemon prio=5 tid=4346 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"JvmPauseMonitor54" daemon prio=5 tid=5824 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=5090 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 8 on default port 42409" daemon prio=5 tid=3467 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode State Machine Task Thread - 0"  prio=5 tid=5573 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1429887399-5666" daemon prio=5 tid=5666 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"efdbc657-ca6b-4fea-9dc1-5411634f3e98-server-thread2" daemon prio=5 tid=5618 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5084 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-SegmentedRaftLogWorker"  prio=5 tid=5880 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 13 on default port 38893" daemon prio=5 tid=4361 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 4 on default port 46021" daemon prio=5 tid=4332 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"FixedThreadPoolWithAffinityExecutor-5-0" daemon prio=5 tid=4301 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"fc27ded4-a200-4b47-a78c-bc930dadcc21-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=5771 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5803 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1373787253-4623" daemon prio=5 tid=4623 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"a1850722-adbb-4bac-9148-47228c85758b@group-F129EA28C032-StateMachineUpdater" daemon prio=5 tid=4014 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"qtp1254266408-4859" daemon prio=5 tid=4859 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1910932943-5561" daemon prio=5 tid=5561 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=5858 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=5856 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@71243cb5" daemon prio=5 tid=5064 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"grpc-default-executor-4" daemon prio=5 tid=924 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=5262 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#2" daemon prio=5 tid=5852 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1214830352-5073" daemon prio=5 tid=5073 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"om1-impl-thread1"  prio=5 tid=4493 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 3 on default port 33597" daemon prio=5 tid=3422 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 7 on default port 36727" daemon prio=5 tid=3549 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"pool-693-thread-3"  prio=5 tid=3894 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=5649 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-StateMachineUpdater" daemon prio=5 tid=5337 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5209 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server idle connection scanner for port 36727" daemon prio=5 tid=3519 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"IPC Server handler 0 on default port 43855" daemon prio=5 tid=4627 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"grpc-default-worker-ELG-3-2" daemon prio=5 tid=482 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait0(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:182)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWait(EpollEventLoop.java:290)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:354)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:750)
"pool-2318-thread-1"  prio=5 tid=5105 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=5704 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor39" daemon prio=5 tid=3945 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 19 on default port 42409" daemon prio=5 tid=3478 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Session-HouseKeeper-7384aaeb-1"  prio=5 tid=4867 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=3780 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 1 on default port 33597" daemon prio=5 tid=3420 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"BlockDeletingService#0" daemon prio=5 tid=3960 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"om1-impl-thread1"  prio=5 tid=5497 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp698964380-5739" daemon prio=5 tid=5739 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=5826 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=5765 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 0 on default port 33597" daemon prio=5 tid=3419 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"735446df-424c-4d38-a683-bd4ef5c8b9e6-server-thread1" daemon prio=5 tid=5962 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-466b0743-1"  prio=5 tid=5591 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 5 on default port 32931" daemon prio=5 tid=5531 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 10 on default port 43297" daemon prio=5 tid=5424 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"om1@group-C5BA1605619E-SegmentedRaftLogWorker"  prio=5 tid=3522 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server Responder" daemon prio=5 tid=3372 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1578)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1561)
"IPC Server handler 5 on default port 44011" daemon prio=5 tid=4313 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 8 on default port 44467" daemon prio=5 tid=3447 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=3169 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=3727 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"10617bfc-881b-4e3d-8722-e7f81aaf7e30-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=5579 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"SstFilteringService#0" daemon prio=5 tid=5514 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2123-thread-1" daemon prio=5 tid=4745 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"7a6d3cb5-ca5e-45e5-99fe-a8e18444e416-server-thread1" daemon prio=5 tid=4098 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=3663 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=4723 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 0 on default port 46383" daemon prio=5 tid=5434 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5-server-thread4"  prio=5 tid=4984 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@5d1caf32" daemon prio=5 tid=3811 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server Responder" daemon prio=5 tid=3381 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1578)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1561)
"Command processor thread" daemon prio=5 tid=3587 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1887242307-5589" daemon prio=5 tid=5589 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-1"  prio=5 tid=4823 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=5725 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@5b70a846" daemon prio=5 tid=5581 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=3634 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"timer5" daemon prio=5 tid=559 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"IPC Server handler 15 on default port 35697" daemon prio=5 tid=5469 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp56975137-4761" daemon prio=5 tid=4761 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderStateImpl" daemon prio=5 tid=5495 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"IPC Server listener on 0" daemon prio=5 tid=4281 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1404)
"Socket Reader #1 for port 0"  prio=5 tid=4292 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1342)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1321)
"qtp1254266408-4861" daemon prio=5 tid=4861 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp96343072-3689" daemon prio=5 tid=3689 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"RatisPipelineUtilsThread - 0"  prio=5 tid=3353 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.pipeline.BackgroundPipelineCreator.run(BackgroundPipelineCreator.java:176)
        at org.apache.hadoop.hdds.scm.pipeline.BackgroundPipelineCreator$$Lambda$409/1972512903.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-260d79ac-1"  prio=5 tid=3541 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 5 on default port 42409" daemon prio=5 tid=3464 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@7e6200aa" daemon prio=5 tid=4663 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"FullTableCache Cleanup Thread - 0" daemon prio=5 tid=703 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=3581 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=3936 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp899835961-3540" daemon prio=5 tid=3540 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@5ed8a2c8" daemon prio=5 tid=3862 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"qtp1827130166-5523" daemon prio=5 tid=5523 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp434207794-5106" daemon prio=5 tid=5106 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=5790 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 9 on default port 44011" daemon prio=5 tid=4317 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=4127 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=5056 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-4"  prio=5 tid=4901 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1741-thread-1"  prio=5 tid=3833 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=5548 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-SegmentedRaftLogWorker"  prio=5 tid=5925 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=5787 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 17 on default port 44467" daemon prio=5 tid=3456 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server listener on 0" daemon prio=5 tid=4291 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1404)
"qtp1117823686-3873" daemon prio=5 tid=3873 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"pool-693-thread-1"  prio=5 tid=3892 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-7-0" daemon prio=5 tid=3395 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"grpc-default-executor-3" daemon prio=5 tid=553 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=5749 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@52c609ea" daemon prio=5 tid=5095 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"ReplicationMonitor" daemon prio=5 tid=4276 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:971)
        at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager$$Lambda$422/684577835.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1010451749-3652" daemon prio=5 tid=3652 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-7"  prio=5 tid=4952 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 4 on default port 36727" daemon prio=5 tid=3546 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"BlockDeletingService#1" daemon prio=5 tid=5770 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderStateImpl" daemon prio=5 tid=5959 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"ChunkWriter-0-0" daemon prio=5 tid=3767 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 3 on default port 38893" daemon prio=5 tid=4351 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5845 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 17 on default port 44011" daemon prio=5 tid=4325 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1827130166-5522" daemon prio=5 tid=5522 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=3926 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 15 on default port 36727" daemon prio=5 tid=3557 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 2 on default port 43297" daemon prio=5 tid=5416 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Session-HouseKeeper-361bc4d1-1"  prio=5 tid=3881 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 15 on default port 43855" daemon prio=5 tid=4642 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1827130166-5519" daemon prio=5 tid=5519 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1010451749-3653" daemon prio=5 tid=3653 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 11 on default port 46383" daemon prio=5 tid=5445 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-SegmentedRaftLogWorker"  prio=5 tid=5279 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp592685883-5778" daemon prio=5 tid=5778 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 5 on default port 36727" daemon prio=5 tid=3547 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderStateImpl" daemon prio=5 tid=5951 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@2622b07d" daemon prio=5 tid=3871 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=3848 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=3932 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp679454742-3835" daemon prio=5 tid=3835 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"qtp1010451749-3647" daemon prio=5 tid=3647 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"om1@group-C5BA1605619E-StateMachineUpdater" daemon prio=5 tid=5508 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#2" daemon prio=5 tid=5894 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 3 on default port 44011" daemon prio=5 tid=4311 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server idle connection scanner for port 35697" daemon prio=5 tid=5389 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"IPC Server handler 7 on default port 46021" daemon prio=5 tid=4335 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 8 on default port 46383" daemon prio=5 tid=5442 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Command processor thread" daemon prio=5 tid=3635 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=4807 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=3956 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@617e442c" daemon prio=5 tid=5024 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 15 on default port 33597" daemon prio=5 tid=3434 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 19 on default port 46383" daemon prio=5 tid=5453 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"FixedThreadPoolWithAffinityExecutor-0-0" daemon prio=5 tid=4296 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-PipelineReportForPipelineReportHandler" daemon prio=5 tid=3912 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 14 on default port 33597" daemon prio=5 tid=3433 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 7 on default port 33597" daemon prio=5 tid=3426 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"pool-2328-thread-1" daemon prio=5 tid=5138 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 19 on default port 33597" daemon prio=5 tid=3438 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@661ff236" daemon prio=5 tid=5753 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderStateImpl" daemon prio=5 tid=5593 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"qtp1117823686-3879" daemon prio=5 tid=3879 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderStateImpl" daemon prio=5 tid=5367 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"qtp1016845872-5041" daemon prio=5 tid=5041 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"FullTableCache Cleanup Thread - 0" daemon prio=5 tid=3289 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=5840 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5@group-ED275D8DA029-SegmentedRaftLogWorker"  prio=5 tid=4032 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-2-0" daemon prio=5 tid=3390 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=3884 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=5568 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FullTableCache Cleanup Thread - 0" daemon prio=5 tid=2177 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"SCMBlockDeletingService#0" daemon prio=5 tid=3479 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-server-thread1" daemon prio=5 tid=5619 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 16 on default port 42409" daemon prio=5 tid=3475 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp2011203216-3603" daemon prio=5 tid=3603 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp899835961-3539" daemon prio=5 tid=3539 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"10617bfc-881b-4e3d-8722-e7f81aaf7e30-impl-thread1"  prio=5 tid=5580 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1910932943-5556" daemon prio=5 tid=5556 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"qtp1117823686-3878" daemon prio=5 tid=3878 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp434207794-5108-acceptor-0@7dda15e3-ServerConnector@2ad56cf9{HTTP/1.1, (http/1.1)}{0.0.0.0:44805}" daemon prio=3 tid=5108 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-7b943ddc-1"  prio=5 tid=5074 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"e5e627c9-cddd-4fde-83a7-403d4c1ca45c-impl-thread1"  prio=5 tid=4754 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=3818 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"Hadoop-Metrics-Updater-0" daemon prio=5 tid=3382 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5772 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-0"  prio=5 tid=4820 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Parameter Sending Thread for 0.0.0.0/0.0.0.0:42409" daemon prio=5 tid=3751 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferQueue.awaitFulfill(SynchronousQueue.java:764)
        at java.util.concurrent.SynchronousQueue$TransferQueue.transfer(SynchronousQueue.java:695)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at org.apache.hadoop.ipc.Client$Connection$RpcRequestSender.run(Client.java:1122)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-2731b0d7-1"  prio=5 tid=4625 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule" daemon prio=5 tid=3910 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=2180 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5011 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ef6aab41-a1eb-41b3-a4a5-2458878a1611-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=3643 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"qtp1887242307-5583" daemon prio=5 tid=5583 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=3728 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderStateImpl" daemon prio=5 tid=5614 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"om1@group-C5BA1605619E-SegmentedRaftLogWorker"  prio=5 tid=4592 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 16 on default port 43855" daemon prio=5 tid=4643 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=4808 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 1 on default port 44467" daemon prio=5 tid=3440 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-StateMachineUpdater" daemon prio=5 tid=5285 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=5080 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 5 on default port 46383" daemon prio=5 tid=5439 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"FixedThreadPoolWithAffinityExecutor-1-0" daemon prio=5 tid=3389 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1591-thread-1" daemon prio=5 tid=3593 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Signal Dispatcher" daemon prio=9 tid=4 runnable
java.lang.Thread.State: RUNNABLE
"FixedThreadPoolWithAffinityExecutor-9-0" daemon prio=5 tid=5411 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"grpc-default-executor-1" daemon prio=5 tid=499 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 2 on default port 36727" daemon prio=5 tid=3544 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 2 on default port 46383" daemon prio=5 tid=5436 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=3658 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=5752 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5711 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-3-0" daemon prio=5 tid=3391 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 11 on default port 33597" daemon prio=5 tid=3430 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"SstFilteringService#0" daemon prio=5 tid=4614 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-3-0" daemon prio=5 tid=4299 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:42409 - 0 "  prio=5 tid=3748 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-1"  prio=5 tid=4819 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-6-0" daemon prio=5 tid=4302 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"KeyDeletingService#0" daemon prio=5 tid=3527 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=5748 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=5764 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 6 on default port 38893" daemon prio=5 tid=4354 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Finalizer" daemon prio=8 tid=3 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:188)
"IPC Server handler 7 on default port 38893" daemon prio=5 tid=4355 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 3 on default port 32931" daemon prio=5 tid=5529 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server idle connection scanner for port 46021" daemon prio=5 tid=4288 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"qtp1429887399-5664" daemon prio=5 tid=5664 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=3824 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-8"  prio=5 tid=4956 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 2 on default port 44467" daemon prio=5 tid=3441 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-StateMachineUpdater" daemon prio=5 tid=5927 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"KeyDeletingService#0" daemon prio=5 tid=5511 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server listener on 0" daemon prio=5 tid=4577 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1404)
"qtp1691761470-4378" daemon prio=5 tid=4378 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp434207794-5107" daemon prio=5 tid=5107 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-StateMachineUpdater" daemon prio=5 tid=5923 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server idle connection scanner for port 46383" daemon prio=5 tid=5394 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"EventQueue-DatanodeCommandQueueUpdatedForDatanodeCommandCountUpdatedHandler" daemon prio=5 tid=3964 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-0-0" daemon prio=5 tid=3388 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@5fbe5ea7" daemon prio=5 tid=5476 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"ContainerReplicationThread-0" daemon prio=5 tid=5348 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.PriorityBlockingQueue.take(PriorityBlockingQueue.java:549)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=5193 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Client (815625593) connection to 0.0.0.0/0.0.0.0:38893 from runner" daemon prio=5 tid=4992 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:1043)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1094)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=3937 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 14 on default port 38893" daemon prio=5 tid=4362 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"null-request--thread1" daemon prio=5 tid=4816 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=5088 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=3933 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderStateImpl" daemon prio=5 tid=5973 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-StateMachineUpdater" daemon prio=5 tid=5117 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"Reference Handler" daemon prio=10 tid=2 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
"IPC Server handler 8 on default port 43855" daemon prio=5 tid=4635 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderStateImpl" daemon prio=5 tid=5359 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"FullTableCache Cleanup Thread - 0" daemon prio=5 tid=3271 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1699819437-3572" daemon prio=5 tid=3572 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:38893 - 0 "  prio=5 tid=5189 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 1 on default port 46021" daemon prio=5 tid=4329 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"EndpointStateMachine task thread for /0.0.0.0:38893 - 0 "  prio=5 tid=4991 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=3823 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 15 on default port 44467" daemon prio=5 tid=3454 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"DatanodeAdminManager-0" daemon prio=5 tid=4280 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=5767 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FollowerState" daemon prio=5 tid=5613 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:128)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=3883 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Hadoop-Metrics-Updater-0" daemon prio=5 tid=3387 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=5703 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 16 on default port 32931" daemon prio=5 tid=5542 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"a1850722-adbb-4bac-9148-47228c85758b@group-F129EA28C032-LeaderStateImpl" daemon prio=5 tid=4094 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"JvmPauseMonitor34" daemon prio=5 tid=3777 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=5801 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=3807 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=5788 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=5603 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-StateMachineUpdater" daemon prio=5 tid=5330 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 11 on default port 44467" daemon prio=5 tid=3450 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"OM StateMachine ApplyTransaction Thread - 0" daemon prio=5 tid=4788 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=5804 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2489-thread-1" daemon prio=5 tid=5550 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor51" daemon prio=5 tid=5766 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=3863 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-SegmentedRaftLogWorker"  prio=5 tid=5283 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 7 on default port 44011" daemon prio=5 tid=4315 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-StateMachineUpdater" daemon prio=5 tid=5916 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=3943 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=3854 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ff409154-2e62-420c-a7a7-066f3a70d145-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=5658 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor46" daemon prio=5 tid=5218 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=3639 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=5710 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-7-0" daemon prio=5 tid=4303 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 18 on default port 44011" daemon prio=5 tid=4326 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkReader-ELG-0" daemon prio=5 tid=5727 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule" daemon prio=5 tid=5173 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1910932943-5562" daemon prio=5 tid=5562 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-SegmentedRaftLogWorker"  prio=5 tid=5115 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=3939 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 9 on default port 46021" daemon prio=5 tid=4337 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-SegmentedRaftLogWorker"  prio=5 tid=5349 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-73cd3f7a-1"  prio=5 tid=3491 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp56975137-4763" daemon prio=5 tid=4763 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 5 on default port 35697" daemon prio=5 tid=5459 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-2"  prio=5 tid=4853 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=3924 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 10 on default port 44467" daemon prio=5 tid=3449 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 15 on default port 42409" daemon prio=5 tid=3474 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-2-0" daemon prio=5 tid=3847 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-2-0" daemon prio=5 tid=4298 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 0 on default port 44467" daemon prio=5 tid=3439 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5861 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5732 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server idle connection scanner for port 44011" daemon prio=5 tid=4293 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"ChunkWriter-2-0" daemon prio=5 tid=3899 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=3731 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 0 on default port 43297" daemon prio=5 tid=5414 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-0-0" daemon prio=5 tid=5808 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FullTableCache Cleanup Thread - 0" daemon prio=5 tid=4243 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 1 on default port 38893" daemon prio=5 tid=4349 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Socket Reader #1 for port 0"  prio=5 tid=3367 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1342)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1321)
"IPC Server handler 15 on default port 43297" daemon prio=5 tid=5429 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 16 on default port 33597" daemon prio=5 tid=3435 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server idle connection scanner for port 44467" daemon prio=5 tid=3380 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"SnapshotDeletingService#0" daemon prio=5 tid=3531 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@2a4df7dc" daemon prio=5 tid=5680 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-StateMachineUpdater" daemon prio=5 tid=5907 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=3585 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@48032577" daemon prio=5 tid=4849 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 10 on default port 46021" daemon prio=5 tid=4338 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 12 on default port 44011" daemon prio=5 tid=4320 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1887242307-5588" daemon prio=5 tid=5588 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-StateMachineUpdater" daemon prio=5 tid=5295 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@74bc2ca7" daemon prio=5 tid=3761 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"SnapshotDiffCleanupService#0" daemon prio=5 tid=5493 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp2011203216-3605" daemon prio=5 tid=3605 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"pool-1536-thread-1"  prio=5 tid=3510 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp679454742-3836" daemon prio=5 tid=3836 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=5181 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-StateMachineUpdater" daemon prio=5 tid=5290 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-PipelineActionsForPipelineActionHandler" daemon prio=5 tid=4845 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1429887399-5663-acceptor-0@56304d61-ServerConnector@4f48c388{HTTP/1.1, (http/1.1)}{0.0.0.0:45617}" daemon prio=3 tid=5663 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5-server-thread1" daemon prio=5 tid=4097 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-SegmentedRaftLogWorker"  prio=5 tid=5328 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"pool-1759-thread-1"  prio=5 tid=4037 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp2011203216-3604" daemon prio=5 tid=3604 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 8 on default port 44011" daemon prio=5 tid=4316 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 15 on default port 32931" daemon prio=5 tid=5541 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-2-0" daemon prio=5 tid=5810 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5827 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"grpc-default-worker-ELG-3-1" daemon prio=5 tid=480 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait0(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:182)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWait(EpollEventLoop.java:290)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:354)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=5600 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 8 on default port 43297" daemon prio=5 tid=5422 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=5569 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=3822 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Hadoop-Metrics-Updater-0" daemon prio=5 tid=5391 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp592685883-5782" daemon prio=5 tid=5782 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=3815 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerReplicationThread-0" daemon prio=5 tid=5297 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.PriorityBlockingQueue.take(PriorityBlockingQueue.java:549)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 10 on default port 32931" daemon prio=5 tid=5536 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 11 on default port 46021" daemon prio=5 tid=4339 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"EndpointStateMachine task thread for /0.0.0.0:35697 - 0 "  prio=5 tid=5850 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DeleteContainerThread-0"  prio=5 tid=5326 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2343-thread-1"  prio=5 tid=5144 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=5841 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1581909030-3484-acceptor-0@5a0745e5-ServerConnector@5c00f0b4{HTTP/1.1, (http/1.1)}{0.0.0.0:44223}" daemon prio=3 tid=3484 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=4806 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 4 on default port 33597" daemon prio=5 tid=3423 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-1-0" daemon prio=5 tid=3955 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=3805 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 3 on default port 43855" daemon prio=5 tid=4630 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1373787253-4619" daemon prio=5 tid=4619 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=5679 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 1" daemon prio=5 tid=3660 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1691761470-4379" daemon prio=5 tid=4379 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:42409 - 0 "  prio=5 tid=3940 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1214830352-5067-acceptor-0@6ac51344-ServerConnector@91e78e8{HTTP/1.1, (http/1.1)}{0.0.0.0:40523}" daemon prio=3 tid=5067 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server Responder" daemon prio=5 tid=4294 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1578)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1561)
"735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderStateImpl" daemon prio=5 tid=5971 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"qtp1581909030-3483" daemon prio=5 tid=3483 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-server-thread3" daemon prio=5 tid=5621 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@6767467b" daemon prio=5 tid=3732 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"qtp2011203216-3600-acceptor-0@5053f2cb-ServerConnector@20b56e22{HTTP/1.1, (http/1.1)}{0.0.0.0:35895}" daemon prio=3 tid=3600 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"FullTableCache Cleanup Thread - 0" daemon prio=5 tid=4789 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1254266408-4857" daemon prio=5 tid=4857 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@6cd68188" daemon prio=5 tid=5572 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"SCM Heartbeat Processing Thread - 0" daemon prio=5 tid=5379 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1728646390-5639" daemon prio=5 tid=5639 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233->109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-GrpcLogAppender-LogAppenderDaemon" daemon prio=5 tid=5369 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.grpc.server.GrpcLogAppender.mayWait(GrpcLogAppender.java:198)
        at org.apache.ratis.grpc.server.GrpcLogAppender.run(GrpcLogAppender.java:148)
        at org.apache.ratis.server.leader.LogAppenderDaemon.run(LogAppenderDaemon.java:78)
        at org.apache.ratis.server.leader.LogAppenderDaemon$$Lambda$1130/481315216.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"a1850722-adbb-4bac-9148-47228c85758b@group-F129EA28C032-SegmentedRaftLogWorker"  prio=5 tid=4012 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=5210 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor52" daemon prio=5 tid=5800 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1373787253-4622" daemon prio=5 tid=4622 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"DeleteContainerThread-1"  prio=5 tid=5357 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor55" daemon prio=5 tid=5842 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"KeyDeletingService#0" daemon prio=5 tid=5512 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1189028704-5151" daemon prio=5 tid=5151 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=3947 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"b85c0d2e-fc3b-4312-9b63-e609b6e2c228-server-thread1" daemon prio=5 tid=5946 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 17 on default port 43855" daemon prio=5 tid=4644 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=5789 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 0 on default port 38893" daemon prio=5 tid=4348 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp2011203216-3606" daemon prio=5 tid=3606 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Under Replicated Processor" daemon prio=5 tid=3362 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.container.replication.UnhealthyReplicationProcessor.run(UnhealthyReplicationProcessor.java:140)
        at java.lang.Thread.run(Thread.java:750)
"qtp1429887399-5668" daemon prio=5 tid=5668 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 6 on default port 43855" daemon prio=5 tid=4633 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-3-0" daemon prio=5 tid=3900 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=3954 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1010451749-3648" daemon prio=5 tid=3648 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ExpiredContainerReplicaOpScrubberThread" daemon prio=5 tid=3355 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.ha.BackgroundSCMService.run(BackgroundSCMService.java:110)
        at org.apache.hadoop.hdds.scm.ha.BackgroundSCMService$$Lambda$412/2019944060.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:42409 - 0 "  prio=5 tid=3925 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1699819437-3573-acceptor-0@22d0ffe5-ServerConnector@3f2958e0{HTTP/1.1, (http/1.1)}{0.0.0.0:38569}" daemon prio=3 tid=3573 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"om1@group-C5BA1605619E-StateMachineUpdater" daemon prio=5 tid=4608 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-05CCBA5F75D2-SegmentedRaftLogWorker"  prio=5 tid=4001 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server Responder" daemon prio=5 tid=4289 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1578)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1561)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5769 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 16 on default port 36727" daemon prio=5 tid=3558 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"pool-1598-thread-1"  prio=5 tid=3983 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5234 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=4724 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-4-0" daemon prio=5 tid=5406 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 5 on default port 44467" daemon prio=5 tid=3444 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Periodic HDDS volume checker" daemon prio=5 tid=3865 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1197505558-4670" daemon prio=5 tid=4670 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 2" daemon prio=5 tid=5705 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@7c9baaa2" daemon prio=5 tid=4726 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-8-0" daemon prio=5 tid=3396 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5-server-thread2" daemon prio=5 tid=4099 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-server-thread2" daemon prio=5 tid=5620 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"65d5be4b-faf3-45c4-8c44-989db5872d1e-impl-thread1"  prio=5 tid=5628 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 11 on default port 38893" daemon prio=5 tid=4359 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"pool-1532-thread-1"  prio=5 tid=3482 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1189028704-5150" daemon prio=5 tid=5150 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-SegmentedRaftLogWorker"  prio=5 tid=5921 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1214830352-5066" daemon prio=5 tid=5066 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@47d706bf" daemon prio=5 tid=5709 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"qtp899835961-3535" daemon prio=5 tid=3535 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=5793 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 6 on default port 33597" daemon prio=5 tid=3425 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=5018 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1581909030-3485" daemon prio=5 tid=3485 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-5fa336c0-1"  prio=5 tid=3694 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5809 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2533-thread-1" daemon prio=5 tid=5625 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=4253 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1373787253-4620" daemon prio=5 tid=4620 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ec732e1e-81da-4c3b-ad71-f2fe790a57c7-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=5721 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"qtp1117823686-3876" daemon prio=5 tid=3876 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-3057bfb4-1"  prio=5 tid=4766 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 12 on default port 44467" daemon prio=5 tid=3451 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"BlockDeletingService#1" daemon prio=5 tid=3906 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#2" daemon prio=5 tid=5866 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-impl-thread1"  prio=5 tid=5063 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 19 on default port 35697" daemon prio=5 tid=5473 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-2399665D7ECA-LeaderStateImpl" daemon prio=5 tid=4081 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-SegmentedRaftLogWorker"  prio=5 tid=5353 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5805 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Client (815625593) connection to 0.0.0.0/0.0.0.0:42409 from runner" daemon prio=5 tid=3750 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:1043)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1094)
"65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-StateMachineUpdater" daemon prio=5 tid=5887 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5794 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-SegmentedRaftLogWorker"  prio=5 tid=5885 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=3808 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=5208 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1827130166-5521" daemon prio=5 tid=5521 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=5025 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp698964380-5741" daemon prio=5 tid=5741 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=5219 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:38893 - 0 "  prio=5 tid=5201 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5-server-thread3" daemon prio=5 tid=4101 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=5077 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2592-thread-1"  prio=5 tid=5690 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=5220 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"om1@group-C5BA1605619E-StateMachineUpdater" daemon prio=5 tid=3524 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=5716 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=3726 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server Responder" daemon prio=5 tid=5400 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1578)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1561)
"pool-1659-thread-1"  prio=5 tid=3685 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Lease Manager-LeaseManager#LeaseMonitor" daemon prio=5 tid=3480 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ozone.lease.LeaseManager$LeaseMonitor.run(LeaseManager.java:269)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=5017 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@44144793" daemon prio=5 tid=5630 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"qtp1233436001-5484" daemon prio=5 tid=5484 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 17 on default port 46021" daemon prio=5 tid=4345 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Hadoop-Metrics-Updater-0" daemon prio=5 tid=5505 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=5707 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 19 on default port 36727" daemon prio=5 tid=3561 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 17 on default port 32931" daemon prio=5 tid=5543 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"BlockDeletingService#1" daemon prio=5 tid=5268 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-991-thread-2"  prio=5 tid=4509 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2192-thread-1" daemon prio=5 tid=4834 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=5180 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=3582 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=5728 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5806 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:38893 - 0 "  prio=5 tid=5258 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=5602 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-3"  prio=5 tid=4864 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Hadoop-Metrics-Updater-0" daemon prio=5 tid=4295 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"null-request--thread1" daemon prio=5 tid=4796 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2039-thread-1" daemon prio=5 tid=4657 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server listener on 0" daemon prio=5 tid=5387 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1404)
"735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-StateMachineUpdater" daemon prio=5 tid=5911 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"qtp1887242307-5585" daemon prio=5 tid=5585 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5013 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 17 on default port 46383" daemon prio=5 tid=5451 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FollowerState" daemon prio=5 tid=5318 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:128)
"pool-2540-thread-1"  prio=5 tid=5884 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=3855 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 3 on default port 43297" daemon prio=5 tid=5417 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1736155493-5695" daemon prio=5 tid=5695 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=5761 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=3779 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 14 on default port 46021" daemon prio=5 tid=4342 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-8"  prio=5 tid=4957 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Socket Reader #1 for port 0"  prio=5 tid=5398 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1342)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1321)
"pool-2644-thread-1"  prio=5 tid=5775 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@22ce719d" daemon prio=5 tid=4755 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"qtp1214830352-5068" daemon prio=5 tid=5068 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 15 on default port 46383" daemon prio=5 tid=5449 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"StaleRecoveringContainerScrubbingService#2" daemon prio=5 tid=5864 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"timer7" daemon prio=5 tid=575 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-server-thread1" daemon prio=5 tid=5372 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1214830352-5072" daemon prio=5 tid=5072 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1373787253-4618-acceptor-0@7d661ef3-ServerConnector@50459c16{HTTP/1.1, (http/1.1)}{0.0.0.0:39267}" daemon prio=3 tid=4618 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-SegmentedRaftLogWorker"  prio=5 tid=5345 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=5058 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=4810 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1010451749-3651" daemon prio=5 tid=3651 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 14 on default port 44011" daemon prio=5 tid=4322 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp2011203216-3602" daemon prio=5 tid=3602 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=5232 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-5ed4f363-1"  prio=5 tid=5154 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=4719 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-StateMachineUpdater" daemon prio=5 tid=5311 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderStateImpl" daemon prio=5 tid=5932 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"qtp899835961-3533" daemon prio=5 tid=3533 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-StateMachineUpdater" daemon prio=5 tid=5355 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=3666 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 10 on default port 46383" daemon prio=5 tid=5444 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-5"  prio=5 tid=4905 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1691761470-4372" daemon prio=5 tid=4372 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 0 on default port 42409" daemon prio=5 tid=3459 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Hadoop-Metrics-Updater-0" daemon prio=5 tid=4285 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"b85c0d2e-fc3b-4312-9b63-e609b6e2c228-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=5552 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"ReplicationMonitor" daemon prio=5 tid=5383 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:971)
        at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager$$Lambda$422/684577835.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 9 on default port 43855" daemon prio=5 tid=4636 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1581909030-3488" daemon prio=5 tid=3488 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"timer3" daemon prio=5 tid=552 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"IPC Server handler 0 on default port 35697" daemon prio=5 tid=5454 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-StateMachineUpdater" daemon prio=5 tid=5347 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=5012 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=5098 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1733-thread-1"  prio=5 tid=4015 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp899835961-3537" daemon prio=5 tid=3537 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Timer for 'StorageContainerManager' metrics system" daemon prio=5 tid=5412 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"qtp1728646390-5633" daemon prio=5 tid=5633 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:42409 - 0 "  prio=5 tid=3953 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1887242307-5590" daemon prio=5 tid=5590 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"grpc-default-executor-8" daemon prio=5 tid=3951 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1197505558-4672" daemon prio=5 tid=4672 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=5654 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-2-0" daemon prio=5 tid=5822 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"70ce52ad-a2f4-4f2c-b96d-cd309621d39c-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=3568 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-c3b4d7d-1"  prio=5 tid=3580 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=3853 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1233436001-5481" daemon prio=5 tid=5481 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"pool-2058-thread-1"  prio=5 tid=4665 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=4130 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-3fe8a8e9-1"  prio=5 tid=4674 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"a1850722-adbb-4bac-9148-47228c85758b-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=3681 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=3952 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 14 on default port 44467" daemon prio=5 tid=3453 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-SegmentedRaftLogWorker"  prio=5 tid=5335 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 2 on default port 44011" daemon prio=5 tid=4310 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"BlockDeletingService#2" daemon prio=5 tid=5870 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-NewNodeForNewNodeHandler" daemon prio=5 tid=5172 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1117823686-3880" daemon prio=5 tid=3880 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"pool-1901-thread-1"  prio=5 tid=4432 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderStateImpl" daemon prio=5 tid=5940 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"EndpointStateMachine task thread for /0.0.0.0:38893 - 0 "  prio=5 tid=5226 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 3 on default port 42409" daemon prio=5 tid=3462 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode State Machine Task Thread - 0"  prio=5 tid=5187 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server idle connection scanner for port 43855" daemon prio=5 tid=4579 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=5645 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 2 on default port 35697" daemon prio=5 tid=5456 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5188 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-9"  prio=5 tid=4959 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ExpiredContainerReplicaOpScrubberThread" daemon prio=5 tid=4275 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdds.scm.ha.BackgroundSCMService.run(BackgroundSCMService.java:110)
        at org.apache.hadoop.hdds.scm.ha.BackgroundSCMService$$Lambda$412/2019944060.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1887242307-5587" daemon prio=5 tid=5587 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1016845872-5043" daemon prio=5 tid=5043 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1197505558-4673" daemon prio=5 tid=4673 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"ChunkReader-ELG-0" daemon prio=5 tid=5009 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native Method)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:209)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.Native.epollWait(Native.java:202)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:294)
        at org.apache.ratis.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:351)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"grpc-default-executor-5" daemon prio=5 tid=1368 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-1"  prio=5 tid=4821 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ec732e1e-81da-4c3b-ad71-f2fe790a57c7-impl-thread1"  prio=5 tid=5724 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule" daemon prio=5 tid=5832 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:35697 - 0 "  prio=5 tid=5795 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp56975137-4758-acceptor-0@77c53bd-ServerConnector@5ae3e6e8{HTTP/1.1, (http/1.1)}{0.0.0.0:41531}" daemon prio=3 tid=4758 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-StateMachineUpdater" daemon prio=5 tid=5351 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"qtp1728646390-5637" daemon prio=5 tid=5637 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"f88c54b2-8776-4954-8dfb-f8bea6a862a6-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=5141 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"qtp434207794-5111" daemon prio=5 tid=5111 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"pool-2548-thread-1"  prio=5 tid=5632 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-server-thread3" daemon prio=5 tid=5376 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=5570 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5211 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"KeyDeletingService#0" daemon prio=5 tid=4612 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2584-thread-1"  prio=5 tid=5908 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=3944 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"7a6d3cb5-ca5e-45e5-99fe-a8e18444e416@group-F129EA28C032-SegmentedRaftLogWorker"  prio=5 tid=4021 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"qtp1189028704-5149" daemon prio=5 tid=5149 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 19 on default port 44011" daemon prio=5 tid=4327 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 5 on default port 33597" daemon prio=5 tid=3424 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 13 on default port 46021" daemon prio=5 tid=4341 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Command processor thread" daemon prio=5 tid=5571 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 1 on default port 36727" daemon prio=5 tid=3543 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"pool-2284-thread-1"  prio=5 tid=5339 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 14 on default port 46383" daemon prio=5 tid=5448 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-3-0" daemon prio=5 tid=3929 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1699819437-3575" daemon prio=5 tid=3575 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1728646390-5640" daemon prio=5 tid=5640 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"pool-2511-thread-1" daemon prio=5 tid=5577 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5@group-F129EA28C032-FollowerState" daemon prio=5 tid=4093 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:128)
"65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-SegmentedRaftLogWorker"  prio=5 tid=5896 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"2cb742ca-d762-4bc5-b311-f495a87c6b6b-server-thread4" daemon prio=5 tid=5375 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=3898 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 2 on default port 42409" daemon prio=5 tid=3461 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 7 on default port 44467" daemon prio=5 tid=3446 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5236 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=5198 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 8 on default port 33597" daemon prio=5 tid=3427 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1429887399-5665" daemon prio=5 tid=5665 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 10 on default port 38893" daemon prio=5 tid=4358 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5225 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 9 on default port 43297" daemon prio=5 tid=5423 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1699819437-3578" daemon prio=5 tid=3578 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-787852ec-1"  prio=5 tid=5641 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=5599 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#0" daemon prio=5 tid=5802 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 16 on default port 46383" daemon prio=5 tid=5450 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1197505558-4666" daemon prio=5 tid=4666 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=3774 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1699819437-3574" daemon prio=5 tid=3574 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1736155493-5698" daemon prio=5 tid=5698 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 0"  prio=5 tid=5681 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 12 on default port 32931" daemon prio=5 tid=5538 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"735446df-424c-4d38-a683-bd4ef5c8b9e6-impl-thread1"  prio=5 tid=5688 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FixedThreadPoolWithAffinityExecutor-5-0" daemon prio=5 tid=5407 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:266)
        at org.apache.hadoop.hdds.scm.server.ContainerReportQueue.poll(ContainerReportQueue.java:42)
        at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:247)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#2" daemon prio=5 tid=5900 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=3927 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=3662 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1728646390-5636" daemon prio=5 tid=5636 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server Responder" daemon prio=5 tid=3520 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1578)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1561)
"Timer-6"  prio=5 tid=5510 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"pool-2144-thread-1"  prio=5 tid=4756 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2518-thread-1"  prio=5 tid=5879 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=3664 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:42409 - 0 "  prio=5 tid=3803 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 18 on default port 43855" daemon prio=5 tid=4645 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@36284f55" daemon prio=5 tid=5792 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=5799 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1233436001-5480-acceptor-0@79b33a2e-ServerConnector@c4aabfd{HTTP/1.1, (http/1.1)}{0.0.0.0:42957}" daemon prio=3 tid=5480 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249)
        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388)
        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:704)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor43" daemon prio=5 tid=5081 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server listener on 0" daemon prio=5 tid=5501 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1404)
"IPC Server handler 17 on default port 42409" daemon prio=5 tid=3476 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A->10617bfc-881b-4e3d-8722-e7f81aaf7e30-GrpcLogAppender-LogAppenderDaemon" daemon prio=5 tid=5942 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.grpc.server.GrpcLogAppender.mayWait(GrpcLogAppender.java:198)
        at org.apache.ratis.grpc.server.GrpcLogAppender.run(GrpcLogAppender.java:148)
        at org.apache.ratis.server.leader.LogAppenderDaemon.run(LogAppenderDaemon.java:78)
        at org.apache.ratis.server.leader.LogAppenderDaemon$$Lambda$1130/481315216.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=5031 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2446-thread-1"  prio=5 tid=5477 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-NewNodeForNewNodeHandler" daemon prio=5 tid=3907 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ContainerOp-8890f9c9-6b8b-4166-944a-f129ea28c032-6"  prio=5 tid=4949 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1584-thread-1"  prio=5 tid=3571 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@3ef6ea7d" daemon prio=5 tid=5729 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=3725 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 0 on default port 36727" daemon prio=5 tid=3542 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=5677 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#2" daemon prio=5 tid=5889 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=5745 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"JvmPauseMonitor47" daemon prio=5 tid=5231 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:342)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:325)
        at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:310)
        at org.apache.ratis.util.JvmPauseMonitor.detectPause(JvmPauseMonitor.java:119)
        at org.apache.ratis.util.JvmPauseMonitor.run(JvmPauseMonitor.java:108)
        at org.apache.ratis.util.JvmPauseMonitor$$Lambda$758/159714626.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 18 on default port 36727" daemon prio=5 tid=3560 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=4129 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=5049 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 17 on default port 33597" daemon prio=5 tid=3436 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5266 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=4725 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=3782 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 9 on default port 46383" daemon prio=5 tid=5443 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Socket Reader #1 for port 0"  prio=5 tid=5393 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1342)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1321)
"ChunkWriter-0-0" daemon prio=5 tid=3845 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@6bcb53fe" daemon prio=5 tid=3570 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"65d5be4b-faf3-45c4-8c44-989db5872d1e-NettyServerStreamRpc-bossGroup--thread1"  prio=5 tid=5627 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:813)
        at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
        at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-1-0" daemon prio=5 tid=5797 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@77407b26" daemon prio=5 tid=5037 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker.run(DeleteBlocksCommandHandler.java:187)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-0-0" daemon prio=5 tid=5720 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5221 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5@group-F129EA28C032-StateMachineUpdater" daemon prio=5 tid=4018 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 10 on default port 33597" daemon prio=5 tid=3429 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode ReportManager Thread - 4" daemon prio=5 tid=5131 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 10 on default port 35697" daemon prio=5 tid=5464 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"IPC Server handler 11 on default port 44011" daemon prio=5 tid=4319 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"om1@group-C5BA1605619E-LeaderStateImpl" daemon prio=5 tid=3657 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"pool-2310-thread-1"  prio=5 tid=5334 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1699819437-3579" daemon prio=5 tid=3579 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-StateMachineUpdater" daemon prio=5 tid=5278 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.server.impl.StateMachineUpdater.waitForCommit(StateMachineUpdater.java:207)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:176)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 12 on default port 46021" daemon prio=5 tid=4340 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@152d222f" daemon prio=5 tid=5651 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#1" daemon prio=5 tid=5817 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp974801434-3770" daemon prio=5 tid=3770 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"qtp1736155493-5691" daemon prio=5 tid=5691 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"PartialTableCache Cleanup Thread - 0" daemon prio=5 tid=3938 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@f2a8186" daemon prio=5 tid=3890 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=3856 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1429887399-5667" daemon prio=5 tid=5667 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=5683 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"DataNode DiskChecker thread 0" daemon prio=5 tid=4833 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=3948 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp2011203216-3599" daemon prio=5 tid=3599 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 15 on default port 44011" daemon prio=5 tid=4323 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-2-0" daemon prio=5 tid=5261 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-1628-thread-1"  prio=5 tid=3646 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2->fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-GrpcLogAppender-LogAppenderDaemon" daemon prio=5 tid=5615 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.grpc.server.GrpcLogAppender.mayWait(GrpcLogAppender.java:198)
        at org.apache.ratis.grpc.server.GrpcLogAppender.run(GrpcLogAppender.java:148)
        at org.apache.ratis.server.leader.LogAppenderDaemon.run(LogAppenderDaemon.java:78)
        at org.apache.ratis.server.leader.LogAppenderDaemon$$Lambda$1130/481315216.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-19aeec42-1"  prio=5 tid=5114 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=5706 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"pool-2606-thread-1"  prio=5 tid=5913 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EndpointStateMachine task thread for /0.0.0.0:35697 - 0 "  prio=5 tid=5756 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp592685883-5779" daemon prio=5 tid=5779 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 11 on default port 43297" daemon prio=5 tid=5425 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-362684FCDA51-LeaderStateImpl" daemon prio=5 tid=4107 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"EventQueue-DatanodeCommandQueueUpdatedForDatanodeCommandCountUpdatedHandler" daemon prio=5 tid=5247 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Daemon Thread" daemon prio=5 tid=5565 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:336)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$886/373900566.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 18 on default port 46383" daemon prio=5 tid=5452 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"qtp1233436001-5485" daemon prio=5 tid=5485 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 0" daemon prio=5 tid=3630 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Datanode ReportManager Thread - 3" daemon prio=5 tid=5130 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule" daemon prio=5 tid=3908 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"BlockDeletingService#2" daemon prio=5 tid=5863 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-362684FCDA51-SegmentedRaftLogWorker"  prio=5 tid=4038 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@33e42c9" daemon prio=5 tid=5186 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=3905 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Command processor thread" daemon prio=5 tid=5650 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:649)
        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine$$Lambda$900/1507182112.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A->b85c0d2e-fc3b-4312-9b63-e609b6e2c228-GrpcLogAppender-LogAppenderDaemon" daemon prio=5 tid=5943 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
        at org.apache.ratis.util.AwaitForSignal.await(AwaitForSignal.java:62)
        at org.apache.ratis.grpc.server.GrpcLogAppender.mayWait(GrpcLogAppender.java:198)
        at org.apache.ratis.grpc.server.GrpcLogAppender.run(GrpcLogAppender.java:148)
        at org.apache.ratis.server.leader.LogAppenderDaemon.run(LogAppenderDaemon.java:78)
        at org.apache.ratis.server.leader.LogAppenderDaemon$$Lambda$1130/481315216.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderStateImpl" daemon prio=5 tid=5672 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventQueue.poll(LeaderStateImpl.java:159)
        at org.apache.ratis.server.impl.LeaderStateImpl$EventProcessor.run(LeaderStateImpl.java:630)
"ff409154-2e62-420c-a7a7-066f3a70d145-impl-thread1"  prio=5 tid=5659 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp1728646390-5635" daemon prio=5 tid=5635 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#0" daemon prio=5 tid=5731 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"ChunkWriter-3-0" daemon prio=5 tid=5006 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Session-HouseKeeper-15075741-1"  prio=5 tid=4380 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"qtp96343072-3686" daemon prio=5 tid=3686 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183)
        at org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606)
        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
        at org.eclipse.jetty.io.ManagedSelector$$Lambda$479/1814642526.run(Unknown Source)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
        at java.lang.Thread.run(Thread.java:750)
"Datanode State Machine Task Thread - 1"  prio=5 tid=5836 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"Periodic HDDS volume checker" daemon prio=5 tid=4832 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 4 on default port 43855" daemon prio=5 tid=4631 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"Datanode State Machine Task Thread - 1"  prio=5 tid=3843 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"om1@group-C5BA1605619E-SegmentedRaftLogWorker"  prio=5 tid=5506 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at org.apache.ratis.util.DataBlockingQueue.poll(DataBlockingQueue.java:148)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:312)
        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$717/1098632404.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:750)
"StaleRecoveringContainerScrubbingService#1" daemon prio=5 tid=3962 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"FullTableCache Cleanup Thread - 0" daemon prio=5 tid=4228 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 1 on default port 43855" daemon prio=5 tid=4628 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)
"ChunkWriter-3-0" daemon prio=5 tid=5811 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
        at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
"IPC Server handler 0 on default port 44011" daemon prio=5 tid=4308 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2992)


	at org.apache.ozone.test.GenericTestUtils.waitFor(GenericTestUtils.java:231)
	at org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance.waitForDnToReachPersistedOpState(TestDecommissionAndMaintenance.java:718)
	at org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance.testSCMHandlesRestartForMaintenanceNode(TestDecommissionAndMaintenance.java:561)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
]]></error>
    <system-out><![CDATA[2023-04-27 06:30:14,430 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:14,431 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:14,431 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:forceCloseContainer(1080)) - Force closing container #3 with BCSID 42, which is in QUASI_CLOSED state.
2023-04-27 06:30:14,431 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33).
2023-04-27 06:30:14,431 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33).
2023-04-27 06:30:14,431 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:14,431 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:14,431 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 11 containers.
2023-04-27 06:30:14,458 [ReplicationMonitor] WARN  net.NetworkTopologyImpl (NetworkTopologyImpl.java:chooseNodeInternal(653)) - No available node in (scope="/" excludedScope="null" excludedNodes="[f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33)]"  ancestorGen="1").
2023-04-27 06:30:14,458 [ReplicationMonitor] WARN  algorithms.SCMContainerPlacementRackAware (SCMContainerPlacementRackAware.java:chooseNode(282)) - Failed to find the datanode for container. excludedNodes:[f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33)], affinityNode:
2023-04-27 06:30:14,458 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #1 is under replicated. Expected replica count is 3, but found 1.
2023-04-27 06:30:14,458 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=1, replicaIndex=0, sourceNodes=[45e99655-b061-40f7-b011-1b1883fa2319(fv-az260-775/10.1.0.33)], priority=NORMAL to f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33)
2023-04-27 06:30:14,458 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=1, replicaIndex=0, sourceNodes=[45e99655-b061-40f7-b011-1b1883fa2319(fv-az260-775/10.1.0.33)], priority=NORMAL to 45e99655-b061-40f7-b011-1b1883fa2319(fv-az260-775/10.1.0.33)
2023-04-27 06:30:14,458 [ReplicationMonitor] WARN  net.NetworkTopologyImpl (NetworkTopologyImpl.java:chooseNodeInternal(653)) - No available node in (scope="/" excludedScope="null" excludedNodes="[f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33)]"  ancestorGen="1").
2023-04-27 06:30:14,458 [ReplicationMonitor] WARN  algorithms.SCMContainerPlacementRackAware (SCMContainerPlacementRackAware.java:chooseNode(282)) - Failed to find the datanode for container. excludedNodes:[f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33)], affinityNode:
2023-04-27 06:30:14,459 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #2 is under replicated. Expected replica count is 3, but found 1.
2023-04-27 06:30:14,459 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=2, replicaIndex=0, sourceNodes=[45e99655-b061-40f7-b011-1b1883fa2319(fv-az260-775/10.1.0.33)], priority=NORMAL to f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33)
2023-04-27 06:30:14,459 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=2, replicaIndex=0, sourceNodes=[45e99655-b061-40f7-b011-1b1883fa2319(fv-az260-775/10.1.0.33)], priority=NORMAL to 45e99655-b061-40f7-b011-1b1883fa2319(fv-az260-775/10.1.0.33)
2023-04-27 06:30:14,459 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33).
2023-04-27 06:30:14,459 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33).
2023-04-27 06:30:14,459 [ReplicationMonitor] WARN  net.NetworkTopologyImpl (NetworkTopologyImpl.java:chooseNodeInternal(653)) - No available node in (scope="/" excludedScope="null" excludedNodes="[f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33)]"  ancestorGen="1").
2023-04-27 06:30:14,459 [ReplicationMonitor] WARN  algorithms.SCMContainerPlacementRackAware (SCMContainerPlacementRackAware.java:chooseNode(282)) - Failed to find the datanode for container. excludedNodes:[f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33)], affinityNode:
2023-04-27 06:30:14,459 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #4 is under replicated. Expected replica count is 3, but found 1.
2023-04-27 06:30:14,459 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=4, replicaIndex=0, sourceNodes=[45e99655-b061-40f7-b011-1b1883fa2319(fv-az260-775/10.1.0.33)], priority=NORMAL to f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33)
2023-04-27 06:30:14,459 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=4, replicaIndex=0, sourceNodes=[45e99655-b061-40f7-b011-1b1883fa2319(fv-az260-775/10.1.0.33)], priority=NORMAL to 45e99655-b061-40f7-b011-1b1883fa2319(fv-az260-775/10.1.0.33)
2023-04-27 06:30:14,460 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33).
2023-04-27 06:30:14,460 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33).
2023-04-27 06:30:14,460 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 1ef0e7b0-3474-409b-a89e-735ba5bcb335(fv-az260-775/10.1.0.33).
2023-04-27 06:30:14,460 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode f83db7c9-4a26-48b7-8329-a6a26bc6bc0f(fv-az260-775/10.1.0.33).
2023-04-27 06:30:14,460 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 3 milliseconds for processing 6 containers.
2023-04-27 06:30:14,496 [main] INFO  rpc.RpcClient (RpcClient.java:createVolume(476)) - Creating Volume: vol1, with user69245 as owner and space quota set to -1 bytes, counts quota set to -1
2023-04-27 06:30:14,529 [OM StateMachine ApplyTransaction Thread - 0] INFO  volume.OMVolumeCreateRequest (OMVolumeCreateRequest.java:validateAndUpdateCache(195)) - created volume:vol1 for user:user69245
2023-04-27 06:30:14,554 [main] INFO  rpc.RpcClient (RpcClient.java:createBucket(697)) - Creating Bucket: vol1/bucket1, with bucket layout LEGACY, runner as owner, Versioning false, Storage Type set to DISK and Encryption set to false, Replication Type set to server-side default replication type, Namespace Quota set to -1, Space Quota set to -1 
2023-04-27 06:30:14,573 [OM StateMachine ApplyTransaction Thread - 0] INFO  bucket.OMBucketCreateRequest (OMBucketCreateRequest.java:validateAndUpdateCache(270)) - created bucket: bucket1 of layout LEGACY in volume: vol1
2023-04-27 06:30:14,599 [IPC Server handler 0 on default port 44467] INFO  ha.SequenceIdGenerator (SequenceIdGenerator.java:getNextId(128)) - Allocate a batch for containerId, change lastId from 0 to 1000.
2023-04-27 06:30:14,599 [IPC Server handler 0 on default port 44467] WARN  ha.SequenceIdGenerator (SequenceIdGenerator.java:allocateBatch(237)) - Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
2023-04-27 06:30:14,599 [IPC Server handler 0 on default port 44467] INFO  ha.SequenceIdGenerator (SequenceIdGenerator.java:getNextId(128)) - Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
2023-04-27 06:30:14,798 [ForkJoinPool.commonPool-worker-0] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-0/data-0/containers/hdds/d25cf8bb-08a8-4312-8236-5e4a36901885/DS-4e7d6fec-c221-4672-b3bb-b50d9a2cdd55/container.db for volume DS-4e7d6fec-c221-4672-b3bb-b50d9a2cdd55
2023-04-27 06:30:14,798 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:14,802 [Mini-Cluster-Provider-Reap] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-d25cf8bb-08a8-4312-8236-5e4a36901885/datanode-5/data-0/containers/hdds/d25cf8bb-08a8-4312-8236-5e4a36901885/DS-a3a27c7e-26c4-4a0d-b42e-11da54c8c58d/container.db for volume DS-a3a27c7e-26c4-4a0d-b42e-11da54c8c58d
2023-04-27 06:30:14,802 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:14,803 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:14,804 [Mini-Cluster-Provider-Reap] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
2023-04-27 06:30:14,819 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:14,821 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
2023-04-27 06:30:14,828 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@54e0c162{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:14,831 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@1f43cf14{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:14,831 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:14,835 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@2befd79f{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:14,837 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@67e1b5cc{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:14,837 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@288cd970{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:14,839 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@8db1b02{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:14,839 [ForkJoinPool.commonPool-worker-0] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:14,839 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@2149ef0a{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:14,839 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@436746da{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:14,843 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopSCM(552)) - Stopping the StorageContainerManager
2023-04-27 06:30:14,843 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1578)) - Container Balancer is not running.
2023-04-27 06:30:14,843 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stopReplicationManager(1709)) - Stopping Replication Manager Service.
2023-04-27 06:30:14,843 [Mini-Cluster-Provider-Reap] INFO  replication.ReplicationManager (ReplicationManager.java:stop(321)) - Stopping Replication Monitor Thread.
2023-04-27 06:30:14,843 [Under Replicated Processor] WARN  replication.UnhealthyReplicationProcessor (UnhealthyReplicationProcessor.java:run(146)) - Under Replicated Processor interrupted. Exiting...
2023-04-27 06:30:14,844 [Over Replicated Processor] WARN  replication.UnhealthyReplicationProcessor (UnhealthyReplicationProcessor.java:run(146)) - Over Replicated Processor interrupted. Exiting...
2023-04-27 06:30:14,848 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1587)) - Stopping the Datanode Admin Monitor.
2023-04-27 06:30:14,848 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:run(975)) - Replication Monitor Thread is stopped
2023-04-27 06:30:14,848 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1594)) - Stopping datanode service RPC server
2023-04-27 06:30:14,848 [Mini-Cluster-Provider-Reap] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:stop(424)) - Stopping the RPC server for DataNodes
2023-04-27 06:30:14,851 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3501)) - Stopping server on 37211
2023-04-27 06:30:14,855 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1563)) - Stopping IPC Server Responder
2023-04-27 06:30:14,856 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1430)) - Stopping IPC Server listener on 0
2023-04-27 06:30:14,867 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@408eeb30{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/meta/webserver/jetty-0_0_0_0-41531-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-1520796640436220703/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:14,871 [Listener at 127.0.0.1/43855] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@5ae3e6e8{HTTP/1.1, (http/1.1)}{0.0.0.0:41531}
2023-04-27 06:30:14,872 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(415)) - Started @141319ms
2023-04-27 06:30:14,872 [Listener at 127.0.0.1/43855] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:14,872 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:41531
2023-04-27 06:30:14,873 [Listener at 127.0.0.1/43855] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:14,876 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:14,884 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6489d4ef] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:14,903 [Listener at 127.0.0.1/43855] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:14,907 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/meta/datanode.id
2023-04-27 06:30:14,917 [SCM Heartbeat Processing Thread - 0] WARN  node.NodeStateManager (NodeStateManager.java:scheduleNextHealthCheck(870)) - Current Thread is interrupted, shutting down HB processing thread for Node Manager.
2023-04-27 06:30:14,917 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1602)) - Stopping block service RPC server
2023-04-27 06:30:14,917 [Mini-Cluster-Provider-Reap] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:stop(161)) - Stopping the RPC server for Block Protocol
2023-04-27 06:30:14,920 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3501)) - Stopping server on 38541
2023-04-27 06:30:14,927 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1563)) - Stopping IPC Server Responder
2023-04-27 06:30:14,927 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1430)) - Stopping IPC Server listener on 0
2023-04-27 06:30:14,928 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1609)) - Stopping the StorageContainerLocationProtocol RPC server
2023-04-27 06:30:14,928 [Mini-Cluster-Provider-Reap] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:stop(206)) - Stopping the RPC server for Client Protocol
2023-04-27 06:30:14,931 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3501)) - Stopping server on 44167
2023-04-27 06:30:14,944 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1430)) - Stopping IPC Server listener on 0
2023-04-27 06:30:14,944 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1563)) - Stopping IPC Server Responder
2023-04-27 06:30:14,954 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1616)) - Stopping Storage Container Manager HTTP server.
2023-04-27 06:30:14,963 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@13d137fb{scm,/,null,STOPPED}{file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/scm}
2023-04-27 06:30:14,966 [Listener at 127.0.0.1/43855] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:14,978 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@2a794b6a{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:14,978 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:14,978 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@1d7a405d{static,/static,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/static,STOPPED}
2023-04-27 06:30:14,978 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@29f43a84{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:14,982 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1624)) - Stopping SCM LayoutVersionManager Service.
2023-04-27 06:30:14,982 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1632)) - Stopping Block Manager Service.
2023-04-27 06:30:14,982 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SCMBlockDeletingService
2023-04-27 06:30:14,983 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SCMBlockDeletingService
2023-04-27 06:30:14,983 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1654)) - Stopping SCM Event Queue.
2023-04-27 06:30:14,987 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1665)) - Stopping SCM HA services.
2023-04-27 06:30:14,987 [Mini-Cluster-Provider-Reap] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:stop(149)) - Stopping RatisPipelineUtilsThread.
2023-04-27 06:30:14,987 [RatisPipelineUtilsThread - 0] WARN  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:run(180)) - RatisPipelineUtilsThread is interrupted.
2023-04-27 06:30:15,001 [Mini-Cluster-Provider-Reap] INFO  BackgroundPipelineScrubber (BackgroundSCMService.java:stop(131)) - Stopping BackgroundPipelineScrubber Service.
2023-04-27 06:30:15,006 [Mini-Cluster-Provider-Reap] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping HddsDatanode metrics system...
2023-04-27 06:30:15,001 [BackgroundPipelineScrubberThread] WARN  BackgroundPipelineScrubber (BackgroundSCMService.java:run(115)) - BackgroundPipelineScrubber is interrupted, exit
2023-04-27 06:30:15,027 [prometheus] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(141)) - prometheus thread interrupted.
2023-04-27 06:30:15,033 [Mini-Cluster-Provider-Reap] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - HddsDatanode metrics system stopped.
2023-04-27 06:30:15,034 [Mini-Cluster-Provider-Reap] WARN  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:stop(145)) - RatisPipelineUtilsThread is not running, just ignore.
2023-04-27 06:30:15,034 [Mini-Cluster-Provider-Reap] INFO  BackgroundPipelineScrubber (BackgroundSCMService.java:stop(126)) - BackgroundPipelineScrubber Service is not running, skip stop.
2023-04-27 06:30:15,034 [Mini-Cluster-Provider-Reap] INFO  ExpiredContainerReplicaOpScrubber (BackgroundSCMService.java:stop(131)) - Stopping ExpiredContainerReplicaOpScrubber Service.
2023-04-27 06:30:15,034 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SCMBlockDeletingService
2023-04-27 06:30:15,034 [Mini-Cluster-Provider-Reap] INFO  replication.ReplicationManager (ReplicationManager.java:stop(331)) - Replication Monitor Thread is not running.
2023-04-27 06:30:15,034 [Mini-Cluster-Provider-Reap] WARN  balancer.ContainerBalancer (ContainerBalancer.java:stop(324)) - Cannot stop Container Balancer because it's not running or stopping
2023-04-27 06:30:15,034 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1700)) - Stopping SCM MetadataStore.
2023-04-27 06:30:15,035 [ExpiredContainerReplicaOpScrubberThread] WARN  ExpiredContainerReplicaOpScrubber (BackgroundSCMService.java:run(115)) - ExpiredContainerReplicaOpScrubber is interrupted, exit
2023-04-27 06:30:15,163 [Listener at 127.0.0.1/43855] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 184 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:15,164 [Listener at 127.0.0.1/43855] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:15,165 [Listener at 127.0.0.1/43855] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:15,166 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:15,166 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data-0/containers/hdds
2023-04-27 06:30:15,166 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data-0/containers/hdds
2023-04-27 06:30:15,242 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis to VolumeSet
2023-04-27 06:30:15,242 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis
2023-04-27 06:30:15,245 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis
2023-04-27 06:30:15,254 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-3] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.persistPutBlock(BlockManagerImpl.java:122)
	at org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.putBlock(BlockManagerImpl.java:103)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handlePutBlock(KeyValueHandler.java:556)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:261)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:232)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:320)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.lambda$dispatch$0(HddsDispatcher.java:171)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:170)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:439)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:449)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$submitTask$8(ContainerStateMachine.java:834)
	at org.apache.ratis.util.TaskQueue.lambda$submit$0(TaskQueue.java:121)
	at org.apache.ratis.util.LogUtils.runAndLog(LogUtils.java:38)
	at org.apache.ratis.util.LogUtils$1.run(LogUtils.java:79)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 26 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 27 more
2023-04-27 06:30:15,255 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-3] INFO  keyvalue.KeyValueHandler (ContainerUtils.java:logAndReturnError(91)) - Operation: PutBlock , Trace ID:  , Message: Error opening DB. Container:3 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db , Result: UNABLE_TO_READ_METADATA_DB , StorageContainerException Occurred.
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:3 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.persistPutBlock(BlockManagerImpl.java:122)
	at org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.putBlock(BlockManagerImpl.java:103)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handlePutBlock(KeyValueHandler.java:556)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:261)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:232)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:320)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.lambda$dispatch$0(HddsDispatcher.java:171)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:170)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:439)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:449)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$submitTask$8(ContainerStateMachine.java:834)
	at org.apache.ratis.util.TaskQueue.lambda$submit$0(TaskQueue.java:121)
	at org.apache.ratis.util.LogUtils.runAndLog(LogUtils.java:38)
	at org.apache.ratis.util.LogUtils$1.run(LogUtils.java:79)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:15,276 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-3] WARN  keyvalue.KeyValueContainer (KeyValueContainer.java:markContainerUnhealthy(348)) - Moving container /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/current/containerDir0/3 to state UNHEALTHY from state:OPEN
2023-04-27 06:30:15,276 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-3] INFO  impl.HddsDispatcher (HddsDispatcher.java:dispatchRequest(361)) - Marked Container UNHEALTHY, ContainerID: 3
06:30:15.283 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-3] ERROR DNAudit - user=null | ip=null | op=PUT_BLOCK {blockData=[blockId=conID: 3 locID: 111677748019200003 bcsId: 0, size=19]} | ret=FAILURE
java.lang.Exception: Error opening DB. Container:3 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:389) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.lambda$dispatch$0(HddsDispatcher.java:171) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87) ~[hdds-server-framework-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:170) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:439) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:449) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$submitTask$8(ContainerStateMachine.java:834) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.ratis.util.TaskQueue.lambda$submit$0(TaskQueue.java:121) ~[ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:2.4.2-8b8bdda-SNAPSHOT]
	at org.apache.ratis.util.LogUtils.runAndLog(LogUtils.java:38) [ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:2.4.2-8b8bdda-SNAPSHOT]
	at org.apache.ratis.util.LogUtils$1.run(LogUtils.java:79) [ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:2.4.2-8b8bdda-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_362]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_362]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_362]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_362]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_362]
2023-04-27 06:30:15,284 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-3] ERROR ratis.ContainerStateMachine (ContainerStateMachine.java:lambda$applyTransaction$10(927)) - gid group-868FC3C697EE : ApplyTransaction failed. cmd PutBlock logIndex 6 msg : Error opening DB. Container:3 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db Container Result: UNABLE_TO_READ_METADATA_DB
2023-04-27 06:30:15,286 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-3] ERROR ratis.XceiverServerRatis (XceiverServerRatis.java:triggerPipelineClose(719)) - pipeline Action CLOSE on pipeline PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee.Reason : Ratis Transaction failure in datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d with role FOLLOWER .Triggering pipeline close action.
2023-04-27 06:30:15,286 [Thread-2696] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data-0/containers/hdds
2023-04-27 06:30:15,286 [Listener at 127.0.0.1/43855] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:15,287 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:shutdown(456)) - Shutting down the Mini Ozone Cluster
2023-04-27 06:30:15,292 [FixedThreadPoolWithAffinityExecutor-0-0] WARN  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(259)) - Container #3 is in OPEN state, but the datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33) reports an UNHEALTHY replica.
2023-04-27 06:30:15,294 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stop(474)) - Stopping the Mini Ozone Cluster
2023-04-27 06:30:15,294 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopOM(560)) - Stopping the OzoneManager
2023-04-27 06:30:15,294 [Mini-Cluster-Provider-Reap] INFO  om.OzoneManager (OzoneManager.java:stop(2165)) - om1[localhost:0]: Stopping Ozone Manager
2023-04-27 06:30:15,299 [Listener at 127.0.0.1/43855] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:15,300 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:15,300 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:15,300 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:15,300 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:15,300 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:15,300 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:15,300 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:15,300 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:15,300 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:15,300 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:15,301 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:15,301 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:15,301 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:15,302 [Listener at 127.0.0.1/43855] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:15,303 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:15,303 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:15,303 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:15,303 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:15,306 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-5] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.persistPutBlock(BlockManagerImpl.java:122)
	at org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.putBlock(BlockManagerImpl.java:103)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handlePutBlock(KeyValueHandler.java:556)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:261)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:232)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:320)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.lambda$dispatch$0(HddsDispatcher.java:171)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:170)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:439)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:449)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$submitTask$8(ContainerStateMachine.java:834)
	at org.apache.ratis.util.TaskQueue.lambda$submit$0(TaskQueue.java:121)
	at org.apache.ratis.util.LogUtils.runAndLog(LogUtils.java:38)
	at org.apache.ratis.util.LogUtils$1.run(LogUtils.java:79)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 26 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 27 more
2023-04-27 06:30:15,306 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-5] INFO  keyvalue.KeyValueHandler (ContainerUtils.java:logAndReturnError(91)) - Operation: PutBlock , Trace ID:  , Message: Error opening DB. Container:4 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db , Result: UNABLE_TO_READ_METADATA_DB , StorageContainerException Occurred.
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:4 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.persistPutBlock(BlockManagerImpl.java:122)
	at org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.putBlock(BlockManagerImpl.java:103)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handlePutBlock(KeyValueHandler.java:556)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:261)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:232)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:320)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.lambda$dispatch$0(HddsDispatcher.java:171)
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:170)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:439)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:449)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$submitTask$8(ContainerStateMachine.java:834)
	at org.apache.ratis.util.TaskQueue.lambda$submit$0(TaskQueue.java:121)
	at org.apache.ratis.util.LogUtils.runAndLog(LogUtils.java:38)
	at org.apache.ratis.util.LogUtils$1.run(LogUtils.java:79)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:15,323 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3501)) - Stopping server on 36903
2023-04-27 06:30:15,326 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:15,326 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:15,327 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #3, current state: OPEN
2023-04-27 06:30:15,327 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:15,327 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:15,327 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:15,327 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:15,328 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:15,328 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:15,328 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:15,328 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:15,329 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis] (custom)
2023-04-27 06:30:15,329 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xcb77e58c] REGISTERED
2023-04-27 06:30:15,329 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xcb77e58c] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:15,329 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xcb77e58c, L:/0:0:0:0:0:0:0:0:39055] ACTIVE
2023-04-27 06:30:15,330 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-5] WARN  keyvalue.KeyValueContainer (KeyValueContainer.java:markContainerUnhealthy(348)) - Moving container /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/current/containerDir0/4 to state UNHEALTHY from state:OPEN
2023-04-27 06:30:15,335 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.PipelineActionHandler (PipelineActionHandler.java:processPipelineAction(83)) - Received pipeline action CLOSE for PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee from datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d. Reason : Ratis Transaction failure in datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d with role FOLLOWER .Triggering pipeline close action.
2023-04-27 06:30:15,335 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #1 closed for pipeline=PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee
2023-04-27 06:30:15,335 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #4 closed for pipeline=PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee
2023-04-27 06:30:15,335 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 1fe65a88-8cef-41d6-b4e0-868fc3c697ee, Nodes: dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33)70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33)ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:ef6aab41-a1eb-41b3-a4a5-2458878a1611, CreationTimestamp2023-04-27T06:29:36.019Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:15,336 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee close command to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d
2023-04-27 06:30:15,336 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee close command to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c
2023-04-27 06:30:15,336 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee close command to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611
2023-04-27 06:30:15,339 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #1, current state: CLOSING
2023-04-27 06:30:15,339 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #4, current state: CLOSING
2023-04-27 06:30:15,339 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-5] INFO  impl.HddsDispatcher (HddsDispatcher.java:dispatchRequest(361)) - Marked Container UNHEALTHY, ContainerID: 4
2023-04-27 06:30:15,356 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1430)) - Stopping IPC Server listener on 0
2023-04-27 06:30:15,356 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 1fe65a88-8cef-41d6-b4e0-868fc3c697ee, Nodes: dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33)70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33)ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:ef6aab41-a1eb-41b3-a4a5-2458878a1611, CreationTimestamp2023-04-27T06:29:36.019Z[Etc/UTC]] removed.
2023-04-27 06:30:15,356 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1563)) - Stopping IPC Server Responder
06:30:15.339 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-5] ERROR DNAudit - user=null | ip=null | op=PUT_BLOCK {blockData=[blockId=conID: 4 locID: 111677748019200004 bcsId: 0, size=19]} | ret=FAILURE
java.lang.Exception: Error opening DB. Container:4 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:389) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.lambda$dispatch$0(HddsDispatcher.java:171) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87) ~[hdds-server-framework-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:170) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:439) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:449) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$submitTask$8(ContainerStateMachine.java:834) ~[hdds-container-service-1.4.0-SNAPSHOT.jar:?]
	at org.apache.ratis.util.TaskQueue.lambda$submit$0(TaskQueue.java:121) ~[ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:2.4.2-8b8bdda-SNAPSHOT]
	at org.apache.ratis.util.LogUtils.runAndLog(LogUtils.java:38) [ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:2.4.2-8b8bdda-SNAPSHOT]
	at org.apache.ratis.util.LogUtils$1.run(LogUtils.java:79) [ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:2.4.2-8b8bdda-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_362]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_362]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_362]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_362]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_362]
2023-04-27 06:30:15,358 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-5] ERROR ratis.ContainerStateMachine (ContainerStateMachine.java:lambda$applyTransaction$10(927)) - gid group-868FC3C697EE : ApplyTransaction failed. cmd PutBlock logIndex 10 msg : Error opening DB. Container:4 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db Container Result: UNABLE_TO_READ_METADATA_DB
2023-04-27 06:30:15,358 [ContainerOp-1fe65a88-8cef-41d6-b4e0-868fc3c697ee-5] ERROR ratis.XceiverServerRatis (XceiverServerRatis.java:triggerPipelineClose(719)) - pipeline Action CLOSE on pipeline PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee.Reason : Ratis Transaction failure in datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d with role FOLLOWER .Triggering pipeline close action.
2023-04-27 06:30:15,363 [EventQueue-PipelineActionsForPipelineActionHandler] INFO  pipeline.PipelineActionHandler (PipelineActionHandler.java:processPipelineAction(83)) - Received pipeline action CLOSE for PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee from datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d. Reason : Ratis Transaction failure in datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d with role FOLLOWER .Triggering pipeline close action.
2023-04-27 06:30:15,363 [EventQueue-PipelineActionsForPipelineActionHandler] WARN  pipeline.PipelineActionHandler (PipelineActionHandler.java:processPipelineAction(94)) - Pipeline action CLOSE received for unknown pipeline PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee, firing close pipeline event.
2023-04-27 06:30:15,363 [Listener at 127.0.0.1/43855] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:15,366 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #4, current state: CLOSING
2023-04-27 06:30:15,368 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:15,368 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:15,369 [Listener at 127.0.0.1/43855] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:15,370 [Listener at 127.0.0.1/43855] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:15,371 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:15,371 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:15,371 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:15,374 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:15,385 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - om1: close
2023-04-27 06:30:15,393 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/meta/webserver
2023-04-27 06:30:15,393 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 46545
2023-04-27 06:30:15,393 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:15,405 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:15,405 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:15,405 [Listener at 127.0.0.1/43855] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 600000ms
2023-04-27 06:30:15,407 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - om1: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:15,408 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - om1: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:15,408 [om1-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - om1@group-C5BA1605619E: shutdown
2023-04-27 06:30:15,408 [om1-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
2023-04-27 06:30:15,408 [om1-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - om1: shutdown om1@group-C5BA1605619E-LeaderStateImpl
2023-04-27 06:30:15,408 [om1-impl-thread2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - om1@group-C5BA1605619E-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:15,412 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:15,412 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@21d81c16{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:15,412 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@6ee46ad6{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:15,425 [om1-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - om1@group-C5BA1605619E-StateMachineUpdater: set stopIndex = 166
2023-04-27 06:30:15,426 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:takeSnapshot(479)) - Current Snapshot Index (t:1, i:166)
2023-04-27 06:30:15,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33).
2023-04-27 06:30:15,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33).
2023-04-27 06:30:15,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:15,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33).
2023-04-27 06:30:15,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33).
2023-04-27 06:30:15,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33).
2023-04-27 06:30:15,430 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33).
2023-04-27 06:30:15,432 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:15,432 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:forceCloseContainer(1080)) - Force closing container #3 with BCSID 42, which is in QUASI_CLOSED state.
2023-04-27 06:30:15,432 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33).
2023-04-27 06:30:15,434 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 4 milliseconds for processing 5 containers.
2023-04-27 06:30:15,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33).
2023-04-27 06:30:15,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:15,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:15,435 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 4 milliseconds for processing 11 containers.
2023-04-27 06:30:15,442 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 42.
2023-04-27 06:30:15,442 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 42.
2023-04-27 06:30:15,443 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:remove(107)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d: remove  FOLLOWER dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE:t1, leader=ef6aab41-a1eb-41b3-a4a5-2458878a1611, voted=ef6aab41-a1eb-41b3-a4a5-2458878a1611, raftlog=Memoized:dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-SegmentedRaftLog:OPENED:c10, conf=0: peers:[70ce52ad-a2f4-4f2c-b96d-cd309621d39c|rpc:10.1.0.33:40471|dataStream:10.1.0.33:37475|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
2023-04-27 06:30:15,443 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE: shutdown
2023-04-27 06:30:15,443 [Command processor thread] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-868FC3C697EE,id=dbe8ec56-fdf9-4402-aab9-993d7a20391d
2023-04-27 06:30:15,443 [Command processor thread] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d: shutdown dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-FollowerState
2023-04-27 06:30:15,443 [Command processor thread] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-StateMachineUpdater: set stopIndex = 10
2023-04-27 06:30:15,443 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-FollowerState was interrupted
2023-04-27 06:30:15,443 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-StateMachineUpdater] ERROR ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(324)) - Failed to take snapshot  for group-868FC3C697EE as the stateMachine is unhealthy. The last applied index is at (t:1, i:5)
2023-04-27 06:30:15,444 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-StateMachineUpdater] ERROR impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(282)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-StateMachineUpdater: Failed to take snapshot
org.apache.ratis.protocol.exceptions.StateMachineException: Failed to take snapshot  for group-868FC3C697EE as the stateMachine is unhealthy. The last applied index is at (t:1, i:5)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:323)
	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:270)
	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:262)
	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:183)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:15,448 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-StateMachineUpdater] ERROR ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(324)) - Failed to take snapshot  for group-868FC3C697EE as the stateMachine is unhealthy. The last applied index is at (t:1, i:5)
2023-04-27 06:30:15,448 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-StateMachineUpdater] ERROR impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(282)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-StateMachineUpdater: Failed to take snapshot
org.apache.ratis.protocol.exceptions.StateMachineException: Failed to take snapshot  for group-868FC3C697EE as the stateMachine is unhealthy. The last applied index is at (t:1, i:5)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:323)
	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:270)
	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:262)
	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:186)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:15,448 [Command processor thread] INFO  server.RaftServer$Division (ServerState.java:close(466)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE: closes. applyIndex: 5
2023-04-27 06:30:15,449 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:15,449 [Command processor thread] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE-SegmentedRaftLogWorker close()
2023-04-27 06:30:15,456 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 3 is closed with bcsId 42.
2023-04-27 06:30:15,458 [FixedThreadPoolWithAffinityExecutor-8-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(318)) - Moving container #3 to CLOSED state, datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) reported CLOSED replica.
2023-04-27 06:30:15,493 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.notifyGroupRemove(ContainerStateMachine.java:1052)
	at org.apache.ratis.server.impl.RaftServerImpl.groupRemove(RaftServerImpl.java:423)
	at org.apache.ratis.server.impl.RaftServerProxy.lambda$groupRemoveAsync$12(RaftServerProxy.java:530)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:529)
	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:479)
	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:459)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:822)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 25 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 26 more
2023-04-27 06:30:15,496 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:groupRemove(428)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-868FC3C697EE: Succeed to remove RaftStorageDirectory Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data/ratis/1fe65a88-8cef-41d6-b4e0-868fc3c697ee
2023-04-27 06:30:15,496 [Command processor thread] INFO  commandhandler.ClosePipelineCommandHandler (ClosePipelineCommandHandler.java:handle(78)) - Close Pipeline PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee command on datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d.
2023-04-27 06:30:15,500 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:15,500 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:15,522 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - om1@group-C5BA1605619E-StateMachineUpdater: Took a snapshot at index 166
2023-04-27 06:30:15,522 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - om1@group-C5BA1605619E-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 166
2023-04-27 06:30:15,522 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:close(533)) - StateMachine has shutdown. Shutdown OzoneManager if not already shutdown.
2023-04-27 06:30:15,522 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:stopDaemon(540)) - Stopping OMDoubleBuffer flush thread
2023-04-27 06:30:15,523 [OMDoubleBufferFlushThread] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:canFlush(625)) - OMDoubleBuffer flush thread OMDoubleBufferFlushThread is interrupted and will exit.
2023-04-27 06:30:15,525 [om1-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - om1@group-C5BA1605619E: closes. applyIndex: 166
2023-04-27 06:30:15,525 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:15,527 [om1-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker close()
2023-04-27 06:30:15,538 [JvmPauseMonitor25] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-om1: Stopped
2023-04-27 06:30:15,539 [Mini-Cluster-Provider-Reap] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:close(533)) - StateMachine has shutdown. Shutdown OzoneManager if not already shutdown.
2023-04-27 06:30:15,539 [Mini-Cluster-Provider-Reap] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:stopDaemon(549)) - OMDoubleBuffer flush thread is not running.
2023-04-27 06:30:15,539 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service KeyDeletingService
2023-04-27 06:30:15,544 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service KeyDeletingService
2023-04-27 06:30:15,544 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service OpenKeyCleanupService
2023-04-27 06:30:15,545 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SstFilteringService
2023-04-27 06:30:15,546 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SnapshotDeletingService
2023-04-27 06:30:15,551 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@47e07d6d{ozoneManager,/,null,STOPPED}{file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/ozoneManager}
2023-04-27 06:30:15,552 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@1b49a46a{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:15,552 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:15,552 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@3cb46ede{static,/static,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/static,STOPPED}
2023-04-27 06:30:15,552 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@4e519460{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:15,562 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SnapshotDiffCleanupService
2023-04-27 06:30:15,567 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopDatanodes(537)) - Stopping the HddsDatanodes
2023-04-27 06:30:15,569 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:15,573 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083: close
2023-04-27 06:30:15,580 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:15,580 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189 Close channels
2023-04-27 06:30:15,580 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B: shutdown
2023-04-27 06:30:15,581 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-FD76DB487B8B,id=a05fa3a3-4f42-4e00-9e5b-4e78c9e14083
2023-04-27 06:30:15,581 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083: shutdown a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B-LeaderStateImpl
2023-04-27 06:30:15,581 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-impl-thread2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:15,581 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d Close channels
2023-04-27 06:30:15,582 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-FD76DB487B8B: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-1/data/ratis/d802d461-95a8-49e9-82b2-fd76db487b8b/sm/snapshot.1_0
2023-04-27 06:30:15,582 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:15,584 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-FD76DB487B8B: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-1/data/ratis/d802d461-95a8-49e9-82b2-fd76db487b8b/sm/snapshot.1_0 took: 2 ms
2023-04-27 06:30:15,584 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:15,584 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:15,584 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B: closes. applyIndex: 0
2023-04-27 06:30:15,584 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:15,585 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083@group-FD76DB487B8B-SegmentedRaftLogWorker close()
2023-04-27 06:30:15,591 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:15,592 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xac015ba8, L:/0:0:0:0:0:0:0:0:35141] CLOSE
2023-04-27 06:30:15,592 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xac015ba8, L:/0:0:0:0:0:0:0:0:35141] INACTIVE
2023-04-27 06:30:15,592 [a05fa3a3-4f42-4e00-9e5b-4e78c9e14083-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xac015ba8, L:/0:0:0:0:0:0:0:0:35141] UNREGISTERED
2023-04-27 06:30:15,609 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 38.
2023-04-27 06:30:15,609 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 38.
2023-04-27 06:30:15,612 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 1 is closed with bcsId 38.
2023-04-27 06:30:15,624 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 42.
2023-04-27 06:30:15,624 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 42.
2023-04-27 06:30:15,626 [Mini-Cluster-Provider-Reap] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:15,628 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce: close
2023-04-27 06:30:15,631 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 3 is closed with bcsId 42.
2023-04-27 06:30:15,632 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D: shutdown
2023-04-27 06:30:15,632 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-03D650ACD99D,id=0e67dfdf-fb52-47b7-aad6-2fa818fce5ce
2023-04-27 06:30:15,632 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce: shutdown 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D-LeaderStateImpl
2023-04-27 06:30:15,636 [grpc-default-executor-6] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - d129cee7-7074-498b-a1c7-3e6cb07a0899: Completed APPEND_ENTRIES, lastRequest: 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce->d129cee7-7074-498b-a1c7-3e6cb07a0899#449-t1,previous=(t:1, i:31),leaderCommit=30,initializing? true,entries: size=1, first=(t:1, i:32), METADATAENTRY(c:30)
2023-04-27 06:30:15,637 [grpc-default-executor-5] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D->d129cee7-7074-498b-a1c7-3e6cb07a0899-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:15,637 [grpc-default-executor-5] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D->d129cee7-7074-498b-a1c7-3e6cb07a0899: nextIndex: updateUnconditionally 33 -> 32
2023-04-27 06:30:15,637 [grpc-default-executor-6] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - d129cee7-7074-498b-a1c7-3e6cb07a0899: Completed APPEND_ENTRIES, lastRequest: null
2023-04-27 06:30:15,638 [grpc-default-executor-5] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D->d129cee7-7074-498b-a1c7-3e6cb07a0899-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:15,638 [grpc-default-executor-5] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D->d129cee7-7074-498b-a1c7-3e6cb07a0899: nextIndex: updateUnconditionally 32 -> 31
2023-04-27 06:30:15,639 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:15,639 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 46.
2023-04-27 06:30:15,640 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 46.
2023-04-27 06:30:15,641 [JvmPauseMonitor27] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-a05fa3a3-4f42-4e00-9e5b-4e78c9e14083: Stopped
2023-04-27 06:30:15,642 [Mini-Cluster-Provider-Reap] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - d129cee7-7074-498b-a1c7-3e6cb07a0899 Close channels
2023-04-27 06:30:15,644 [Mini-Cluster-Provider-Reap] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58 Close channels
2023-04-27 06:30:15,647 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:15,649 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 4 is closed with bcsId 46.
2023-04-27 06:30:15,648 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D->63017b46-a8f7-4bfc-aab4-ba20a30a5f58-GrpcLogAppender-LogAppenderDaemon] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(200)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D->63017b46-a8f7-4bfc-aab4-ba20a30a5f58-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2023-04-27 06:30:15,655 [grpc-default-executor-6] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58: Completed APPEND_ENTRIES, lastRequest: 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce->63017b46-a8f7-4bfc-aab4-ba20a30a5f58#206-t1,previous=(t:1, i:31),leaderCommit=30,initializing? true,entries: size=1, first=(t:1, i:32), METADATAENTRY(c:30)
2023-04-27 06:30:15,656 [grpc-default-executor-5] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D->63017b46-a8f7-4bfc-aab4-ba20a30a5f58-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:15,656 [grpc-default-executor-6] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58: Completed APPEND_ENTRIES, lastRequest: null
2023-04-27 06:30:15,663 [grpc-default-executor-5] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(137)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D->63017b46-a8f7-4bfc-aab4-ba20a30a5f58-GrpcLogAppender: Failed to getClient for 63017b46-a8f7-4bfc-aab4-ba20a30a5f58
org.apache.ratis.protocol.exceptions.AlreadyClosedException: 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce is already CLOSED
	at org.apache.ratis.util.PeerProxyMap$PeerAndProxy.getProxy(PeerProxyMap.java:61)
	at org.apache.ratis.util.PeerProxyMap.getProxy(PeerProxyMap.java:115)
	at org.apache.ratis.grpc.server.GrpcLogAppender.getClient(GrpcLogAppender.java:116)
	at org.apache.ratis.grpc.server.GrpcLogAppender.resetClient(GrpcLogAppender.java:121)
	at org.apache.ratis.grpc.server.GrpcLogAppender.access$500(GrpcLogAppender.java:58)
	at org.apache.ratis.grpc.server.GrpcLogAppender$AppendLogResponseHandler.onCompleted(GrpcLogAppender.java:416)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:485)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:562)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:743)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:722)
	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:15,666 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread3] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF: shutdown
2023-04-27 06:30:15,666 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread3] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-914FAF6CF0EF,id=0e67dfdf-fb52-47b7-aad6-2fa818fce5ce
2023-04-27 06:30:15,666 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread3] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce: shutdown 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF-LeaderStateImpl
2023-04-27 06:30:15,667 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread3] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:15,667 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-03D650ACD99D: Taking a snapshot at:(t:1, i:32) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-4/data/ratis/bb0486ea-5b79-457f-a4db-03d650acd99d/sm/snapshot.1_32
2023-04-27 06:30:15,668 [grpc-default-executor-5] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D->63017b46-a8f7-4bfc-aab4-ba20a30a5f58-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:15,668 [grpc-default-executor-5] WARN  server.GrpcLogAppender (GrpcLogAppender.java:resetClient(137)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D->63017b46-a8f7-4bfc-aab4-ba20a30a5f58-GrpcLogAppender: Failed to getClient for 63017b46-a8f7-4bfc-aab4-ba20a30a5f58
org.apache.ratis.protocol.exceptions.AlreadyClosedException: 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce is already CLOSED
	at org.apache.ratis.util.PeerProxyMap$PeerAndProxy.getProxy(PeerProxyMap.java:61)
	at org.apache.ratis.util.PeerProxyMap.getProxy(PeerProxyMap.java:115)
	at org.apache.ratis.grpc.server.GrpcLogAppender.getClient(GrpcLogAppender.java:116)
	at org.apache.ratis.grpc.server.GrpcLogAppender.resetClient(GrpcLogAppender.java:121)
	at org.apache.ratis.grpc.server.GrpcLogAppender.access$500(GrpcLogAppender.java:58)
	at org.apache.ratis.grpc.server.GrpcLogAppender$AppendLogResponseHandler.onCompleted(GrpcLogAppender.java:416)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:485)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:562)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:743)
	at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:722)
	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:15,669 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-03D650ACD99D: Finished taking a snapshot at:(t:1, i:32) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-4/data/ratis/bb0486ea-5b79-457f-a4db-03d650acd99d/sm/snapshot.1_32 took: 2 ms
2023-04-27 06:30:15,669 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D-StateMachineUpdater: Took a snapshot at index 32
2023-04-27 06:30:15,670 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 32
2023-04-27 06:30:15,670 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:15,667 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D-StateMachineUpdater: set stopIndex = 32
2023-04-27 06:30:15,671 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x647d8399, L:/0:0:0:0:0:0:0:0:32911] CLOSE
2023-04-27 06:30:15,671 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D: closes. applyIndex: 32
2023-04-27 06:30:15,671 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x647d8399, L:/0:0:0:0:0:0:0:0:32911] INACTIVE
2023-04-27 06:30:15,671 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x647d8399, L:/0:0:0:0:0:0:0:0:32911] UNREGISTERED
2023-04-27 06:30:15,671 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:15,671 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-03D650ACD99D-SegmentedRaftLogWorker close()
2023-04-27 06:30:15,673 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-914FAF6CF0EF: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-4/data/ratis/1a69e094-d42e-467a-ae88-914faf6cf0ef/sm/snapshot.1_0
2023-04-27 06:30:15,673 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread3] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:15,679 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-914FAF6CF0EF: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-4/data/ratis/1a69e094-d42e-467a-ae88-914faf6cf0ef/sm/snapshot.1_0 took: 7 ms
2023-04-27 06:30:15,680 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:15,680 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:15,683 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread3] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF: closes. applyIndex: 0
2023-04-27 06:30:15,683 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:15,683 [0e67dfdf-fb52-47b7-aad6-2fa818fce5ce-impl-thread3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce@group-914FAF6CF0EF-SegmentedRaftLogWorker close()
2023-04-27 06:30:15,692 [JvmPauseMonitor30] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-0e67dfdf-fb52-47b7-aad6-2fa818fce5ce: Stopped
2023-04-27 06:30:15,997 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-cb55ad6f-5dfd-44fc-8fd8-2d474435e374/container.db to cache
2023-04-27 06:30:15,997 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-cb55ad6f-5dfd-44fc-8fd8-2d474435e374/container.db for volume DS-cb55ad6f-5dfd-44fc-8fd8-2d474435e374
2023-04-27 06:30:16,000 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:16,001 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:16,004 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 42155
2023-04-27 06:30:16,004 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis 2cb742ca-d762-4bc5-b311-f495a87c6b6b
2023-04-27 06:30:16,041 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: start RPC server
2023-04-27 06:30:16,041 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: GrpcService started, listening on 41533
2023-04-27 06:30:16,041 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 2cb742ca-d762-4bc5-b311-f495a87c6b6b is started using port 41533 for RATIS
2023-04-27 06:30:16,042 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 2cb742ca-d762-4bc5-b311-f495a87c6b6b is started using port 41533 for RATIS_ADMIN
2023-04-27 06:30:16,042 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 2cb742ca-d762-4bc5-b311-f495a87c6b6b is started using port 41533 for RATIS_SERVER
2023-04-27 06:30:16,042 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 2cb742ca-d762-4bc5-b311-f495a87c6b6b is started using port 42543 for RATIS_DATASTREAM
2023-04-27 06:30:16,047 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc 2cb742ca-d762-4bc5-b311-f495a87c6b6b is started using port 39123
2023-04-27 06:30:16,047 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:16,057 [JvmPauseMonitor42] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-2cb742ca-d762-4bc5-b311-f495a87c6b6b: Started
2023-04-27 06:30:16,075 [IPC Server handler 2 on default port 33597] INFO  node.NodeDecommissionManager (NodeDecommissionManager.java:startMaintenance(366)) - Starting Maintenance for node 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33)
2023-04-27 06:30:16,093 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO  node.ReadOnlyHealthyToHealthyNodeHandler (ReadOnlyHealthyToHealthyNodeHandler.java:onMessage(51)) - Datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) moved to HEALTHY state.
2023-04-27 06:30:16,093 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:16,094 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d to datanode:dbe8ec56-fdf9-4402-aab9-993d7a20391d
2023-04-27 06:30:16,094 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d to datanode:ef6aab41-a1eb-41b3-a4a5-2458878a1611
2023-04-27 06:30:16,094 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d to datanode:7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b
2023-04-27 06:30:16,095 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d, Nodes: dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33)ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:16.094Z[Etc/UTC]].
2023-04-27 06:30:16,096 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(160)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 6.
2023-04-27 06:30:16,104 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 42.
2023-04-27 06:30:16,104 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 42.
2023-04-27 06:30:16,108 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 3 is closed with bcsId 42.
2023-04-27 06:30:16,178 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@6d79a3e6{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/meta/webserver/jetty-0_0_0_0-46545-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-6610266909159441503/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:16,181 [Listener at 127.0.0.1/43855] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@30d9ab2a{HTTP/1.1, (http/1.1)}{0.0.0.0:46545}
2023-04-27 06:30:16,181 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(415)) - Started @142629ms
2023-04-27 06:30:16,182 [Listener at 127.0.0.1/43855] ERROR ozone.HddsDatanodeService (HddsDatanodeService.java:start(312)) - HttpServer failed to start.
java.lang.NullPointerException: config
	at org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkNotNull(Preconditions.java:899)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSink(MetricsSystemImpl.java:298)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:277)
	at org.apache.hadoop.hdds.server.http.BaseHttpServer.start(BaseHttpServer.java:315)
	at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:301)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.lambda$startHddsDatanodes$4(MiniOzoneClusterImpl.java:500)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.startHddsDatanodes(MiniOzoneClusterImpl.java:494)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.build(MiniOzoneClusterImpl.java:624)
	at org.apache.hadoop.ozone.MiniOzoneClusterProvider.lambda$createClusters$1(MiniOzoneClusterProvider.java:234)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:16,183 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:16,187 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee is not found
2023-04-27 06:30:16,189 [IPC Server handler 9 on default port 42409] INFO  node.SCMNodeManager (SCMNodeManager.java:updateDatanodeOpState(565)) - Scheduling a command to update the operationalState persisted on 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) as the reported value (IN_SERVICE, 0) does not match the value stored in SCM (ENTERING_MAINTENANCE, 0)
2023-04-27 06:30:16,193 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@617e442c] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:16,195 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/meta/datanode.id
2023-04-27 06:30:16,195 [Listener at 127.0.0.1/43855] WARN  impl.MetricsConfig (MetricsConfig.java:loadFirst(136)) - Cannot locate configuration: tried hadoop-metrics2-hddsdatanode.properties,hadoop-metrics2.properties
2023-04-27 06:30:16,195 [Listener at 127.0.0.1/43855] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(378)) - Scheduled Metric snapshot period at 10 second(s).
2023-04-27 06:30:16,195 [Listener at 127.0.0.1/43855] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - HddsDatanode metrics system started
2023-04-27 06:30:16,225 [Listener at 127.0.0.1/43855] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:start(204)) - Sink prometheus started
2023-04-27 06:30:16,225 [Listener at 127.0.0.1/43855] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:registerSink(305)) - Registered sink prometheus
2023-04-27 06:30:16,245 [Listener at 127.0.0.1/43855] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:16,283 [Listener at 127.0.0.1/43855] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:16,350 [Listener at 127.0.0.1/43855] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 66 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:16,352 [Listener at 127.0.0.1/43855] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:16,354 [Listener at 127.0.0.1/43855] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:16,354 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:16,354 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data-0/containers/hdds
2023-04-27 06:30:16,355 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data-0/containers/hdds
2023-04-27 06:30:16,368 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis to VolumeSet
2023-04-27 06:30:16,368 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis
2023-04-27 06:30:16,368 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis
2023-04-27 06:30:16,381 [Thread-2822] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data-0/containers/hdds
2023-04-27 06:30:16,381 [Listener at 127.0.0.1/43855] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:16,382 [Listener at 127.0.0.1/43855] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:16,382 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:16,382 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:16,382 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:16,382 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:16,383 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:16,383 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:16,383 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:16,383 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:16,383 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:16,383 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:16,383 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:16,383 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:16,383 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:16,385 [Listener at 127.0.0.1/43855] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:16,386 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:16,386 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:16,386 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:16,386 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:16,386 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:16,386 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:16,386 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:16,387 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:16,387 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:16,387 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:16,394 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:16,395 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:16,395 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:16,395 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:16,395 [3804776e-bffa-42a7-addb-662dc325de80-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x45b14185] REGISTERED
2023-04-27 06:30:16,395 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis] (custom)
2023-04-27 06:30:16,395 [3804776e-bffa-42a7-addb-662dc325de80-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x45b14185] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:16,395 [3804776e-bffa-42a7-addb-662dc325de80-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x45b14185, L:/0:0:0:0:0:0:0:0:45163] ACTIVE
2023-04-27 06:30:16,396 [Listener at 127.0.0.1/43855] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:16,400 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:16,400 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:16,402 [Listener at 127.0.0.1/43855] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:16,407 [Listener at 127.0.0.1/43855] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:16,408 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:16,409 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:16,409 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:16,409 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:16,410 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/meta/webserver
2023-04-27 06:30:16,410 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 41095
2023-04-27 06:30:16,410 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:16,412 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:16,417 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:16,417 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:16,417 [Listener at 127.0.0.1/43855] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 600000ms
2023-04-27 06:30:16,418 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@5c426393{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:16,419 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@fe00017{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:16,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33).
2023-04-27 06:30:16,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:16,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33).
2023-04-27 06:30:16,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33).
2023-04-27 06:30:16,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33).
2023-04-27 06:30:16,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33).
2023-04-27 06:30:16,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33).
2023-04-27 06:30:16,435 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:16,435 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleOverReplicatedExcessUnhealthy(1234)) - Container 3 has 1 excess unhealthy replicas. Excess unhealthy replicas will be deleted.
2023-04-27 06:30:16,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendDeleteCommand(1477)) - Sending delete container command for container #3 to datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)
2023-04-27 06:30:16,437 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 2 milliseconds for processing 11 containers.
2023-04-27 06:30:16,638 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@464b0d1b{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/meta/webserver/jetty-0_0_0_0-41095-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-3330763935361470596/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:16,642 [Listener at 127.0.0.1/43855] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@48fc2c4c{HTTP/1.1, (http/1.1)}{0.0.0.0:41095}
2023-04-27 06:30:16,642 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(415)) - Started @143090ms
2023-04-27 06:30:16,642 [Listener at 127.0.0.1/43855] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:16,644 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:41095
2023-04-27 06:30:16,647 [Listener at 127.0.0.1/43855] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:16,648 [Listener at 127.0.0.1/43855] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:16,648 [Listener at 127.0.0.1/43855] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:16,655 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:16,661 [Listener at 127.0.0.1/43855] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:16,675 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@74f0e5fb] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:16,680 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/meta/datanode.id
2023-04-27 06:30:16,695 [Listener at 127.0.0.1/43855] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:16,744 [Listener at 127.0.0.1/43855] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 48 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:16,745 [Listener at 127.0.0.1/43855] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:16,747 [Listener at 127.0.0.1/43855] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:16,747 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:16,747 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data-0/containers/hdds
2023-04-27 06:30:16,750 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data-0/containers/hdds
2023-04-27 06:30:16,760 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis to VolumeSet
2023-04-27 06:30:16,760 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis
2023-04-27 06:30:16,761 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis
2023-04-27 06:30:16,770 [Thread-2836] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data-0/containers/hdds
2023-04-27 06:30:16,770 [Listener at 127.0.0.1/43855] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:16,771 [Listener at 127.0.0.1/43855] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:16,772 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:16,774 [Listener at 127.0.0.1/43855] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:16,774 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:16,774 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:16,774 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:16,774 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:16,774 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:16,774 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:16,775 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:16,775 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:16,775 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:16,775 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:16,775 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:16,775 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:16,775 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:16,776 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:16,776 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis] (custom)
2023-04-27 06:30:16,776 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x21e31ad4] REGISTERED
2023-04-27 06:30:16,776 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x21e31ad4] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:16,776 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x21e31ad4, L:/0:0:0:0:0:0:0:0:46313] ACTIVE
2023-04-27 06:30:16,778 [Listener at 127.0.0.1/43855] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:16,780 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:16,781 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:16,781 [Listener at 127.0.0.1/43855] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:16,782 [Listener at 127.0.0.1/43855] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:16,783 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:16,783 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:16,783 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:16,784 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:16,784 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/meta/webserver
2023-04-27 06:30:16,784 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 40523
2023-04-27 06:30:16,784 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:16,788 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:16,788 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:16,788 [Listener at 127.0.0.1/43855] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 660000ms
2023-04-27 06:30:16,789 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@3086f4f5{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:16,789 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@27509b24{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:16,934 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-7b9c319c-58e0-436a-9ede-e251e9be7104/container.db to cache
2023-04-27 06:30:16,934 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-7b9c319c-58e0-436a-9ede-e251e9be7104/container.db for volume DS-7b9c319c-58e0-436a-9ede-e251e9be7104
2023-04-27 06:30:16,935 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:16,935 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:16,935 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 40489
2023-04-27 06:30:16,937 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis e5e627c9-cddd-4fde-83a7-403d4c1ca45c
2023-04-27 06:30:16,946 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: start RPC server
2023-04-27 06:30:16,946 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: GrpcService started, listening on 33449
2023-04-27 06:30:16,947 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis e5e627c9-cddd-4fde-83a7-403d4c1ca45c is started using port 33449 for RATIS
2023-04-27 06:30:16,947 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis e5e627c9-cddd-4fde-83a7-403d4c1ca45c is started using port 33449 for RATIS_ADMIN
2023-04-27 06:30:16,947 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis e5e627c9-cddd-4fde-83a7-403d4c1ca45c is started using port 33449 for RATIS_SERVER
2023-04-27 06:30:16,947 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis e5e627c9-cddd-4fde-83a7-403d4c1ca45c is started using port 43189 for RATIS_DATASTREAM
2023-04-27 06:30:16,948 [JvmPauseMonitor43] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-e5e627c9-cddd-4fde-83a7-403d4c1ca45c: Started
2023-04-27 06:30:16,952 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc e5e627c9-cddd-4fde-83a7-403d4c1ca45c is started using port 39667
2023-04-27 06:30:16,952 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:17,029 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkPipelinesClosedOnNode(326)) - Waiting for pipelines to close for 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33). There are 1 pipelines
2023-04-27 06:30:17,029 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:17,032 [EventQueue-StartAdminOnNodeForStartDatanodeAdminHandler] INFO  node.StartDatanodeAdminHandler (StartDatanodeAdminHandler.java:onMessage(57)) - Admin start on datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33). Finalizing its pipelines [PipelineID=efb2dabe-8701-4149-82a1-f2e99ef36468]
2023-04-27 06:30:17,032 [EventQueue-StartAdminOnNodeForStartDatanodeAdminHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: efb2dabe-8701-4149-82a1-f2e99ef36468, Nodes: 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:70ce52ad-a2f4-4f2c-b96d-cd309621d39c, CreationTimestamp2023-04-27T06:29:35.036Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:17,032 [EventQueue-StartAdminOnNodeForStartDatanodeAdminHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=efb2dabe-8701-4149-82a1-f2e99ef36468 close command to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c
2023-04-27 06:30:17,032 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@a3b672d{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/meta/webserver/jetty-0_0_0_0-40523-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-3319150558815035083/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:17,033 [EventQueue-StartAdminOnNodeForStartDatanodeAdminHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: efb2dabe-8701-4149-82a1-f2e99ef36468, Nodes: 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:70ce52ad-a2f4-4f2c-b96d-cd309621d39c, CreationTimestamp2023-04-27T06:29:35.036Z[Etc/UTC]] removed.
2023-04-27 06:30:17,036 [Listener at 127.0.0.1/43855] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@91e78e8{HTTP/1.1, (http/1.1)}{0.0.0.0:40523}
2023-04-27 06:30:17,036 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(415)) - Started @143484ms
2023-04-27 06:30:17,036 [Listener at 127.0.0.1/43855] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:17,037 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:40523
2023-04-27 06:30:17,037 [Listener at 127.0.0.1/43855] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:17,037 [Listener at 127.0.0.1/43855] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:17,037 [Listener at 127.0.0.1/43855] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:17,043 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:17,049 [Listener at 127.0.0.1/43855] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:17,052 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@52c609ea] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:17,056 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/meta/datanode.id
2023-04-27 06:30:17,075 [Listener at 127.0.0.1/43855] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:17,126 [Listener at 127.0.0.1/43855] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 50 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:17,127 [Listener at 127.0.0.1/43855] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:17,130 [Listener at 127.0.0.1/43855] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:17,131 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:17,131 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data-0/containers/hdds
2023-04-27 06:30:17,132 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data-0/containers/hdds
2023-04-27 06:30:17,143 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis to VolumeSet
2023-04-27 06:30:17,143 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis
2023-04-27 06:30:17,143 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis
2023-04-27 06:30:17,153 [Thread-2856] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data-0/containers/hdds
2023-04-27 06:30:17,153 [Listener at 127.0.0.1/43855] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:17,155 [Listener at 127.0.0.1/43855] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:17,155 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:17,155 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:17,156 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:17,156 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:17,156 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:17,156 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:17,156 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:17,156 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:17,156 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:17,156 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:17,156 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:17,156 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:17,156 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:17,158 [Listener at 127.0.0.1/43855] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:17,158 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:17,158 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:17,158 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:17,158 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:17,158 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:17,158 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:17,159 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:17,159 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:17,159 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:17,159 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:17,159 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:17,160 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:17,160 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:17,160 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:17,160 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis] (custom)
2023-04-27 06:30:17,160 [efdbc657-ca6b-4fea-9dc1-5411634f3e98-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xb1158053] REGISTERED
2023-04-27 06:30:17,160 [efdbc657-ca6b-4fea-9dc1-5411634f3e98-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xb1158053] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:17,161 [efdbc657-ca6b-4fea-9dc1-5411634f3e98-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xb1158053, L:/0:0:0:0:0:0:0:0:38671] ACTIVE
2023-04-27 06:30:17,162 [Listener at 127.0.0.1/43855] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:17,164 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:17,164 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:17,165 [Listener at 127.0.0.1/43855] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:17,165 [Listener at 127.0.0.1/43855] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:17,166 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:17,167 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:17,167 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:17,167 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:17,167 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/meta/webserver
2023-04-27 06:30:17,167 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 44805
2023-04-27 06:30:17,167 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:17,169 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:17,169 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:17,169 [Listener at 127.0.0.1/43855] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 600000ms
2023-04-27 06:30:17,169 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@2f2681c5{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:17,170 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@722930ae{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:17,185 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:remove(107)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611: remove    LEADER ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE:t1, leader=ef6aab41-a1eb-41b3-a4a5-2458878a1611, voted=ef6aab41-a1eb-41b3-a4a5-2458878a1611, raftlog=Memoized:ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-SegmentedRaftLog:OPENED:c12, conf=0: peers:[70ce52ad-a2f4-4f2c-b96d-cd309621d39c|rpc:10.1.0.33:40471|dataStream:10.1.0.33:37475|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
2023-04-27 06:30:17,185 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE: shutdown
2023-04-27 06:30:17,185 [Command processor thread] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-868FC3C697EE,id=ef6aab41-a1eb-41b3-a4a5-2458878a1611
2023-04-27 06:30:17,185 [Command processor thread] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611: shutdown ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-LeaderStateImpl
2023-04-27 06:30:17,185 [Command processor thread] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:17,186 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE->70ce52ad-a2f4-4f2c-b96d-cd309621d39c-GrpcLogAppender-LogAppenderDaemon] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(200)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE->70ce52ad-a2f4-4f2c-b96d-cd309621d39c-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2023-04-27 06:30:17,186 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee is not found
2023-04-27 06:30:17,186 [grpc-default-executor-5] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c: Completed APPEND_ENTRIES, lastRequest: ef6aab41-a1eb-41b3-a4a5-2458878a1611->70ce52ad-a2f4-4f2c-b96d-cd309621d39c#75-t1,previous=(t:1, i:11),leaderCommit=10,initializing? true,entries: size=1, first=(t:1, i:12), METADATAENTRY(c:10)
2023-04-27 06:30:17,186 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE->dbe8ec56-fdf9-4402-aab9-993d7a20391d-GrpcLogAppender-LogAppenderDaemon] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(200)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE->dbe8ec56-fdf9-4402-aab9-993d7a20391d-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2023-04-27 06:30:17,194 [grpc-default-executor-8] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE->70ce52ad-a2f4-4f2c-b96d-cd309621d39c-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:17,194 [grpc-default-executor-6] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d: Completed APPEND_ENTRIES, lastRequest: ef6aab41-a1eb-41b3-a4a5-2458878a1611->dbe8ec56-fdf9-4402-aab9-993d7a20391d#76-t1,previous=(t:1, i:11),leaderCommit=10,initializing? true,entries: size=1, first=(t:1, i:12), METADATAENTRY(c:10)
2023-04-27 06:30:17,194 [grpc-default-executor-8] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE->70ce52ad-a2f4-4f2c-b96d-cd309621d39c: nextIndex: updateUnconditionally 13 -> 12
2023-04-27 06:30:17,195 [grpc-default-executor-1] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d: Completed APPEND_ENTRIES, lastRequest: null
2023-04-27 06:30:17,195 [grpc-default-executor-5] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(143)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c: Completed APPEND_ENTRIES, lastRequest: null
2023-04-27 06:30:17,197 [Command processor thread] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-StateMachineUpdater: set stopIndex = 12
2023-04-27 06:30:17,198 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-868FC3C697EE: Taking a snapshot at:(t:1, i:12) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-2/data/ratis/1fe65a88-8cef-41d6-b4e0-868fc3c697ee/sm/snapshot.1_12
2023-04-27 06:30:17,200 [grpc-default-executor-5] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE->dbe8ec56-fdf9-4402-aab9-993d7a20391d-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:17,201 [grpc-default-executor-5] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE->dbe8ec56-fdf9-4402-aab9-993d7a20391d: nextIndex: updateUnconditionally 13 -> 12
2023-04-27 06:30:17,201 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:remove(107)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c: remove  FOLLOWER 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE:t1, leader=ef6aab41-a1eb-41b3-a4a5-2458878a1611, voted=ef6aab41-a1eb-41b3-a4a5-2458878a1611, raftlog=Memoized:70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-SegmentedRaftLog:OPENED:c10, conf=0: peers:[70ce52ad-a2f4-4f2c-b96d-cd309621d39c|rpc:10.1.0.33:40471|dataStream:10.1.0.33:37475|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
2023-04-27 06:30:17,200 [IPC Server handler 14 on default port 42409] INFO  node.SCMNodeManager (SCMNodeManager.java:updateDatanodeOpState(565)) - Scheduling a command to update the operationalState persisted on 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) as the reported value (IN_SERVICE, 0) does not match the value stored in SCM (ENTERING_MAINTENANCE, 0)
2023-04-27 06:30:17,200 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-868FC3C697EE: Finished taking a snapshot at:(t:1, i:12) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-2/data/ratis/1fe65a88-8cef-41d6-b4e0-868fc3c697ee/sm/snapshot.1_12 took: 2 ms
2023-04-27 06:30:17,204 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-StateMachineUpdater: Took a snapshot at index 12
2023-04-27 06:30:17,204 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 12
2023-04-27 06:30:17,204 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee is not found
2023-04-27 06:30:17,204 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=efb2dabe-8701-4149-82a1-f2e99ef36468 is not found
2023-04-27 06:30:17,203 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE: shutdown
2023-04-27 06:30:17,205 [Command processor thread] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-868FC3C697EE,id=70ce52ad-a2f4-4f2c-b96d-cd309621d39c
2023-04-27 06:30:17,205 [Command processor thread] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c: shutdown 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-FollowerState
2023-04-27 06:30:17,205 [Command processor thread] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-StateMachineUpdater: set stopIndex = 10
2023-04-27 06:30:17,205 [70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-FollowerState was interrupted
2023-04-27 06:30:17,205 [70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-868FC3C697EE: Taking a snapshot at:(t:1, i:10) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-0/data/ratis/1fe65a88-8cef-41d6-b4e0-868fc3c697ee/sm/snapshot.1_10
2023-04-27 06:30:17,205 [grpc-default-executor-3] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE->70ce52ad-a2f4-4f2c-b96d-cd309621d39c-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:17,206 [grpc-default-executor-3] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE->70ce52ad-a2f4-4f2c-b96d-cd309621d39c: nextIndex: updateUnconditionally 12 -> 11
2023-04-27 06:30:17,205 [grpc-default-executor-1] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(415)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE->dbe8ec56-fdf9-4402-aab9-993d7a20391d-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2023-04-27 06:30:17,206 [grpc-default-executor-1] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE->dbe8ec56-fdf9-4402-aab9-993d7a20391d: nextIndex: updateUnconditionally 12 -> 11
2023-04-27 06:30:17,206 [Command processor thread] INFO  server.RaftServer$Division (ServerState.java:close(466)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE: closes. applyIndex: 12
2023-04-27 06:30:17,207 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:17,207 [Command processor thread] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE-SegmentedRaftLogWorker close()
2023-04-27 06:30:17,207 [70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-868FC3C697EE: Finished taking a snapshot at:(t:1, i:10) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-0/data/ratis/1fe65a88-8cef-41d6-b4e0-868fc3c697ee/sm/snapshot.1_10 took: 2 ms
2023-04-27 06:30:17,207 [70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-StateMachineUpdater: Took a snapshot at index 10
2023-04-27 06:30:17,207 [70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 10
2023-04-27 06:30:17,208 [Command processor thread] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE: closes. applyIndex: 10
2023-04-27 06:30:17,208 [70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:17,209 [Command processor thread] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE-SegmentedRaftLogWorker close()
2023-04-27 06:30:17,214 [IPC Server handler 3 on default port 42409] INFO  node.SCMNodeManager (SCMNodeManager.java:updateDatanodeOpState(565)) - Scheduling a command to update the operationalState persisted on 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) as the reported value (IN_SERVICE, 0) does not match the value stored in SCM (ENTERING_MAINTENANCE, 0)
2023-04-27 06:30:17,217 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 2.
2023-04-27 06:30:17,218 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 2.
2023-04-27 06:30:17,219 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 2.
2023-04-27 06:30:17,219 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 2.
2023-04-27 06:30:17,221 [FixedThreadPoolWithAffinityExecutor-9-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(285)) - Moving container #1 to QUASI_CLOSED state, datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33) reported QUASI_CLOSED replica.
2023-04-27 06:30:17,223 [IPC Server handler 13 on default port 42409] INFO  node.SCMNodeManager (SCMNodeManager.java:updateDatanodeOpState(565)) - Scheduling a command to update the operationalState persisted on 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) as the reported value (IN_SERVICE, 0) does not match the value stored in SCM (ENTERING_MAINTENANCE, 0)
2023-04-27 06:30:17,225 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 6.
2023-04-27 06:30:17,225 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 6.
2023-04-27 06:30:17,227 [IPC Server handler 16 on default port 42409] INFO  node.SCMNodeManager (SCMNodeManager.java:updateDatanodeOpState(565)) - Scheduling a command to update the operationalState persisted on 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) as the reported value (IN_SERVICE, 0) does not match the value stored in SCM (ENTERING_MAINTENANCE, 0)
2023-04-27 06:30:17,228 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 6.
2023-04-27 06:30:17,228 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 6.
2023-04-27 06:30:17,230 [FixedThreadPoolWithAffinityExecutor-9-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(285)) - Moving container #3 to QUASI_CLOSED state, datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33) reported QUASI_CLOSED replica.
2023-04-27 06:30:17,233 [IPC Server handler 0 on default port 42409] INFO  node.SCMNodeManager (SCMNodeManager.java:updateDatanodeOpState(565)) - Scheduling a command to update the operationalState persisted on 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) as the reported value (IN_SERVICE, 0) does not match the value stored in SCM (ENTERING_MAINTENANCE, 0)
2023-04-27 06:30:17,235 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 10.
2023-04-27 06:30:17,235 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 10.
2023-04-27 06:30:17,237 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 10.
2023-04-27 06:30:17,237 [IPC Server handler 19 on default port 42409] INFO  node.SCMNodeManager (SCMNodeManager.java:updateDatanodeOpState(565)) - Scheduling a command to update the operationalState persisted on 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) as the reported value (IN_SERVICE, 0) does not match the value stored in SCM (ENTERING_MAINTENANCE, 0)
2023-04-27 06:30:17,237 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 10.
2023-04-27 06:30:17,239 [FixedThreadPoolWithAffinityExecutor-9-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(285)) - Moving container #4 to QUASI_CLOSED state, datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33) reported QUASI_CLOSED replica.
2023-04-27 06:30:17,240 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:groupRemove(428)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-868FC3C697EE: Succeed to remove RaftStorageDirectory Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-2/data/ratis/1fe65a88-8cef-41d6-b4e0-868fc3c697ee
2023-04-27 06:30:17,240 [Command processor thread] INFO  commandhandler.ClosePipelineCommandHandler (ClosePipelineCommandHandler.java:handle(78)) - Close Pipeline PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee command on datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611.
2023-04-27 06:30:17,242 [pool-1620-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611: new RaftServerImpl for group-50FF7EBD4A6D:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:17,242 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:17,242 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:17,242 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:17,242 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:17,242 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:17,243 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:17,243 [pool-1620-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D: ConfigurationManager, init=-1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:17,243 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-2/data/ratis] (custom)
2023-04-27 06:30:17,243 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:17,243 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:17,243 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:17,243 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:17,243 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:17,247 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:17,247 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:17,247 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:17,247 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:17,247 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:17,248 [IPC Server handler 18 on default port 42409] INFO  node.SCMNodeManager (SCMNodeManager.java:updateDatanodeOpState(565)) - Scheduling a command to update the operationalState persisted on 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) as the reported value (IN_SERVICE, 0) does not match the value stored in SCM (ENTERING_MAINTENANCE, 0)
2023-04-27 06:30:17,249 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:groupRemove(428)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-868FC3C697EE: Succeed to remove RaftStorageDirectory Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-0/data/ratis/1fe65a88-8cef-41d6-b4e0-868fc3c697ee
2023-04-27 06:30:17,249 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611: addNew group-50FF7EBD4A6D:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER] returns      null ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
2023-04-27 06:30:17,249 [pool-1620-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-2/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d does not exist. Creating ...
2023-04-27 06:30:17,250 [Command processor thread] INFO  commandhandler.ClosePipelineCommandHandler (ClosePipelineCommandHandler.java:handle(78)) - Close Pipeline PipelineID=1fe65a88-8cef-41d6-b4e0-868fc3c697ee command on datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c.
2023-04-27 06:30:17,251 [pool-1620-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-2/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:17,253 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:remove(107)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c: remove    LEADER 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468:t1, leader=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, voted=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, raftlog=Memoized:70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-SegmentedRaftLog:OPENED:c0, conf=0: peers:[70ce52ad-a2f4-4f2c-b96d-cd309621d39c|rpc:10.1.0.33:40471|dataStream:10.1.0.33:37475|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
2023-04-27 06:30:17,253 [pool-1620-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-2/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d has been successfully formatted.
2023-04-27 06:30:17,254 [pool-1620-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-50FF7EBD4A6D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:17,255 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:17,255 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:17,255 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:17,255 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:17,255 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:17,255 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:17,256 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:17,256 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:17,256 [pool-1620-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-2/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d
2023-04-27 06:30:17,256 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:17,256 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:17,256 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:17,256 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:17,256 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:17,256 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:17,256 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:17,256 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:17,257 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468: shutdown
2023-04-27 06:30:17,257 [Command processor thread] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-F2E99EF36468,id=70ce52ad-a2f4-4f2c-b96d-cd309621d39c
2023-04-27 06:30:17,258 [Command processor thread] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c: shutdown 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-LeaderStateImpl
2023-04-27 06:30:17,258 [Command processor thread] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:17,258 [Command processor thread] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:17,259 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:17,259 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:17,261 [70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-F2E99EF36468: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-0/data/ratis/efb2dabe-8701-4149-82a1-f2e99ef36468/sm/snapshot.1_0
2023-04-27 06:30:17,263 [70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-F2E99EF36468: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-0/data/ratis/efb2dabe-8701-4149-82a1-f2e99ef36468/sm/snapshot.1_0 took: 3 ms
2023-04-27 06:30:17,264 [70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:17,264 [70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:17,264 [Command processor thread] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468: closes. applyIndex: 0
2023-04-27 06:30:17,264 [70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:17,264 [Command processor thread] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468-SegmentedRaftLogWorker close()
2023-04-27 06:30:17,265 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:groupRemove(428)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c@group-F2E99EF36468: Succeed to remove RaftStorageDirectory Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-0/data/ratis/efb2dabe-8701-4149-82a1-f2e99ef36468
2023-04-27 06:30:17,265 [Command processor thread] INFO  commandhandler.ClosePipelineCommandHandler (ClosePipelineCommandHandler.java:handle(78)) - Close Pipeline PipelineID=efb2dabe-8701-4149-82a1-f2e99ef36468 command on datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c.
2023-04-27 06:30:17,272 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:17,273 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:17,273 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:17,273 [pool-1620-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:17,273 [pool-1620-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:17,273 [pool-1620-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D: start as a follower, conf=-1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:17,273 [pool-1620-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:17,273 [pool-1620-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611: start ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-FollowerState
2023-04-27 06:30:17,275 [pool-1620-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-50FF7EBD4A6D,id=ef6aab41-a1eb-41b3-a4a5-2458878a1611
2023-04-27 06:30:17,275 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:17,276 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:17,276 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:17,276 [pool-1620-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:17,276 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:17,276 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:17,278 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d
2023-04-27 06:30:17,288 [grpc-default-executor-1] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d: addNew group-50FF7EBD4A6D:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER] returns group-50FF7EBD4A6D:java.util.concurrent.CompletableFuture@583db155[Not completed]
2023-04-27 06:30:17,289 [pool-1598-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d: new RaftServerImpl for group-50FF7EBD4A6D:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:17,289 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:17,289 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:17,289 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:17,289 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:17,289 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:17,289 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:17,289 [pool-1598-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D: ConfigurationManager, init=-1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:17,290 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data/ratis] (custom)
2023-04-27 06:30:17,290 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:17,290 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:17,290 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:17,290 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:17,290 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:17,291 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:17,291 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:17,291 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:17,292 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:17,292 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:17,292 [pool-1598-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d does not exist. Creating ...
2023-04-27 06:30:17,293 [pool-1598-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:17,295 [pool-1598-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d has been successfully formatted.
2023-04-27 06:30:17,295 [pool-1598-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-50FF7EBD4A6D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:17,295 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:17,296 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:17,296 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:17,296 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:17,296 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:17,296 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:17,297 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:17,297 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:17,297 [pool-1598-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d
2023-04-27 06:30:17,297 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:17,297 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:17,297 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:17,297 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:17,297 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:17,297 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:17,298 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:17,298 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:17,299 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:17,299 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:17,353 [Finalizer] WARN  managed.ManagedRocksObjectUtils (ManagedRocksObjectUtils.java:assertClosed(54)) - Checkpoint is not closed properly
 StackTrace for unclosed instance: org.apache.hadoop.hdds.utils.db.managed.ManagedObject.<init>(ManagedObject.java:35)
org.apache.hadoop.hdds.utils.db.managed.ManagedCheckpoint.<init>(ManagedCheckpoint.java:27)
org.apache.hadoop.hdds.utils.db.managed.ManagedCheckpoint.create(ManagedCheckpoint.java:31)
org.apache.hadoop.hdds.utils.db.RocksDatabase$RocksCheckpoint.<init>(RocksDatabase.java:226)
org.apache.hadoop.hdds.utils.db.RocksDatabase$RocksCheckpoint.<init>(RocksDatabase.java:222)
org.apache.hadoop.hdds.utils.db.RocksDatabase.createCheckpoint(RocksDatabase.java:601)
org.apache.hadoop.hdds.utils.db.RDBCheckpointManager.<init>(RDBCheckpointManager.java:53)
org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:174)
org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.persistPutBlock(BlockManagerImpl.java:122)
org.apache.hadoop.ozone.container.keyvalue.impl.BlockManagerImpl.putBlock(BlockManagerImpl.java:103)
org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handlePutBlock(KeyValueHandler.java:556)
org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:261)
org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:232)
org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:320)
org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.lambda$dispatch$0(HddsDispatcher.java:171)
org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:170)
org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:439)
org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:449)
org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$submitTask$8(ContainerStateMachine.java:834)
org.apache.ratis.util.TaskQueue.lambda$submit$0(TaskQueue.java:121)
org.apache.ratis.util.LogUtils.runAndLog(LogUtils.java:38)
org.apache.ratis.util.LogUtils$1.run(LogUtils.java:79)
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
java.util.concurrent.FutureTask.run(FutureTask.java:266)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
java.lang.Thread.run(Thread.java:750)

2023-04-27 06:30:17,355 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:17,355 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:17,361 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:17,361 [pool-1598-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:17,361 [pool-1598-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:17,362 [pool-1598-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D: start as a follower, conf=-1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:17,362 [pool-1598-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:17,362 [pool-1598-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d: start dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FollowerState
2023-04-27 06:30:17,362 [pool-1598-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-50FF7EBD4A6D,id=dbe8ec56-fdf9-4402-aab9-993d7a20391d
2023-04-27 06:30:17,362 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:17,362 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:17,362 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:17,365 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:17,365 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:17,365 [pool-1598-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:17,375 [grpc-default-executor-1] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b: addNew group-50FF7EBD4A6D:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER] returns group-50FF7EBD4A6D:java.util.concurrent.CompletableFuture@78fee155[Not completed]
2023-04-27 06:30:17,376 [pool-1759-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b: new RaftServerImpl for group-50FF7EBD4A6D:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:17,376 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:17,376 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:17,376 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:17,376 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:17,376 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:17,376 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:17,376 [pool-1759-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D: ConfigurationManager, init=-1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:17,376 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-6/data/ratis] (custom)
2023-04-27 06:30:17,377 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:17,377 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:17,377 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:17,377 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:17,377 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:17,378 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:17,378 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:17,378 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:17,378 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:17,378 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:17,378 [pool-1759-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-6/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d does not exist. Creating ...
2023-04-27 06:30:17,382 [pool-1759-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-6/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:17,383 [pool-1759-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-6/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d has been successfully formatted.
2023-04-27 06:30:17,384 [pool-1759-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-50FF7EBD4A6D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:17,384 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:17,384 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:17,384 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:17,385 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:17,385 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:17,385 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:17,385 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:17,385 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:17,385 [pool-1759-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-6/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d
2023-04-27 06:30:17,385 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:17,386 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:17,386 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:17,386 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:17,386 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:17,386 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:17,386 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:17,386 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:17,388 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:17,390 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:17,396 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:17,396 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:17,396 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:17,396 [pool-1759-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:17,397 [pool-1759-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:17,397 [pool-1759-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D: start as a follower, conf=-1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:17,397 [pool-1759-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:17,397 [pool-1759-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b: start 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FollowerState
2023-04-27 06:30:17,397 [pool-1759-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-50FF7EBD4A6D,id=7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b
2023-04-27 06:30:17,397 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:17,397 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:17,398 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:17,398 [pool-1759-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:17,398 [7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:17,398 [7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:17,409 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS THREE PipelineID=03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d.
2023-04-27 06:30:17,412 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:17,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:forceCloseContainer(1080)) - Force closing container #1 with BCSID 2, which is in QUASI_CLOSED state.
2023-04-27 06:30:17,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33).
2023-04-27 06:30:17,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33).
2023-04-27 06:30:17,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:forceCloseContainer(1080)) - Force closing container #3 with BCSID 6, which is in QUASI_CLOSED state.
2023-04-27 06:30:17,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33).
2023-04-27 06:30:17,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33).
2023-04-27 06:30:17,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:forceCloseContainer(1080)) - Force closing container #4 with BCSID 10, which is in QUASI_CLOSED state.
2023-04-27 06:30:17,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33).
2023-04-27 06:30:17,436 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33).
2023-04-27 06:30:17,436 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:17,437 [ReplicationMonitor] WARN  net.NetworkTopologyImpl (NetworkTopologyImpl.java:chooseNodeInternal(653)) - No available node in (scope="/" excludedScope="null" excludedNodes="[220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33), 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33), 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)]"  ancestorGen="1").
2023-04-27 06:30:17,437 [ReplicationMonitor] WARN  algorithms.SCMContainerPlacementRackAware (SCMContainerPlacementRackAware.java:chooseNode(282)) - Failed to find the datanode for container. excludedNodes:[220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33), 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33), 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)], affinityNode:
2023-04-27 06:30:17,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #3 is under replicated. Expected replica count is 3, but found 2.
2023-04-27 06:30:17,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=3, replicaIndex=0, sourceNodes=[220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33), 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33), 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)], priority=NORMAL to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)
2023-04-27 06:30:17,438 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 11 containers.
2023-04-27 06:30:17,494 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:17,494 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:17,496 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:17,499 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:17,544 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@463c250{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/meta/webserver/jetty-0_0_0_0-44805-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-3187200202333563156/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:17,551 [Listener at 127.0.0.1/43855] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@2ad56cf9{HTTP/1.1, (http/1.1)}{0.0.0.0:44805}
2023-04-27 06:30:17,551 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(415)) - Started @143999ms
2023-04-27 06:30:17,551 [Listener at 127.0.0.1/43855] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:17,552 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:44805
2023-04-27 06:30:17,552 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:17,552 [Listener at 127.0.0.1/43855] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:17,552 [Listener at 127.0.0.1/43855] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:17,553 [Listener at 127.0.0.1/43855] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:17,565 [Listener at 127.0.0.1/43855] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:17,566 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6d6fbf9d] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:17,570 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/meta/datanode.id
2023-04-27 06:30:17,593 [Listener at 127.0.0.1/43855] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:17,644 [Listener at 127.0.0.1/43855] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 50 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:17,645 [Listener at 127.0.0.1/43855] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:17,646 [Listener at 127.0.0.1/43855] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:17,647 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:17,647 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data-0/containers/hdds
2023-04-27 06:30:17,654 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data-0/containers/hdds
2023-04-27 06:30:17,675 [Listener at 127.0.0.1/43855] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data/ratis to VolumeSet
2023-04-27 06:30:17,675 [Listener at 127.0.0.1/43855] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data/ratis
2023-04-27 06:30:17,676 [ForkJoinPool.commonPool-worker-0] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-1/data-0/containers/hdds/96a9b0ac-a466-4378-aad7-6952c9070703/DS-6d5b5c2c-7852-435f-85ea-d5c448e13564/container.db for volume DS-6d5b5c2c-7852-435f-85ea-d5c448e13564
2023-04-27 06:30:17,676 [Listener at 127.0.0.1/43855] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data/ratis
2023-04-27 06:30:17,679 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:17,680 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:17,681 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
2023-04-27 06:30:17,691 [Thread-2878] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data-0/containers/hdds
2023-04-27 06:30:17,691 [Listener at 127.0.0.1/43855] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:17,693 [Listener at 127.0.0.1/43855] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:17,693 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:17,693 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:17,693 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:17,693 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:17,693 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:17,693 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:17,694 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@2ffc21f8{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:17,694 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:17,694 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:17,694 [Listener at 127.0.0.1/43855] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:17,694 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:17,694 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:17,694 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@31df6883{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:17,694 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:17,695 [ForkJoinPool.commonPool-worker-0] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:17,695 [Listener at 127.0.0.1/43855] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:17,695 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@436b06e3{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:17,696 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@5a36c0dc{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:17,696 [Listener at 127.0.0.1/43855] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:17,697 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:17,698 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:17,698 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:17,698 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:17,698 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:17,698 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:17,699 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:17,699 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:17,699 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:17,699 [Listener at 127.0.0.1/43855] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:17,700 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:17,700 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:17,700 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:17,700 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:17,700 [Listener at 127.0.0.1/43855] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data/ratis] (custom)
2023-04-27 06:30:17,700 [f88c54b2-8776-4954-8dfb-f8bea6a862a6-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x81b73538] REGISTERED
2023-04-27 06:30:17,701 [f88c54b2-8776-4954-8dfb-f8bea6a862a6-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x81b73538] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:17,701 [f88c54b2-8776-4954-8dfb-f8bea6a862a6-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x81b73538, L:/0:0:0:0:0:0:0:0:46011] ACTIVE
2023-04-27 06:30:17,703 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:17,703 [Listener at 127.0.0.1/43855] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:17,712 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:17,712 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:17,713 [Listener at 127.0.0.1/43855] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:17,713 [Listener at 127.0.0.1/43855] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:17,714 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189: close
2023-04-27 06:30:17,714 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:17,714 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:17,715 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:17,715 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:17,715 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:17,715 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - a05fa3a3-4f42-4e00-9e5b-4e78c9e14083 Close channels
2023-04-27 06:30:17,715 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/meta/webserver
2023-04-27 06:30:17,715 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d Close channels
2023-04-27 06:30:17,715 [Listener at 127.0.0.1/43855] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 39005
2023-04-27 06:30:17,717 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:17,717 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D: shutdown
2023-04-27 06:30:17,717 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:17,718 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-C898FAF9593D,id=2b022b8d-8bf4-4c13-9ad2-4e7e18903189
2023-04-27 06:30:17,718 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189: shutdown 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D-LeaderStateImpl
2023-04-27 06:30:17,718 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189-impl-thread2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:17,718 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:17,719 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-C898FAF9593D: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-2/data/ratis/b2ba4b9e-5d53-4979-8d36-c898faf9593d/sm/snapshot.1_0
2023-04-27 06:30:17,719 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:17,719 [Listener at 127.0.0.1/43855] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:17,719 [Listener at 127.0.0.1/43855] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 600000ms
2023-04-27 06:30:17,719 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@55cf34b8{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:17,720 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@33be0702{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:17,722 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x20eee4fc, L:/0:0:0:0:0:0:0:0:43489] CLOSE
2023-04-27 06:30:17,722 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x20eee4fc, L:/0:0:0:0:0:0:0:0:43489] INACTIVE
2023-04-27 06:30:17,722 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x20eee4fc, L:/0:0:0:0:0:0:0:0:43489] UNREGISTERED
2023-04-27 06:30:17,724 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-C898FAF9593D: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-2/data/ratis/b2ba4b9e-5d53-4979-8d36-c898faf9593d/sm/snapshot.1_0 took: 5 ms
2023-04-27 06:30:17,724 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:17,724 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:17,726 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D: closes. applyIndex: 0
2023-04-27 06:30:17,727 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:17,727 [2b022b8d-8bf4-4c13-9ad2-4e7e18903189-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 2b022b8d-8bf4-4c13-9ad2-4e7e18903189@group-C898FAF9593D-SegmentedRaftLogWorker close()
2023-04-27 06:30:17,727 [JvmPauseMonitor28] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-2b022b8d-8bf4-4c13-9ad2-4e7e18903189: Stopped
2023-04-27 06:30:17,758 [Mini-Cluster-Provider-Reap] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-4/data-0/containers/hdds/96a9b0ac-a466-4378-aad7-6952c9070703/DS-b4c731da-858b-43c3-9884-ce9c3944875e/container.db for volume DS-b4c731da-858b-43c3-9884-ce9c3944875e
2023-04-27 06:30:17,758 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:17,760 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:17,761 [Mini-Cluster-Provider-Reap] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
2023-04-27 06:30:17,790 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@134d4403{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:17,792 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@2b61b451{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:17,792 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:17,793 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@41d93fe3{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:17,793 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@4fc1097d{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:17,801 [Mini-Cluster-Provider-Reap] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:17,805 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - d129cee7-7074-498b-a1c7-3e6cb07a0899: close
2023-04-27 06:30:17,806 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D: shutdown
2023-04-27 06:30:17,806 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-03D650ACD99D,id=d129cee7-7074-498b-a1c7-3e6cb07a0899
2023-04-27 06:30:17,806 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - d129cee7-7074-498b-a1c7-3e6cb07a0899: shutdown d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-FollowerState
2023-04-27 06:30:17,806 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - d129cee7-7074-498b-a1c7-3e6cb07a0899: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:17,807 [Mini-Cluster-Provider-Reap] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58 Close channels
2023-04-27 06:30:17,808 [Mini-Cluster-Provider-Reap] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce Close channels
2023-04-27 06:30:17,809 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - d129cee7-7074-498b-a1c7-3e6cb07a0899: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:17,809 [d129cee7-7074-498b-a1c7-3e6cb07a0899-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xec91d056, L:/0:0:0:0:0:0:0:0:41377] CLOSE
2023-04-27 06:30:17,809 [d129cee7-7074-498b-a1c7-3e6cb07a0899-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xec91d056, L:/0:0:0:0:0:0:0:0:41377] INACTIVE
2023-04-27 06:30:17,809 [d129cee7-7074-498b-a1c7-3e6cb07a0899-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xec91d056, L:/0:0:0:0:0:0:0:0:41377] UNREGISTERED
2023-04-27 06:30:17,809 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread3] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C: shutdown
2023-04-27 06:30:17,810 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread3] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-47E71C502C0C,id=d129cee7-7074-498b-a1c7-3e6cb07a0899
2023-04-27 06:30:17,810 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread3] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - d129cee7-7074-498b-a1c7-3e6cb07a0899: shutdown d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C-LeaderStateImpl
2023-04-27 06:30:17,810 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread3] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:17,810 [d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-47E71C502C0C: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-3/data/ratis/057b00eb-9ef1-46bf-b442-47e71c502c0c/sm/snapshot.1_0
2023-04-27 06:30:17,811 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread3] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:17,813 [d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-47E71C502C0C: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-3/data/ratis/057b00eb-9ef1-46bf-b442-47e71c502c0c/sm/snapshot.1_0 took: 2 ms
2023-04-27 06:30:17,813 [d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:17,813 [d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:17,814 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread3] INFO  server.RaftServer$Division (ServerState.java:close(466)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C: closes. applyIndex: 0
2023-04-27 06:30:17,838 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-StateMachineUpdater: set stopIndex = 32
2023-04-27 06:30:17,838 [d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-FollowerState was interrupted
2023-04-27 06:30:17,839 [d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-03D650ACD99D: Taking a snapshot at:(t:1, i:32) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-3/data/ratis/bb0486ea-5b79-457f-a4db-03d650acd99d/sm/snapshot.1_32
2023-04-27 06:30:17,839 [d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:17,841 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-47E71C502C0C-SegmentedRaftLogWorker close()
2023-04-27 06:30:17,842 [d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-03D650ACD99D: Finished taking a snapshot at:(t:1, i:32) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-3/data/ratis/bb0486ea-5b79-457f-a4db-03d650acd99d/sm/snapshot.1_32 took: 3 ms
2023-04-27 06:30:17,842 [d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-StateMachineUpdater: Took a snapshot at index 32
2023-04-27 06:30:17,842 [d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 32
2023-04-27 06:30:17,843 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D: closes. applyIndex: 32
2023-04-27 06:30:17,843 [d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:17,843 [d129cee7-7074-498b-a1c7-3e6cb07a0899-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - d129cee7-7074-498b-a1c7-3e6cb07a0899@group-03D650ACD99D-SegmentedRaftLogWorker close()
2023-04-27 06:30:17,845 [JvmPauseMonitor29] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-d129cee7-7074-498b-a1c7-3e6cb07a0899: Stopped
2023-04-27 06:30:17,850 [IPC Server handler 13 on default port 38893] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/2cb742ca-d762-4bc5-b311-f495a87c6b6b
2023-04-27 06:30:17,850 [IPC Server handler 13 on default port 38893] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : 2cb742ca-d762-4bc5-b311-f495a87c6b6b{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=35319, REPLICATION=42155, RATIS=41533, RATIS_ADMIN=41533, RATIS_SERVER=41533, RATIS_DATASTREAM=42543, STANDALONE=39123], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:17,861 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:17,861 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=9f565819-4a1c-445f-b823-e8b5ba640bdf to datanode:2cb742ca-d762-4bc5-b311-f495a87c6b6b
2023-04-27 06:30:17,863 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(200)) - ContainerSafeModeRule rule is successfully validated
2023-04-27 06:30:17,868 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 9f565819-4a1c-445f-b823-e8b5ba640bdf, Nodes: 2cb742ca-d762-4bc5-b311-f495a87c6b6b(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:17.861Z[Etc/UTC]].
2023-04-27 06:30:17,870 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (DataNodeSafeModeRule.java:process(71)) - SCM in safe mode. 1 DataNodes registered, 3 required.
2023-04-27 06:30:17,887 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(200)) - AtleastOneDatanodeReportedRule rule is successfully validated
2023-04-27 06:30:17,988 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) moved to stale state. Finalizing its pipelines [PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, PipelineID=fc22a943-ba2e-46ef-a412-943475173222, PipelineID=d802d461-95a8-49e9-82b2-fd76db487b8b, PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24]
2023-04-27 06:30:17,988 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #7 closed for pipeline=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332
2023-04-27 06:30:17,988 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #7, current state: CLOSING
2023-04-27 06:30:17,989 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 37ee7bc9-b834-49c7-bf6a-23376a986332, Nodes: 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:OPEN, leaderId:, CreationTimestamp2023-04-27T06:30:09.998Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:17,989 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #8 closed for pipeline=PipelineID=fc22a943-ba2e-46ef-a412-943475173222
2023-04-27 06:30:17,989 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #8, current state: CLOSING
2023-04-27 06:30:17,990 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: fc22a943-ba2e-46ef-a412-943475173222, Nodes: d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:OPEN, leaderId:, CreationTimestamp2023-04-27T06:30:10.665Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:17,990 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: d802d461-95a8-49e9-82b2-fd76db487b8b, Nodes: a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:a05fa3a3-4f42-4e00-9e5b-4e78c9e14083, CreationTimestamp2023-04-27T06:28:59.422Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:17,990 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #10 closed for pipeline=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317
2023-04-27 06:30:17,990 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #10, current state: CLOSING
2023-04-27 06:30:17,991 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 3541dabf-3351-48cd-a257-9e51dd0a6317, Nodes: a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33)63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:OPEN, leaderId:, CreationTimestamp2023-04-27T06:30:11.878Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:17,991 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #9 closed for pipeline=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1
2023-04-27 06:30:17,991 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #9, current state: CLOSING
2023-04-27 06:30:17,991 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: e68537c7-dd5e-4934-bd5b-0f78194036e1, Nodes: d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:OPEN, leaderId:, CreationTimestamp2023-04-27T06:30:11.602Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:17,993 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #11 closed for pipeline=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24
2023-04-27 06:30:17,993 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #11, current state: CLOSING
2023-04-27 06:30:17,993 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 7a77590e-48e7-4a17-b391-b680b8534a24, Nodes: 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:OPEN, leaderId:, CreationTimestamp2023-04-27T06:30:12.203Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:18,025 [Listener at 127.0.0.1/43855] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@5fbc7b54{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/meta/webserver/jetty-0_0_0_0-39005-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-6735643064869914312/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:18,029 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:18,029 [Listener at 127.0.0.1/43855] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@774a1373{HTTP/1.1, (http/1.1)}{0.0.0.0:39005}
2023-04-27 06:30:18,029 [Listener at 127.0.0.1/43855] INFO  server.Server (Server.java:doStart(415)) - Started @144477ms
2023-04-27 06:30:18,029 [Listener at 127.0.0.1/43855] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:18,030 [Listener at 127.0.0.1/43855] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:39005
2023-04-27 06:30:18,030 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Waiting for nodes to be ready. Got 1 of 7 DN Heartbeats.
2023-04-27 06:30:18,030 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:18,030 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:18,031 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:18,034 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@33e42c9] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:18,038 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/meta/datanode.id
2023-04-27 06:30:18,232 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-17544770-539a-4c76-9ade-a6b9b4b4bc77/container.db to cache
2023-04-27 06:30:18,232 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-17544770-539a-4c76-9ade-a6b9b4b4bc77/container.db for volume DS-17544770-539a-4c76-9ade-a6b9b4b4bc77
2023-04-27 06:30:18,233 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:18,233 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:18,233 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 43409
2023-04-27 06:30:18,236 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3
2023-04-27 06:30:18,238 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: start RPC server
2023-04-27 06:30:18,239 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: GrpcService started, listening on 41035
2023-04-27 06:30:18,239 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3 is started using port 41035 for RATIS
2023-04-27 06:30:18,239 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3 is started using port 41035 for RATIS_ADMIN
2023-04-27 06:30:18,239 [JvmPauseMonitor44] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: Started
2023-04-27 06:30:18,239 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3 is started using port 41035 for RATIS_SERVER
2023-04-27 06:30:18,239 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3 is started using port 39055 for RATIS_DATASTREAM
2023-04-27 06:30:18,240 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3 is started using port 39131
2023-04-27 06:30:18,241 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:18,412 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:18,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:forceCloseContainer(1080)) - Force closing container #1 with BCSID 2, which is in QUASI_CLOSED state.
2023-04-27 06:30:18,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33).
2023-04-27 06:30:18,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33).
2023-04-27 06:30:18,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:forceCloseContainer(1080)) - Force closing container #3 with BCSID 6, which is in QUASI_CLOSED state.
2023-04-27 06:30:18,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33).
2023-04-27 06:30:18,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #3 to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33).
2023-04-27 06:30:18,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:forceCloseContainer(1080)) - Force closing container #4 with BCSID 10, which is in QUASI_CLOSED state.
2023-04-27 06:30:18,437 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33).
2023-04-27 06:30:18,438 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #4 to datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33).
2023-04-27 06:30:18,438 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:18,438 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578788438 and scm deadline 1682578818438
2023-04-27 06:30:18,439 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578788439 and scm deadline 1682578818439
2023-04-27 06:30:18,439 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578788439 and scm deadline 1682578818439
2023-04-27 06:30:18,439 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578788439 and scm deadline 1682578818439
2023-04-27 06:30:18,439 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) with datanode deadline 1682578788439 and scm deadline 1682578818439
2023-04-27 06:30:18,439 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578788439 and scm deadline 1682578818439
2023-04-27 06:30:18,439 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578788439 and scm deadline 1682578818439
2023-04-27 06:30:18,439 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) with datanode deadline 1682578788439 and scm deadline 1682578818439
2023-04-27 06:30:18,439 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578788439 and scm deadline 1682578818439
2023-04-27 06:30:18,439 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578788439 and scm deadline 1682578818439
2023-04-27 06:30:18,439 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578788439 and scm deadline 1682578818439
2023-04-27 06:30:18,440 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578788439 and scm deadline 1682578818439
2023-04-27 06:30:18,440 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578788440 and scm deadline 1682578818440
2023-04-27 06:30:18,440 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578788440 and scm deadline 1682578818440
2023-04-27 06:30:18,440 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578788440 and scm deadline 1682578818440
2023-04-27 06:30:18,440 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) with datanode deadline 1682578788440 and scm deadline 1682578818440
2023-04-27 06:30:18,440 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578788440 and scm deadline 1682578818440
2023-04-27 06:30:18,440 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578788440 and scm deadline 1682578818440
2023-04-27 06:30:18,440 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578788440 and scm deadline 1682578818440
2023-04-27 06:30:18,442 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578788442 and scm deadline 1682578818442
2023-04-27 06:30:18,442 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578788442 and scm deadline 1682578818442
2023-04-27 06:30:18,442 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) with datanode deadline 1682578788442 and scm deadline 1682578818442
2023-04-27 06:30:18,442 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578788442 and scm deadline 1682578818442
2023-04-27 06:30:18,443 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578788443 and scm deadline 1682578818443
2023-04-27 06:30:18,443 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578788443 and scm deadline 1682578818443
2023-04-27 06:30:18,443 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 5 milliseconds for processing 11 containers.
2023-04-27 06:30:18,488 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) moved to stale state. Finalizing its pipelines [PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, PipelineID=fc22a943-ba2e-46ef-a412-943475173222, PipelineID=bb0486ea-5b79-457f-a4db-03d650acd99d, PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, PipelineID=1a69e094-d42e-467a-ae88-914faf6cf0ef, PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24]
2023-04-27 06:30:18,489 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #2 closed for pipeline=PipelineID=bb0486ea-5b79-457f-a4db-03d650acd99d
2023-04-27 06:30:18,489 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #5 closed for pipeline=PipelineID=bb0486ea-5b79-457f-a4db-03d650acd99d
2023-04-27 06:30:18,489 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #6 closed for pipeline=PipelineID=bb0486ea-5b79-457f-a4db-03d650acd99d
2023-04-27 06:30:18,489 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: bb0486ea-5b79-457f-a4db-03d650acd99d, Nodes: 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:0e67dfdf-fb52-47b7-aad6-2fa818fce5ce, CreationTimestamp2023-04-27T06:29:01.477Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:18,490 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 1a69e094-d42e-467a-ae88-914faf6cf0ef, Nodes: 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:0e67dfdf-fb52-47b7-aad6-2fa818fce5ce, CreationTimestamp2023-04-27T06:29:00.905Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:18,490 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #2, current state: CLOSING
2023-04-27 06:30:18,490 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #5, current state: CLOSING
2023-04-27 06:30:18,490 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(89)) - Close container Event triggered for container : #6, current state: CLOSING
2023-04-27 06:30:18,730 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-be396f20-53f7-4816-97c1-44f3405d8327/container.db to cache
2023-04-27 06:30:18,730 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-be396f20-53f7-4816-97c1-44f3405d8327/container.db for volume DS-be396f20-53f7-4816-97c1-44f3405d8327
2023-04-27 06:30:18,731 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:18,731 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:18,731 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 38685
2023-04-27 06:30:18,731 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis 3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:18,737 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - 3804776e-bffa-42a7-addb-662dc325de80: start RPC server
2023-04-27 06:30:18,737 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - 3804776e-bffa-42a7-addb-662dc325de80: GrpcService started, listening on 46607
2023-04-27 06:30:18,737 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 3804776e-bffa-42a7-addb-662dc325de80 is started using port 46607 for RATIS
2023-04-27 06:30:18,737 [JvmPauseMonitor45] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-3804776e-bffa-42a7-addb-662dc325de80: Started
2023-04-27 06:30:18,737 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 3804776e-bffa-42a7-addb-662dc325de80 is started using port 46607 for RATIS_ADMIN
2023-04-27 06:30:18,737 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 3804776e-bffa-42a7-addb-662dc325de80 is started using port 46607 for RATIS_SERVER
2023-04-27 06:30:18,737 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 3804776e-bffa-42a7-addb-662dc325de80 is started using port 45163 for RATIS_DATASTREAM
2023-04-27 06:30:18,738 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc 3804776e-bffa-42a7-addb-662dc325de80 is started using port 37537
2023-04-27 06:30:18,739 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:18,889 [IPC Server handler 9 on default port 38893] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/e5e627c9-cddd-4fde-83a7-403d4c1ca45c
2023-04-27 06:30:18,889 [IPC Server handler 9 on default port 38893] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : e5e627c9-cddd-4fde-83a7-403d4c1ca45c{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=41531, REPLICATION=40489, RATIS=33449, RATIS_ADMIN=33449, RATIS_SERVER=33449, RATIS_DATASTREAM=43189, STANDALONE=39667], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:18,891 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (DataNodeSafeModeRule.java:process(71)) - SCM in safe mode. 2 DataNodes registered, 3 required.
2023-04-27 06:30:18,891 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:18,892 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=08f3233d-81d6-42ac-8cfc-cb9e9b7e8b9e to datanode:e5e627c9-cddd-4fde-83a7-403d4c1ca45c
2023-04-27 06:30:18,892 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 08f3233d-81d6-42ac-8cfc-cb9e9b7e8b9e, Nodes: e5e627c9-cddd-4fde-83a7-403d4c1ca45c(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:18.892Z[Etc/UTC]].
2023-04-27 06:30:19,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: QUASI_CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=QUASI_CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=QUASI_CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:19,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: QUASI_CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=QUASI_CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=QUASI_CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:19,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #3 Container State: QUASI_CLOSED Replica Count: 3 Healthy Count: 1 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#3, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#3, state=QUASI_CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=QUASI_CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=6, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:19,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #3 Container State: QUASI_CLOSED Replica Count: 3 Healthy Count: 1 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#3, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#3, state=QUASI_CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=QUASI_CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=6, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:19,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #4 Container State: QUASI_CLOSED Replica Count: 3 Healthy Count: 1 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=QUASI_CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=QUASI_CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:19,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #4 Container State: QUASI_CLOSED Replica Count: 3 Healthy Count: 1 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=QUASI_CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=QUASI_CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:19,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 0 sufficientlyReplicated, 3 underReplicated and 3 unhealthy containers
2023-04-27 06:30:19,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:19,031 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Waiting for nodes to be ready. Got 2 of 7 DN Heartbeats.
2023-04-27 06:30:19,031 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:19,031 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:19,096 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-a4b65777-370f-43fb-b519-5025235142df/container.db to cache
2023-04-27 06:30:19,096 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-a4b65777-370f-43fb-b519-5025235142df/container.db for volume DS-a4b65777-370f-43fb-b519-5025235142df
2023-04-27 06:30:19,096 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:19,096 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:19,099 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 42159
2023-04-27 06:30:19,099 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e
2023-04-27 06:30:19,106 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: start RPC server
2023-04-27 06:30:19,110 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: GrpcService started, listening on 45409
2023-04-27 06:30:19,111 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e is started using port 45409 for RATIS
2023-04-27 06:30:19,111 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e is started using port 45409 for RATIS_ADMIN
2023-04-27 06:30:19,111 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e is started using port 45409 for RATIS_SERVER
2023-04-27 06:30:19,111 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e is started using port 46313 for RATIS_DATASTREAM
2023-04-27 06:30:19,111 [JvmPauseMonitor46] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: Started
2023-04-27 06:30:19,112 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e is started using port 37937
2023-04-27 06:30:19,113 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:19,282 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 2.
2023-04-27 06:30:19,283 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 2.
2023-04-27 06:30:19,285 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 1 is closed with bcsId 2.
2023-04-27 06:30:19,286 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 6.
2023-04-27 06:30:19,286 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 6.
2023-04-27 06:30:19,287 [FixedThreadPoolWithAffinityExecutor-9-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(318)) - Moving container #1 to CLOSED state, datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) reported CLOSED replica.
2023-04-27 06:30:19,288 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 3 is closed with bcsId 6.
2023-04-27 06:30:19,288 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 10.
2023-04-27 06:30:19,288 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 10.
2023-04-27 06:30:19,290 [FixedThreadPoolWithAffinityExecutor-9-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(318)) - Moving container #3 to CLOSED state, datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) reported CLOSED replica.
2023-04-27 06:30:19,291 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 4 is closed with bcsId 10.
2023-04-27 06:30:19,293 [FixedThreadPoolWithAffinityExecutor-9-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(318)) - Moving container #4 to CLOSED state, datanode 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) reported CLOSED replica.
2023-04-27 06:30:19,315 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 2.
2023-04-27 06:30:19,315 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 1 is synced with bcsId 2.
2023-04-27 06:30:19,320 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 1 is closed with bcsId 2.
2023-04-27 06:30:19,321 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 6.
2023-04-27 06:30:19,321 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 3 is synced with bcsId 6.
2023-04-27 06:30:19,324 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 3 is closed with bcsId 6.
2023-04-27 06:30:19,324 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 10.
2023-04-27 06:30:19,324 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 4 is synced with bcsId 10.
2023-04-27 06:30:19,326 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 4 is closed with bcsId 10.
2023-04-27 06:30:19,413 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:19,438 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:19,438 [ReplicationMonitor] WARN  net.NetworkTopologyImpl (NetworkTopologyImpl.java:chooseNodeInternal(653)) - No available node in (scope="/" excludedScope="null" excludedNodes="[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)]"  ancestorGen="1").
2023-04-27 06:30:19,438 [ReplicationMonitor] WARN  algorithms.SCMContainerPlacementRackAware (SCMContainerPlacementRackAware.java:chooseNode(282)) - Failed to find the datanode for container. excludedNodes:[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], affinityNode:
2023-04-27 06:30:19,438 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #3 is under replicated. Expected replica count is 3, but found 2.
2023-04-27 06:30:19,439 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=3, replicaIndex=0, sourceNodes=[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], priority=NORMAL to a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33)
2023-04-27 06:30:19,439 [ReplicationMonitor] WARN  net.NetworkTopologyImpl (NetworkTopologyImpl.java:chooseNodeInternal(653)) - No available node in (scope="/" excludedScope="null" excludedNodes="[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)]"  ancestorGen="1").
2023-04-27 06:30:19,439 [ReplicationMonitor] WARN  algorithms.SCMContainerPlacementRackAware (SCMContainerPlacementRackAware.java:chooseNode(282)) - Failed to find the datanode for container. excludedNodes:[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], affinityNode:
2023-04-27 06:30:19,439 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #4 is under replicated. Expected replica count is 3, but found 2.
2023-04-27 06:30:19,439 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=4, replicaIndex=0, sourceNodes=[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], priority=NORMAL to b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33)
2023-04-27 06:30:19,439 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:19,443 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #2 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:19,443 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #2 to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33).
2023-04-27 06:30:19,443 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #2 to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33).
2023-04-27 06:30:19,443 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33).
2023-04-27 06:30:19,443 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33).
2023-04-27 06:30:19,443 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:19,444 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:19,444 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33).
2023-04-27 06:30:19,444 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33).
2023-04-27 06:30:19,444 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578789444 and scm deadline 1682578819444
2023-04-27 06:30:19,444 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578789444 and scm deadline 1682578819444
2023-04-27 06:30:19,444 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578789444 and scm deadline 1682578819444
2023-04-27 06:30:19,444 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578789444 and scm deadline 1682578819444
2023-04-27 06:30:19,444 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) with datanode deadline 1682578789444 and scm deadline 1682578819444
2023-04-27 06:30:19,444 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578789444 and scm deadline 1682578819444
2023-04-27 06:30:19,444 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578789444 and scm deadline 1682578819444
2023-04-27 06:30:19,444 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) with datanode deadline 1682578789444 and scm deadline 1682578819444
2023-04-27 06:30:19,444 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578789444 and scm deadline 1682578819444
2023-04-27 06:30:19,445 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578789444 and scm deadline 1682578819444
2023-04-27 06:30:19,445 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578789445 and scm deadline 1682578819445
2023-04-27 06:30:19,445 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578789445 and scm deadline 1682578819445
2023-04-27 06:30:19,445 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578789445 and scm deadline 1682578819445
2023-04-27 06:30:19,445 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578789445 and scm deadline 1682578819445
2023-04-27 06:30:19,445 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578789445 and scm deadline 1682578819445
2023-04-27 06:30:19,445 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) with datanode deadline 1682578789445 and scm deadline 1682578819445
2023-04-27 06:30:19,445 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578789445 and scm deadline 1682578819445
2023-04-27 06:30:19,445 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578789445 and scm deadline 1682578819445
2023-04-27 06:30:19,445 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578789445 and scm deadline 1682578819445
2023-04-27 06:30:19,446 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578789445 and scm deadline 1682578819445
2023-04-27 06:30:19,446 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578789446 and scm deadline 1682578819446
2023-04-27 06:30:19,447 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) with datanode deadline 1682578789447 and scm deadline 1682578819447
2023-04-27 06:30:19,447 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578789447 and scm deadline 1682578819447
2023-04-27 06:30:19,447 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578789447 and scm deadline 1682578819447
2023-04-27 06:30:19,447 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578789447 and scm deadline 1682578819447
2023-04-27 06:30:19,447 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 4 milliseconds for processing 11 containers.
2023-04-27 06:30:19,609 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-0bd0e29d-271d-4dc0-b196-f6ae22d1625e/container.db to cache
2023-04-27 06:30:19,609 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-0bd0e29d-271d-4dc0-b196-f6ae22d1625e/container.db for volume DS-0bd0e29d-271d-4dc0-b196-f6ae22d1625e
2023-04-27 06:30:19,609 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:19,609 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:19,610 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 39623
2023-04-27 06:30:19,610 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis efdbc657-ca6b-4fea-9dc1-5411634f3e98
2023-04-27 06:30:19,614 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: start RPC server
2023-04-27 06:30:19,614 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: GrpcService started, listening on 44129
2023-04-27 06:30:19,614 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis efdbc657-ca6b-4fea-9dc1-5411634f3e98 is started using port 44129 for RATIS
2023-04-27 06:30:19,614 [JvmPauseMonitor47] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-efdbc657-ca6b-4fea-9dc1-5411634f3e98: Started
2023-04-27 06:30:19,614 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis efdbc657-ca6b-4fea-9dc1-5411634f3e98 is started using port 44129 for RATIS_ADMIN
2023-04-27 06:30:19,616 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis efdbc657-ca6b-4fea-9dc1-5411634f3e98 is started using port 44129 for RATIS_SERVER
2023-04-27 06:30:19,616 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis efdbc657-ca6b-4fea-9dc1-5411634f3e98 is started using port 38671 for RATIS_DATASTREAM
2023-04-27 06:30:19,616 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc efdbc657-ca6b-4fea-9dc1-5411634f3e98 is started using port 37667
2023-04-27 06:30:19,617 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:19,741 [ForkJoinPool.commonPool-worker-0] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-2/data-0/containers/hdds/96a9b0ac-a466-4378-aad7-6952c9070703/DS-e5e0b7f9-af19-4d44-ac3e-63c251e05a1d/container.db for volume DS-e5e0b7f9-af19-4d44-ac3e-63c251e05a1d
2023-04-27 06:30:19,742 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:19,742 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:19,746 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
2023-04-27 06:30:19,763 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@64f436d8{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:19,764 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@3fd75065{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:19,764 [ForkJoinPool.commonPool-worker-0] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:19,764 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@3d9361cf{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:19,764 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@5a0b0907{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:19,768 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:19,779 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d: close
2023-04-27 06:30:19,779 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:19,783 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:19,783 [220eace7-05d2-4cc3-8ea2-3a8f6657333d-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51: shutdown
2023-04-27 06:30:19,783 [220eace7-05d2-4cc3-8ea2-3a8f6657333d-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x5e6aeaf2, L:/0:0:0:0:0:0:0:0:40337] CLOSE
2023-04-27 06:30:19,783 [220eace7-05d2-4cc3-8ea2-3a8f6657333d-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-364E7F2FCE51,id=220eace7-05d2-4cc3-8ea2-3a8f6657333d
2023-04-27 06:30:19,783 [220eace7-05d2-4cc3-8ea2-3a8f6657333d-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d: shutdown 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51-LeaderStateImpl
2023-04-27 06:30:19,784 [220eace7-05d2-4cc3-8ea2-3a8f6657333d-impl-thread2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:19,783 [220eace7-05d2-4cc3-8ea2-3a8f6657333d-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x5e6aeaf2, L:/0:0:0:0:0:0:0:0:40337] INACTIVE
2023-04-27 06:30:19,784 [220eace7-05d2-4cc3-8ea2-3a8f6657333d-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x5e6aeaf2, L:/0:0:0:0:0:0:0:0:40337] UNREGISTERED
2023-04-27 06:30:19,784 [220eace7-05d2-4cc3-8ea2-3a8f6657333d-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:19,785 [220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-364E7F2FCE51: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-0/data/ratis/5d215173-4283-4f76-b761-364e7f2fce51/sm/snapshot.1_0
2023-04-27 06:30:19,787 [220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-364E7F2FCE51: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-0/data/ratis/5d215173-4283-4f76-b761-364e7f2fce51/sm/snapshot.1_0 took: 2 ms
2023-04-27 06:30:19,787 [220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:19,787 [220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:19,787 [220eace7-05d2-4cc3-8ea2-3a8f6657333d-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51: closes. applyIndex: 0
2023-04-27 06:30:19,788 [220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:19,788 [220eace7-05d2-4cc3-8ea2-3a8f6657333d-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 220eace7-05d2-4cc3-8ea2-3a8f6657333d@group-364E7F2FCE51-SegmentedRaftLogWorker close()
2023-04-27 06:30:19,791 [JvmPauseMonitor26] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-220eace7-05d2-4cc3-8ea2-3a8f6657333d: Stopped
2023-04-27 06:30:19,857 [Mini-Cluster-Provider-Reap] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-3/data-0/containers/hdds/96a9b0ac-a466-4378-aad7-6952c9070703/DS-4a9d4c83-7718-4c89-b147-a7f785414de8/container.db for volume DS-4a9d4c83-7718-4c89-b147-a7f785414de8
2023-04-27 06:30:19,857 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:19,858 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:19,868 [Mini-Cluster-Provider-Reap] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
2023-04-27 06:30:19,882 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@71018383{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:19,882 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@727fff61{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:19,882 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:19,882 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@4748880f{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:19,883 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@7e4d168d{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:19,887 [Mini-Cluster-Provider-Reap] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:19,887 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - 8ee12707-a777-4421-9344-714c0bb69310: close
2023-04-27 06:30:19,889 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - 8ee12707-a777-4421-9344-714c0bb69310: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:19,889 [8ee12707-a777-4421-9344-714c0bb69310-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E: shutdown
2023-04-27 06:30:19,889 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - 8ee12707-a777-4421-9344-714c0bb69310: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:19,889 [8ee12707-a777-4421-9344-714c0bb69310-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-83B16DCF869E,id=8ee12707-a777-4421-9344-714c0bb69310
2023-04-27 06:30:19,890 [8ee12707-a777-4421-9344-714c0bb69310-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x9f55a9e9, L:/0:0:0:0:0:0:0:0:42285] CLOSE
2023-04-27 06:30:19,890 [8ee12707-a777-4421-9344-714c0bb69310-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 8ee12707-a777-4421-9344-714c0bb69310: shutdown 8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E-LeaderStateImpl
2023-04-27 06:30:19,890 [8ee12707-a777-4421-9344-714c0bb69310-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x9f55a9e9, L:/0:0:0:0:0:0:0:0:42285] INACTIVE
2023-04-27 06:30:19,890 [8ee12707-a777-4421-9344-714c0bb69310-impl-thread2] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:19,890 [8ee12707-a777-4421-9344-714c0bb69310-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x9f55a9e9, L:/0:0:0:0:0:0:0:0:42285] UNREGISTERED
2023-04-27 06:30:19,891 [8ee12707-a777-4421-9344-714c0bb69310-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:19,891 [8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-83B16DCF869E: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-6/data/ratis/782195fe-43fe-41c4-aa0a-83b16dcf869e/sm/snapshot.1_0
2023-04-27 06:30:19,893 [8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-83B16DCF869E: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-6/data/ratis/782195fe-43fe-41c4-aa0a-83b16dcf869e/sm/snapshot.1_0 took: 2 ms
2023-04-27 06:30:19,894 [8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:19,894 [8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:19,894 [8ee12707-a777-4421-9344-714c0bb69310-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E: closes. applyIndex: 0
2023-04-27 06:30:19,896 [8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:19,896 [8ee12707-a777-4421-9344-714c0bb69310-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 8ee12707-a777-4421-9344-714c0bb69310@group-83B16DCF869E-SegmentedRaftLogWorker close()
2023-04-27 06:30:19,898 [JvmPauseMonitor32] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-8ee12707-a777-4421-9344-714c0bb69310: Stopped
2023-04-27 06:30:20,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:20,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:20,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #3 Container State: CLOSED Replica Count: 3 Healthy Count: 1 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#3, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=6, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:20,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #3 Container State: CLOSED Replica Count: 3 Healthy Count: 1 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#3, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=6, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:20,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #4 Container State: CLOSED Replica Count: 3 Healthy Count: 1 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:20,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #4 Container State: CLOSED Replica Count: 3 Healthy Count: 1 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:20,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 0 sufficientlyReplicated, 3 underReplicated and 3 unhealthy containers
2023-04-27 06:30:20,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:20,031 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Waiting for nodes to be ready. Got 2 of 7 DN Heartbeats.
2023-04-27 06:30:20,031 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:20,031 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:20,085 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-141b9038-1532-4175-b419-73571d6c2d55/container.db to cache
2023-04-27 06:30:20,085 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data-0/containers/hdds/bd3209a3-83fe-44cb-83c2-665bdcf7ddee/DS-141b9038-1532-4175-b419-73571d6c2d55/container.db for volume DS-141b9038-1532-4175-b419-73571d6c2d55
2023-04-27 06:30:20,086 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:20,086 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:20,086 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 41121
2023-04-27 06:30:20,086 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis f88c54b2-8776-4954-8dfb-f8bea6a862a6
2023-04-27 06:30:20,090 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6: start RPC server
2023-04-27 06:30:20,090 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6: GrpcService started, listening on 38947
2023-04-27 06:30:20,091 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis f88c54b2-8776-4954-8dfb-f8bea6a862a6 is started using port 38947 for RATIS
2023-04-27 06:30:20,091 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis f88c54b2-8776-4954-8dfb-f8bea6a862a6 is started using port 38947 for RATIS_ADMIN
2023-04-27 06:30:20,091 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis f88c54b2-8776-4954-8dfb-f8bea6a862a6 is started using port 38947 for RATIS_SERVER
2023-04-27 06:30:20,091 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis f88c54b2-8776-4954-8dfb-f8bea6a862a6 is started using port 46011 for RATIS_DATASTREAM
2023-04-27 06:30:20,091 [JvmPauseMonitor48] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-f88c54b2-8776-4954-8dfb-f8bea6a862a6: Started
2023-04-27 06:30:20,092 [EndpointStateMachine task thread for /0.0.0.0:38893 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc f88c54b2-8776-4954-8dfb-f8bea6a862a6 is started using port 33551
2023-04-27 06:30:20,093 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:20,192 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) moved to stale state. Finalizing its pipelines [PipelineID=b2ba4b9e-5d53-4979-8d36-c898faf9593d, PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24]
2023-04-27 06:30:20,193 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: b2ba4b9e-5d53-4979-8d36-c898faf9593d, Nodes: 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:2b022b8d-8bf4-4c13-9ad2-4e7e18903189, CreationTimestamp2023-04-27T06:28:59.845Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:20,194 [IPC Server handler 14 on default port 38893] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/109e7eb5-e5f2-4565-a541-6a0b1e1f79c3
2023-04-27 06:30:20,194 [IPC Server handler 14 on default port 38893] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3{ip: 10.1.0.33, host: fv-az260-775, ports: [REPLICATION=43409, RATIS=41035, RATIS_ADMIN=41035, RATIS_SERVER=41035, RATIS_DATASTREAM=39055, STANDALONE=39131], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:20,198 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:20,198 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=b0ff91ae-07f7-40d4-aaaf-926679737466 to datanode:109e7eb5-e5f2-4565-a541-6a0b1e1f79c3
2023-04-27 06:30:20,198 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: b0ff91ae-07f7-40d4-aaaf-926679737466, Nodes: 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:20.198Z[Etc/UTC]].
2023-04-27 06:30:20,198 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (DataNodeSafeModeRule.java:process(71)) - SCM in safe mode. 3 DataNodes registered, 3 required.
2023-04-27 06:30:20,199 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(200)) - DataNodeSafeModeRule rule is successfully validated
2023-04-27 06:30:20,199 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:completePreCheck(229)) - All SCM safe mode pre check rules have passed
2023-04-27 06:30:20,199 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  ha.SCMContext (SCMContext.java:updateSafeModeStatus(228)) - Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
2023-04-27 06:30:20,199 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:20,200 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=7f18b1ba-62bd-48bf-be88-d6a15093a233 to datanode:2cb742ca-d762-4bc5-b311-f495a87c6b6b
2023-04-27 06:30:20,200 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=7f18b1ba-62bd-48bf-be88-d6a15093a233 to datanode:109e7eb5-e5f2-4565-a541-6a0b1e1f79c3
2023-04-27 06:30:20,200 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=7f18b1ba-62bd-48bf-be88-d6a15093a233 to datanode:e5e627c9-cddd-4fde-83a7-403d4c1ca45c
2023-04-27 06:30:20,200 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 7f18b1ba-62bd-48bf-be88-d6a15093a233, Nodes: 2cb742ca-d762-4bc5-b311-f495a87c6b6b(fv-az260-775/10.1.0.33)109e7eb5-e5f2-4565-a541-6a0b1e1f79c3(fv-az260-775/10.1.0.33)e5e627c9-cddd-4fde-83a7-403d4c1ca45c(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:20.200Z[Etc/UTC]].
2023-04-27 06:30:20,201 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(160)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
2023-04-27 06:30:20,393 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) moved to stale state. Finalizing its pipelines [PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, PipelineID=fc22a943-ba2e-46ef-a412-943475173222, PipelineID=bb0486ea-5b79-457f-a4db-03d650acd99d, PipelineID=057b00eb-9ef1-46bf-b442-47e71c502c0c, PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24]
2023-04-27 06:30:20,394 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 057b00eb-9ef1-46bf-b442-47e71c502c0c, Nodes: d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d129cee7-7074-498b-a1c7-3e6cb07a0899, CreationTimestamp2023-04-27T06:29:00.902Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:20,414 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:20,443 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:20,443 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:20,447 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #2 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:20,448 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #2 to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33).
2023-04-27 06:30:20,448 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #2 to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33).
2023-04-27 06:30:20,448 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33).
2023-04-27 06:30:20,448 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33).
2023-04-27 06:30:20,448 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:20,448 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:20,448 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33).
2023-04-27 06:30:20,448 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33).
2023-04-27 06:30:20,448 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578790448 and scm deadline 1682578820448
2023-04-27 06:30:20,448 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578790448 and scm deadline 1682578820448
2023-04-27 06:30:20,448 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578790448 and scm deadline 1682578820448
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSING, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSING, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578790449 and scm deadline 1682578820449
2023-04-27 06:30:20,450 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578790450 and scm deadline 1682578820450
2023-04-27 06:30:20,450 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578790450 and scm deadline 1682578820450
2023-04-27 06:30:20,450 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSING, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578790450 and scm deadline 1682578820450
2023-04-27 06:30:20,450 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578790450 and scm deadline 1682578820450
2023-04-27 06:30:20,450 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) with datanode deadline 1682578790450 and scm deadline 1682578820450
2023-04-27 06:30:20,450 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578790450 and scm deadline 1682578820450
2023-04-27 06:30:20,450 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578790450 and scm deadline 1682578820450
2023-04-27 06:30:20,450 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSING, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33) with datanode deadline 1682578790450 and scm deadline 1682578820450
2023-04-27 06:30:20,450 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 3 milliseconds for processing 11 containers.
2023-04-27 06:30:20,460 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(73)) - Starting replication of container 3 from [70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)] using NO_COMPRESSION
2023-04-27 06:30:20,467 [grpc-default-executor-3] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(62)) - Streaming container data (3) to other datanode with compression NO_COMPRESSION
2023-04-27 06:30:20,503 [grpc-default-executor-3] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(111)) - Sent 9728 bytes for container 3
2023-04-27 06:30:20,506 [grpc-default-executor-1] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(218)) - Container 3 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-3/data-0/containers/tmp/container-copy/container-3.tar
2023-04-27 06:30:20,506 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(88)) - Container 3 is downloaded with size 9728, starting to import.
2023-04-27 06:30:20,568 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(95)) - Container 3 is replicated successfully
2023-04-27 06:30:20,568 [ContainerReplicationThread-0] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(220)) - Successful DONE replicateContainerCommand: containerId=3, replicaIndex=0, sourceNodes=[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], priority=NORMAL, transferred 9728 bytes
2023-04-27 06:30:20,676 [IPC Server handler 8 on default port 38893] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:20,676 [IPC Server handler 8 on default port 38893] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : 3804776e-bffa-42a7-addb-662dc325de80{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=41095, REPLICATION=38685, RATIS=46607, RATIS_ADMIN=46607, RATIS_SERVER=46607, RATIS_DATASTREAM=45163, STANDALONE=37537], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:20,676 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:20,677 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=6bf5f405-714f-49bb-b293-6fc4b564c533 to datanode:3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:20,677 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 6bf5f405-714f-49bb-b293-6fc4b564c533, Nodes: 3804776e-bffa-42a7-addb-662dc325de80(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:20.677Z[Etc/UTC]].
2023-04-27 06:30:20,677 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(160)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 3.
2023-04-27 06:30:20,726 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5895983737ns, electionTimeout:5125ms
2023-04-27 06:30:20,727 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58: shutdown 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-FollowerState
2023-04-27 06:30:20,727 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
2023-04-27 06:30:20,727 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:20,727 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58: start 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74
2023-04-27 06:30:20,727 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D: change Leader from 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce to null at term 1 for PRE_VOTE
2023-04-27 06:30:20,727 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[d129cee7-7074-498b-a1c7-3e6cb07a0899|rpc:10.1.0.33:33147|dataStream:10.1.0.33:41377|priority:0|startupRole:FOLLOWER, 63017b46-a8f7-4bfc-aab4-ba20a30a5f58|rpc:10.1.0.33:43641|dataStream:10.1.0.33:42701|priority:0|startupRole:FOLLOWER, 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce|rpc:10.1.0.33:38137|dataStream:10.1.0.33:32911|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:20,730 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74] INFO  impl.LeaderElection (LogUtils.java:infoOrTrace(137)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:20,730 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74] INFO  impl.LeaderElection (LogUtils.java:infoOrTrace(137)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:20,730 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
2023-04-27 06:30:20,730 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74] INFO  impl.LeaderElection (LogUtils.java:infoOrTrace(137)) -   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:20,730 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74] INFO  impl.LeaderElection (LogUtils.java:infoOrTrace(137)) -   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2023-04-27 06:30:20,730 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74 PRE_VOTE round 0: result REJECTED
2023-04-27 06:30:20,730 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
2023-04-27 06:30:20,731 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58: shutdown 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74
2023-04-27 06:30:20,731 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-LeaderElection74] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58: start 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-FollowerState
2023-04-27 06:30:20,846 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: addNew group-E8B5BA640BDF:[2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:1|startupRole:FOLLOWER] returns group-E8B5BA640BDF:java.util.concurrent.CompletableFuture@24be6263[Not completed]
2023-04-27 06:30:20,847 [pool-2046-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: new RaftServerImpl for group-E8B5BA640BDF:[2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:20,847 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:20,847 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:20,847 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:20,847 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:20,847 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:20,847 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:20,847 [pool-2046-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF: ConfigurationManager, init=-1: peers:[2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:20,847 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis] (custom)
2023-04-27 06:30:20,847 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:20,847 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:20,847 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:20,848 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:20,848 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:20,849 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:20,849 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:20,849 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:20,849 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:20,849 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:20,849 [pool-2046-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis/9f565819-4a1c-445f-b823-e8b5ba640bdf does not exist. Creating ...
2023-04-27 06:30:20,851 [pool-2046-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis/9f565819-4a1c-445f-b823-e8b5ba640bdf/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:20,854 [pool-2046-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis/9f565819-4a1c-445f-b823-e8b5ba640bdf has been successfully formatted.
2023-04-27 06:30:20,855 [pool-2046-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-E8B5BA640BDF: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:20,855 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:20,855 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:20,856 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:20,856 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:20,856 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:20,856 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 9f565819-4a1c-445f-b823-e8b5ba640bdf, Nodes: 2cb742ca-d762-4bc5-b311-f495a87c6b6b(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:2cb742ca-d762-4bc5-b311-f495a87c6b6b, CreationTimestamp2023-04-27T06:30:17.861Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:20,861 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:20,862 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:20,862 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:20,862 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:20,862 [pool-2046-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis/9f565819-4a1c-445f-b823-e8b5ba640bdf
2023-04-27 06:30:20,862 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:20,862 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:20,862 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:20,862 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:20,862 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:20,862 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:20,862 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:20,863 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:20,864 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:20,864 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:20,870 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:20,870 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:20,870 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:20,870 [pool-2046-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:20,870 [pool-2046-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:20,870 [pool-2046-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF: start as a follower, conf=-1: peers:[2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:20,870 [pool-2046-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:20,870 [pool-2046-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: start 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-FollowerState
2023-04-27 06:30:20,871 [pool-2046-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E8B5BA640BDF,id=2cb742ca-d762-4bc5-b311-f495a87c6b6b
2023-04-27 06:30:20,871 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:20,871 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:20,871 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:20,871 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:20,871 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:20,871 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:20,872 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=9f565819-4a1c-445f-b823-e8b5ba640bdf
2023-04-27 06:30:20,872 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=9f565819-4a1c-445f-b823-e8b5ba640bdf.
2023-04-27 06:30:20,872 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: addNew group-D6A15093A233:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER] returns group-D6A15093A233:java.util.concurrent.CompletableFuture@2081e22c[Not completed]
2023-04-27 06:30:20,877 [pool-2046-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: new RaftServerImpl for group-D6A15093A233:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:20,877 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:20,878 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:20,878 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:20,878 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:20,878 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:20,878 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:20,878 [pool-2046-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233: ConfigurationManager, init=-1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:20,878 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis] (custom)
2023-04-27 06:30:20,878 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:20,878 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:20,878 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:20,878 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:20,878 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:20,879 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:20,880 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:20,880 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:20,880 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:20,880 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:20,880 [pool-2046-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233 does not exist. Creating ...
2023-04-27 06:30:20,881 [pool-2046-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:20,883 [pool-2046-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233 has been successfully formatted.
2023-04-27 06:30:20,883 [pool-2046-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-D6A15093A233: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:20,886 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:20,887 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:20,886 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:20,887 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:20,887 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:20,887 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:20,887 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:20,887 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:20,888 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:20,888 [pool-2046-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233
2023-04-27 06:30:20,888 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:20,888 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:20,888 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:20,888 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:20,888 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:20,888 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:20,888 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:20,888 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:20,889 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:20,889 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:20,895 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:20,896 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:20,896 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:20,896 [pool-2046-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:20,896 [pool-2046-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:20,896 [pool-2046-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233: start as a follower, conf=-1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:20,896 [pool-2046-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:20,897 [pool-2046-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: start 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-FollowerState
2023-04-27 06:30:20,897 [pool-2046-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D6A15093A233,id=2cb742ca-d762-4bc5-b311-f495a87c6b6b
2023-04-27 06:30:20,897 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:20,897 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:20,897 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:20,897 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:20,897 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:20,897 [pool-2046-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:20,898 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=7f18b1ba-62bd-48bf-be88-d6a15093a233
2023-04-27 06:30:20,905 [grpc-default-executor-3] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: addNew group-D6A15093A233:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER] returns group-D6A15093A233:java.util.concurrent.CompletableFuture@133e3abb[Not completed]
2023-04-27 06:30:20,907 [pool-2208-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: new RaftServerImpl for group-D6A15093A233:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:20,907 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:20,907 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:20,907 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:20,907 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:20,907 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:20,907 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:20,907 [pool-2208-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233: ConfigurationManager, init=-1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:20,908 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis] (custom)
2023-04-27 06:30:20,908 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:20,908 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:20,908 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:20,908 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:20,908 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:20,909 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:20,909 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:20,909 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:20,909 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:20,909 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:20,909 [pool-2208-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233 does not exist. Creating ...
2023-04-27 06:30:20,911 [pool-2208-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:20,912 [pool-2208-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233 has been successfully formatted.
2023-04-27 06:30:20,912 [pool-2208-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-D6A15093A233: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:20,913 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:20,913 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:20,913 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:20,913 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:20,913 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:20,913 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:20,914 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:20,914 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:20,914 [pool-2208-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233
2023-04-27 06:30:20,914 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:20,914 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:20,914 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:20,914 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:20,914 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:20,914 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:20,914 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:20,914 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:20,915 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:20,915 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:20,921 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:20,922 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:20,922 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:20,922 [pool-2208-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:20,922 [pool-2208-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:20,929 [pool-2208-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233: start as a follower, conf=-1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:20,929 [pool-2208-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:20,930 [pool-2208-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: start 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-FollowerState
2023-04-27 06:30:20,930 [pool-2208-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D6A15093A233,id=109e7eb5-e5f2-4565-a541-6a0b1e1f79c3
2023-04-27 06:30:20,930 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:20,930 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:20,930 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:20,930 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:20,931 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:20,931 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:20,944 [grpc-default-executor-3] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: addNew group-D6A15093A233:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER] returns group-D6A15093A233:java.util.concurrent.CompletableFuture@5c065774[Not completed]
2023-04-27 06:30:20,945 [pool-2136-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: new RaftServerImpl for group-D6A15093A233:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:20,945 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:20,945 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:20,945 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:20,945 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:20,945 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:20,945 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:20,945 [pool-2136-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233: ConfigurationManager, init=-1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:20,945 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis] (custom)
2023-04-27 06:30:20,946 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:20,946 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:20,946 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:20,946 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:20,946 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:20,948 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:20,948 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:20,948 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:20,948 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:20,948 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:20,948 [pool-2136-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233 does not exist. Creating ...
2023-04-27 06:30:20,949 [pool-2136-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:20,951 [pool-2136-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233 has been successfully formatted.
2023-04-27 06:30:20,952 [pool-2136-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-D6A15093A233: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:20,952 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:20,952 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:20,952 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:20,952 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:20,952 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:20,952 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:20,953 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:20,953 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:20,954 [pool-2136-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233
2023-04-27 06:30:20,954 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:20,954 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:20,954 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:20,954 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:20,954 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:20,954 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:20,954 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:20,954 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:20,956 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:20,956 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:20,963 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:20,963 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:20,963 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:20,963 [pool-2136-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:20,963 [pool-2136-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:20,964 [pool-2136-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233: start as a follower, conf=-1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:20,964 [pool-2136-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:20,964 [pool-2136-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: start e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-FollowerState
2023-04-27 06:30:20,964 [pool-2136-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D6A15093A233,id=e5e627c9-cddd-4fde-83a7-403d4c1ca45c
2023-04-27 06:30:20,964 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:20,965 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:20,965 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:20,965 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:20,965 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:20,965 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:20,969 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS THREE PipelineID=7f18b1ba-62bd-48bf-be88-d6a15093a233.
2023-04-27 06:30:20,994 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(83)) - A dead datanode is detected. a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)
2023-04-27 06:30:20,994 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 37ee7bc9-b834-49c7-bf6a-23376a986332, Nodes: 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2023-04-27T06:30:09.998Z[Etc/UTC]] removed.
2023-04-27 06:30:20,994 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: fc22a943-ba2e-46ef-a412-943475173222, Nodes: d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2023-04-27T06:30:10.665Z[Etc/UTC]] removed.
2023-04-27 06:30:20,995 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=d802d461-95a8-49e9-82b2-fd76db487b8b close command to datanode a05fa3a3-4f42-4e00-9e5b-4e78c9e14083
2023-04-27 06:30:20,995 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: d802d461-95a8-49e9-82b2-fd76db487b8b, Nodes: a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:a05fa3a3-4f42-4e00-9e5b-4e78c9e14083, CreationTimestamp2023-04-27T06:28:59.422Z[Etc/UTC]] removed.
2023-04-27 06:30:20,995 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 3541dabf-3351-48cd-a257-9e51dd0a6317, Nodes: a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33)63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2023-04-27T06:30:11.878Z[Etc/UTC]] removed.
2023-04-27 06:30:20,997 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: e68537c7-dd5e-4934-bd5b-0f78194036e1, Nodes: d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2023-04-27T06:30:11.602Z[Etc/UTC]] removed.
2023-04-27 06:30:20,998 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 7a77590e-48e7-4a17-b391-b680b8534a24, Nodes: 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33), ReplicationConfig: EC{rs-3-2-1024k}, State:CLOSED, leaderId:, CreationTimestamp2023-04-27T06:30:12.203Z[Etc/UTC]] removed.
2023-04-27 06:30:20,998 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(96)) - Clearing command queue of size 22 for DN a05fa3a3-4f42-4e00-9e5b-4e78c9e14083(fv-az260-775/10.1.0.33)
2023-04-27 06:30:20,998 [EventQueue-DeadNodeForDeadNodeHandler] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:remove(190)) - Removed a node: /default-rack/a05fa3a3-4f42-4e00-9e5b-4e78c9e14083
2023-04-27 06:30:21,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:21,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:21,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #3 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#3, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=6, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:21,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #4 Container State: CLOSED Replica Count: 3 Healthy Count: 1 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:21,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #4 Container State: CLOSED Replica Count: 3 Healthy Count: 1 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:21,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 1 sufficientlyReplicated, 2 underReplicated and 3 unhealthy containers
2023-04-27 06:30:21,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:21,032 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Waiting for nodes to be ready. Got 4 of 7 DN Heartbeats.
2023-04-27 06:30:21,032 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:21,032 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:21,056 [IPC Server handler 16 on default port 38893] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e
2023-04-27 06:30:21,056 [IPC Server handler 16 on default port 38893] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=40523, REPLICATION=42159, RATIS=45409, RATIS_ADMIN=45409, RATIS_SERVER=45409, RATIS_DATASTREAM=46313, STANDALONE=37937], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:21,056 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:21,059 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=5084858f-2ede-4f2d-b349-cc149f761a2e to datanode:fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e
2023-04-27 06:30:21,060 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 5084858f-2ede-4f2d-b349-cc149f761a2e, Nodes: fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:21.059Z[Etc/UTC]].
2023-04-27 06:30:21,060 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(160)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
2023-04-27 06:30:21,194 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: addNew group-926679737466:[109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:1|startupRole:FOLLOWER] returns group-926679737466:java.util.concurrent.CompletableFuture@3c14eb1e[Not completed]
2023-04-27 06:30:21,194 [pool-2208-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: new RaftServerImpl for group-926679737466:[109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:21,195 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:21,195 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:21,195 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:21,195 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:21,195 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:21,195 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:21,195 [pool-2208-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466: ConfigurationManager, init=-1: peers:[109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:21,195 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis] (custom)
2023-04-27 06:30:21,195 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:21,195 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:21,195 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:21,196 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:21,196 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:21,197 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:21,197 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:21,197 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:21,197 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:21,197 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:21,197 [pool-2208-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis/b0ff91ae-07f7-40d4-aaaf-926679737466 does not exist. Creating ...
2023-04-27 06:30:21,199 [pool-2208-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis/b0ff91ae-07f7-40d4-aaaf-926679737466/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:21,201 [pool-2208-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis/b0ff91ae-07f7-40d4-aaaf-926679737466 has been successfully formatted.
2023-04-27 06:30:21,201 [pool-2208-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-926679737466: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:21,202 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:21,202 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:21,202 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:21,202 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:21,202 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:21,202 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:21,203 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: b0ff91ae-07f7-40d4-aaaf-926679737466, Nodes: 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:109e7eb5-e5f2-4565-a541-6a0b1e1f79c3, CreationTimestamp2023-04-27T06:30:20.198Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:21,203 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:21,203 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:21,203 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:21,203 [pool-2208-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis/b0ff91ae-07f7-40d4-aaaf-926679737466
2023-04-27 06:30:21,203 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:21,203 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:21,203 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:21,203 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:21,203 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:21,203 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:21,203 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:21,204 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:21,205 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:21,206 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:21,213 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:21,213 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:21,213 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:21,213 [pool-2208-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:21,213 [pool-2208-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:21,214 [pool-2208-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466: start as a follower, conf=-1: peers:[109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:21,214 [pool-2208-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:21,214 [pool-2208-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: start 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-FollowerState
2023-04-27 06:30:21,214 [pool-2208-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-926679737466,id=109e7eb5-e5f2-4565-a541-6a0b1e1f79c3
2023-04-27 06:30:21,214 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:21,214 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:21,214 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:21,214 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:21,214 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:21,214 [pool-2208-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:21,215 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=b0ff91ae-07f7-40d4-aaaf-926679737466
2023-04-27 06:30:21,215 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=b0ff91ae-07f7-40d4-aaaf-926679737466.
2023-04-27 06:30:21,297 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:21,298 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:21,300 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:21,301 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:21,382 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 7 is synced with bcsId 0.
2023-04-27 06:30:21,382 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 7 is synced with bcsId 0.
2023-04-27 06:30:21,391 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 7 is closed with bcsId 0.
2023-04-27 06:30:21,393 [FixedThreadPoolWithAffinityExecutor-0-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(292)) - Moving container #7 to CLOSED state, datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) reported CLOSED replica.
2023-04-27 06:30:21,394 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 8 is synced with bcsId 0.
2023-04-27 06:30:21,394 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 8 is synced with bcsId 0.
2023-04-27 06:30:21,397 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 8 is closed with bcsId 0.
2023-04-27 06:30:21,398 [FixedThreadPoolWithAffinityExecutor-0-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(292)) - Moving container #8 to CLOSED state, datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) reported CLOSED replica.
2023-04-27 06:30:21,400 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 10 is synced with bcsId 0.
2023-04-27 06:30:21,400 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 10 is synced with bcsId 0.
2023-04-27 06:30:21,402 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 10 is closed with bcsId 0.
2023-04-27 06:30:21,402 [FixedThreadPoolWithAffinityExecutor-0-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(292)) - Moving container #10 to CLOSED state, datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) reported CLOSED replica.
2023-04-27 06:30:21,404 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 11 is synced with bcsId 0.
2023-04-27 06:30:21,404 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(446)) - Container 11 is synced with bcsId 0.
2023-04-27 06:30:21,406 [Command processor thread] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(361)) - Container 11 is closed with bcsId 0.
2023-04-27 06:30:21,407 [FixedThreadPoolWithAffinityExecutor-0-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(292)) - Moving container #11 to CLOSED state, datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33) reported CLOSED replica.
2023-04-27 06:30:21,414 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:21,444 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:21,444 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleOverReplicatedExcessUnhealthy(1234)) - Container 3 has 1 excess unhealthy replicas. Excess unhealthy replicas will be deleted.
2023-04-27 06:30:21,444 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendDeleteCommand(1477)) - Sending delete container command for container #3 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33)
2023-04-27 06:30:21,444 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:21,451 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #1 is under replicated. Expected replica count is 3, but found 2.
2023-04-27 06:30:21,451 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=1, replicaIndex=0, sourceNodes=[220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33)], priority=NORMAL to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33)
2023-04-27 06:30:21,451 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #2 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:21,451 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #2 to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33).
2023-04-27 06:30:21,452 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #2 to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33).
2023-04-27 06:30:21,452 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #4 is under replicated. Expected replica count is 3, but found 2.
2023-04-27 06:30:21,452 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=4, replicaIndex=0, sourceNodes=[220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33)], priority=NORMAL to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33)
2023-04-27 06:30:21,452 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33).
2023-04-27 06:30:21,452 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33).
2023-04-27 06:30:21,452 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:21,452 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:21,452 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33).
2023-04-27 06:30:21,452 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33).
2023-04-27 06:30:21,452 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSED, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578791452 and scm deadline 1682578821452
2023-04-27 06:30:21,452 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSED, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578791452 and scm deadline 1682578821452
2023-04-27 06:30:21,453 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSED, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578791452 and scm deadline 1682578821452
2023-04-27 06:30:21,453 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSED, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578791453 and scm deadline 1682578821453
2023-04-27 06:30:21,453 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSED, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578791453 and scm deadline 1682578821453
2023-04-27 06:30:21,453 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSED, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578791453 and scm deadline 1682578821453
2023-04-27 06:30:21,453 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578791453 and scm deadline 1682578821453
2023-04-27 06:30:21,453 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578791453 and scm deadline 1682578821453
2023-04-27 06:30:21,453 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578791453 and scm deadline 1682578821453
2023-04-27 06:30:21,453 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578791453 and scm deadline 1682578821453
2023-04-27 06:30:21,453 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSED, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578791453 and scm deadline 1682578821453
2023-04-27 06:30:21,454 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSED, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578791454 and scm deadline 1682578821454
2023-04-27 06:30:21,454 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSED, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578791454 and scm deadline 1682578821454
2023-04-27 06:30:21,455 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSED, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578791455 and scm deadline 1682578821455
2023-04-27 06:30:21,456 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSED, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33) with datanode deadline 1682578791456 and scm deadline 1682578821456
2023-04-27 06:30:21,456 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSED, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578791456 and scm deadline 1682578821456
2023-04-27 06:30:21,456 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 5 milliseconds for processing 11 containers.
2023-04-27 06:30:21,462 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(73)) - Starting replication of container 4 from [70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)] using NO_COMPRESSION
2023-04-27 06:30:21,467 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processMissingIndexes(341)) - Cannot proceed for EC container reconstruction for #7, due to insufficient source replicas found. Number of source replicas needed: 3. Number of available source replicas are: 1. Available sources are: {1=(ContainerReplica{containerID=#7, state=CLOSED, datanodeDetails=63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33), placeOfBirth=63017b46-a8f7-4bfc-aab4-ba20a30a5f58, sequenceId=0, keyCount=2, bytesUsed=38,replicaIndex=1},OperationalState: IN_SERVICE Health: HEALTHY OperationStateExpiry: 0)}
2023-04-27 06:30:21,467 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processAndSendCommands(222)) - Container #7 is under replicated, but no commands were created to correct it
2023-04-27 06:30:21,467 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processMissingIndexes(341)) - Cannot proceed for EC container reconstruction for #11, due to insufficient source replicas found. Number of source replicas needed: 3. Number of available source replicas are: 1. Available sources are: {1=(ContainerReplica{containerID=#11, state=CLOSED, datanodeDetails=63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33), placeOfBirth=63017b46-a8f7-4bfc-aab4-ba20a30a5f58, sequenceId=0, keyCount=7, bytesUsed=133,replicaIndex=1},OperationalState: IN_SERVICE Health: HEALTHY OperationStateExpiry: 0)}
2023-04-27 06:30:21,467 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processAndSendCommands(222)) - Container #11 is under replicated, but no commands were created to correct it
2023-04-27 06:30:21,467 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processMissingIndexes(341)) - Cannot proceed for EC container reconstruction for #10, due to insufficient source replicas found. Number of source replicas needed: 3. Number of available source replicas are: 1. Available sources are: {3=(ContainerReplica{containerID=#10, state=CLOSED, datanodeDetails=63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33), placeOfBirth=63017b46-a8f7-4bfc-aab4-ba20a30a5f58, sequenceId=0, keyCount=2, bytesUsed=0,replicaIndex=3},OperationalState: IN_SERVICE Health: HEALTHY OperationStateExpiry: 0)}
2023-04-27 06:30:21,467 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processAndSendCommands(222)) - Container #10 is under replicated, but no commands were created to correct it
2023-04-27 06:30:21,467 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processMissingIndexes(341)) - Cannot proceed for EC container reconstruction for #8, due to insufficient source replicas found. Number of source replicas needed: 3. Number of available source replicas are: 1. Available sources are: {4=(ContainerReplica{containerID=#8, state=CLOSED, datanodeDetails=63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33), placeOfBirth=63017b46-a8f7-4bfc-aab4-ba20a30a5f58, sequenceId=0, keyCount=5, bytesUsed=95,replicaIndex=4},OperationalState: IN_SERVICE Health: HEALTHY OperationStateExpiry: 0)}
2023-04-27 06:30:21,468 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processAndSendCommands(222)) - Container #8 is under replicated, but no commands were created to correct it
2023-04-27 06:30:21,468 [Under Replicated Processor] INFO  replication.UnhealthyReplicationProcessor (UnhealthyReplicationProcessor.java:processAll(110)) - Processed 4 containers with health state counts {UNDER_REPLICATED=4}, failed processing 0
2023-04-27 06:30:21,470 [grpc-default-executor-3] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(62)) - Streaming container data (4) to other datanode with compression NO_COMPRESSION
2023-04-27 06:30:21,492 [grpc-default-executor-3] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(111)) - Sent 9728 bytes for container 4
2023-04-27 06:30:21,492 [grpc-default-executor-1] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(218)) - Container 4 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-5/data-0/containers/tmp/container-copy/container-4.tar
2023-04-27 06:30:21,493 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(88)) - Container 4 is downloaded with size 9728, starting to import.
2023-04-27 06:30:21,495 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(83)) - A dead datanode is detected. 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)
2023-04-27 06:30:21,495 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=bb0486ea-5b79-457f-a4db-03d650acd99d close command to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce
2023-04-27 06:30:21,496 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=bb0486ea-5b79-457f-a4db-03d650acd99d close command to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899
2023-04-27 06:30:21,496 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=bb0486ea-5b79-457f-a4db-03d650acd99d close command to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58
2023-04-27 06:30:21,496 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: bb0486ea-5b79-457f-a4db-03d650acd99d, Nodes: 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:0e67dfdf-fb52-47b7-aad6-2fa818fce5ce, CreationTimestamp2023-04-27T06:29:01.477Z[Etc/UTC]] removed.
2023-04-27 06:30:21,496 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=1a69e094-d42e-467a-ae88-914faf6cf0ef close command to datanode 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce
2023-04-27 06:30:21,497 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 1a69e094-d42e-467a-ae88-914faf6cf0ef, Nodes: 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:0e67dfdf-fb52-47b7-aad6-2fa818fce5ce, CreationTimestamp2023-04-27T06:29:00.905Z[Etc/UTC]] removed.
2023-04-27 06:30:21,497 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(96)) - Clearing command queue of size 34 for DN 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce(fv-az260-775/10.1.0.33)
2023-04-27 06:30:21,497 [EventQueue-DeadNodeForDeadNodeHandler] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:remove(190)) - Removed a node: /default-rack/0e67dfdf-fb52-47b7-aad6-2fa818fce5ce
2023-04-27 06:30:21,526 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(95)) - Container 4 is replicated successfully
2023-04-27 06:30:21,526 [ContainerReplicationThread-0] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(220)) - Successful DONE replicateContainerCommand: containerId=4, replicaIndex=0, sourceNodes=[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], priority=NORMAL, transferred 9728 bytes
2023-04-27 06:30:21,570 [IPC Server handler 2 on default port 38893] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/efdbc657-ca6b-4fea-9dc1-5411634f3e98
2023-04-27 06:30:21,570 [IPC Server handler 2 on default port 38893] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : efdbc657-ca6b-4fea-9dc1-5411634f3e98{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=44805, REPLICATION=39623, RATIS=44129, RATIS_ADMIN=44129, RATIS_SERVER=44129, RATIS_DATASTREAM=38671, STANDALONE=37667], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:21,570 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:21,570 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=b5275d90-33a5-467d-a65a-ba28176f28a3 to datanode:efdbc657-ca6b-4fea-9dc1-5411634f3e98
2023-04-27 06:30:21,571 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: b5275d90-33a5-467d-a65a-ba28176f28a3, Nodes: efdbc657-ca6b-4fea-9dc1-5411634f3e98(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:21.570Z[Etc/UTC]].
2023-04-27 06:30:21,571 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=454b071d-716d-4e22-a45d-1ab8bd3cbbb2 to datanode:efdbc657-ca6b-4fea-9dc1-5411634f3e98
2023-04-27 06:30:21,571 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=454b071d-716d-4e22-a45d-1ab8bd3cbbb2 to datanode:3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:21,571 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=454b071d-716d-4e22-a45d-1ab8bd3cbbb2 to datanode:fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e
2023-04-27 06:30:21,572 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 454b071d-716d-4e22-a45d-1ab8bd3cbbb2, Nodes: efdbc657-ca6b-4fea-9dc1-5411634f3e98(fv-az260-775/10.1.0.33)3804776e-bffa-42a7-addb-662dc325de80(fv-az260-775/10.1.0.33)fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:21.571Z[Etc/UTC]].
2023-04-27 06:30:21,572 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(160)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 6.
2023-04-27 06:30:21,805 [ForkJoinPool.commonPool-worker-0] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-0/data-0/containers/hdds/96a9b0ac-a466-4378-aad7-6952c9070703/DS-85f43363-2793-46af-a2f0-8f2d61b305e6/container.db for volume DS-85f43363-2793-46af-a2f0-8f2d61b305e6
2023-04-27 06:30:21,805 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:21,806 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:21,807 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
2023-04-27 06:30:21,825 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@ff87e74{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:21,825 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@26bc7d8a{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:21,825 [ForkJoinPool.commonPool-worker-0] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:21,826 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@5446859b{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:21,826 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@4a7532d6{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:21,830 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(419)) - Attempting to stop container services.
2023-04-27 06:30:21,830 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$6(409)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58: close
2023-04-27 06:30:21,830 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread2] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D: shutdown
2023-04-27 06:30:21,831 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread2] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-03D650ACD99D,id=63017b46-a8f7-4bfc-aab4-ba20a30a5f58
2023-04-27 06:30:21,831 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread2] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58: shutdown 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-FollowerState
2023-04-27 06:30:21,831 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(271)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58: shutdown server GrpcServerProtocolService now
2023-04-27 06:30:21,831 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-FollowerState was interrupted
2023-04-27 06:30:21,832 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-03D650ACD99D: Taking a snapshot at:(t:1, i:32) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-5/data/ratis/bb0486ea-5b79-457f-a4db-03d650acd99d/sm/snapshot.1_32
2023-04-27 06:30:21,832 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread2] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-StateMachineUpdater: set stopIndex = 32
2023-04-27 06:30:21,832 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread3] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(458)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506: shutdown
2023-04-27 06:30:21,832 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread3] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-9858800FF506,id=63017b46-a8f7-4bfc-aab4-ba20a30a5f58
2023-04-27 06:30:21,832 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread3] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58: shutdown 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506-LeaderStateImpl
2023-04-27 06:30:21,833 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread3] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506-PendingRequests: sendNotLeaderResponses
2023-04-27 06:30:21,833 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - d129cee7-7074-498b-a1c7-3e6cb07a0899 Close channels
2023-04-27 06:30:21,833 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:close(101)) - 0e67dfdf-fb52-47b7-aad6-2fa818fce5ce Close channels
2023-04-27 06:30:21,833 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(280)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58: shutdown server GrpcServerProtocolService successfully
2023-04-27 06:30:21,834 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xc41a30ac, L:/0:0:0:0:0:0:0:0:42701] CLOSE
2023-04-27 06:30:21,834 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread3] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506-StateMachineUpdater: set stopIndex = 0
2023-04-27 06:30:21,835 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xc41a30ac, L:/0:0:0:0:0:0:0:0:42701] INACTIVE
2023-04-27 06:30:21,835 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xc41a30ac, L:/0:0:0:0:0:0:0:0:42701] UNREGISTERED
2023-04-27 06:30:21,834 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(330)) - group-9858800FF506: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-5/data/ratis/b1fceeb3-8016-4ca7-852b-9858800ff506/sm/snapshot.1_0
2023-04-27 06:30:21,835 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-03D650ACD99D: Finished taking a snapshot at:(t:1, i:32) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-5/data/ratis/bb0486ea-5b79-457f-a4db-03d650acd99d/sm/snapshot.1_32 took: 3 ms
2023-04-27 06:30:21,836 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-StateMachineUpdater: Took a snapshot at index 32
2023-04-27 06:30:21,836 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 32
2023-04-27 06:30:21,836 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(341)) - group-9858800FF506: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-5/data/ratis/b1fceeb3-8016-4ca7-852b-9858800ff506/sm/snapshot.1_0 took: 2 ms
2023-04-27 06:30:21,837 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506-StateMachineUpdater: Took a snapshot at index 0
2023-04-27 06:30:21,837 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2023-04-27 06:30:21,837 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread3] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506: closes. applyIndex: 0
2023-04-27 06:30:21,838 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread2] INFO  server.RaftServer$Division (ServerState.java:close(466)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D: closes. applyIndex: 32
2023-04-27 06:30:21,838 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:21,839 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(347)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2023-04-27 06:30:21,839 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-9858800FF506-SegmentedRaftLogWorker close()
2023-04-27 06:30:21,839 [63017b46-a8f7-4bfc-aab4-ba20a30a5f58-impl-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(257)) - 63017b46-a8f7-4bfc-aab4-ba20a30a5f58@group-03D650ACD99D-SegmentedRaftLogWorker close()
2023-04-27 06:30:21,845 [JvmPauseMonitor31] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(111)) - JvmPauseMonitor-63017b46-a8f7-4bfc-aab4-ba20a30a5f58: Stopped
2023-04-27 06:30:21,884 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: addNew group-CB9E9B7E8B9E:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER] returns group-CB9E9B7E8B9E:java.util.concurrent.CompletableFuture@7996e86a[Not completed]
2023-04-27 06:30:21,885 [pool-2136-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: new RaftServerImpl for group-CB9E9B7E8B9E:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:21,885 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:21,885 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:21,885 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:21,885 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:21,885 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:21,886 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:21,886 [pool-2136-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E: ConfigurationManager, init=-1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:21,886 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis] (custom)
2023-04-27 06:30:21,886 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:21,886 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:21,886 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:21,886 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:21,886 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:21,887 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:21,887 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:21,887 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:21,888 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:21,888 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:21,888 [pool-2136-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis/08f3233d-81d6-42ac-8cfc-cb9e9b7e8b9e does not exist. Creating ...
2023-04-27 06:30:21,889 [pool-2136-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis/08f3233d-81d6-42ac-8cfc-cb9e9b7e8b9e/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:21,890 [pool-2136-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis/08f3233d-81d6-42ac-8cfc-cb9e9b7e8b9e has been successfully formatted.
2023-04-27 06:30:21,890 [pool-2136-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-CB9E9B7E8B9E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:21,890 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:21,891 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:21,891 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 08f3233d-81d6-42ac-8cfc-cb9e9b7e8b9e, Nodes: e5e627c9-cddd-4fde-83a7-403d4c1ca45c(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e5e627c9-cddd-4fde-83a7-403d4c1ca45c, CreationTimestamp2023-04-27T06:30:18.892Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:21,891 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:21,892 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:21,892 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:21,892 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:21,892 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:21,893 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:21,893 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:21,893 [pool-2136-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis/08f3233d-81d6-42ac-8cfc-cb9e9b7e8b9e
2023-04-27 06:30:21,893 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:21,893 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:21,893 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:21,893 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:21,893 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:21,893 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:21,893 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:21,893 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:21,894 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:21,895 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:21,902 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:21,902 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:21,902 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:21,902 [pool-2136-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:21,902 [pool-2136-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:21,903 [pool-2136-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E: start as a follower, conf=-1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:21,903 [pool-2136-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:21,903 [pool-2136-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: start e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-FollowerState
2023-04-27 06:30:21,903 [pool-2136-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CB9E9B7E8B9E,id=e5e627c9-cddd-4fde-83a7-403d4c1ca45c
2023-04-27 06:30:21,903 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:21,903 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:21,903 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:21,903 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:21,904 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:21,904 [pool-2136-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:21,904 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=08f3233d-81d6-42ac-8cfc-cb9e9b7e8b9e
2023-04-27 06:30:21,905 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=08f3233d-81d6-42ac-8cfc-cb9e9b7e8b9e.
2023-04-27 06:30:21,907 [Mini-Cluster-Provider-Reap] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-6/data-0/containers/hdds/96a9b0ac-a466-4378-aad7-6952c9070703/DS-a228b3d4-4c8f-4e59-b5c0-b12fdebd0b8c/container.db for volume DS-a228b3d4-4c8f-4e59-b5c0-b12fdebd0b8c
2023-04-27 06:30:21,907 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:21,908 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:21,909 [Mini-Cluster-Provider-Reap] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
2023-04-27 06:30:21,922 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@379cb33b{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:21,922 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@69dd93bf{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:21,922 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:21,923 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@2cbabb{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:21,923 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@5e6f01a0{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:22,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:22,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:22,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #3 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#3, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=6, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:22,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #3 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#3, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=6, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:22,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #4 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:22,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 1 sufficientlyReplicated, 2 underReplicated and 3 unhealthy containers
2023-04-27 06:30:22,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:22,032 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Waiting for nodes to be ready. Got 6 of 7 DN Heartbeats.
2023-04-27 06:30:22,032 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:22,032 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:22,034 [IPC Server handler 16 on default port 38893] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/f88c54b2-8776-4954-8dfb-f8bea6a862a6
2023-04-27 06:30:22,034 [IPC Server handler 16 on default port 38893] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : f88c54b2-8776-4954-8dfb-f8bea6a862a6{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=39005, REPLICATION=41121, RATIS=38947, RATIS_ADMIN=38947, RATIS_SERVER=38947, RATIS_DATASTREAM=46011, STANDALONE=33551], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:22,035 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:22,035 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=d15a6767-9072-4c9d-abb6-94f17cb32b95 to datanode:f88c54b2-8776-4954-8dfb-f8bea6a862a6
2023-04-27 06:30:22,036 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: d15a6767-9072-4c9d-abb6-94f17cb32b95, Nodes: f88c54b2-8776-4954-8dfb-f8bea6a862a6(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:22.035Z[Etc/UTC]].
2023-04-27 06:30:22,036 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(160)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 6.
2023-04-27 06:30:22,196 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) moved to stale state. Finalizing its pipelines [PipelineID=782195fe-43fe-41c4-aa0a-83b16dcf869e]
2023-04-27 06:30:22,197 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 782195fe-43fe-41c4-aa0a-83b16dcf869e, Nodes: 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:8ee12707-a777-4421-9344-714c0bb69310, CreationTimestamp2023-04-27T06:29:01.934Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:22,204 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:22,301 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5027137655ns, electionTimeout:5024ms
2023-04-27 06:30:22,301 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611: shutdown ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-FollowerState
2023-04-27 06:30:22,301 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:22,301 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:22,301 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611: start ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75
2023-04-27 06:30:22,302 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:22,302 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75-1] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b
2023-04-27 06:30:22,302 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:22,303 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:22,304 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D: receive requestVote(PRE_VOTE, ef6aab41-a1eb-41b3-a4a5-2458878a1611, group-50FF7EBD4A6D, 0, (t:0, i:0))
2023-04-27 06:30:22,304 [grpc-default-executor-1] INFO  impl.VoteContext (VoteContext.java:log(49)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FOLLOWER: accept PRE_VOTE from ef6aab41-a1eb-41b3-a4a5-2458878a1611: our priority 0 <= candidate's priority 1
2023-04-27 06:30:22,304 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D replies to PRE_VOTE vote request: ef6aab41-a1eb-41b3-a4a5-2458878a1611<-dbe8ec56-fdf9-4402-aab9-993d7a20391d#0:OK-t0. Peer's state: dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D:t0, leader=null, voted=, raftlog=Memoized:dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:22,305 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
2023-04-27 06:30:22,305 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: ef6aab41-a1eb-41b3-a4a5-2458878a1611<-dbe8ec56-fdf9-4402-aab9-993d7a20391d#0:OK-t0
2023-04-27 06:30:22,305 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75 PRE_VOTE round 0: result PASSED
2023-04-27 06:30:22,310 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75 ELECTION round 0: submit vote requests at term 1 for -1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:22,311 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:22,311 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:22,313 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D: receive requestVote(PRE_VOTE, ef6aab41-a1eb-41b3-a4a5-2458878a1611, group-50FF7EBD4A6D, 0, (t:0, i:0))
2023-04-27 06:30:22,313 [grpc-default-executor-1] INFO  impl.VoteContext (VoteContext.java:log(49)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FOLLOWER: accept PRE_VOTE from ef6aab41-a1eb-41b3-a4a5-2458878a1611: our priority 0 <= candidate's priority 1
2023-04-27 06:30:22,314 [grpc-default-executor-3] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D: receive requestVote(ELECTION, ef6aab41-a1eb-41b3-a4a5-2458878a1611, group-50FF7EBD4A6D, 1, (t:0, i:0))
2023-04-27 06:30:22,314 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D: receive requestVote(ELECTION, ef6aab41-a1eb-41b3-a4a5-2458878a1611, group-50FF7EBD4A6D, 1, (t:0, i:0))
2023-04-27 06:30:22,314 [grpc-default-executor-5] INFO  impl.VoteContext (VoteContext.java:log(49)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FOLLOWER: accept ELECTION from ef6aab41-a1eb-41b3-a4a5-2458878a1611: our priority 0 <= candidate's priority 1
2023-04-27 06:30:22,314 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:ef6aab41-a1eb-41b3-a4a5-2458878a1611
2023-04-27 06:30:22,314 [grpc-default-executor-5] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d: shutdown dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FollowerState
2023-04-27 06:30:22,315 [grpc-default-executor-5] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d: start dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FollowerState
2023-04-27 06:30:22,315 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FollowerState was interrupted
2023-04-27 06:30:22,314 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D replies to PRE_VOTE vote request: ef6aab41-a1eb-41b3-a4a5-2458878a1611<-7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b#0:OK-t0. Peer's state: 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D:t0, leader=null, voted=, raftlog=Memoized:7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:22,316 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:22,316 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:22,317 [grpc-default-executor-3] INFO  impl.VoteContext (VoteContext.java:log(49)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FOLLOWER: accept ELECTION from ef6aab41-a1eb-41b3-a4a5-2458878a1611: our priority 0 <= candidate's priority 1
2023-04-27 06:30:22,317 [grpc-default-executor-3] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:ef6aab41-a1eb-41b3-a4a5-2458878a1611
2023-04-27 06:30:22,317 [grpc-default-executor-3] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b: shutdown 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FollowerState
2023-04-27 06:30:22,317 [grpc-default-executor-3] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b: start 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FollowerState
2023-04-27 06:30:22,317 [7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FollowerState was interrupted
2023-04-27 06:30:22,318 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D replies to ELECTION vote request: ef6aab41-a1eb-41b3-a4a5-2458878a1611<-dbe8ec56-fdf9-4402-aab9-993d7a20391d#0:OK-t1. Peer's state: dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D:t1, leader=null, voted=ef6aab41-a1eb-41b3-a4a5-2458878a1611, raftlog=Memoized:dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:22,318 [7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:22,318 [7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:22,319 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75: ELECTION PASSED received 1 response(s) and 0 exception(s):
2023-04-27 06:30:22,319 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: ef6aab41-a1eb-41b3-a4a5-2458878a1611<-dbe8ec56-fdf9-4402-aab9-993d7a20391d#0:OK-t1
2023-04-27 06:30:22,319 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75 ELECTION round 0: result PASSED
2023-04-27 06:30:22,319 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611: shutdown ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75
2023-04-27 06:30:22,319 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:22,319 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-50FF7EBD4A6D with new leaderId: ef6aab41-a1eb-41b3-a4a5-2458878a1611
2023-04-27 06:30:22,319 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D: change Leader from null to ef6aab41-a1eb-41b3-a4a5-2458878a1611 at term 1 for becomeLeader, leader elected after 5075ms
2023-04-27 06:30:22,319 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:22,320 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:22,320 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:22,320 [grpc-default-executor-3] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D replies to ELECTION vote request: ef6aab41-a1eb-41b3-a4a5-2458878a1611<-7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b#0:OK-t1. Peer's state: 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D:t1, leader=null, voted=ef6aab41-a1eb-41b3-a4a5-2458878a1611, raftlog=Memoized:7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:22,320 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:22,320 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:22,320 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:22,320 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:22,320 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:22,321 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2023-04-27 06:30:22,321 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:22,321 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2023-04-27 06:30:22,321 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2023-04-27 06:30:22,321 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:22,321 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:22,322 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:22,322 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.wait-time.min = 10ms (default)
2023-04-27 06:30:22,323 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2023-04-27 06:30:22,323 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:22,323 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2023-04-27 06:30:22,323 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d, Nodes: dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33)ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:ef6aab41-a1eb-41b3-a4a5-2458878a1611, CreationTimestamp2023-04-27T06:30:16.094Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:22,323 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2023-04-27 06:30:22,324 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:22,324 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:22,324 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:22,324 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.wait-time.min = 10ms (default)
2023-04-27 06:30:22,324 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611: start ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderStateImpl
2023-04-27 06:30:22,325 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:22,327 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-2/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d/current/log_inprogress_0
2023-04-27 06:30:22,342 [ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D-LeaderElection75] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - ef6aab41-a1eb-41b3-a4a5-2458878a1611@group-50FF7EBD4A6D: set configuration 0: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:22,347 [7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b-server-thread1] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-50FF7EBD4A6D with new leaderId: ef6aab41-a1eb-41b3-a4a5-2458878a1611
2023-04-27 06:30:22,347 [7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D: change Leader from null to ef6aab41-a1eb-41b3-a4a5-2458878a1611 at term 1 for appendEntries, leader elected after 4969ms
2023-04-27 06:30:22,349 [dbe8ec56-fdf9-4402-aab9-993d7a20391d-server-thread1] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-50FF7EBD4A6D with new leaderId: ef6aab41-a1eb-41b3-a4a5-2458878a1611
2023-04-27 06:30:22,349 [dbe8ec56-fdf9-4402-aab9-993d7a20391d-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D: change Leader from null to ef6aab41-a1eb-41b3-a4a5-2458878a1611 at term 1 for appendEntries, leader elected after 5058ms
2023-04-27 06:30:22,352 [7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D: set configuration 0: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:22,353 [7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b-server-thread1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:22,356 [7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-6/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d/current/log_inprogress_0
2023-04-27 06:30:22,356 [dbe8ec56-fdf9-4402-aab9-993d7a20391d-server-thread2] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D: set configuration 0: peers:[7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b|rpc:10.1.0.33:36513|dataStream:10.1.0.33:34595|priority:0|startupRole:FOLLOWER, dbe8ec56-fdf9-4402-aab9-993d7a20391d|rpc:10.1.0.33:35821|dataStream:10.1.0.33:36227|priority:0|startupRole:FOLLOWER, ef6aab41-a1eb-41b3-a4a5-2458878a1611|rpc:10.1.0.33:39551|dataStream:10.1.0.33:39907|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:22,359 [dbe8ec56-fdf9-4402-aab9-993d7a20391d-server-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:22,360 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data/ratis/03c9c77b-4bca-4bd4-a475-50ff7ebd4a6d/current/log_inprogress_0
2023-04-27 06:30:22,414 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:22,445 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:22,445 [ReplicationMonitor] WARN  net.NetworkTopologyImpl (NetworkTopologyImpl.java:chooseNodeInternal(653)) - No available node in (scope="/" excludedScope="null" excludedNodes="[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)]"  ancestorGen="1").
2023-04-27 06:30:22,445 [ReplicationMonitor] WARN  algorithms.SCMContainerPlacementRackAware (SCMContainerPlacementRackAware.java:chooseNode(282)) - Failed to find the datanode for container. excludedNodes:[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], affinityNode:
2023-04-27 06:30:22,446 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #3 is under replicated. Expected replica count is 3, but found 2.
2023-04-27 06:30:22,446 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=3, replicaIndex=0, sourceNodes=[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], priority=NORMAL to dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33)
2023-04-27 06:30:22,446 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleOverReplicatedExcessUnhealthy(1234)) - Container 4 has 1 excess unhealthy replicas. Excess unhealthy replicas will be deleted.
2023-04-27 06:30:22,446 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendDeleteCommand(1477)) - Sending delete container command for container #4 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33)
2023-04-27 06:30:22,446 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:22,457 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #1 is under replicated. Expected replica count is 3, but found 2.
2023-04-27 06:30:22,457 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=1, replicaIndex=0, sourceNodes=[220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33)], priority=NORMAL to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)
2023-04-27 06:30:22,457 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #2 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:22,457 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #2 to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33).
2023-04-27 06:30:22,457 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #3 is under replicated. Expected replica count is 3, but found 2.
2023-04-27 06:30:22,457 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=3, replicaIndex=0, sourceNodes=[220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33)], priority=NORMAL to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)
2023-04-27 06:30:22,458 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #4 is under replicated. Expected replica count is 3, but found 2.
2023-04-27 06:30:22,458 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=4, replicaIndex=0, sourceNodes=[220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33)], priority=NORMAL to 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33)
2023-04-27 06:30:22,458 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33).
2023-04-27 06:30:22,458 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:22,458 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:22,458 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33).
2023-04-27 06:30:22,458 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSED, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578792458 and scm deadline 1682578822458
2023-04-27 06:30:22,459 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSED, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578792459 and scm deadline 1682578822459
2023-04-27 06:30:22,460 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSED, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578792460 and scm deadline 1682578822460
2023-04-27 06:30:22,460 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSED, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578792460 and scm deadline 1682578822460
2023-04-27 06:30:22,461 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578792461 and scm deadline 1682578822461
2023-04-27 06:30:22,461 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578792461 and scm deadline 1682578822461
2023-04-27 06:30:22,461 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578792461 and scm deadline 1682578822461
2023-04-27 06:30:22,461 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSED, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578792461 and scm deadline 1682578822461
2023-04-27 06:30:22,462 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSED, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578792462 and scm deadline 1682578822462
2023-04-27 06:30:22,462 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSED, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578792462 and scm deadline 1682578822462
2023-04-27 06:30:22,462 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSED, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33) with datanode deadline 1682578792462 and scm deadline 1682578822462
2023-04-27 06:30:22,462 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 11, pipelineID: PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, force: true] for container ContainerInfo{id=#11, state=CLOSED, pipelineID=PipelineID=7a77590e-48e7-4a17-b391-b680b8534a24, stateEnterTime=2023-04-27T06:30:12.204Z, owner=om1} to 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33) with datanode deadline 1682578792462 and scm deadline 1682578822462
2023-04-27 06:30:22,462 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 6 milliseconds for processing 11 containers.
2023-04-27 06:30:22,468 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processMissingIndexes(341)) - Cannot proceed for EC container reconstruction for #7, due to insufficient source replicas found. Number of source replicas needed: 3. Number of available source replicas are: 1. Available sources are: {1=(ContainerReplica{containerID=#7, state=CLOSED, datanodeDetails=63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33), placeOfBirth=63017b46-a8f7-4bfc-aab4-ba20a30a5f58, sequenceId=0, keyCount=2, bytesUsed=38,replicaIndex=1},OperationalState: IN_SERVICE Health: HEALTHY OperationStateExpiry: 0)}
2023-04-27 06:30:22,468 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processAndSendCommands(222)) - Container #7 is under replicated, but no commands were created to correct it
2023-04-27 06:30:22,468 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processMissingIndexes(341)) - Cannot proceed for EC container reconstruction for #11, due to insufficient source replicas found. Number of source replicas needed: 3. Number of available source replicas are: 1. Available sources are: {1=(ContainerReplica{containerID=#11, state=CLOSED, datanodeDetails=63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33), placeOfBirth=63017b46-a8f7-4bfc-aab4-ba20a30a5f58, sequenceId=0, keyCount=7, bytesUsed=133,replicaIndex=1},OperationalState: IN_SERVICE Health: HEALTHY OperationStateExpiry: 0)}
2023-04-27 06:30:22,468 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processAndSendCommands(222)) - Container #11 is under replicated, but no commands were created to correct it
2023-04-27 06:30:22,468 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processMissingIndexes(341)) - Cannot proceed for EC container reconstruction for #8, due to insufficient source replicas found. Number of source replicas needed: 3. Number of available source replicas are: 1. Available sources are: {4=(ContainerReplica{containerID=#8, state=CLOSED, datanodeDetails=63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33), placeOfBirth=63017b46-a8f7-4bfc-aab4-ba20a30a5f58, sequenceId=0, keyCount=5, bytesUsed=95,replicaIndex=4},OperationalState: IN_SERVICE Health: HEALTHY OperationStateExpiry: 0)}
2023-04-27 06:30:22,468 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processAndSendCommands(222)) - Container #8 is under replicated, but no commands were created to correct it
2023-04-27 06:30:22,468 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processMissingIndexes(341)) - Cannot proceed for EC container reconstruction for #10, due to insufficient source replicas found. Number of source replicas needed: 3. Number of available source replicas are: 1. Available sources are: {3=(ContainerReplica{containerID=#10, state=CLOSED, datanodeDetails=63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33), placeOfBirth=63017b46-a8f7-4bfc-aab4-ba20a30a5f58, sequenceId=0, keyCount=2, bytesUsed=0,replicaIndex=3},OperationalState: IN_SERVICE Health: HEALTHY OperationStateExpiry: 0)}
2023-04-27 06:30:22,469 [Under Replicated Processor] WARN  replication.ECUnderReplicationHandler (ECUnderReplicationHandler.java:processAndSendCommands(222)) - Container #10 is under replicated, but no commands were created to correct it
2023-04-27 06:30:22,469 [Under Replicated Processor] INFO  replication.UnhealthyReplicationProcessor (UnhealthyReplicationProcessor.java:processAll(110)) - Processed 4 containers with health state counts {UNDER_REPLICATED=4}, failed processing 0
2023-04-27 06:30:22,696 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33) moved to stale state. Finalizing its pipelines [PipelineID=5d215173-4283-4f76-b761-364e7f2fce51]
2023-04-27 06:30:22,697 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 5d215173-4283-4f76-b761-364e7f2fce51, Nodes: 220eace7-05d2-4cc3-8ea2-3a8f6657333d(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:220eace7-05d2-4cc3-8ea2-3a8f6657333d, CreationTimestamp2023-04-27T06:28:59.004Z[Etc/UTC]] moved to CLOSED state
2023-04-27 06:30:22,885 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:22,891 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:23,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:23,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:23,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #3 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#3, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=6, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:23,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #3 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#3, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=6, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:23,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #4 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:23,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #4 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:23,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 0 sufficientlyReplicated, 3 underReplicated and 3 unhealthy containers
2023-04-27 06:30:23,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:23,032 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Nodes are ready. Got 7 of 7 DN Heartbeats.
2023-04-27 06:30:23,033 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:23,033 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:23,200 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(83)) - A dead datanode is detected. 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)
2023-04-27 06:30:23,200 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=b2ba4b9e-5d53-4979-8d36-c898faf9593d close command to datanode 2b022b8d-8bf4-4c13-9ad2-4e7e18903189
2023-04-27 06:30:23,200 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: b2ba4b9e-5d53-4979-8d36-c898faf9593d, Nodes: 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:2b022b8d-8bf4-4c13-9ad2-4e7e18903189, CreationTimestamp2023-04-27T06:28:59.845Z[Etc/UTC]] removed.
2023-04-27 06:30:23,201 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(96)) - Clearing command queue of size 19 for DN 2b022b8d-8bf4-4c13-9ad2-4e7e18903189(fv-az260-775/10.1.0.33)
2023-04-27 06:30:23,201 [EventQueue-DeadNodeForDeadNodeHandler] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:remove(190)) - Removed a node: /default-rack/2b022b8d-8bf4-4c13-9ad2-4e7e18903189
2023-04-27 06:30:23,301 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:23,302 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:23,305 [DeleteContainerThread-0] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.removeContainerFromDB(BlockUtils.java:248)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.removeContainer(KeyValueContainerUtil.java:160)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.delete(KeyValueContainer.java:297)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteInternal(KeyValueHandler.java:1403)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteContainer(KeyValueHandler.java:1163)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.deleteContainer(ContainerController.java:181)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.handleInternal(DeleteContainerCommandHandler.java:108)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.lambda$handle$0(DeleteContainerCommandHandler.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 17 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 18 more
2023-04-27 06:30:23,314 [DeleteContainerThread-0] ERROR helpers.KeyValueContainerUtil (KeyValueContainerUtil.java:removeContainer(162)) - DB failure, unable to remove container. Disk need to be replaced.
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:3 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.removeContainerFromDB(BlockUtils.java:248)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.removeContainer(KeyValueContainerUtil.java:160)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.delete(KeyValueContainer.java:297)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteInternal(KeyValueHandler.java:1403)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteContainer(KeyValueHandler.java:1163)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.deleteContainer(ContainerController.java:181)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.handleInternal(DeleteContainerCommandHandler.java:108)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.lambda$handle$0(DeleteContainerCommandHandler.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:23,314 [DeleteContainerThread-0] ERROR keyvalue.KeyValueContainer (KeyValueContainer.java:delete(308)) - Failed to cleanup container. ID: 3
java.io.IOException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:3 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.removeContainer(KeyValueContainerUtil.java:164)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.delete(KeyValueContainer.java:297)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteInternal(KeyValueHandler.java:1403)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteContainer(KeyValueHandler.java:1163)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.deleteContainer(ContainerController.java:181)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.handleInternal(DeleteContainerCommandHandler.java:108)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.lambda$handle$0(DeleteContainerCommandHandler.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:3 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.removeContainerFromDB(BlockUtils.java:248)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.removeContainer(KeyValueContainerUtil.java:160)
	... 9 more
2023-04-27 06:30:23,315 [DeleteContainerThread-0] ERROR commandhandler.DeleteContainerCommandHandler (DeleteContainerCommandHandler.java:handleInternal(111)) - Exception occurred while deleting the container.
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Failed to cleanup container. ID: 3
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.delete(KeyValueContainer.java:309)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteInternal(KeyValueHandler.java:1403)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteContainer(KeyValueHandler.java:1163)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.deleteContainer(ContainerController.java:181)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.handleInternal(DeleteContainerCommandHandler.java:108)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.lambda$handle$0(DeleteContainerCommandHandler.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:3 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.removeContainer(KeyValueContainerUtil.java:164)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.delete(KeyValueContainer.java:297)
	... 8 more
Caused by: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:3 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.removeContainerFromDB(BlockUtils.java:248)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.removeContainer(KeyValueContainerUtil.java:160)
	... 9 more
2023-04-27 06:30:23,400 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(83)) - A dead datanode is detected. d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)
2023-04-27 06:30:23,400 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(272)) - Send pipeline:PipelineID=057b00eb-9ef1-46bf-b442-47e71c502c0c close command to datanode d129cee7-7074-498b-a1c7-3e6cb07a0899
2023-04-27 06:30:23,400 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 057b00eb-9ef1-46bf-b442-47e71c502c0c, Nodes: d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:d129cee7-7074-498b-a1c7-3e6cb07a0899, CreationTimestamp2023-04-27T06:29:00.902Z[Etc/UTC]] removed.
2023-04-27 06:30:23,401 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(96)) - Clearing command queue of size 48 for DN d129cee7-7074-498b-a1c7-3e6cb07a0899(fv-az260-775/10.1.0.33)
2023-04-27 06:30:23,401 [EventQueue-DeadNodeForDeadNodeHandler] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:remove(190)) - Removed a node: /default-rack/d129cee7-7074-498b-a1c7-3e6cb07a0899
2023-04-27 06:30:23,414 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:23,446 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:23,447 [ReplicationMonitor] WARN  net.NetworkTopologyImpl (NetworkTopologyImpl.java:chooseNodeInternal(653)) - No available node in (scope="/" excludedScope="null" excludedNodes="[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)]"  ancestorGen="1").
2023-04-27 06:30:23,447 [ReplicationMonitor] WARN  algorithms.SCMContainerPlacementRackAware (SCMContainerPlacementRackAware.java:chooseNode(282)) - Failed to find the datanode for container. excludedNodes:[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], affinityNode:
2023-04-27 06:30:23,447 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #3 is under replicated. Expected replica count is 3, but found 2.
2023-04-27 06:30:23,447 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=3, replicaIndex=0, sourceNodes=[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], priority=NORMAL to 7a6d3cb5-ca5e-45e5-99fe-a8e18444e416(fv-az260-775/10.1.0.33)
2023-04-27 06:30:23,447 [ReplicationMonitor] WARN  net.NetworkTopologyImpl (NetworkTopologyImpl.java:chooseNodeInternal(653)) - No available node in (scope="/" excludedScope="null" excludedNodes="[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)]"  ancestorGen="1").
2023-04-27 06:30:23,447 [ReplicationMonitor] WARN  algorithms.SCMContainerPlacementRackAware (SCMContainerPlacementRackAware.java:chooseNode(282)) - Failed to find the datanode for container. excludedNodes:[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], affinityNode:
2023-04-27 06:30:23,448 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2199)) - Container #4 is under replicated. Expected replica count is 3, but found 2.
2023-04-27 06:30:23,448 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1454)) - Sending replicateContainerCommand: containerId=4, replicaIndex=0, sourceNodes=[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], priority=NORMAL to 7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b(fv-az260-775/10.1.0.33)
2023-04-27 06:30:23,448 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2023-04-27 06:30:23,462 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2228)) - Cannot replicate container #1, no healthy datanodes with replica found.
2023-04-27 06:30:23,463 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #2 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:23,463 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2228)) - Cannot replicate container #3, no healthy datanodes with replica found.
2023-04-27 06:30:23,463 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:replicateAnyWithTopology(2228)) - Cannot replicate container #4, no healthy datanodes with replica found.
2023-04-27 06:30:23,463 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #5 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:23,463 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #6 to datanode 63017b46-a8f7-4bfc-aab4-ba20a30a5f58(fv-az260-775/10.1.0.33).
2023-04-27 06:30:23,463 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 7, pipelineID: PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, force: true] for container ContainerInfo{id=#7, state=CLOSED, pipelineID=PipelineID=37ee7bc9-b834-49c7-bf6a-23376a986332, stateEnterTime=2023-04-27T06:30:10.005Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578793463 and scm deadline 1682578823463
2023-04-27 06:30:23,463 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 8, pipelineID: PipelineID=fc22a943-ba2e-46ef-a412-943475173222, force: true] for container ContainerInfo{id=#8, state=CLOSED, pipelineID=PipelineID=fc22a943-ba2e-46ef-a412-943475173222, stateEnterTime=2023-04-27T06:30:10.666Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578793463 and scm deadline 1682578823463
2023-04-27 06:30:23,463 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 9, pipelineID: PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, force: true] for container ContainerInfo{id=#9, state=CLOSING, pipelineID=PipelineID=e68537c7-dd5e-4934-bd5b-0f78194036e1, stateEnterTime=2023-04-27T06:30:11.603Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578793463 and scm deadline 1682578823463
2023-04-27 06:30:23,464 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:sendDatanodeCommand(678)) - Sending command [closeContainerCommand: containerID: 10, pipelineID: PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, force: true] for container ContainerInfo{id=#10, state=CLOSED, pipelineID=PipelineID=3541dabf-3351-48cd-a257-9e51dd0a6317, stateEnterTime=2023-04-27T06:30:11.879Z, owner=om1} to 8ee12707-a777-4421-9344-714c0bb69310(fv-az260-775/10.1.0.33) with datanode deadline 1682578793464 and scm deadline 1682578823464
2023-04-27 06:30:23,464 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 2 milliseconds for processing 11 containers.
2023-04-27 06:30:23,675 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 3804776e-bffa-42a7-addb-662dc325de80: addNew group-6FC4B564C533:[3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER] returns group-6FC4B564C533:java.util.concurrent.CompletableFuture@6817c0f6[Not completed]
2023-04-27 06:30:23,677 [pool-2262-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 3804776e-bffa-42a7-addb-662dc325de80: new RaftServerImpl for group-6FC4B564C533:[3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:23,677 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:23,677 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:23,677 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:23,677 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:23,677 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:23,677 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:23,677 [pool-2262-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533: ConfigurationManager, init=-1: peers:[3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:23,677 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis] (custom)
2023-04-27 06:30:23,678 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:23,678 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:23,678 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:23,678 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:23,678 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:23,679 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:23,679 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:23,679 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:23,679 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:23,680 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:23,680 [pool-2262-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis/6bf5f405-714f-49bb-b293-6fc4b564c533 does not exist. Creating ...
2023-04-27 06:30:23,681 [pool-2262-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis/6bf5f405-714f-49bb-b293-6fc4b564c533/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:23,683 [pool-2262-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis/6bf5f405-714f-49bb-b293-6fc4b564c533 has been successfully formatted.
2023-04-27 06:30:23,684 [pool-2262-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-6FC4B564C533: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:23,684 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:23,684 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:23,684 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:23,684 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 6bf5f405-714f-49bb-b293-6fc4b564c533, Nodes: 3804776e-bffa-42a7-addb-662dc325de80(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3804776e-bffa-42a7-addb-662dc325de80, CreationTimestamp2023-04-27T06:30:20.677Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:23,685 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:23,684 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:23,687 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:23,687 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:23,687 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:23,687 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:23,687 [pool-2262-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis/6bf5f405-714f-49bb-b293-6fc4b564c533
2023-04-27 06:30:23,687 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:23,687 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:23,687 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:23,687 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:23,687 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:23,688 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:23,688 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:23,688 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:23,688 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:23,689 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:23,696 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:23,696 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:23,697 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:23,697 [pool-2262-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:23,697 [pool-2262-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:23,697 [pool-2262-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533: start as a follower, conf=-1: peers:[3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:23,697 [pool-2262-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:23,697 [pool-2262-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 3804776e-bffa-42a7-addb-662dc325de80: start 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-FollowerState
2023-04-27 06:30:23,698 [pool-2262-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6FC4B564C533,id=3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:23,698 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:23,698 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:23,698 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:23,698 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:23,699 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:23,699 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:23,699 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=6bf5f405-714f-49bb-b293-6fc4b564c533
2023-04-27 06:30:23,699 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=6bf5f405-714f-49bb-b293-6fc4b564c533.
2023-04-27 06:30:23,699 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 3804776e-bffa-42a7-addb-662dc325de80: addNew group-1AB8BD3CBBB2:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER] returns group-1AB8BD3CBBB2:java.util.concurrent.CompletableFuture@6c28592c[Not completed]
2023-04-27 06:30:23,700 [pool-2262-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 3804776e-bffa-42a7-addb-662dc325de80: new RaftServerImpl for group-1AB8BD3CBBB2:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:23,700 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:23,700 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:23,700 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:23,700 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:23,700 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:23,701 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:23,701 [pool-2262-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2: ConfigurationManager, init=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:23,701 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis] (custom)
2023-04-27 06:30:23,701 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:23,701 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:23,701 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:23,701 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:23,701 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:23,702 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:23,702 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:23,702 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:23,702 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:23,702 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:23,703 [pool-2262-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2 does not exist. Creating ...
2023-04-27 06:30:23,704 [pool-2262-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:23,705 [pool-2262-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2 has been successfully formatted.
2023-04-27 06:30:23,706 [pool-2262-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-1AB8BD3CBBB2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:23,706 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:23,706 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:23,706 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:23,706 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:23,706 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:23,706 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:23,707 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:23,707 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:23,707 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:23,707 [pool-2262-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2
2023-04-27 06:30:23,707 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:23,707 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:23,707 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:23,707 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:23,707 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:23,707 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:23,707 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:23,707 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:23,708 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:23,710 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:23,717 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:23,718 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:23,718 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:23,718 [pool-2262-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:23,718 [pool-2262-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:23,718 [pool-2262-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2: start as a follower, conf=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:23,718 [pool-2262-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:23,718 [pool-2262-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 3804776e-bffa-42a7-addb-662dc325de80: start 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-FollowerState
2023-04-27 06:30:23,719 [pool-2262-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1AB8BD3CBBB2,id=3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:23,719 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:23,719 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:23,719 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:23,719 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:23,719 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:23,719 [pool-2262-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:23,720 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=454b071d-716d-4e22-a45d-1ab8bd3cbbb2
2023-04-27 06:30:23,727 [grpc-default-executor-1] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: addNew group-1AB8BD3CBBB2:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER] returns group-1AB8BD3CBBB2:java.util.concurrent.CompletableFuture@35253293[Not completed]
2023-04-27 06:30:23,732 [pool-2310-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: new RaftServerImpl for group-1AB8BD3CBBB2:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:23,732 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:23,732 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:23,732 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:23,732 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:23,732 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:23,732 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:23,732 [pool-2310-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2: ConfigurationManager, init=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:23,733 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis] (custom)
2023-04-27 06:30:23,733 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:23,733 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:23,733 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:23,733 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:23,733 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:23,734 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:23,734 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:23,734 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:23,734 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:23,734 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:23,735 [pool-2310-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2 does not exist. Creating ...
2023-04-27 06:30:23,736 [pool-2310-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:23,737 [pool-2310-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2 has been successfully formatted.
2023-04-27 06:30:23,738 [pool-2310-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-1AB8BD3CBBB2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:23,738 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:23,738 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:23,738 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:23,738 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:23,738 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:23,738 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:23,739 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:23,739 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:23,739 [pool-2310-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2
2023-04-27 06:30:23,739 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:23,740 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:23,740 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:23,740 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:23,740 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:23,740 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:23,740 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:23,740 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:23,741 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:23,742 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:23,749 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:23,750 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:23,750 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:23,750 [pool-2310-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:23,750 [pool-2310-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:23,751 [pool-2310-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2: start as a follower, conf=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:23,751 [pool-2310-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:23,751 [pool-2310-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: start efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FollowerState
2023-04-27 06:30:23,751 [pool-2310-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1AB8BD3CBBB2,id=efdbc657-ca6b-4fea-9dc1-5411634f3e98
2023-04-27 06:30:23,751 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:23,751 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:23,751 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:23,754 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:23,754 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:23,754 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:23,765 [grpc-default-executor-1] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: addNew group-1AB8BD3CBBB2:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER] returns group-1AB8BD3CBBB2:java.util.concurrent.CompletableFuture@d197b9f[Not completed]
2023-04-27 06:30:23,766 [pool-2284-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: new RaftServerImpl for group-1AB8BD3CBBB2:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:23,766 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:23,767 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:23,767 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:23,767 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:23,767 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:23,767 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:23,767 [pool-2284-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2: ConfigurationManager, init=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:23,767 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis] (custom)
2023-04-27 06:30:23,767 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:23,767 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:23,767 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:23,768 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:23,768 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:23,769 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:23,769 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:23,769 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:23,769 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:23,769 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:23,769 [pool-2284-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2 does not exist. Creating ...
2023-04-27 06:30:23,771 [pool-2284-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:23,772 [pool-2284-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2 has been successfully formatted.
2023-04-27 06:30:23,773 [pool-2284-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-1AB8BD3CBBB2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:23,773 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:23,773 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:23,773 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:23,773 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:23,773 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:23,773 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:23,774 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:23,774 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:23,774 [pool-2284-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2
2023-04-27 06:30:23,774 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:23,774 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:23,774 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:23,774 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:23,774 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:23,774 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:23,774 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:23,774 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:23,775 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:23,776 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:23,783 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:23,784 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:23,784 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:23,784 [pool-2284-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:23,784 [pool-2284-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:23,784 [pool-2284-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2: start as a follower, conf=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:23,784 [pool-2284-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:23,785 [pool-2284-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: start fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState
2023-04-27 06:30:23,785 [pool-2284-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1AB8BD3CBBB2,id=fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e
2023-04-27 06:30:23,785 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:23,785 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:23,785 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:23,785 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:23,785 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:23,785 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:23,788 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS THREE PipelineID=454b071d-716d-4e22-a45d-1ab8bd3cbbb2.
2023-04-27 06:30:23,855 [ForkJoinPool.commonPool-worker-0] INFO  volume.HddsVolume (HddsVolume.java:closeDbStore(437)) - SchemaV3 db is stopped at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-96a9b0ac-a466-4378-aad7-6952c9070703/datanode-5/data-0/containers/hdds/96a9b0ac-a466-4378-aad7-6952c9070703/DS-d1896e60-a422-41a6-b02f-8412a7df82cb/container.db for volume DS-d1896e60-a422-41a6-b02f-8412a7df82cb
2023-04-27 06:30:23,855 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2023-04-27 06:30:23,856 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2023-04-27 06:30:23,859 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(601)) - Ozone container server stopped.
2023-04-27 06:30:23,873 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@244c17a8{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:23,873 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@1628d18e{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:23,873 [ForkJoinPool.commonPool-worker-0] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:23,874 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@6e325dfc{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2023-04-27 06:30:23,874 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@427f0a90{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:23,877 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopSCM(552)) - Stopping the StorageContainerManager
2023-04-27 06:30:23,877 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1578)) - Container Balancer is not running.
2023-04-27 06:30:23,877 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stopReplicationManager(1709)) - Stopping Replication Manager Service.
2023-04-27 06:30:23,877 [Mini-Cluster-Provider-Reap] INFO  replication.ReplicationManager (ReplicationManager.java:stop(321)) - Stopping Replication Monitor Thread.
2023-04-27 06:30:23,886 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1587)) - Stopping the Datanode Admin Monitor.
2023-04-27 06:30:23,886 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:run(975)) - Replication Monitor Thread is stopped
2023-04-27 06:30:23,886 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1594)) - Stopping datanode service RPC server
2023-04-27 06:30:23,886 [Mini-Cluster-Provider-Reap] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:stop(424)) - Stopping the RPC server for DataNodes
2023-04-27 06:30:23,887 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3501)) - Stopping server on 34637
2023-04-27 06:30:23,888 [Over Replicated Processor] WARN  replication.UnhealthyReplicationProcessor (UnhealthyReplicationProcessor.java:run(146)) - Over Replicated Processor interrupted. Exiting...
2023-04-27 06:30:23,888 [Under Replicated Processor] WARN  replication.UnhealthyReplicationProcessor (UnhealthyReplicationProcessor.java:run(146)) - Under Replicated Processor interrupted. Exiting...
2023-04-27 06:30:23,893 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1430)) - Stopping IPC Server listener on 0
2023-04-27 06:30:23,893 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:23,895 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1563)) - Stopping IPC Server Responder
2023-04-27 06:30:23,902 [SCM Heartbeat Processing Thread - 0] WARN  node.NodeStateManager (NodeStateManager.java:scheduleNextHealthCheck(870)) - Current Thread is interrupted, shutting down HB processing thread for Node Manager.
2023-04-27 06:30:23,902 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1602)) - Stopping block service RPC server
2023-04-27 06:30:23,903 [Mini-Cluster-Provider-Reap] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:stop(161)) - Stopping the RPC server for Block Protocol
2023-04-27 06:30:23,904 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3501)) - Stopping server on 37175
2023-04-27 06:30:23,914 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1563)) - Stopping IPC Server Responder
2023-04-27 06:30:23,914 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1430)) - Stopping IPC Server listener on 0
2023-04-27 06:30:23,915 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1609)) - Stopping the StorageContainerLocationProtocol RPC server
2023-04-27 06:30:23,915 [Mini-Cluster-Provider-Reap] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:stop(206)) - Stopping the RPC server for Client Protocol
2023-04-27 06:30:23,917 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3501)) - Stopping server on 40303
2023-04-27 06:30:23,922 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1563)) - Stopping IPC Server Responder
2023-04-27 06:30:23,922 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1430)) - Stopping IPC Server listener on 0
2023-04-27 06:30:23,922 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1616)) - Stopping Storage Container Manager HTTP server.
2023-04-27 06:30:23,932 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@2a52c9e5{scm,/,null,STOPPED}{file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/scm}
2023-04-27 06:30:23,933 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@ad323f2{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2023-04-27 06:30:23,933 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2023-04-27 06:30:23,933 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@12ec6c17{static,/static,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/static,STOPPED}
2023-04-27 06:30:23,933 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@142e1162{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2023-04-27 06:30:23,935 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1624)) - Stopping SCM LayoutVersionManager Service.
2023-04-27 06:30:23,935 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1632)) - Stopping Block Manager Service.
2023-04-27 06:30:23,935 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SCMBlockDeletingService
2023-04-27 06:30:23,935 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SCMBlockDeletingService
2023-04-27 06:30:23,936 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1654)) - Stopping SCM Event Queue.
2023-04-27 06:30:23,938 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1665)) - Stopping SCM HA services.
2023-04-27 06:30:23,938 [Mini-Cluster-Provider-Reap] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:stop(149)) - Stopping RatisPipelineUtilsThread.
2023-04-27 06:30:23,938 [RatisPipelineUtilsThread - 0] WARN  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:run(180)) - RatisPipelineUtilsThread is interrupted.
2023-04-27 06:30:23,939 [BackgroundPipelineScrubberThread] WARN  BackgroundPipelineScrubber (BackgroundSCMService.java:run(115)) - BackgroundPipelineScrubber is interrupted, exit
2023-04-27 06:30:23,940 [Mini-Cluster-Provider-Reap] INFO  BackgroundPipelineScrubber (BackgroundSCMService.java:stop(131)) - Stopping BackgroundPipelineScrubber Service.
2023-04-27 06:30:23,940 [Mini-Cluster-Provider-Reap] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping HddsDatanode metrics system...
2023-04-27 06:30:24,001 [prometheus] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(141)) - prometheus thread interrupted.
2023-04-27 06:30:24,006 [Mini-Cluster-Provider-Reap] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - HddsDatanode metrics system stopped.
2023-04-27 06:30:24,006 [Mini-Cluster-Provider-Reap] WARN  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:stop(145)) - RatisPipelineUtilsThread is not running, just ignore.
2023-04-27 06:30:24,006 [Mini-Cluster-Provider-Reap] INFO  BackgroundPipelineScrubber (BackgroundSCMService.java:stop(126)) - BackgroundPipelineScrubber Service is not running, skip stop.
2023-04-27 06:30:24,006 [Mini-Cluster-Provider-Reap] INFO  ExpiredContainerReplicaOpScrubber (BackgroundSCMService.java:stop(131)) - Stopping ExpiredContainerReplicaOpScrubber Service.
2023-04-27 06:30:24,006 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SCMBlockDeletingService
2023-04-27 06:30:24,006 [ExpiredContainerReplicaOpScrubberThread] WARN  ExpiredContainerReplicaOpScrubber (BackgroundSCMService.java:run(115)) - ExpiredContainerReplicaOpScrubber is interrupted, exit
2023-04-27 06:30:24,006 [Mini-Cluster-Provider-Reap] INFO  replication.ReplicationManager (ReplicationManager.java:stop(331)) - Replication Monitor Thread is not running.
2023-04-27 06:30:24,006 [Mini-Cluster-Provider-Reap] WARN  balancer.ContainerBalancer (ContainerBalancer.java:stop(324)) - Cannot stop Container Balancer because it's not running or stopping
2023-04-27 06:30:24,006 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1700)) - Stopping SCM MetadataStore.
2023-04-27 06:30:24,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:24,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:24,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #3 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#3, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=6, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:24,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #3 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#3, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=6, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#3, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=6, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:24,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #4 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:24,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #4 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:24,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 0 sufficientlyReplicated, 3 underReplicated and 3 unhealthy containers
2023-04-27 06:30:24,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:24,033 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Nodes are ready. Got 7 of 7 DN Heartbeats.
2023-04-27 06:30:24,033 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:24,033 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:24,052 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: addNew group-CC149F761A2E:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:1|startupRole:FOLLOWER] returns group-CC149F761A2E:java.util.concurrent.CompletableFuture@73c85ae2[Not completed]
2023-04-27 06:30:24,053 [pool-2284-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: new RaftServerImpl for group-CC149F761A2E:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:24,053 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:24,053 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:24,053 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:24,054 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:24,054 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:24,054 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:24,054 [pool-2284-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E: ConfigurationManager, init=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:24,054 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis] (custom)
2023-04-27 06:30:24,054 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:24,054 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:24,055 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:24,055 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:24,055 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:24,056 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:24,056 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:24,056 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:24,056 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:24,056 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:24,057 [pool-2284-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis/5084858f-2ede-4f2d-b349-cc149f761a2e does not exist. Creating ...
2023-04-27 06:30:24,061 [pool-2284-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis/5084858f-2ede-4f2d-b349-cc149f761a2e/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:24,063 [pool-2284-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis/5084858f-2ede-4f2d-b349-cc149f761a2e has been successfully formatted.
2023-04-27 06:30:24,063 [pool-2284-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-CC149F761A2E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:24,064 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:24,064 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:24,064 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:24,064 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:24,064 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:24,065 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:24,065 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 5084858f-2ede-4f2d-b349-cc149f761a2e, Nodes: fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e, CreationTimestamp2023-04-27T06:30:21.059Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:24,065 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:24,065 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:24,065 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:24,065 [pool-2284-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis/5084858f-2ede-4f2d-b349-cc149f761a2e
2023-04-27 06:30:24,065 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:24,065 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:24,065 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:24,066 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:24,067 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:24,067 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:24,068 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:24,068 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:24,069 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:24,069 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:24,083 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:24,083 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:24,083 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:24,085 [pool-2284-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:24,085 [pool-2284-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:24,092 [pool-2284-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E: start as a follower, conf=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:24,092 [pool-2284-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:24,092 [pool-2284-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: start fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-FollowerState
2023-04-27 06:30:24,094 [pool-2284-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CC149F761A2E,id=fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e
2023-04-27 06:30:24,094 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:24,094 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:24,094 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:24,094 [pool-2284-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:24,095 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:24,095 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:24,099 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=5084858f-2ede-4f2d-b349-cc149f761a2e
2023-04-27 06:30:24,099 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=5084858f-2ede-4f2d-b349-cc149f761a2e.
2023-04-27 06:30:24,129 [Mini-Cluster-Provider-Reap] ERROR util.ExitUtils (ExitUtils.java:terminate(133)) - Terminating with exit status -1: Thread[Mini-Cluster-Provider-Reap,5,main] has thrown an uncaught exception
java.lang.AssertionError: Found 1 leaked objects, check logs
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksObjectMetrics.assertNoLeaks(ManagedRocksObjectMetrics.java:60)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.shutdown(MiniOzoneClusterImpl.java:466)
	at org.apache.hadoop.ozone.MiniOzoneClusterProvider.lambda$reapClusters$0(MiniOzoneClusterProvider.java:202)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:24,208 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:24,415 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:24,448 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:24,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:24,463 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(73)) - Starting replication of container 3 from [70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)] using NO_COMPRESSION
2023-04-27 06:30:24,469 [grpc-default-executor-5] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(62)) - Streaming container data (3) to other datanode with compression NO_COMPRESSION
2023-04-27 06:30:24,500 [grpc-default-executor-5] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(111)) - Sent 9728 bytes for container 3
2023-04-27 06:30:24,503 [grpc-default-executor-1] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(218)) - Container 3 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-4/data-0/containers/tmp/container-copy/container-3.tar
2023-04-27 06:30:24,505 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(88)) - Container 3 is downloaded with size 9728, starting to import.
2023-04-27 06:30:24,559 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(95)) - Container 3 is replicated successfully
2023-04-27 06:30:24,559 [ContainerReplicationThread-0] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(220)) - Successful DONE replicateContainerCommand: containerId=3, replicaIndex=0, sourceNodes=[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], priority=NORMAL, transferred 9728 bytes
2023-04-27 06:30:24,566 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: addNew group-BA28176F28A3:[efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:1|startupRole:FOLLOWER] returns group-BA28176F28A3:java.util.concurrent.CompletableFuture@119e45d8[Not completed]
2023-04-27 06:30:24,567 [pool-2310-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: new RaftServerImpl for group-BA28176F28A3:[efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:24,567 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:24,567 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:24,567 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:24,567 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:24,567 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:24,567 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:24,567 [pool-2310-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3: ConfigurationManager, init=-1: peers:[efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:24,567 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis] (custom)
2023-04-27 06:30:24,568 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:24,568 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:24,568 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:24,568 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:24,568 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:24,570 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:24,570 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:24,570 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:24,570 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:24,570 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:24,570 [pool-2310-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis/b5275d90-33a5-467d-a65a-ba28176f28a3 does not exist. Creating ...
2023-04-27 06:30:24,572 [pool-2310-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis/b5275d90-33a5-467d-a65a-ba28176f28a3/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:24,573 [pool-2310-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis/b5275d90-33a5-467d-a65a-ba28176f28a3 has been successfully formatted.
2023-04-27 06:30:24,574 [pool-2310-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-BA28176F28A3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:24,574 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:24,574 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:24,574 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:24,574 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: b5275d90-33a5-467d-a65a-ba28176f28a3, Nodes: efdbc657-ca6b-4fea-9dc1-5411634f3e98(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:efdbc657-ca6b-4fea-9dc1-5411634f3e98, CreationTimestamp2023-04-27T06:30:21.570Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:24,575 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:24,575 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:24,575 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:24,575 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:24,576 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:24,576 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:24,576 [pool-2310-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis/b5275d90-33a5-467d-a65a-ba28176f28a3
2023-04-27 06:30:24,576 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:24,576 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:24,576 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:24,576 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:24,576 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:24,576 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:24,576 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:24,576 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:24,577 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:24,578 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:24,585 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:24,585 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:24,585 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:24,586 [pool-2310-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:24,586 [pool-2310-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:24,586 [pool-2310-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3: start as a follower, conf=-1: peers:[efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:24,586 [pool-2310-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:24,586 [pool-2310-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: start efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-FollowerState
2023-04-27 06:30:24,586 [pool-2310-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BA28176F28A3,id=efdbc657-ca6b-4fea-9dc1-5411634f3e98
2023-04-27 06:30:24,586 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:24,586 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:24,586 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:24,587 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:24,587 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:24,587 [pool-2310-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:24,587 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=b5275d90-33a5-467d-a65a-ba28176f28a3
2023-04-27 06:30:24,588 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=b5275d90-33a5-467d-a65a-ba28176f28a3.
2023-04-27 06:30:24,707 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:24,887 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:24,895 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:25,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:25,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:25,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #4 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:25,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #4 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:25,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 1 sufficientlyReplicated, 2 underReplicated and 2 unhealthy containers
2023-04-27 06:30:25,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:25,033 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Nodes are ready. Got 7 of 7 DN Heartbeats.
2023-04-27 06:30:25,033 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:25,033 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:25,034 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6: addNew group-94F17CB32B95:[f88c54b2-8776-4954-8dfb-f8bea6a862a6|rpc:10.1.0.33:38947|dataStream:10.1.0.33:46011|priority:1|startupRole:FOLLOWER] returns group-94F17CB32B95:java.util.concurrent.CompletableFuture@676d9ec[Not completed]
2023-04-27 06:30:25,038 [pool-2335-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6: new RaftServerImpl for group-94F17CB32B95:[f88c54b2-8776-4954-8dfb-f8bea6a862a6|rpc:10.1.0.33:38947|dataStream:10.1.0.33:46011|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:25,038 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:25,038 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:25,038 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:25,038 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:25,038 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:25,038 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:25,039 [pool-2335-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95: ConfigurationManager, init=-1: peers:[f88c54b2-8776-4954-8dfb-f8bea6a862a6|rpc:10.1.0.33:38947|dataStream:10.1.0.33:46011|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:25,039 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data/ratis] (custom)
2023-04-27 06:30:25,039 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:25,039 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:25,039 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:25,039 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:25,039 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:25,040 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:25,040 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:25,040 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:25,040 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:25,040 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:25,041 [pool-2335-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data/ratis/d15a6767-9072-4c9d-abb6-94f17cb32b95 does not exist. Creating ...
2023-04-27 06:30:25,042 [pool-2335-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data/ratis/d15a6767-9072-4c9d-abb6-94f17cb32b95/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:25,044 [pool-2335-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data/ratis/d15a6767-9072-4c9d-abb6-94f17cb32b95 has been successfully formatted.
2023-04-27 06:30:25,045 [pool-2335-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-94F17CB32B95: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:25,045 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:25,045 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:25,045 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:25,045 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:25,045 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:25,045 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: d15a6767-9072-4c9d-abb6-94f17cb32b95, Nodes: f88c54b2-8776-4954-8dfb-f8bea6a862a6(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f88c54b2-8776-4954-8dfb-f8bea6a862a6, CreationTimestamp2023-04-27T06:30:22.035Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:25,046 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:25,046 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:25,046 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:25,046 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:25,046 [pool-2335-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data/ratis/d15a6767-9072-4c9d-abb6-94f17cb32b95
2023-04-27 06:30:25,046 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:25,046 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:25,046 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:25,046 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:25,046 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:25,047 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:25,047 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:25,047 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:25,047 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:25,048 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:25,054 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:25,055 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:25,055 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:25,055 [pool-2335-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:25,055 [pool-2335-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:25,055 [pool-2335-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95: start as a follower, conf=-1: peers:[f88c54b2-8776-4954-8dfb-f8bea6a862a6|rpc:10.1.0.33:38947|dataStream:10.1.0.33:46011|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:25,055 [pool-2335-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:25,055 [pool-2335-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6: start f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-FollowerState
2023-04-27 06:30:25,056 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:25,056 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:25,057 [pool-2335-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-94F17CB32B95,id=f88c54b2-8776-4954-8dfb-f8bea6a862a6
2023-04-27 06:30:25,057 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:25,057 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:25,057 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:25,057 [pool-2335-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:25,058 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=d15a6767-9072-4c9d-abb6-94f17cb32b95
2023-04-27 06:30:25,058 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=d15a6767-9072-4c9d-abb6-94f17cb32b95.
2023-04-27 06:30:25,305 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:25,306 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:25,311 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(73)) - Starting replication of container 3 from [70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)] using NO_COMPRESSION
2023-04-27 06:30:25,331 [grpc-default-executor-3] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(62)) - Streaming container data (3) to other datanode with compression NO_COMPRESSION
2023-04-27 06:30:25,334 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:25,335 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:25,337 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:25,338 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:25,338 [grpc-default-executor-3] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(111)) - Sent 9728 bytes for container 3
2023-04-27 06:30:25,346 [grpc-default-executor-1] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(218)) - Container 3 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/tmp/container-copy/container-3.tar
2023-04-27 06:30:25,348 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(88)) - Container 3 is downloaded with size 9728, starting to import.
2023-04-27 06:30:25,355 [DeleteContainerThread-1] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.removeContainerFromDB(BlockUtils.java:248)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.removeContainer(KeyValueContainerUtil.java:160)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.delete(KeyValueContainer.java:297)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteInternal(KeyValueHandler.java:1403)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteContainer(KeyValueHandler.java:1163)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.deleteContainer(ContainerController.java:181)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.handleInternal(DeleteContainerCommandHandler.java:108)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.lambda$handle$0(DeleteContainerCommandHandler.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 17 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 18 more
2023-04-27 06:30:25,356 [DeleteContainerThread-1] ERROR helpers.KeyValueContainerUtil (KeyValueContainerUtil.java:removeContainer(162)) - DB failure, unable to remove container. Disk need to be replaced.
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:4 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.removeContainerFromDB(BlockUtils.java:248)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.removeContainer(KeyValueContainerUtil.java:160)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.delete(KeyValueContainer.java:297)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteInternal(KeyValueHandler.java:1403)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteContainer(KeyValueHandler.java:1163)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.deleteContainer(ContainerController.java:181)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.handleInternal(DeleteContainerCommandHandler.java:108)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.lambda$handle$0(DeleteContainerCommandHandler.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:25,356 [DeleteContainerThread-1] ERROR keyvalue.KeyValueContainer (KeyValueContainer.java:delete(308)) - Failed to cleanup container. ID: 4
java.io.IOException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:4 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.removeContainer(KeyValueContainerUtil.java:164)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.delete(KeyValueContainer.java:297)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteInternal(KeyValueHandler.java:1403)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteContainer(KeyValueHandler.java:1163)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.deleteContainer(ContainerController.java:181)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.handleInternal(DeleteContainerCommandHandler.java:108)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.lambda$handle$0(DeleteContainerCommandHandler.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:4 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.removeContainerFromDB(BlockUtils.java:248)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.removeContainer(KeyValueContainerUtil.java:160)
	... 9 more
2023-04-27 06:30:25,357 [DeleteContainerThread-1] ERROR commandhandler.DeleteContainerCommandHandler (DeleteContainerCommandHandler.java:handleInternal(111)) - Exception occurred while deleting the container.
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Failed to cleanup container. ID: 4
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.delete(KeyValueContainer.java:309)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteInternal(KeyValueHandler.java:1403)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteContainer(KeyValueHandler.java:1163)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.deleteContainer(ContainerController.java:181)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.handleInternal(DeleteContainerCommandHandler.java:108)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteContainerCommandHandler.lambda$handle$0(DeleteContainerCommandHandler.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:4 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.removeContainer(KeyValueContainerUtil.java:164)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.delete(KeyValueContainer.java:297)
	... 8 more
Caused by: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:4 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.removeContainerFromDB(BlockUtils.java:248)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.removeContainer(KeyValueContainerUtil.java:160)
	... 9 more
2023-04-27 06:30:25,357 [ContainerReplicationThread-0] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.loadKVContainerDataFromFiles(BlockUtils.java:304)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.importContainerData(KeyValueContainer.java:600)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.importContainerData(KeyValueContainer.java:546)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.importContainer(KeyValueHandler.java:1043)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.importContainer(ContainerController.java:161)
	at org.apache.hadoop.ozone.container.replication.ContainerImporter.importContainer(ContainerImporter.java:101)
	at org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator.replicate(DownloadAndImportReplicator.java:92)
	at org.apache.hadoop.ozone.container.replication.MeasuredReplicator.replicate(MeasuredReplicator.java:83)
	at org.apache.hadoop.ozone.container.replication.ReplicationTask.runTask(ReplicationTask.java:122)
	at org.apache.hadoop.ozone.container.replication.ReplicationSupervisor$TaskRunner.run(ReplicationSupervisor.java:215)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 19 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 20 more
2023-04-27 06:30:25,359 [ContainerReplicationThread-0] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.removeContainerFromDB(BlockUtils.java:248)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.importContainerData(KeyValueContainer.java:571)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.importContainer(KeyValueHandler.java:1043)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.importContainer(ContainerController.java:161)
	at org.apache.hadoop.ozone.container.replication.ContainerImporter.importContainer(ContainerImporter.java:101)
	at org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator.replicate(DownloadAndImportReplicator.java:92)
	at org.apache.hadoop.ozone.container.replication.MeasuredReplicator.replicate(MeasuredReplicator.java:83)
	at org.apache.hadoop.ozone.container.replication.ReplicationTask.runTask(ReplicationTask.java:122)
	at org.apache.hadoop.ozone.container.replication.ReplicationSupervisor$TaskRunner.run(ReplicationSupervisor.java:215)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 18 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 19 more
2023-04-27 06:30:25,360 [ContainerReplicationThread-0] ERROR keyvalue.KeyValueContainer (KeyValueContainer.java:importContainerData(578)) - Can not cleanup destination directories after a container import error (cid: 3
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:3 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.removeContainerFromDB(BlockUtils.java:248)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.importContainerData(KeyValueContainer.java:571)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.importContainer(KeyValueHandler.java:1043)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.importContainer(ContainerController.java:161)
	at org.apache.hadoop.ozone.container.replication.ContainerImporter.importContainer(ContainerImporter.java:101)
	at org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator.replicate(DownloadAndImportReplicator.java:92)
	at org.apache.hadoop.ozone.container.replication.MeasuredReplicator.replicate(MeasuredReplicator.java:83)
	at org.apache.hadoop.ozone.container.replication.ReplicationTask.runTask(ReplicationTask.java:122)
	at org.apache.hadoop.ozone.container.replication.ReplicationSupervisor$TaskRunner.run(ReplicationSupervisor.java:215)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:25,360 [ContainerReplicationThread-0] ERROR replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(98)) - Container 3 replication was unsuccessful.
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:3 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.loadKVContainerDataFromFiles(BlockUtils.java:304)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.importContainerData(KeyValueContainer.java:600)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.importContainerData(KeyValueContainer.java:546)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.importContainer(KeyValueHandler.java:1043)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.importContainer(ContainerController.java:161)
	at org.apache.hadoop.ozone.container.replication.ContainerImporter.importContainer(ContainerImporter.java:101)
	at org.apache.hadoop.ozone.container.replication.DownloadAndImportReplicator.replicate(DownloadAndImportReplicator.java:92)
	at org.apache.hadoop.ozone.container.replication.MeasuredReplicator.replicate(MeasuredReplicator.java:83)
	at org.apache.hadoop.ozone.container.replication.ReplicationTask.runTask(ReplicationTask.java:122)
	at org.apache.hadoop.ozone.container.replication.ReplicationSupervisor$TaskRunner.run(ReplicationSupervisor.java:215)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:25,360 [ContainerReplicationThread-0] WARN  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(217)) - Failed FAILED replicateContainerCommand: containerId=3, replicaIndex=0, sourceNodes=[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), a1850722-adbb-4bac-9148-47228c85758b(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], priority=NORMAL, transferred 9728 bytes
2023-04-27 06:30:25,415 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:25,449 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:25,449 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:25,574 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:25,707 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:25,887 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:25,895 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:25,978 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5107585269ns, electionTimeout:5107ms
2023-04-27 06:30:25,978 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: shutdown 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-FollowerState
2023-04-27 06:30:25,979 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:25,979 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:25,979 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: start 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76
2023-04-27 06:30:25,980 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:25,980 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:25,984 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76 ELECTION round 0: submit vote requests at term 1 for -1: peers:[2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:25,984 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:25,984 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: shutdown 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76
2023-04-27 06:30:25,984 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:25,984 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-E8B5BA640BDF with new leaderId: 2cb742ca-d762-4bc5-b311-f495a87c6b6b
2023-04-27 06:30:25,984 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF: change Leader from null to 2cb742ca-d762-4bc5-b311-f495a87c6b6b at term 1 for becomeLeader, leader elected after 5136ms
2023-04-27 06:30:25,984 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:25,984 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:25,985 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:25,985 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:25,985 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:25,985 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:25,985 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:25,986 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:25,986 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:25,986 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: start 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderStateImpl
2023-04-27 06:30:25,986 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:25,989 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-LeaderElection76] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF: set configuration 0: peers:[2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:25,989 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-E8B5BA640BDF-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis/9f565819-4a1c-445f-b823-e8b5ba640bdf/current/log_inprogress_0
2023-04-27 06:30:26,002 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5037913559ns, electionTimeout:5037ms
2023-04-27 06:30:26,002 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: shutdown e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-FollowerState
2023-04-27 06:30:26,002 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:26,002 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:26,002 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: start e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77
2023-04-27 06:30:26,003 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,003 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77-1] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for 2cb742ca-d762-4bc5-b311-f495a87c6b6b
2023-04-27 06:30:26,009 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233: receive requestVote(PRE_VOTE, e5e627c9-cddd-4fde-83a7-403d4c1ca45c, group-D6A15093A233, 0, (t:0, i:0))
2023-04-27 06:30:26,009 [grpc-default-executor-1] INFO  impl.VoteContext (VoteContext.java:log(49)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-FOLLOWER: accept PRE_VOTE from e5e627c9-cddd-4fde-83a7-403d4c1ca45c: our priority 0 <= candidate's priority 1
2023-04-27 06:30:26,009 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233 replies to PRE_VOTE vote request: e5e627c9-cddd-4fde-83a7-403d4c1ca45c<-2cb742ca-d762-4bc5-b311-f495a87c6b6b#0:OK-t0. Peer's state: 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233:t0, leader=null, voted=, raftlog=Memoized:2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,010 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:26,010 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77-2] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3
2023-04-27 06:30:26,010 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:26,012 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
2023-04-27 06:30:26,012 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: e5e627c9-cddd-4fde-83a7-403d4c1ca45c<-2cb742ca-d762-4bc5-b311-f495a87c6b6b#0:OK-t0
2023-04-27 06:30:26,012 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77 PRE_VOTE round 0: result PASSED
2023-04-27 06:30:26,015 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,016 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233: receive requestVote(PRE_VOTE, e5e627c9-cddd-4fde-83a7-403d4c1ca45c, group-D6A15093A233, 0, (t:0, i:0))
2023-04-27 06:30:26,016 [grpc-default-executor-1] INFO  impl.VoteContext (VoteContext.java:log(49)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-FOLLOWER: accept PRE_VOTE from e5e627c9-cddd-4fde-83a7-403d4c1ca45c: our priority 0 <= candidate's priority 1
2023-04-27 06:30:26,016 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233 replies to PRE_VOTE vote request: e5e627c9-cddd-4fde-83a7-403d4c1ca45c<-109e7eb5-e5f2-4565-a541-6a0b1e1f79c3#0:OK-t0. Peer's state: 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233:t0, leader=null, voted=, raftlog=Memoized:109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,017 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233: receive requestVote(ELECTION, e5e627c9-cddd-4fde-83a7-403d4c1ca45c, group-D6A15093A233, 1, (t:0, i:0))
2023-04-27 06:30:26,017 [grpc-default-executor-1] INFO  impl.VoteContext (VoteContext.java:log(49)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-FOLLOWER: accept ELECTION from e5e627c9-cddd-4fde-83a7-403d4c1ca45c: our priority 0 <= candidate's priority 1
2023-04-27 06:30:26,017 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:e5e627c9-cddd-4fde-83a7-403d4c1ca45c
2023-04-27 06:30:26,017 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: shutdown 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-FollowerState
2023-04-27 06:30:26,019 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:26,019 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b: start 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-FollowerState
2023-04-27 06:30:26,019 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-FollowerState was interrupted
2023-04-27 06:30:26,019 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:26,020 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:26,021 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:26,021 [grpc-default-executor-3] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233: receive requestVote(ELECTION, e5e627c9-cddd-4fde-83a7-403d4c1ca45c, group-D6A15093A233, 1, (t:0, i:0))
2023-04-27 06:30:26,021 [grpc-default-executor-3] INFO  impl.VoteContext (VoteContext.java:log(49)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-FOLLOWER: accept ELECTION from e5e627c9-cddd-4fde-83a7-403d4c1ca45c: our priority 0 <= candidate's priority 1
2023-04-27 06:30:26,021 [grpc-default-executor-3] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:e5e627c9-cddd-4fde-83a7-403d4c1ca45c
2023-04-27 06:30:26,021 [grpc-default-executor-3] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: shutdown 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-FollowerState
2023-04-27 06:30:26,021 [grpc-default-executor-3] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: start 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-FollowerState
2023-04-27 06:30:26,021 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-FollowerState was interrupted
2023-04-27 06:30:26,022 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233 replies to ELECTION vote request: e5e627c9-cddd-4fde-83a7-403d4c1ca45c<-2cb742ca-d762-4bc5-b311-f495a87c6b6b#0:OK-t1. Peer's state: 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233:t1, leader=null, voted=e5e627c9-cddd-4fde-83a7-403d4c1ca45c, raftlog=Memoized:2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,023 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77: ELECTION PASSED received 1 response(s) and 0 exception(s):
2023-04-27 06:30:26,023 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: e5e627c9-cddd-4fde-83a7-403d4c1ca45c<-2cb742ca-d762-4bc5-b311-f495a87c6b6b#0:OK-t1
2023-04-27 06:30:26,023 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77 ELECTION round 0: result PASSED
2023-04-27 06:30:26,023 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: shutdown e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77
2023-04-27 06:30:26,023 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:26,023 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-D6A15093A233 with new leaderId: e5e627c9-cddd-4fde-83a7-403d4c1ca45c
2023-04-27 06:30:26,023 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233: change Leader from null to e5e627c9-cddd-4fde-83a7-403d4c1ca45c at term 1 for becomeLeader, leader elected after 5077ms
2023-04-27 06:30:26,023 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:26,024 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:26,023 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:26,026 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:26,025 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:26,026 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:26,026 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:26,026 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:26,026 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:26,026 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:26,027 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2023-04-27 06:30:26,027 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:26,027 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2023-04-27 06:30:26,027 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2023-04-27 06:30:26,027 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:26,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:26,025 [grpc-default-executor-3] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233 replies to ELECTION vote request: e5e627c9-cddd-4fde-83a7-403d4c1ca45c<-109e7eb5-e5f2-4565-a541-6a0b1e1f79c3#0:OK-t1. Peer's state: 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233:t1, leader=null, voted=e5e627c9-cddd-4fde-83a7-403d4c1ca45c, raftlog=Memoized:109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,024 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 7f18b1ba-62bd-48bf-be88-d6a15093a233, Nodes: 2cb742ca-d762-4bc5-b311-f495a87c6b6b(fv-az260-775/10.1.0.33)109e7eb5-e5f2-4565-a541-6a0b1e1f79c3(fv-az260-775/10.1.0.33)e5e627c9-cddd-4fde-83a7-403d4c1ca45c(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:e5e627c9-cddd-4fde-83a7-403d4c1ca45c, CreationTimestamp2023-04-27T06:30:20.200Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:26,024 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:26,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:26,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #4 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:26,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #4 Container State: CLOSED Replica Count: 4 Healthy Count: 2 Unhealthy Count: 1 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 1 inFightDel Count: 1 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#4, state=UNHEALTHY, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=0, keyCount=0, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#4, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=10, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:26,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 1 sufficientlyReplicated, 2 underReplicated and 2 unhealthy containers
2023-04-27 06:30:26,027 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:26,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:26,028 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
2023-04-27 06:30:26,028 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(200)) - HealthyPipelineSafeModeRule rule is successfully validated
2023-04-27 06:30:26,029 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(215)) - ScmSafeModeManager, all rules are successfully validated
2023-04-27 06:30:26,029 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:exitSafeMode(244)) - SCM exiting safe mode.
2023-04-27 06:30:26,029 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  ha.SCMContext (SCMContext.java:updateSafeModeStatus(228)) - Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
2023-04-27 06:30:26,029 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyStatusChanged(254)) - Service BackgroundPipelineCreator transitions to RUNNING.
2023-04-27 06:30:26,029 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  BackgroundPipelineScrubber (BackgroundSCMService.java:notifyStatusChanged(82)) - Service BackgroundPipelineScrubber transitions to RUNNING.
2023-04-27 06:30:26,029 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  ExpiredContainerReplicaOpScrubber (BackgroundSCMService.java:notifyStatusChanged(82)) - Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
2023-04-27 06:30:26,029 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  replication.ReplicationManager (ReplicationManager.java:notifyStatusChanged(1369)) - Service ReplicationManager transitions to RUNNING.
2023-04-27 06:30:26,029 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN  balancer.ContainerBalancer (ContainerBalancer.java:shouldRun(132)) - Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
2023-04-27 06:30:26,028 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:26,029 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.wait-time.min = 10ms (default)
2023-04-27 06:30:26,030 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2023-04-27 06:30:26,030 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:26,030 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2023-04-27 06:30:26,030 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2023-04-27 06:30:26,030 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:26,030 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:26,030 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:26,030 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.wait-time.min = 10ms (default)
2023-04-27 06:30:26,031 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: start e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderStateImpl
2023-04-27 06:30:26,032 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:26,034 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Nodes are ready. Got 7 of 7 DN Heartbeats.
2023-04-27 06:30:26,034 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Cluster exits safe mode
2023-04-27 06:30:26,034 [Listener at 127.0.0.1/43855] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:26,041 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-LeaderElection77] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233: set configuration 0: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,042 [Listener at 127.0.0.1/43855] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(148)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:26,042 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-D6A15093A233-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233/current/log_inprogress_0
2023-04-27 06:30:26,044 [Listener at 127.0.0.1/43855] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(148)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:26,044 [Listener at 127.0.0.1/43855] INFO  ha.SCMHANodeDetails (SCMHANodeDetails.java:loadSCMHAConfig(209)) - ServiceID for StorageContainerManager is null
2023-04-27 06:30:26,044 [Listener at 127.0.0.1/43855] INFO  ha.SCMHANodeDetails (SCMHANodeDetails.java:loadSCMHAConfig(214)) - ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2023-04-27 06:30:26,044 [Listener at 127.0.0.1/43855] WARN  utils.HAUtils (HAUtils.java:getMetaDir(350)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:26,045 [Listener at 127.0.0.1/43855] WARN  db.DBStoreBuilder (DBStoreBuilder.java:applyDBDefinition(182)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:26,066 [2cb742ca-d762-4bc5-b311-f495a87c6b6b-server-thread1] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-D6A15093A233 with new leaderId: e5e627c9-cddd-4fde-83a7-403d4c1ca45c
2023-04-27 06:30:26,066 [2cb742ca-d762-4bc5-b311-f495a87c6b6b-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233: change Leader from null to e5e627c9-cddd-4fde-83a7-403d4c1ca45c at term 1 for appendEntries, leader elected after 5186ms
2023-04-27 06:30:26,069 [2cb742ca-d762-4bc5-b311-f495a87c6b6b-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233: set configuration 0: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,071 [2cb742ca-d762-4bc5-b311-f495a87c6b6b-server-thread1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:26,075 [2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 2cb742ca-d762-4bc5-b311-f495a87c6b6b@group-D6A15093A233-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-0/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233/current/log_inprogress_0
2023-04-27 06:30:26,077 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-server-thread1] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-D6A15093A233 with new leaderId: e5e627c9-cddd-4fde-83a7-403d4c1ca45c
2023-04-27 06:30:26,078 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233: change Leader from null to e5e627c9-cddd-4fde-83a7-403d4c1ca45c at term 1 for appendEntries, leader elected after 5169ms
2023-04-27 06:30:26,085 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-server-thread2] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233: set configuration 0: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER, 2cb742ca-d762-4bc5-b311-f495a87c6b6b|rpc:10.1.0.33:41533|dataStream:10.1.0.33:42543|priority:0|startupRole:FOLLOWER, 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,085 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3-server-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:26,087 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-D6A15093A233-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis/7f18b1ba-62bd-48bf-be88-d6a15093a233/current/log_inprogress_0
2023-04-27 06:30:26,204 [Listener at 127.0.0.1/43855] INFO  net.NodeSchemaLoader (NodeSchemaLoader.java:loadSchemaFromFile(129)) - Loading schema from [jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-common/1.4.0-SNAPSHOT/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
2023-04-27 06:30:26,204 [Listener at 127.0.0.1/43855] INFO  net.NodeSchemaLoader (NodeSchemaLoader.java:loadSchema(176)) - Loading network topology layer schema file
2023-04-27 06:30:26,210 [Listener at 127.0.0.1/43855] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:26,210 [Listener at 127.0.0.1/43855] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:26,210 [Listener at 127.0.0.1/43855] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:26,249 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5035764905ns, electionTimeout:5035ms
2023-04-27 06:30:26,250 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: shutdown 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-FollowerState
2023-04-27 06:30:26,250 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:26,250 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:26,250 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: start 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78
2023-04-27 06:30:26,252 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,252 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:26,254 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78 ELECTION round 0: submit vote requests at term 1 for -1: peers:[109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,254 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:26,254 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: shutdown 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78
2023-04-27 06:30:26,254 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:26,254 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-926679737466 with new leaderId: 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3
2023-04-27 06:30:26,255 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466: change Leader from null to 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3 at term 1 for becomeLeader, leader elected after 5058ms
2023-04-27 06:30:26,255 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:26,255 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:26,255 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:26,256 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:26,256 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:26,256 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:26,256 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:26,256 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:26,256 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: start 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderStateImpl
2023-04-27 06:30:26,257 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:26,258 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-2/data/ratis/b0ff91ae-07f7-40d4-aaaf-926679737466/current/log_inprogress_0
2023-04-27 06:30:26,262 [109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466-LeaderElection78] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 109e7eb5-e5f2-4565-a541-6a0b1e1f79c3@group-926679737466: set configuration 0: peers:[109e7eb5-e5f2-4565-a541-6a0b1e1f79c3|rpc:10.1.0.33:41035|dataStream:10.1.0.33:39055|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,268 [Listener at 127.0.0.1/43855] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 56 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:26,270 [Listener at 127.0.0.1/43855] INFO  ha.SequenceIdGenerator (SequenceIdGenerator.java:upgradeToSequenceId(349)) - upgrade localId to 111677748019200000
2023-04-27 06:30:26,270 [Listener at 127.0.0.1/43855] INFO  ha.SequenceIdGenerator (SequenceIdGenerator.java:upgradeToSequenceId(359)) - upgrade delTxnId to 0
2023-04-27 06:30:26,270 [Listener at 127.0.0.1/43855] INFO  ha.SequenceIdGenerator (SequenceIdGenerator.java:upgradeToSequenceId(376)) - upgrade containerId to 0
2023-04-27 06:30:26,270 [Listener at 127.0.0.1/43855] INFO  ha.SequenceIdGenerator (SequenceIdGenerator.java:<init>(220)) - Init the HA SequenceIdGenerator.
2023-04-27 06:30:26,271 [Listener at 127.0.0.1/43855] INFO  node.SCMNodeManager (SCMNodeManager.java:<init>(156)) - Entering startup safe mode.
2023-04-27 06:30:26,271 [Listener at 127.0.0.1/43855] INFO  algorithms.ContainerPlacementPolicyFactory (ContainerPlacementPolicyFactory.java:getPolicyInternal(86)) - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
2023-04-27 06:30:26,271 [Listener at 127.0.0.1/43855] INFO  algorithms.ContainerPlacementPolicyFactory (ContainerPlacementPolicyFactory.java:getPolicyInternal(86)) - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2023-04-27 06:30:26,271 [Listener at 127.0.0.1/43855] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:initialize(78)) - No pipeline exists in current db
2023-04-27 06:30:26,271 [Listener at 127.0.0.1/43855] INFO  algorithms.LeaderChoosePolicyFactory (LeaderChoosePolicyFactory.java:getPolicy(57)) - Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
2023-04-27 06:30:26,271 [Listener at 127.0.0.1/43855] INFO  algorithms.ContainerPlacementPolicyFactory (ContainerPlacementPolicyFactory.java:getPolicyInternal(86)) - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
2023-04-27 06:30:26,272 [Listener at 127.0.0.1/43855] INFO  ha.SCMServiceManager (SCMServiceManager.java:register(42)) - Registering service BackgroundPipelineCreator.
2023-04-27 06:30:26,272 [Listener at 127.0.0.1/43855] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:start(124)) - Starting RatisPipelineUtilsThread.
2023-04-27 06:30:26,272 [Listener at 127.0.0.1/43855] INFO  BackgroundPipelineScrubber (BackgroundSCMService.java:start(68)) - Starting BackgroundPipelineScrubber Service.
2023-04-27 06:30:26,272 [Listener at 127.0.0.1/43855] INFO  ha.SCMServiceManager (SCMServiceManager.java:register(42)) - Registering service BackgroundPipelineScrubber.
2023-04-27 06:30:26,272 [Listener at 127.0.0.1/43855] INFO  ExpiredContainerReplicaOpScrubber (BackgroundSCMService.java:start(68)) - Starting ExpiredContainerReplicaOpScrubber Service.
2023-04-27 06:30:26,272 [Listener at 127.0.0.1/43855] INFO  ha.SCMServiceManager (SCMServiceManager.java:register(42)) - Registering service ExpiredContainerReplicaOpScrubber.
2023-04-27 06:30:26,273 [Listener at 127.0.0.1/43855] INFO  algorithms.PipelineChoosePolicyFactory (PipelineChoosePolicyFactory.java:createPipelineChoosePolicyFromClass(73)) - Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
2023-04-27 06:30:26,274 [Listener at 127.0.0.1/43855] INFO  ha.SCMServiceManager (SCMServiceManager.java:register(42)) - Registering service SCMBlockDeletingService.
2023-04-27 06:30:26,274 [Listener at 127.0.0.1/43855] INFO  replication.ReplicationManager (ReplicationManager.java:start(288)) - Starting Replication Monitor Thread.
2023-04-27 06:30:26,276 [Listener at 127.0.0.1/43855] INFO  ha.SCMServiceManager (SCMServiceManager.java:register(42)) - Registering service ReplicationManager.
2023-04-27 06:30:26,276 [Listener at 127.0.0.1/43855] INFO  safemode.ContainerSafeModeRule (ContainerSafeModeRule.java:<init>(89)) - containers with one replica threshold count 0
2023-04-27 06:30:26,276 [Listener at 127.0.0.1/43855] INFO  safemode.HealthyPipelineSafeModeRule (HealthyPipelineSafeModeRule.java:initializeRule(169)) - Total pipeline count is 0, healthy pipeline threshold count is 1
2023-04-27 06:30:26,276 [Listener at 127.0.0.1/43855] INFO  safemode.OneReplicaPipelineSafeModeRule (OneReplicaPipelineSafeModeRule.java:initializeRule(180)) - Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
2023-04-27 06:30:26,277 [Listener at 127.0.0.1/43855] INFO  server.StorageContainerManager (StorageContainerManager.java:<init>(403)) - SCM start with adminUsers: [runner]
2023-04-27 06:30:26,277 [Listener at 127.0.0.1/43855] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(90)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-04-27 06:30:26,278 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1319)) - Starting Socket Reader #1 for port 0
2023-04-27 06:30:26,280 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:26,278 [Listener at 0.0.0.0/35697] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(90)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-04-27 06:30:26,281 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1319)) - Starting Socket Reader #1 for port 0
2023-04-27 06:30:26,281 [Listener at 0.0.0.0/46383] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(90)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-04-27 06:30:26,282 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1319)) - Starting Socket Reader #1 for port 0
2023-04-27 06:30:26,285 [Listener at 0.0.0.0/43297] INFO  ha.SCMServiceManager (SCMServiceManager.java:register(42)) - Registering service ContainerBalancer.
2023-04-27 06:30:26,285 [Listener at 0.0.0.0/43297] INFO  server.StorageContainerManager (StorageContainerManager.java:<init>(420)) - 
Container Balancer status:
Key                            Value
Running                        false
Container Balancer Configuration values:
Key                                                Value
Threshold                                          10
Max Datanodes to Involve per Iteration(percent)    20
Max Size to Move per Iteration                     500GB
Max Size Entering Target per Iteration             26GB
Max Size Leaving Source per Iteration              26GB

2023-04-27 06:30:26,285 [Listener at 0.0.0.0/43297] INFO  ha.SCMContext (SCMContext.java:updateSafeModeStatus(228)) - Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
2023-04-27 06:30:26,285 [Listener at 0.0.0.0/43297] INFO  server.StorageContainerManager (StorageContainerManager.java:start(1477)) - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:43297
2023-04-27 06:30:26,287 [Listener at 0.0.0.0/43297] WARN  impl.MetricsConfig (MetricsConfig.java:loadFirst(136)) - Cannot locate configuration: tried hadoop-metrics2-storagecontainermanager.properties,hadoop-metrics2.properties
2023-04-27 06:30:26,300 [Listener at 0.0.0.0/43297] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(378)) - Scheduled Metric snapshot period at 10 second(s).
2023-04-27 06:30:26,300 [Listener at 0.0.0.0/43297] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - StorageContainerManager metrics system started
2023-04-27 06:30:26,322 [Listener at 0.0.0.0/43297] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:start(204)) - Sink prometheus started
2023-04-27 06:30:26,322 [Listener at 0.0.0.0/43297] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:registerSink(305)) - Registered sink prometheus
2023-04-27 06:30:26,354 [Listener at 0.0.0.0/43297] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:start(197)) - RPC server for Client  is listening at /0.0.0.0:43297
2023-04-27 06:30:26,359 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1558)) - IPC Server Responder: starting
2023-04-27 06:30:26,360 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1398)) - IPC Server listener on 0: starting
2023-04-27 06:30:26,377 [Listener at 0.0.0.0/43297] INFO  server.StorageContainerManager (StorageContainerManager.java:start(1491)) - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:46383
2023-04-27 06:30:26,377 [Listener at 0.0.0.0/43297] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:start(152)) - RPC server for Block Protocol is listening at /0.0.0.0:46383
2023-04-27 06:30:26,379 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1558)) - IPC Server Responder: starting
2023-04-27 06:30:26,387 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1398)) - IPC Server listener on 0: starting
2023-04-27 06:30:26,389 [Listener at 0.0.0.0/43297] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:start(193)) - ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:35697
2023-04-27 06:30:26,389 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1558)) - IPC Server Responder: starting
2023-04-27 06:30:26,390 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1398)) - IPC Server listener on 0: starting
2023-04-27 06:30:26,392 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5fbe5ea7] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:26,393 [Listener at 0.0.0.0/43297] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for scm at: http://0.0.0.0:0
2023-04-27 06:30:26,393 [Listener at 0.0.0.0/43297] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:26,394 [Listener at 0.0.0.0/43297] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:26,402 [Listener at 0.0.0.0/43297] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.scm is not defined
2023-04-27 06:30:26,403 [Listener at 0.0.0.0/43297] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:26,403 [Listener at 0.0.0.0/43297] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
2023-04-27 06:30:26,403 [Listener at 0.0.0.0/43297] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:26,403 [Listener at 0.0.0.0/43297] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:26,404 [Listener at 0.0.0.0/43297] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of scm uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/ozone-meta/webserver
2023-04-27 06:30:26,404 [Listener at 0.0.0.0/43297] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 42957
2023-04-27 06:30:26,404 [Listener at 0.0.0.0/43297] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:26,411 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(73)) - Starting replication of container 4 from [70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)] using NO_COMPRESSION
2023-04-27 06:30:26,417 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:26,419 [grpc-default-executor-1] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(62)) - Streaming container data (4) to other datanode with compression NO_COMPRESSION
2023-04-27 06:30:26,430 [Listener at 0.0.0.0/43297] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:26,431 [Listener at 0.0.0.0/43297] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:26,431 [Listener at 0.0.0.0/43297] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 600000ms
2023-04-27 06:30:26,435 [Listener at 0.0.0.0/43297] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@15b4469f{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:26,436 [Listener at 0.0.0.0/43297] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@232f4bc{static,/static,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/static,AVAILABLE}
2023-04-27 06:30:26,441 [Listener at 0.0.0.0/43297] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@c5e38c2{scm,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/scm/,AVAILABLE}{file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/scm}
2023-04-27 06:30:26,458 [Listener at 0.0.0.0/43297] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@c4aabfd{HTTP/1.1, (http/1.1)}{0.0.0.0:42957}
2023-04-27 06:30:26,458 [Listener at 0.0.0.0/43297] INFO  server.Server (Server.java:doStart(415)) - Started @152906ms
2023-04-27 06:30:26,458 [Listener at 0.0.0.0/43297] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:26,459 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:26,459 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:26,459 [Listener at 0.0.0.0/43297] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of scm listening at http://0.0.0.0:42957
2023-04-27 06:30:26,460 [Listener at 0.0.0.0/43297] WARN  server.ServerUtils (ServerUtils.java:getDBPath(225)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:26,462 [Listener at 0.0.0.0/43297] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:loadOMHAConfig(115)) - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
2023-04-27 06:30:26,462 [Listener at 0.0.0.0/43297] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:loadOMHAConfig(226)) - Configuration does not have ozone.om.address set. Falling back to the default OM address /127.0.0.1:0
2023-04-27 06:30:26,462 [Listener at 0.0.0.0/43297] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:getOMNodeDetailsForNonHA(254)) - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
2023-04-27 06:30:26,462 [Listener at 0.0.0.0/43297] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:getOMNodeDetailsForNonHA(261)) - OM Node ID is not set. Setting it to the default ID: om1
2023-04-27 06:30:26,463 [Listener at 0.0.0.0/43297] WARN  server.ServerUtils (ServerUtils.java:getDBPath(225)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:26,467 [Listener at 0.0.0.0/43297] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = HSYNC (version = 4), software layout = HSYNC (version = 4)
2023-04-27 06:30:26,496 [grpc-default-executor-1] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(111)) - Sent 9728 bytes for container 4
2023-04-27 06:30:26,497 [grpc-default-executor-4] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(218)) - Container 4 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-6/data-0/containers/tmp/container-copy/container-4.tar
2023-04-27 06:30:26,498 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(88)) - Container 4 is downloaded with size 9728, starting to import.
2023-04-27 06:30:26,579 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(95)) - Container 4 is replicated successfully
2023-04-27 06:30:26,579 [ContainerReplicationThread-0] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(220)) - Successful DONE replicateContainerCommand: containerId=4, replicaIndex=0, sourceNodes=[70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5(fv-az260-775/10.1.0.33), ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33)], priority=NORMAL, transferred 9728 bytes
2023-04-27 06:30:26,608 [Listener at 0.0.0.0/43297] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 138 ms to scan 2 urls, producing 172 keys and 478 values [using 2 cores]
2023-04-27 06:30:26,608 [Listener at 0.0.0.0/43297] INFO  upgrade.OMLayoutVersionManager (OMLayoutVersionManager.java:lambda$0(115)) - Skipping Upgrade Action MockOmUpgradeAction since it has been finalized.
2023-04-27 06:30:26,609 [Listener at 0.0.0.0/43297] WARN  server.ServerUtils (ServerUtils.java:getDBPath(225)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:26,609 [Listener at 0.0.0.0/43297] INFO  proxy.SCMBlockLocationFailoverProxyProvider (SCMBlockLocationFailoverProxyProvider.java:<init>(114)) - Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=/0.0.0.0:46383]
2023-04-27 06:30:26,611 [Listener at 0.0.0.0/43297] INFO  proxy.SCMBlockLocationFailoverProxyProvider (SCMBlockLocationFailoverProxyProvider.java:<init>(114)) - Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=/0.0.0.0:46383]
2023-04-27 06:30:26,622 [Listener at 0.0.0.0/43297] INFO  om.OzoneManager (OzoneManager.java:<init>(634)) - OM start with adminUsers: [runner]
2023-04-27 06:30:26,622 [Listener at 0.0.0.0/43297] WARN  server.ServerUtils (ServerUtils.java:getDBPath(225)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:26,622 [Listener at 0.0.0.0/43297] INFO  codec.OmKeyInfoCodec (OmKeyInfoCodec.java:<init>(49)) - OmKeyInfoCodec ignorePipeline = true
2023-04-27 06:30:26,623 [Listener at 0.0.0.0/43297] INFO  codec.RepeatedOmKeyInfoCodec (RepeatedOmKeyInfoCodec.java:<init>(41)) - RepeatedOmKeyInfoCodec ignorePipeline = true
2023-04-27 06:30:26,877 [Listener at 0.0.0.0/43297] INFO  om.OzoneManager (OzoneManager.java:instantiateServices(764)) - S3 Multi-Tenancy is disabled
2023-04-27 06:30:26,878 [Listener at 0.0.0.0/43297] WARN  server.ServerUtils (ServerUtils.java:getDBPath(225)) - ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2023-04-27 06:30:26,904 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5000698933ns, electionTimeout:5000ms
2023-04-27 06:30:26,904 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: shutdown e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-FollowerState
2023-04-27 06:30:26,904 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:26,904 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:26,904 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: start e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79
2023-04-27 06:30:26,905 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,905 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:26,905 [Listener at 0.0.0.0/43297] INFO  om.OzoneManager (OzoneManager.java:addS3GVolumeToDB(4263)) - Created Volume s3v With Owner runner required for S3Gateway operations.
2023-04-27 06:30:26,905 [Listener at 0.0.0.0/43297] WARN  server.ServerUtils (ServerUtils.java:getDefaultRatisDirectory(237)) - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
2023-04-27 06:30:26,905 [Listener at 0.0.0.0/43297] WARN  utils.OzoneManagerRatisUtils (OzoneManagerRatisUtils.java:getOMRatisSnapshotDirectory(459)) - ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
2023-04-27 06:30:26,906 [Listener at 0.0.0.0/43297] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:26,906 [Listener at 0.0.0.0/43297] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:26,906 [Listener at 0.0.0.0/43297] WARN  server.ServerUtils (ServerUtils.java:getDefaultRatisDirectory(237)) - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
2023-04-27 06:30:26,906 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,907 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:26,907 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: shutdown e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79
2023-04-27 06:30:26,907 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:26,907 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-CB9E9B7E8B9E with new leaderId: e5e627c9-cddd-4fde-83a7-403d4c1ca45c
2023-04-27 06:30:26,908 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E: change Leader from null to e5e627c9-cddd-4fde-83a7-403d4c1ca45c at term 1 for becomeLeader, leader elected after 5020ms
2023-04-27 06:30:26,908 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:26,909 [Listener at 0.0.0.0/43297] INFO  ratis.OzoneManagerRatisServer (OzoneManagerRatisServer.java:<init>(163)) - Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: localhost:33323
2023-04-27 06:30:26,909 [Listener at 0.0.0.0/43297] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:loadSnapshotInfoFromDB(670)) - LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
2023-04-27 06:30:26,909 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:26,909 [Listener at 0.0.0.0/43297] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:26,909 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:26,909 [Listener at 0.0.0.0/43297] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:26,909 [Listener at 0.0.0.0/43297] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.port = 33323 (fallback to raft.grpc.server.port)
2023-04-27 06:30:26,909 [Listener at 0.0.0.0/43297] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:26,910 [Listener at 0.0.0.0/43297] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.port = 33323 (fallback to raft.grpc.server.port)
2023-04-27 06:30:26,910 [Listener at 0.0.0.0/43297] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:26,910 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:26,910 [Listener at 0.0.0.0/43297] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 33323 (custom)
2023-04-27 06:30:26,910 [Listener at 0.0.0.0/43297] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 33554432 (custom)
2023-04-27 06:30:26,910 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:26,910 [Listener at 0.0.0.0/43297] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:26,910 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:26,910 [Listener at 0.0.0.0/43297] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2023-04-27 06:30:26,910 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:26,910 [Listener at 0.0.0.0/43297] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 3000ms (default)
2023-04-27 06:30:26,910 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:26,911 [Listener at 0.0.0.0/43297] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:26,911 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c: start e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderStateImpl
2023-04-27 06:30:26,911 [Listener at 0.0.0.0/43297] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:26,911 [Listener at 0.0.0.0/43297] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:26,911 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:26,917 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-LeaderElection79] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E: set configuration 0: peers:[e5e627c9-cddd-4fde-83a7-403d4c1ca45c|rpc:10.1.0.33:33449|dataStream:10.1.0.33:43189|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:26,917 [e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - e5e627c9-cddd-4fde-83a7-403d4c1ca45c@group-CB9E9B7E8B9E-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-1/data/ratis/08f3233d-81d6-42ac-8cfc-cb9e9b7e8b9e/current/log_inprogress_0
2023-04-27 06:30:26,919 [Listener at 0.0.0.0/43297] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = DISABLED (default)
2023-04-27 06:30:26,919 [Listener at 0.0.0.0/43297] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:26,919 [Listener at 0.0.0.0/43297] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:26,919 [Listener at 0.0.0.0/43297] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 120s (custom)
2023-04-27 06:30:26,919 [Listener at 0.0.0.0/43297] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:26,919 [Listener at 0.0.0.0/43297] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/ozone-meta/ratis] (custom)
2023-04-27 06:30:26,920 [Listener at 0.0.0.0/43297] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - om1: addNew group-C5BA1605619E:[om1|rpc:localhost:33323|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@6ac5d07b[Not completed]
2023-04-27 06:30:26,920 [Listener at 0.0.0.0/43297] INFO  om.OzoneManager (OzoneManager.java:initializeRatisServer(2106)) - OzoneManager Ratis server initialized at port 33323
2023-04-27 06:30:26,920 [Listener at 0.0.0.0/43297] INFO  om.OzoneManager (OzoneManager.java:getRpcServer(1153)) - Creating RPC Server
2023-04-27 06:30:26,921 [pool-2481-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:localhost:33323|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
2023-04-27 06:30:26,922 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 1s (custom)
2023-04-27 06:30:26,922 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 1200ms (custom)
2023-04-27 06:30:26,922 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:26,922 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 120s (custom)
2023-04-27 06:30:26,922 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:26,923 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:26,923 [pool-2481-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:localhost:33323|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:26,923 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/ozone-meta/ratis] (custom)
2023-04-27 06:30:26,923 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:26,923 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:26,923 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 120s (custom)
2023-04-27 06:30:26,923 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 300s (custom)
2023-04-27 06:30:26,923 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:26,925 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:26,925 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:26,925 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:26,925 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:26,925 [pool-2481-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:27,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:27,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:27,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:27,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:27,743 [JvmPauseMonitor46] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: Detected pause in JVM or host machine (eg GC): pause of approximately 124959149ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,743 [JvmPauseMonitor47] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-efdbc657-ca6b-4fea-9dc1-5411634f3e98: Detected pause in JVM or host machine (eg GC): pause of approximately 125329069ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,744 [JvmPauseMonitor48] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-f88c54b2-8776-4954-8dfb-f8bea6a862a6: Detected pause in JVM or host machine (eg GC): pause of approximately 151418925ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,745 [JvmPauseMonitor42] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-2cb742ca-d762-4bc5-b311-f495a87c6b6b: Detected pause in JVM or host machine (eg GC): pause of approximately 177537081ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,746 [JvmPauseMonitor37] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-a1850722-adbb-4bac-9148-47228c85758b: Detected pause in JVM or host machine (eg GC): pause of approximately 240115071ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,746 [JvmPauseMonitor34] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-70ce52ad-a2f4-4f2c-b96d-cd309621d39c: Detected pause in JVM or host machine (eg GC): pause of approximately 240185374ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,746 [JvmPauseMonitor38] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-7a6d3cb5-ca5e-45e5-99fe-a8e18444e416: Detected pause in JVM or host machine (eg GC): pause of approximately 240335383ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,746 [JvmPauseMonitor43] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-e5e627c9-cddd-4fde-83a7-403d4c1ca45c: Detected pause in JVM or host machine (eg GC): pause of approximately 240597297ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,746 [JvmPauseMonitor39] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-b2fca75a-ef6a-4f56-9857-0ffdedbdf9d5: Detected pause in JVM or host machine (eg GC): pause of approximately 242674513ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,747 [JvmPauseMonitor35] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-dbe8ec56-fdf9-4402-aab9-993d7a20391d: Detected pause in JVM or host machine (eg GC): pause of approximately 242825122ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,748 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:27,748 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:27,748 [JvmPauseMonitor36] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-ef6aab41-a1eb-41b3-a4a5-2458878a1611: Detected pause in JVM or host machine (eg GC): pause of approximately 312676917ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,748 [JvmPauseMonitor33] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-om1: Detected pause in JVM or host machine (eg GC): pause of approximately 312755021ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,748 [JvmPauseMonitor40] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b: Detected pause in JVM or host machine (eg GC): pause of approximately 312833825ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,749 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:27,749 [7aa2e1ad-d98e-4944-abb6-de8c1f6ef39b@group-50FF7EBD4A6D-FollowerState] WARN  impl.FollowerState (FollowerState.java:run(130)) - Unexpected long sleep: sleep 5066ms but took extra 364738188ns (> threshold = 300ms)
2023-04-27 06:30:27,750 [dbe8ec56-fdf9-4402-aab9-993d7a20391d@group-50FF7EBD4A6D-FollowerState] WARN  impl.FollowerState (FollowerState.java:run(130)) - Unexpected long sleep: sleep 5034ms but took extra 399220023ns (> threshold = 300ms)
2023-04-27 06:30:27,752 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:27,753 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:27,753 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:27,756 [JvmPauseMonitor41] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-om1: Detected pause in JVM or host machine (eg GC): pause of approximately 498680789ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,757 [JvmPauseMonitor44] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-109e7eb5-e5f2-4565-a541-6a0b1e1f79c3: Detected pause in JVM or host machine (eg GC): pause of approximately 515406121ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,757 [JvmPauseMonitor45] WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:detectPause(126)) - JvmPauseMonitor-3804776e-bffa-42a7-addb-662dc325de80: Detected pause in JVM or host machine (eg GC): pause of approximately 517351830ns.
GC pool 'PS MarkSweep' had collection(s): count=1 time=470ms
GC pool 'PS Scavenge' had collection(s): count=1 time=97ms
2023-04-27 06:30:27,914 [Listener at 0.0.0.0/43297] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 993 ms to scan 19 urls, producing 68 keys and 5104 values [using 2 cores]
2023-04-27 06:30:27,915 [Listener at 0.0.0.0/43297] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(90)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2023-04-27 06:30:27,916 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1319)) - Starting Socket Reader #1 for port 0
2023-04-27 06:30:27,941 [Listener at 127.0.0.1/32931] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - OzoneManager metrics system started (again)
2023-04-27 06:30:27,951 [Listener at 127.0.0.1/32931] INFO  om.OzoneManager (OzoneManager.java:start(1574)) - OzoneManager RPC server is listening at localhost/127.0.0.1:32931
2023-04-27 06:30:27,951 [Listener at 127.0.0.1/32931] INFO  ratis.OzoneManagerRatisServer (OzoneManagerRatisServer.java:start(558)) - Starting OzoneManagerRatisServer om1 at port 33323
2023-04-27 06:30:27,951 [om1-impl-thread1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/ozone-meta/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
2023-04-27 06:30:27,953 [om1-impl-thread1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/ozone-meta/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:27,954 [om1-impl-thread1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/ozone-meta/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
2023-04-27 06:30:27,955 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:27,955 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:27,955 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:27,955 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:27,955 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:27,955 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 4194304 (custom)
2023-04-27 06:30:27,956 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:27,956 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:27,956 [om1-impl-thread1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/ozone-meta/ratis/bf265839-605b-3f16-9796-c5ba1605619e
2023-04-27 06:30:27,956 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2023-04-27 06:30:27,956 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 4096 (default)
2023-04-27 06:30:27,956 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 4194304 (custom)
2023-04-27 06:30:27,956 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 4194304 (custom)
2023-04-27 06:30:27,956 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:27,956 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:27,956 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:27,956 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:27,957 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 64KB (=65536) (default)
2023-04-27 06:30:27,957 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:27,964 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:27,965 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:27,965 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = false (default)
2023-04-27 06:30:27,965 [om1-impl-thread1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:27,965 [om1-impl-thread1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:27,965 [om1-impl-thread1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:localhost:33323|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:27,965 [om1-impl-thread1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:27,966 [om1-impl-thread1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - om1: start om1@group-C5BA1605619E-FollowerState
2023-04-27 06:30:27,966 [om1-impl-thread1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
2023-04-27 06:30:27,966 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:27,966 [om1@group-C5BA1605619E-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 1s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:27,966 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 400000 (default)
2023-04-27 06:30:27,967 [om1@group-C5BA1605619E-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 1200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:27,967 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = -1 (default)
2023-04-27 06:30:27,967 [om1-impl-thread1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = true (custom)
2023-04-27 06:30:27,967 [Listener at 127.0.0.1/32931] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - om1: start RPC server
2023-04-27 06:30:27,968 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - om1: GrpcService started, listening on 33323
2023-04-27 06:30:27,968 [Listener at 127.0.0.1/32931] INFO  om.OzoneManager (OzoneManager.java:start(1590)) - Version File has different layout version (4) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
2023-04-27 06:30:27,968 [JvmPauseMonitor49] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-om1: Started
2023-04-27 06:30:27,971 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for ozoneManager at: http://0.0.0.0:0
2023-04-27 06:30:27,971 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:27,981 [Listener at 127.0.0.1/32931] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:27,982 [Listener at 127.0.0.1/32931] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.ozoneManager is not defined
2023-04-27 06:30:27,983 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:27,984 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
2023-04-27 06:30:27,984 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:27,984 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:27,984 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of ozoneManager uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/ozone-meta/webserver
2023-04-27 06:30:27,988 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 35641
2023-04-27 06:30:27,989 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:27,997 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:27,997 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:28,006 [Listener at 127.0.0.1/32931] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 600000ms
2023-04-27 06:30:28,006 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@371c485e{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:28,007 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@43f362ca{static,/static,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/static,AVAILABLE}
2023-04-27 06:30:28,009 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@20334300{ozoneManager,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/ozoneManager/,AVAILABLE}{file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/ozoneManager}
2023-04-27 06:30:28,015 [Listener at 127.0.0.1/32931] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@590cf1eb{HTTP/1.1, (http/1.1)}{0.0.0.0:35641}
2023-04-27 06:30:28,015 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(415)) - Started @154462ms
2023-04-27 06:30:28,015 [Listener at 127.0.0.1/32931] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:28,015 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of ozoneManager listening at http://0.0.0.0:35641
2023-04-27 06:30:28,016 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1558)) - IPC Server Responder: starting
2023-04-27 06:30:28,016 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1398)) - IPC Server listener on 0: starting
2023-04-27 06:30:28,021 [Listener at 127.0.0.1/32931] INFO  om.OzoneManager (OzoneManager.java:startTrashEmptier(2050)) - Trash Interval set to 0. Files deleted won't move to trash
2023-04-27 06:30:28,021 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6cd1f569] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:28,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:28,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:28,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:28,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:28,039 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:28,039 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:28,039 [Listener at 127.0.0.1/32931] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:28,052 [Listener at 127.0.0.1/32931] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:28,089 [Listener at 127.0.0.1/32931] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:28,144 [Listener at 127.0.0.1/32931] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 53 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:28,145 [Listener at 127.0.0.1/32931] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:28,147 [Listener at 127.0.0.1/32931] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:28,147 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:28,147 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data-0/containers/hdds
2023-04-27 06:30:28,148 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data-0/containers/hdds
2023-04-27 06:30:28,160 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis to VolumeSet
2023-04-27 06:30:28,162 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis
2023-04-27 06:30:28,162 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis
2023-04-27 06:30:28,176 [Thread-3174] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data-0/containers/hdds
2023-04-27 06:30:28,176 [Listener at 127.0.0.1/32931] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:28,178 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:28,179 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:28,179 [Listener at 127.0.0.1/32931] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:28,180 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:28,180 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:28,180 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:28,180 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:28,180 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:28,180 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:28,180 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:28,181 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:28,181 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:28,181 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:28,182 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:28,183 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:28,183 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x4fb3dd29] REGISTERED
2023-04-27 06:30:28,183 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:28,183 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:28,183 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis] (custom)
2023-04-27 06:30:28,183 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x4fb3dd29] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:28,184 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x4fb3dd29, L:/0:0:0:0:0:0:0:0:40691] ACTIVE
2023-04-27 06:30:28,185 [Listener at 127.0.0.1/32931] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:28,188 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:28,188 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:28,188 [Listener at 127.0.0.1/32931] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:28,189 [Listener at 127.0.0.1/32931] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:28,190 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:28,190 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:28,190 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:28,190 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:28,190 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/meta/webserver
2023-04-27 06:30:28,190 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 43203
2023-04-27 06:30:28,190 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:28,201 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:28,201 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:28,201 [Listener at 127.0.0.1/32931] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 600000ms
2023-04-27 06:30:28,202 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@22335d{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:28,203 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@20d62366{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:28,415 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@5ef277e4{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/meta/webserver/jetty-0_0_0_0-43203-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-4636002931462154340/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:28,420 [Listener at 127.0.0.1/32931] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@2c9d021c{HTTP/1.1, (http/1.1)}{0.0.0.0:43203}
2023-04-27 06:30:28,420 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(415)) - Started @154867ms
2023-04-27 06:30:28,420 [Listener at 127.0.0.1/32931] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:28,421 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:43203
2023-04-27 06:30:28,427 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:28,427 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:28,427 [Listener at 127.0.0.1/32931] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:28,428 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:28,440 [Listener at 127.0.0.1/32931] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:28,441 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6cd68188] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:28,446 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/meta/datanode.id
2023-04-27 06:30:28,472 [Listener at 127.0.0.1/32931] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:28,525 [Listener at 127.0.0.1/32931] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 52 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:28,527 [Listener at 127.0.0.1/32931] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:28,528 [Listener at 127.0.0.1/32931] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:28,528 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:28,529 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data-0/containers/hdds
2023-04-27 06:30:28,529 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data-0/containers/hdds
2023-04-27 06:30:28,539 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis to VolumeSet
2023-04-27 06:30:28,539 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis
2023-04-27 06:30:28,540 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis
2023-04-27 06:30:28,549 [Thread-3188] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data-0/containers/hdds
2023-04-27 06:30:28,549 [Listener at 127.0.0.1/32931] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:28,551 [Listener at 127.0.0.1/32931] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:28,551 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:28,551 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:28,551 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:28,551 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:28,551 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:28,551 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:28,551 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:28,551 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:28,551 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:28,552 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:28,552 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:28,552 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:28,552 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:28,553 [Listener at 127.0.0.1/32931] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:28,553 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:28,553 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:28,553 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:28,553 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:28,553 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:28,553 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:28,554 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:28,554 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:28,554 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:28,554 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:28,554 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:28,554 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:28,554 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:28,555 [10617bfc-881b-4e3d-8722-e7f81aaf7e30-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x28124867] REGISTERED
2023-04-27 06:30:28,555 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:28,555 [10617bfc-881b-4e3d-8722-e7f81aaf7e30-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x28124867] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:28,555 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis] (custom)
2023-04-27 06:30:28,555 [10617bfc-881b-4e3d-8722-e7f81aaf7e30-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x28124867, L:/0:0:0:0:0:0:0:0:37703] ACTIVE
2023-04-27 06:30:28,556 [Listener at 127.0.0.1/32931] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:28,560 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:28,560 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:28,561 [Listener at 127.0.0.1/32931] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:28,563 [Listener at 127.0.0.1/32931] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:28,564 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:28,564 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:28,564 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:28,564 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:28,565 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/meta/webserver
2023-04-27 06:30:28,565 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 46477
2023-04-27 06:30:28,565 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:28,570 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:28,570 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:28,570 [Listener at 127.0.0.1/32931] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 660000ms
2023-04-27 06:30:28,571 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@3c358615{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:28,571 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@1dd3ad57{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:28,732 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5034399258ns, electionTimeout:5033ms
2023-04-27 06:30:28,732 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 3804776e-bffa-42a7-addb-662dc325de80: shutdown 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-FollowerState
2023-04-27 06:30:28,732 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:28,732 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:28,732 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 3804776e-bffa-42a7-addb-662dc325de80: start 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80
2023-04-27 06:30:28,733 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,733 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:28,737 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,738 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:28,738 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - 3804776e-bffa-42a7-addb-662dc325de80: shutdown 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80
2023-04-27 06:30:28,738 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:28,738 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-6FC4B564C533 with new leaderId: 3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:28,738 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533: change Leader from null to 3804776e-bffa-42a7-addb-662dc325de80 at term 1 for becomeLeader, leader elected after 5059ms
2023-04-27 06:30:28,738 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:28,739 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:28,739 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:28,740 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:28,740 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:28,740 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:28,740 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:28,740 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:28,740 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 3804776e-bffa-42a7-addb-662dc325de80: start 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderStateImpl
2023-04-27 06:30:28,741 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:28,746 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-LeaderElection80] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533: set configuration 0: peers:[3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,748 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:28,749 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:28,749 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:28,753 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:28,756 [3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 3804776e-bffa-42a7-addb-662dc325de80@group-6FC4B564C533-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis/6bf5f405-714f-49bb-b293-6fc4b564c533/current/log_inprogress_0
2023-04-27 06:30:28,828 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5043828646ns, electionTimeout:5043ms
2023-04-27 06:30:28,829 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: shutdown fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState
2023-04-27 06:30:28,829 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:28,829 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:28,829 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: start fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81
2023-04-27 06:30:28,830 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,830 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81-1] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for efdbc657-ca6b-4fea-9dc1-5411634f3e98
2023-04-27 06:30:28,834 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:28,834 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:28,834 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81-2] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for 3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:28,837 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2: receive requestVote(PRE_VOTE, fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e, group-1AB8BD3CBBB2, 0, (t:0, i:0))
2023-04-27 06:30:28,837 [grpc-default-executor-4] INFO  impl.VoteContext (VoteContext.java:log(49)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FOLLOWER: accept PRE_VOTE from fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: our priority 0 <= candidate's priority 0
2023-04-27 06:30:28,837 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2 replies to PRE_VOTE vote request: fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e<-efdbc657-ca6b-4fea-9dc1-5411634f3e98#0:OK-t0. Peer's state: efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2:t0, leader=null, voted=, raftlog=Memoized:efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,840 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2: receive requestVote(PRE_VOTE, fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e, group-1AB8BD3CBBB2, 0, (t:0, i:0))
2023-04-27 06:30:28,840 [grpc-default-executor-4] INFO  impl.VoteContext (VoteContext.java:log(49)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-FOLLOWER: reject PRE_VOTE from fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: our priority 1 > candidate's priority 0
2023-04-27 06:30:28,840 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2 replies to PRE_VOTE vote request: fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e<-3804776e-bffa-42a7-addb-662dc325de80#0:FAIL-t0. Peer's state: 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2:t0, leader=null, voted=, raftlog=Memoized:3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,841 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
2023-04-27 06:30:28,842 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e<-efdbc657-ca6b-4fea-9dc1-5411634f3e98#0:OK-t0
2023-04-27 06:30:28,842 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 1: fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e<-3804776e-bffa-42a7-addb-662dc325de80#0:FAIL-t0
2023-04-27 06:30:28,842 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81 PRE_VOTE round 0: result REJECTED
2023-04-27 06:30:28,842 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
2023-04-27 06:30:28,842 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: shutdown fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81
2023-04-27 06:30:28,842 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-LeaderElection81] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: start fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState
2023-04-27 06:30:28,846 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:28,846 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:28,880 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@7a22aa3d{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/meta/webserver/jetty-0_0_0_0-46477-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-1418622311417915043/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:28,885 [Listener at 127.0.0.1/32931] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@704a231{HTTP/1.1, (http/1.1)}{0.0.0.0:46477}
2023-04-27 06:30:28,885 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(415)) - Started @155333ms
2023-04-27 06:30:28,885 [Listener at 127.0.0.1/32931] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:28,886 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:46477
2023-04-27 06:30:28,887 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:28,887 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:28,887 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:28,887 [Listener at 127.0.0.1/32931] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:28,893 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7200c794] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:28,899 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5180230728ns, electionTimeout:5179ms
2023-04-27 06:30:28,899 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 3804776e-bffa-42a7-addb-662dc325de80: shutdown 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-FollowerState
2023-04-27 06:30:28,899 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:28,899 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:28,899 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 3804776e-bffa-42a7-addb-662dc325de80: start 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82
2023-04-27 06:30:28,899 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/meta/datanode.id
2023-04-27 06:30:28,903 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,904 [Listener at 127.0.0.1/32931] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:28,904 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82-1] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e
2023-04-27 06:30:28,904 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:28,904 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:28,904 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82-2] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for efdbc657-ca6b-4fea-9dc1-5411634f3e98
2023-04-27 06:30:28,911 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2: receive requestVote(PRE_VOTE, 3804776e-bffa-42a7-addb-662dc325de80, group-1AB8BD3CBBB2, 0, (t:0, i:0))
2023-04-27 06:30:28,912 [grpc-default-executor-4] INFO  impl.VoteContext (VoteContext.java:log(49)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FOLLOWER: accept PRE_VOTE from 3804776e-bffa-42a7-addb-662dc325de80: our priority 0 <= candidate's priority 1
2023-04-27 06:30:28,912 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2 replies to PRE_VOTE vote request: 3804776e-bffa-42a7-addb-662dc325de80<-efdbc657-ca6b-4fea-9dc1-5411634f3e98#0:OK-t0. Peer's state: efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2:t0, leader=null, voted=, raftlog=Memoized:efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,914 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
2023-04-27 06:30:28,914 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: 3804776e-bffa-42a7-addb-662dc325de80<-efdbc657-ca6b-4fea-9dc1-5411634f3e98#0:OK-t0
2023-04-27 06:30:28,914 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82 PRE_VOTE round 0: result PASSED
2023-04-27 06:30:28,920 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82 ELECTION round 0: submit vote requests at term 1 for -1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,920 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2: receive requestVote(PRE_VOTE, 3804776e-bffa-42a7-addb-662dc325de80, group-1AB8BD3CBBB2, 0, (t:0, i:0))
2023-04-27 06:30:28,921 [grpc-default-executor-4] INFO  impl.VoteContext (VoteContext.java:log(49)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FOLLOWER: accept PRE_VOTE from 3804776e-bffa-42a7-addb-662dc325de80: our priority 0 <= candidate's priority 1
2023-04-27 06:30:28,921 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2 replies to PRE_VOTE vote request: 3804776e-bffa-42a7-addb-662dc325de80<-fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e#0:OK-t0. Peer's state: fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2:t0, leader=null, voted=, raftlog=Memoized:fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,922 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:28,922 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:28,923 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2: receive requestVote(ELECTION, 3804776e-bffa-42a7-addb-662dc325de80, group-1AB8BD3CBBB2, 1, (t:0, i:0))
2023-04-27 06:30:28,923 [grpc-default-executor-4] INFO  impl.VoteContext (VoteContext.java:log(49)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FOLLOWER: accept ELECTION from 3804776e-bffa-42a7-addb-662dc325de80: our priority 0 <= candidate's priority 1
2023-04-27 06:30:28,923 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:28,923 [grpc-default-executor-4] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: shutdown fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState
2023-04-27 06:30:28,923 [grpc-default-executor-4] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: start fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState
2023-04-27 06:30:28,923 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState was interrupted
2023-04-27 06:30:28,924 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:28,924 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:28,924 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2: receive requestVote(ELECTION, 3804776e-bffa-42a7-addb-662dc325de80, group-1AB8BD3CBBB2, 1, (t:0, i:0))
2023-04-27 06:30:28,924 [grpc-default-executor-1] INFO  impl.VoteContext (VoteContext.java:log(49)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FOLLOWER: accept ELECTION from 3804776e-bffa-42a7-addb-662dc325de80: our priority 0 <= candidate's priority 1
2023-04-27 06:30:28,924 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:28,924 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: shutdown efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FollowerState
2023-04-27 06:30:28,925 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: start efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FollowerState
2023-04-27 06:30:28,925 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:28,925 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:28,926 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FollowerState was interrupted
2023-04-27 06:30:28,926 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:28,926 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:28,927 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2 replies to ELECTION vote request: 3804776e-bffa-42a7-addb-662dc325de80<-fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e#0:OK-t1. Peer's state: fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2:t1, leader=null, voted=3804776e-bffa-42a7-addb-662dc325de80, raftlog=Memoized:fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,928 [grpc-default-executor-1] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2 replies to ELECTION vote request: 3804776e-bffa-42a7-addb-662dc325de80<-efdbc657-ca6b-4fea-9dc1-5411634f3e98#0:OK-t1. Peer's state: efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2:t1, leader=null, voted=3804776e-bffa-42a7-addb-662dc325de80, raftlog=Memoized:efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,928 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82: ELECTION PASSED received 1 response(s) and 0 exception(s):
2023-04-27 06:30:28,928 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: 3804776e-bffa-42a7-addb-662dc325de80<-fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e#0:OK-t1
2023-04-27 06:30:28,928 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82 ELECTION round 0: result PASSED
2023-04-27 06:30:28,928 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - 3804776e-bffa-42a7-addb-662dc325de80: shutdown 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82
2023-04-27 06:30:28,928 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:28,928 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-1AB8BD3CBBB2 with new leaderId: 3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:28,928 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2: change Leader from null to 3804776e-bffa-42a7-addb-662dc325de80 at term 1 for becomeLeader, leader elected after 5227ms
2023-04-27 06:30:28,928 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:28,929 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:28,929 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:28,929 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:28,929 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:28,929 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:28,929 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:28,930 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:28,930 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 454b071d-716d-4e22-a45d-1ab8bd3cbbb2, Nodes: efdbc657-ca6b-4fea-9dc1-5411634f3e98(fv-az260-775/10.1.0.33)3804776e-bffa-42a7-addb-662dc325de80(fv-az260-775/10.1.0.33)fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:3804776e-bffa-42a7-addb-662dc325de80, CreationTimestamp2023-04-27T06:30:21.571Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:28,931 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2023-04-27 06:30:28,931 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:28,931 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2023-04-27 06:30:28,932 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2023-04-27 06:30:28,932 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:28,932 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:28,932 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:28,932 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.wait-time.min = 10ms (default)
2023-04-27 06:30:28,933 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2023-04-27 06:30:28,933 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:28,933 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2023-04-27 06:30:28,933 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2023-04-27 06:30:28,933 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:28,933 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:28,933 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:28,933 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.wait-time.min = 10ms (default)
2023-04-27 06:30:28,934 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 3804776e-bffa-42a7-addb-662dc325de80: start 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderStateImpl
2023-04-27 06:30:28,934 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:28,945 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-LeaderElection82] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2: set configuration 0: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,947 [3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 3804776e-bffa-42a7-addb-662dc325de80@group-1AB8BD3CBBB2-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-3/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2/current/log_inprogress_0
2023-04-27 06:30:28,948 [Listener at 127.0.0.1/32931] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:28,962 [efdbc657-ca6b-4fea-9dc1-5411634f3e98-server-thread1] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-1AB8BD3CBBB2 with new leaderId: 3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:28,962 [efdbc657-ca6b-4fea-9dc1-5411634f3e98-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2: change Leader from null to 3804776e-bffa-42a7-addb-662dc325de80 at term 1 for appendEntries, leader elected after 5228ms
2023-04-27 06:30:28,973 [efdbc657-ca6b-4fea-9dc1-5411634f3e98-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2: set configuration 0: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,973 [efdbc657-ca6b-4fea-9dc1-5411634f3e98-server-thread1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:28,975 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-server-thread1] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-1AB8BD3CBBB2 with new leaderId: 3804776e-bffa-42a7-addb-662dc325de80
2023-04-27 06:30:28,976 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2: change Leader from null to 3804776e-bffa-42a7-addb-662dc325de80 at term 1 for appendEntries, leader elected after 5207ms
2023-04-27 06:30:28,976 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-1AB8BD3CBBB2-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2/current/log_inprogress_0
2023-04-27 06:30:28,983 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-server-thread2] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2: set configuration 0: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:0|startupRole:FOLLOWER, efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:0|startupRole:FOLLOWER, 3804776e-bffa-42a7-addb-662dc325de80|rpc:10.1.0.33:46607|dataStream:10.1.0.33:45163|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:28,983 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e-server-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:28,985 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-1AB8BD3CBBB2-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis/454b071d-716d-4e22-a45d-1ab8bd3cbbb2/current/log_inprogress_0
2023-04-27 06:30:29,026 [Listener at 127.0.0.1/32931] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 77 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:29,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:29,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:29,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:29,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:29,028 [Listener at 127.0.0.1/32931] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:29,029 [Listener at 127.0.0.1/32931] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:29,029 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:29,029 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data-0/containers/hdds
2023-04-27 06:30:29,030 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data-0/containers/hdds
2023-04-27 06:30:29,044 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis to VolumeSet
2023-04-27 06:30:29,044 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis
2023-04-27 06:30:29,044 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis
2023-04-27 06:30:29,061 [Thread-3223] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data-0/containers/hdds
2023-04-27 06:30:29,061 [Listener at 127.0.0.1/32931] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:29,065 [Listener at 127.0.0.1/32931] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:29,067 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:29,067 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:29,067 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:29,067 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:29,067 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:29,068 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:29,068 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:29,068 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:29,068 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:29,068 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:29,068 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:29,068 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:29,068 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:29,070 [Listener at 127.0.0.1/32931] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:29,070 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:29,070 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:29,070 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:29,070 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:29,071 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:29,071 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:29,072 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:29,072 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:29,072 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:29,072 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:29,073 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:29,073 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:29,073 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:29,073 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:29,073 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis] (custom)
2023-04-27 06:30:29,074 [om1@group-C5BA1605619E-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:1108222796ns, electionTimeout:1107ms
2023-04-27 06:30:29,074 [om1@group-C5BA1605619E-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - om1: shutdown om1@group-C5BA1605619E-FollowerState
2023-04-27 06:30:29,077 [om1@group-C5BA1605619E-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:29,077 [om1@group-C5BA1605619E-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:29,077 [om1@group-C5BA1605619E-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - om1: start om1@group-C5BA1605619E-LeaderElection83
2023-04-27 06:30:29,077 [65d5be4b-faf3-45c4-8c44-989db5872d1e-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x56e0b50c] REGISTERED
2023-04-27 06:30:29,078 [65d5be4b-faf3-45c4-8c44-989db5872d1e-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x56e0b50c] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:29,078 [65d5be4b-faf3-45c4-8c44-989db5872d1e-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x56e0b50c, L:/0:0:0:0:0:0:0:0:46609] ACTIVE
2023-04-27 06:30:29,078 [Listener at 127.0.0.1/32931] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:29,079 [om1@group-C5BA1605619E-LeaderElection83] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - om1@group-C5BA1605619E-LeaderElection83 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:localhost:33323|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:29,079 [om1@group-C5BA1605619E-LeaderElection83] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - om1@group-C5BA1605619E-LeaderElection83 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:29,081 [om1@group-C5BA1605619E-LeaderElection83] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - om1@group-C5BA1605619E-LeaderElection83 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:localhost:33323|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:29,081 [om1@group-C5BA1605619E-LeaderElection83] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - om1@group-C5BA1605619E-LeaderElection83 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:29,081 [om1@group-C5BA1605619E-LeaderElection83] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - om1: shutdown om1@group-C5BA1605619E-LeaderElection83
2023-04-27 06:30:29,081 [om1@group-C5BA1605619E-LeaderElection83] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:29,081 [om1@group-C5BA1605619E-LeaderElection83] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 2157ms
2023-04-27 06:30:29,081 [om1@group-C5BA1605619E-LeaderElection83] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:29,082 [om1@group-C5BA1605619E-LeaderElection83] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 4096 (default)
2023-04-27 06:30:29,082 [om1@group-C5BA1605619E-LeaderElection83] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 64MB (=67108864) (default)
2023-04-27 06:30:29,082 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:29,082 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:29,083 [om1@group-C5BA1605619E-LeaderElection83] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 10s (default)
2023-04-27 06:30:29,083 [om1@group-C5BA1605619E-LeaderElection83] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:29,083 [om1@group-C5BA1605619E-LeaderElection83] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:29,083 [om1@group-C5BA1605619E-LeaderElection83] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 4096 (default)
2023-04-27 06:30:29,083 [om1@group-C5BA1605619E-LeaderElection83] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:29,083 [om1@group-C5BA1605619E-LeaderElection83] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - om1: start om1@group-C5BA1605619E-LeaderStateImpl
2023-04-27 06:30:29,084 [om1@group-C5BA1605619E-LeaderElection83] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:29,099 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/ozone-meta/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
2023-04-27 06:30:29,103 [Listener at 127.0.0.1/32931] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:29,104 [om1@group-C5BA1605619E-LeaderElection83] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:localhost:33323|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:29,107 [Listener at 127.0.0.1/32931] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:29,108 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:notifyConfigurationChanged(201)) - Received Configuration change notification from Ratis. New Peer list:
[id: "om1"
address: "localhost:33323"
startupRole: FOLLOWER
]
2023-04-27 06:30:29,108 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:29,109 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:29,109 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:29,109 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:29,109 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/meta/webserver
2023-04-27 06:30:29,110 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 45569
2023-04-27 06:30:29,110 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:29,125 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:29,125 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:29,125 [Listener at 127.0.0.1/32931] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 600000ms
2023-04-27 06:30:29,126 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@52800461{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:29,126 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@6a295d32{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:29,187 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5095299189ns, electionTimeout:5092ms
2023-04-27 06:30:29,187 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: shutdown fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-FollowerState
2023-04-27 06:30:29,187 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:29,187 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:29,188 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: start fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84
2023-04-27 06:30:29,188 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:29,188 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:29,190 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84 ELECTION round 0: submit vote requests at term 1 for -1: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:29,190 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:29,190 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: shutdown fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84
2023-04-27 06:30:29,190 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:29,190 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-CC149F761A2E with new leaderId: fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e
2023-04-27 06:30:29,190 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E: change Leader from null to fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e at term 1 for becomeLeader, leader elected after 5135ms
2023-04-27 06:30:29,190 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:29,191 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:29,192 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:29,192 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:29,192 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:29,192 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:29,192 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:29,192 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:29,192 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e: start fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderStateImpl
2023-04-27 06:30:29,192 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:29,197 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-4/data/ratis/5084858f-2ede-4f2d-b349-cc149f761a2e/current/log_inprogress_0
2023-04-27 06:30:29,198 [fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E-LeaderElection84] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e@group-CC149F761A2E: set configuration 0: peers:[fef9ad2a-bc25-4aa3-b5e5-1b6e493a930e|rpc:10.1.0.33:45409|dataStream:10.1.0.33:46313|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:29,373 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@301ee283{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/meta/webserver/jetty-0_0_0_0-45569-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-4273860050179053154/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:29,377 [Listener at 127.0.0.1/32931] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@3251c6ea{HTTP/1.1, (http/1.1)}{0.0.0.0:45569}
2023-04-27 06:30:29,377 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(415)) - Started @155825ms
2023-04-27 06:30:29,378 [Listener at 127.0.0.1/32931] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:29,378 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:45569
2023-04-27 06:30:29,379 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:29,379 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:29,379 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:29,379 [Listener at 127.0.0.1/32931] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:29,387 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@152d222f] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:29,399 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/meta/datanode.id
2023-04-27 06:30:29,402 [Listener at 127.0.0.1/32931] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:29,435 [Listener at 127.0.0.1/32931] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:29,492 [Listener at 127.0.0.1/32931] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 55 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:29,494 [Listener at 127.0.0.1/32931] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:29,496 [Listener at 127.0.0.1/32931] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:29,496 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:29,496 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data-0/containers/hdds
2023-04-27 06:30:29,499 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data-0/containers/hdds
2023-04-27 06:30:29,514 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis to VolumeSet
2023-04-27 06:30:29,514 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis
2023-04-27 06:30:29,514 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis
2023-04-27 06:30:29,526 [Thread-3241] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data-0/containers/hdds
2023-04-27 06:30:29,526 [Listener at 127.0.0.1/32931] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:29,529 [Listener at 127.0.0.1/32931] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:29,529 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:29,529 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:29,529 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:29,529 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:29,529 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:29,529 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:29,529 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:29,529 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:29,529 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:29,529 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:29,530 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:29,530 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:29,530 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:29,531 [Listener at 127.0.0.1/32931] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:29,531 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:29,531 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:29,531 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:29,531 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:29,531 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:29,531 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:29,532 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:29,532 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:29,532 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:29,532 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:29,537 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:29,537 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:29,538 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:29,538 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:29,538 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis] (custom)
2023-04-27 06:30:29,539 [ff409154-2e62-420c-a7a7-066f3a70d145-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x4ef7aa91] REGISTERED
2023-04-27 06:30:29,539 [ff409154-2e62-420c-a7a7-066f3a70d145-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x4ef7aa91] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:29,539 [ff409154-2e62-420c-a7a7-066f3a70d145-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x4ef7aa91, L:/0:0:0:0:0:0:0:0:34185] ACTIVE
2023-04-27 06:30:29,539 [Listener at 127.0.0.1/32931] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:29,544 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:29,544 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:29,545 [Listener at 127.0.0.1/32931] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:29,547 [Listener at 127.0.0.1/32931] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:29,549 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:29,549 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:29,549 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:29,549 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:29,550 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/meta/webserver
2023-04-27 06:30:29,550 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 45617
2023-04-27 06:30:29,550 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:29,570 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:29,571 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:29,571 [Listener at 127.0.0.1/32931] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 600000ms
2023-04-27 06:30:29,572 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@11dd3df9{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:29,572 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@453800c9{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:29,609 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5023224291ns, electionTimeout:5022ms
2023-04-27 06:30:29,609 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: shutdown efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-FollowerState
2023-04-27 06:30:29,610 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:29,610 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:29,610 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: start efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85
2023-04-27 06:30:29,610 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:29,610 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:29,612 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85 ELECTION round 0: submit vote requests at term 1 for -1: peers:[efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:29,613 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:29,613 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: shutdown efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85
2023-04-27 06:30:29,613 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:29,613 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-BA28176F28A3 with new leaderId: efdbc657-ca6b-4fea-9dc1-5411634f3e98
2023-04-27 06:30:29,613 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3: change Leader from null to efdbc657-ca6b-4fea-9dc1-5411634f3e98 at term 1 for becomeLeader, leader elected after 5044ms
2023-04-27 06:30:29,614 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:29,614 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:29,614 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:29,615 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:29,615 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:29,615 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:29,615 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:29,615 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:29,615 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98: start efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderStateImpl
2023-04-27 06:30:29,616 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:29,622 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-LeaderElection85] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3: set configuration 0: peers:[efdbc657-ca6b-4fea-9dc1-5411634f3e98|rpc:10.1.0.33:44129|dataStream:10.1.0.33:38671|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:29,626 [efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - efdbc657-ca6b-4fea-9dc1-5411634f3e98@group-BA28176F28A3-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-5/data/ratis/b5275d90-33a5-467d-a65a-ba28176f28a3/current/log_inprogress_0
2023-04-27 06:30:29,750 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:29,750 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:29,751 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:29,757 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:29,764 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:29,765 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:29,778 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:29,785 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:29,787 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:29,793 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:29,796 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:29,796 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:29,866 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@9cb4c33{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/meta/webserver/jetty-0_0_0_0-45617-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-861346205686011577/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:29,872 [Listener at 127.0.0.1/32931] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@4f48c388{HTTP/1.1, (http/1.1)}{0.0.0.0:45617}
2023-04-27 06:30:29,873 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(415)) - Started @156321ms
2023-04-27 06:30:29,873 [Listener at 127.0.0.1/32931] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:29,874 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:45617
2023-04-27 06:30:29,874 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:29,874 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:29,874 [Listener at 127.0.0.1/32931] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:29,880 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:29,887 [Listener at 127.0.0.1/32931] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:29,887 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2a4df7dc] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:29,896 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/meta/datanode.id
2023-04-27 06:30:29,918 [Listener at 127.0.0.1/32931] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:30,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:30,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:30,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:30,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:30,029 [Listener at 127.0.0.1/32931] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 110 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:30,030 [Listener at 127.0.0.1/32931] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:30,032 [Listener at 127.0.0.1/32931] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:30,032 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:30,032 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data-0/containers/hdds
2023-04-27 06:30:30,033 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data-0/containers/hdds
2023-04-27 06:30:30,046 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis to VolumeSet
2023-04-27 06:30:30,046 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis
2023-04-27 06:30:30,047 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis
2023-04-27 06:30:30,065 [Thread-3257] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data-0/containers/hdds
2023-04-27 06:30:30,065 [Listener at 127.0.0.1/32931] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:30,068 [Listener at 127.0.0.1/32931] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:30,068 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:30,068 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:30,068 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:30,068 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:30,068 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:30,068 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:30,068 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:30,068 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:30,068 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:30,069 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:30,069 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:30,069 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:30,069 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:30,070 [Listener at 127.0.0.1/32931] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:30,070 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:30,071 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:30,071 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:30,071 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:30,071 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:30,071 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:30,071 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:30,077 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:30,077 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:30,077 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:30,077 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:30,079 [735446df-424c-4d38-a683-bd4ef5c8b9e6-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x7c357b6e] REGISTERED
2023-04-27 06:30:30,079 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:30,079 [735446df-424c-4d38-a683-bd4ef5c8b9e6-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x7c357b6e] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:30,080 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:30,080 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:30,080 [735446df-424c-4d38-a683-bd4ef5c8b9e6-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x7c357b6e, L:/0:0:0:0:0:0:0:0:41265] ACTIVE
2023-04-27 06:30:30,080 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis] (custom)
2023-04-27 06:30:30,082 [Listener at 127.0.0.1/32931] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:30,088 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:30,088 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:30,089 [Listener at 127.0.0.1/32931] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:30,090 [Listener at 127.0.0.1/32931] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:30,090 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:30,091 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:30,091 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:30,091 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:30,094 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/meta/webserver
2023-04-27 06:30:30,094 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 36753
2023-04-27 06:30:30,094 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:30,096 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:30,102 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:30,102 [Listener at 127.0.0.1/32931] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 660000ms
2023-04-27 06:30:30,103 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@4f50a5fd{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:30,103 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@2daab3b6{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:30,144 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5088492705ns, electionTimeout:5088ms
2023-04-27 06:30:30,144 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6: shutdown f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-FollowerState
2023-04-27 06:30:30,144 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:30,144 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:30,144 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6: start f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86
2023-04-27 06:30:30,145 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f88c54b2-8776-4954-8dfb-f8bea6a862a6|rpc:10.1.0.33:38947|dataStream:10.1.0.33:46011|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:30,145 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:30,148 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f88c54b2-8776-4954-8dfb-f8bea6a862a6|rpc:10.1.0.33:38947|dataStream:10.1.0.33:46011|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:30,148 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:30,148 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6: shutdown f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86
2023-04-27 06:30:30,148 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:30,148 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-94F17CB32B95 with new leaderId: f88c54b2-8776-4954-8dfb-f8bea6a862a6
2023-04-27 06:30:30,148 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95: change Leader from null to f88c54b2-8776-4954-8dfb-f8bea6a862a6 at term 1 for becomeLeader, leader elected after 5108ms
2023-04-27 06:30:30,149 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:30,149 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:30,149 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:30,149 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:30,149 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:30,150 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:30,150 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:30,150 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:30,150 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6: start f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderStateImpl
2023-04-27 06:30:30,150 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:30,154 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-LeaderElection86] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95: set configuration 0: peers:[f88c54b2-8776-4954-8dfb-f8bea6a862a6|rpc:10.1.0.33:38947|dataStream:10.1.0.33:46011|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:30,156 [f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - f88c54b2-8776-4954-8dfb-f8bea6a862a6@group-94F17CB32B95-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bd3209a3-83fe-44cb-83c2-665bdcf7ddee/datanode-6/data/ratis/d15a6767-9072-4c9d-abb6-94f17cb32b95/current/log_inprogress_0
2023-04-27 06:30:30,368 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@6f3008e9{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/meta/webserver/jetty-0_0_0_0-36753-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-4074231499302162589/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:30,372 [Listener at 127.0.0.1/32931] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@1a933422{HTTP/1.1, (http/1.1)}{0.0.0.0:36753}
2023-04-27 06:30:30,372 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(415)) - Started @156820ms
2023-04-27 06:30:30,372 [Listener at 127.0.0.1/32931] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:30,373 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:36753
2023-04-27 06:30:30,373 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:30,373 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:30,373 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:30,374 [Listener at 127.0.0.1/32931] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:30,381 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@47d706bf] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:30,383 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/meta/datanode.id
2023-04-27 06:30:30,388 [Listener at 127.0.0.1/32931] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:30,413 [Listener at 127.0.0.1/32931] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:30,474 [Listener at 127.0.0.1/32931] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 61 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:30,475 [Listener at 127.0.0.1/32931] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:30,477 [Listener at 127.0.0.1/32931] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:30,477 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:30,477 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data-0/containers/hdds
2023-04-27 06:30:30,479 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data-0/containers/hdds
2023-04-27 06:30:30,492 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis to VolumeSet
2023-04-27 06:30:30,492 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis
2023-04-27 06:30:30,492 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis
2023-04-27 06:30:30,505 [Thread-3274] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data-0/containers/hdds
2023-04-27 06:30:30,506 [Listener at 127.0.0.1/32931] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:30,508 [Listener at 127.0.0.1/32931] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:30,508 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:30,508 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:30,508 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:30,508 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:30,508 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:30,508 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:30,508 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:30,509 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:30,509 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:30,509 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-ddcfa67c-e2e3-4ee4-a1ab-735228e46375/container.db to cache
2023-04-27 06:30:30,509 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:30,509 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-ddcfa67c-e2e3-4ee4-a1ab-735228e46375/container.db for volume DS-ddcfa67c-e2e3-4ee4-a1ab-735228e46375
2023-04-27 06:30:30,509 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:30,509 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:30,509 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:30,510 [Listener at 127.0.0.1/32931] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:30,510 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:30,510 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:30,511 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:30,511 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:30,511 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:30,511 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:30,511 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:30,511 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:30,511 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:30,511 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:30,511 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:30,512 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 37613
2023-04-27 06:30:30,512 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:30,512 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis b85c0d2e-fc3b-4312-9b63-e609b6e2c228
2023-04-27 06:30:30,512 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:30,512 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:30,512 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:30,512 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:30,512 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis] (custom)
2023-04-27 06:30:30,513 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xa12c16aa] REGISTERED
2023-04-27 06:30:30,513 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xa12c16aa] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:30,513 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0xa12c16aa, L:/0:0:0:0:0:0:0:0:36675] ACTIVE
2023-04-27 06:30:30,517 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: start RPC server
2023-04-27 06:30:30,518 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: GrpcService started, listening on 41235
2023-04-27 06:30:30,518 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis b85c0d2e-fc3b-4312-9b63-e609b6e2c228 is started using port 41235 for RATIS
2023-04-27 06:30:30,518 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis b85c0d2e-fc3b-4312-9b63-e609b6e2c228 is started using port 41235 for RATIS_ADMIN
2023-04-27 06:30:30,518 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis b85c0d2e-fc3b-4312-9b63-e609b6e2c228 is started using port 41235 for RATIS_SERVER
2023-04-27 06:30:30,518 [JvmPauseMonitor50] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-b85c0d2e-fc3b-4312-9b63-e609b6e2c228: Started
2023-04-27 06:30:30,518 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis b85c0d2e-fc3b-4312-9b63-e609b6e2c228 is started using port 40691 for RATIS_DATASTREAM
2023-04-27 06:30:30,519 [Listener at 127.0.0.1/32931] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:30,520 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc b85c0d2e-fc3b-4312-9b63-e609b6e2c228 is started using port 41369
2023-04-27 06:30:30,523 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:30,524 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:30,524 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:30,525 [Listener at 127.0.0.1/32931] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:30,525 [Listener at 127.0.0.1/32931] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:30,526 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:30,526 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:30,526 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:30,526 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:30,527 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/meta/webserver
2023-04-27 06:30:30,527 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 36927
2023-04-27 06:30:30,527 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:30,536 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:30,536 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:30,536 [Listener at 127.0.0.1/32931] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 660000ms
2023-04-27 06:30:30,536 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@1d6e316e{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:30,537 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@2c938599{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:30,751 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:30,751 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:30,751 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:30,757 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:30,811 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@11fe2f10{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/meta/webserver/jetty-0_0_0_0-36927-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-1311092451589723760/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:30,816 [Listener at 127.0.0.1/32931] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@56ebc398{HTTP/1.1, (http/1.1)}{0.0.0.0:36927}
2023-04-27 06:30:30,816 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(415)) - Started @157264ms
2023-04-27 06:30:30,816 [Listener at 127.0.0.1/32931] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:30,817 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:36927
2023-04-27 06:30:30,817 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:30,817 [Listener at 127.0.0.1/32931] WARN  impl.MetricRegistriesImpl (MetricRegistriesImpl.java:addReporterRegistration(111)) - New reporters are added after registries were created. Some metrics will be missing from the reporter. Please add reporter before adding any new registry.
2023-04-27 06:30:30,817 [Listener at 127.0.0.1/32931] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2023-04-27 06:30:30,825 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:30,831 [Listener at 127.0.0.1/32931] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(252)) - HddsDatanodeService host:fv-az260-775 ip:10.1.0.33
2023-04-27 06:30:30,838 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@661ff236] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:30,851 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/meta/datanode.id
2023-04-27 06:30:30,869 [Listener at 127.0.0.1/32931] INFO  upgrade.AbstractLayoutVersionManager (AbstractLayoutVersionManager.java:init(83)) - Initializing Layout version manager with metadata layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
2023-04-27 06:30:30,923 [Listener at 127.0.0.1/32931] INFO  reflections.Reflections (Reflections.java:scan(232)) - Reflections took 53 ms to scan 7 urls, producing 156 keys and 369 values 
2023-04-27 06:30:30,925 [Listener at 127.0.0.1/32931] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:getEndPointTaskThreadPoolSize(260)) - Datanode State Machine Task Thread Pool size 2
2023-04-27 06:30:30,927 [Listener at 127.0.0.1/32931] INFO  volume.HddsVolume (HddsVolume.java:<init>(130)) - Creating HddsVolume: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data-0/containers/hdds of storage type : DISK capacity : 9223372036854775807
2023-04-27 06:30:30,927 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data-0/containers/hdds to VolumeSet
2023-04-27 06:30:30,927 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data-0/containers/hdds
2023-04-27 06:30:30,928 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data-0/containers/hdds
2023-04-27 06:30:30,946 [Listener at 127.0.0.1/32931] INFO  volume.MutableVolumeSet (MutableVolumeSet.java:initializeVolumeSet(175)) - Added Volume : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data/ratis to VolumeSet
2023-04-27 06:30:30,946 [Listener at 127.0.0.1/32931] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(141)) - Scheduling a check for /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data/ratis
2023-04-27 06:30:30,946 [Listener at 127.0.0.1/32931] INFO  volume.StorageVolumeChecker (StorageVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data/ratis
2023-04-27 06:30:30,955 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-b36f31c4-8b09-4969-bd35-e2389bbcd846/container.db to cache
2023-04-27 06:30:30,955 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-b36f31c4-8b09-4969-bd35-e2389bbcd846/container.db for volume DS-b36f31c4-8b09-4969-bd35-e2389bbcd846
2023-04-27 06:30:30,955 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:30,955 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:30,956 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 46361
2023-04-27 06:30:30,956 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis 10617bfc-881b-4e3d-8722-e7f81aaf7e30
2023-04-27 06:30:30,962 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: start RPC server
2023-04-27 06:30:30,962 [Thread-3293] INFO  ozoneimpl.ContainerReader (ContainerReader.java:readVolume(175)) - Finish verifying containers on volume /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data-0/containers/hdds
2023-04-27 06:30:30,962 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: GrpcService started, listening on 37309
2023-04-27 06:30:30,963 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 10617bfc-881b-4e3d-8722-e7f81aaf7e30 is started using port 37309 for RATIS
2023-04-27 06:30:30,963 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 10617bfc-881b-4e3d-8722-e7f81aaf7e30 is started using port 37309 for RATIS_ADMIN
2023-04-27 06:30:30,963 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 10617bfc-881b-4e3d-8722-e7f81aaf7e30 is started using port 37309 for RATIS_SERVER
2023-04-27 06:30:30,963 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 10617bfc-881b-4e3d-8722-e7f81aaf7e30 is started using port 37703 for RATIS_DATASTREAM
2023-04-27 06:30:30,963 [JvmPauseMonitor51] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-10617bfc-881b-4e3d-8722-e7f81aaf7e30: Started
2023-04-27 06:30:30,963 [Listener at 127.0.0.1/32931] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:buildContainerSet(303)) - Build ContainerSet costs 0s
2023-04-27 06:30:30,963 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc 10617bfc-881b-4e3d-8722-e7f81aaf7e30 is started using port 38295
2023-04-27 06:30:30,964 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:30,965 [Listener at 127.0.0.1/32931] INFO  server.RaftServer (ConfUtils.java:logGet(46)) - raft.rpc.type = GRPC (default)
2023-04-27 06:30:30,965 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:30,965 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.admin.port = 0 (custom)
2023-04-27 06:30:30,965 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logFallback(53)) - raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2023-04-27 06:30:30,965 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.client.port = 0 (custom)
2023-04-27 06:30:30,965 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.host = null (default)
2023-04-27 06:30:30,965 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.port = 0 (default)
2023-04-27 06:30:30,965 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.message.size.max = 32MB (=33554432) (custom)
2023-04-27 06:30:30,966 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:30,966 [Listener at 127.0.0.1/32931] INFO  server.GrpcService (ConfUtils.java:logGet(46)) - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
2023-04-27 06:30:30,966 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:30,966 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:30,966 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.cached = true (default)
2023-04-27 06:30:30,966 [Listener at 127.0.0.1/32931] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.async.request.thread.pool.size = 32 (default)
2023-04-27 06:30:30,968 [Listener at 127.0.0.1/32931] INFO  impl.DataStreamServerImpl (ConfUtils.java:logGet(46)) - raft.datastream.type = NETTY (custom)
2023-04-27 06:30:30,968 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.cached = false (default)
2023-04-27 06:30:30,968 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.request.thread.pool.size = 20 (custom)
2023-04-27 06:30:30,968 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.async.write.thread.pool.size = 16 (default)
2023-04-27 06:30:30,970 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.data-stream.client.pool.size = 10 (default)
2023-04-27 06:30:30,970 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.use-epoll = false (default)
2023-04-27 06:30:30,970 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.boss-group.size = 0 (default)
2023-04-27 06:30:30,970 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.worker-group.size = 0 (default)
2023-04-27 06:30:30,971 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.server.tls.conf = null (default)
2023-04-27 06:30:30,971 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.host = null (default)
2023-04-27 06:30:30,971 [Listener at 127.0.0.1/32931] INFO  netty.NettyConfigKeys$DataStream (ConfUtils.java:logGet(46)) - raft.netty.dataStream.port = 0 (default)
2023-04-27 06:30:30,971 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.cached = true (default)
2023-04-27 06:30:30,971 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.proxy.size = 0 (default)
2023-04-27 06:30:30,972 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:30,972 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:30,972 [Listener at 127.0.0.1/32931] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data/ratis] (custom)
2023-04-27 06:30:30,972 [fc27ded4-a200-4b47-a78c-bc930dadcc21-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x1d89fd2c] REGISTERED
2023-04-27 06:30:30,973 [fc27ded4-a200-4b47-a78c-bc930dadcc21-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x1d89fd2c] BIND: 0.0.0.0/0.0.0.0:0
2023-04-27 06:30:30,973 [fc27ded4-a200-4b47-a78c-bc930dadcc21-NettyServerStreamRpc-bossGroup--thread1] INFO  logging.LoggingHandler (AbstractInternalLogger.java:log(148)) - [id: 0x1d89fd2c, L:/0:0:0:0:0:0:0:0:35947] ACTIVE
2023-04-27 06:30:30,974 [Listener at 127.0.0.1/32931] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:<init>(132)) - GrpcServer channel type EpollServerSocketChannel
2023-04-27 06:30:30,977 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(224)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2023-04-27 06:30:30,977 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(111)) - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
2023-04-27 06:30:30,980 [Listener at 127.0.0.1/32931] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/runner/hadoop-http-auth-signature-secret
2023-04-27 06:30:30,989 [Listener at 127.0.0.1/32931] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(86)) - Http request log for http.requests.hddsDatanode is not defined
2023-04-27 06:30:30,992 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1031)) - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2023-04-27 06:30:30,993 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1007)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2023-04-27 06:30:30,994 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-27 06:30:30,994 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1015)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-27 06:30:30,996 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:<init>(190)) - HTTP server of hddsDatanode uses base directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/meta/webserver
2023-04-27 06:30:30,996 [Listener at 127.0.0.1/32931] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1250)) - Jetty bound to port 40463
2023-04-27 06:30:30,996 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_362-b09
2023-04-27 06:30:31,008 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2023-04-27 06:30:31,008 [Listener at 127.0.0.1/32931] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2023-04-27 06:30:31,008 [Listener at 127.0.0.1/32931] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 660000ms
2023-04-27 06:30:31,009 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@4f4ecc72{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,AVAILABLE}
2023-04-27 06:30:31,009 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@497f1c8a{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2023-04-27 06:30:31,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:31,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:31,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:31,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:31,246 [Listener at 127.0.0.1/32931] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@b5d6897{hddsDatanode,/,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/meta/webserver/jetty-0_0_0_0-40463-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-2929890094778761060/webapp/,AVAILABLE}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.4.0-SNAPSHOT/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2023-04-27 06:30:31,250 [Listener at 127.0.0.1/32931] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@78ff36ff{HTTP/1.1, (http/1.1)}{0.0.0.0:40463}
2023-04-27 06:30:31,250 [Listener at 127.0.0.1/32931] INFO  server.Server (Server.java:doStart(415)) - Started @157698ms
2023-04-27 06:30:31,250 [Listener at 127.0.0.1/32931] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(279)) - Sink prometheus already exists!
2023-04-27 06:30:31,255 [Listener at 127.0.0.1/32931] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(344)) - HTTP server of hddsDatanode listening at http://0.0.0.0:40463
2023-04-27 06:30:31,256 [Datanode State Machine Daemon Thread] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(517)) - Ozone container server started.
2023-04-27 06:30:31,256 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Waiting for nodes to be ready. Got 0 of 7 DN Heartbeats.
2023-04-27 06:30:31,256 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:31,256 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:31,257 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@36284f55] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2023-04-27 06:30:31,260 [Datanode State Machine Task Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(138)) - DatanodeDetails is persisted to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/meta/datanode.id
2023-04-27 06:30:31,435 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-a51375c0-82fa-4ab7-a4f3-5c3c8e8c0287/container.db to cache
2023-04-27 06:30:31,435 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-a51375c0-82fa-4ab7-a4f3-5c3c8e8c0287/container.db for volume DS-a51375c0-82fa-4ab7-a4f3-5c3c8e8c0287
2023-04-27 06:30:31,436 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:31,436 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:31,436 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 39579
2023-04-27 06:30:31,436 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis 65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:31,442 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: start RPC server
2023-04-27 06:30:31,442 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: GrpcService started, listening on 43177
2023-04-27 06:30:31,442 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 65d5be4b-faf3-45c4-8c44-989db5872d1e is started using port 43177 for RATIS
2023-04-27 06:30:31,442 [JvmPauseMonitor52] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-65d5be4b-faf3-45c4-8c44-989db5872d1e: Started
2023-04-27 06:30:31,442 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 65d5be4b-faf3-45c4-8c44-989db5872d1e is started using port 43177 for RATIS_ADMIN
2023-04-27 06:30:31,443 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 65d5be4b-faf3-45c4-8c44-989db5872d1e is started using port 43177 for RATIS_SERVER
2023-04-27 06:30:31,443 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 65d5be4b-faf3-45c4-8c44-989db5872d1e is started using port 46609 for RATIS_DATASTREAM
2023-04-27 06:30:31,443 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc 65d5be4b-faf3-45c4-8c44-989db5872d1e is started using port 43595
2023-04-27 06:30:31,444 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:31,751 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:31,752 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:31,752 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:31,755 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:31,756 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:31,757 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:31,759 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:31,759 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:31,927 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-d15fbadc-58e5-4a5f-b65f-c5d5401504e3/container.db to cache
2023-04-27 06:30:31,927 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-d15fbadc-58e5-4a5f-b65f-c5d5401504e3/container.db for volume DS-d15fbadc-58e5-4a5f-b65f-c5d5401504e3
2023-04-27 06:30:31,928 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:31,928 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:31,928 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 41211
2023-04-27 06:30:31,928 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis ff409154-2e62-420c-a7a7-066f3a70d145
2023-04-27 06:30:31,933 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - ff409154-2e62-420c-a7a7-066f3a70d145: start RPC server
2023-04-27 06:30:31,933 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - ff409154-2e62-420c-a7a7-066f3a70d145: GrpcService started, listening on 38661
2023-04-27 06:30:31,935 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis ff409154-2e62-420c-a7a7-066f3a70d145 is started using port 38661 for RATIS
2023-04-27 06:30:31,935 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis ff409154-2e62-420c-a7a7-066f3a70d145 is started using port 38661 for RATIS_ADMIN
2023-04-27 06:30:31,935 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis ff409154-2e62-420c-a7a7-066f3a70d145 is started using port 38661 for RATIS_SERVER
2023-04-27 06:30:31,935 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis ff409154-2e62-420c-a7a7-066f3a70d145 is started using port 34185 for RATIS_DATASTREAM
2023-04-27 06:30:31,936 [JvmPauseMonitor53] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-ff409154-2e62-420c-a7a7-066f3a70d145: Started
2023-04-27 06:30:31,941 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc ff409154-2e62-420c-a7a7-066f3a70d145 is started using port 35187
2023-04-27 06:30:31,941 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:32,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:32,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:32,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:32,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:32,256 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Waiting for nodes to be ready. Got 0 of 7 DN Heartbeats.
2023-04-27 06:30:32,256 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:32,256 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:32,425 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-247405fc-3cbc-40fe-829c-d9164d60f277/container.db to cache
2023-04-27 06:30:32,425 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-247405fc-3cbc-40fe-829c-d9164d60f277/container.db for volume DS-247405fc-3cbc-40fe-829c-d9164d60f277
2023-04-27 06:30:32,426 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:32,426 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:32,426 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 37653
2023-04-27 06:30:32,426 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis 735446df-424c-4d38-a683-bd4ef5c8b9e6
2023-04-27 06:30:32,437 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: start RPC server
2023-04-27 06:30:32,438 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: GrpcService started, listening on 38151
2023-04-27 06:30:32,438 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 735446df-424c-4d38-a683-bd4ef5c8b9e6 is started using port 38151 for RATIS
2023-04-27 06:30:32,438 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 735446df-424c-4d38-a683-bd4ef5c8b9e6 is started using port 38151 for RATIS_ADMIN
2023-04-27 06:30:32,438 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 735446df-424c-4d38-a683-bd4ef5c8b9e6 is started using port 38151 for RATIS_SERVER
2023-04-27 06:30:32,438 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis 735446df-424c-4d38-a683-bd4ef5c8b9e6 is started using port 41265 for RATIS_DATASTREAM
2023-04-27 06:30:32,438 [JvmPauseMonitor54] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-735446df-424c-4d38-a683-bd4ef5c8b9e6: Started
2023-04-27 06:30:32,439 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc 735446df-424c-4d38-a683-bd4ef5c8b9e6 is started using port 36889
2023-04-27 06:30:32,444 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:32,449 [IPC Server handler 5 on default port 35697] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/b85c0d2e-fc3b-4312-9b63-e609b6e2c228
2023-04-27 06:30:32,449 [IPC Server handler 5 on default port 35697] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : b85c0d2e-fc3b-4312-9b63-e609b6e2c228{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=43203, REPLICATION=37613, RATIS=41235, RATIS_ADMIN=41235, RATIS_SERVER=41235, RATIS_DATASTREAM=40691, STANDALONE=41369], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:32,457 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (DataNodeSafeModeRule.java:process(71)) - SCM in safe mode. 1 DataNodes registered, 3 required.
2023-04-27 06:30:32,457 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(200)) - ContainerSafeModeRule rule is successfully validated
2023-04-27 06:30:32,463 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:32,463 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(200)) - AtleastOneDatanodeReportedRule rule is successfully validated
2023-04-27 06:30:32,463 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=530b76c3-23b6-4f35-bf64-ed7ecff91685 to datanode:b85c0d2e-fc3b-4312-9b63-e609b6e2c228
2023-04-27 06:30:32,464 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 530b76c3-23b6-4f35-bf64-ed7ecff91685, Nodes: b85c0d2e-fc3b-4312-9b63-e609b6e2c228(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:32.463Z[Etc/UTC]].
2023-04-27 06:30:32,752 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:32,752 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:32,752 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:32,758 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:32,878 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-e95eaafb-444b-4fb8-99a4-625c9c096f6f/container.db to cache
2023-04-27 06:30:32,878 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-e95eaafb-444b-4fb8-99a4-625c9c096f6f/container.db for volume DS-e95eaafb-444b-4fb8-99a4-625c9c096f6f
2023-04-27 06:30:32,879 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:32,879 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:32,880 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 34143
2023-04-27 06:30:32,884 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis ec732e1e-81da-4c3b-ad71-f2fe790a57c7
2023-04-27 06:30:32,887 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: start RPC server
2023-04-27 06:30:32,887 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: GrpcService started, listening on 41649
2023-04-27 06:30:32,888 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis ec732e1e-81da-4c3b-ad71-f2fe790a57c7 is started using port 41649 for RATIS
2023-04-27 06:30:32,889 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis ec732e1e-81da-4c3b-ad71-f2fe790a57c7 is started using port 41649 for RATIS_ADMIN
2023-04-27 06:30:32,888 [JvmPauseMonitor55] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-ec732e1e-81da-4c3b-ad71-f2fe790a57c7: Started
2023-04-27 06:30:32,889 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis ec732e1e-81da-4c3b-ad71-f2fe790a57c7 is started using port 41649 for RATIS_SERVER
2023-04-27 06:30:32,890 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis ec732e1e-81da-4c3b-ad71-f2fe790a57c7 is started using port 36675 for RATIS_DATASTREAM
2023-04-27 06:30:32,890 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc ec732e1e-81da-4c3b-ad71-f2fe790a57c7 is started using port 39751
2023-04-27 06:30:32,891 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:32,895 [IPC Server handler 4 on default port 35697] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/10617bfc-881b-4e3d-8722-e7f81aaf7e30
2023-04-27 06:30:32,895 [IPC Server handler 4 on default port 35697] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : 10617bfc-881b-4e3d-8722-e7f81aaf7e30{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=46477, REPLICATION=46361, RATIS=37309, RATIS_ADMIN=37309, RATIS_SERVER=37309, RATIS_DATASTREAM=37703, STANDALONE=38295], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:32,895 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:32,895 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (DataNodeSafeModeRule.java:process(71)) - SCM in safe mode. 2 DataNodes registered, 3 required.
2023-04-27 06:30:32,896 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=e9f594c3-20a8-4fb8-8977-85090f82d0e9 to datanode:10617bfc-881b-4e3d-8722-e7f81aaf7e30
2023-04-27 06:30:32,896 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: e9f594c3-20a8-4fb8-8977-85090f82d0e9, Nodes: 10617bfc-881b-4e3d-8722-e7f81aaf7e30(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:32.896Z[Etc/UTC]].
2023-04-27 06:30:33,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:33,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:33,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:33,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:33,256 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Waiting for nodes to be ready. Got 2 of 7 DN Heartbeats.
2023-04-27 06:30:33,256 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:33,257 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:33,259 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:33,300 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-cd8cfd15-cadf-4899-bb1f-261442fad98b/container.db to cache
2023-04-27 06:30:33,300 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  volume.HddsVolume (HddsVolume.java:createDbStore(350)) - SchemaV3 db is created and loaded at /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data-0/containers/hdds/4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/DS-cd8cfd15-cadf-4899-bb1f-261442fad98b/container.db for volume DS-cd8cfd15-cadf-4899-bb1f-261442fad98b
2023-04-27 06:30:33,300 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(397)) - Attempting to start container services.
2023-04-27 06:30:33,300 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(314)) - Scheduled background container scanners and the on-demand container scanner have been disabled.
2023-04-27 06:30:33,301 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  replication.ReplicationServer (ReplicationServer.java:start(109)) - ReplicationServer is started using port 46611
2023-04-27 06:30:33,301 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(517)) - Starting XceiverServerRatis fc27ded4-a200-4b47-a78c-bc930dadcc21
2023-04-27 06:30:33,309 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.RaftServer (RaftServerProxy.java:startImpl(393)) - fc27ded4-a200-4b47-a78c-bc930dadcc21: start RPC server
2023-04-27 06:30:33,312 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.GrpcService (GrpcService.java:startImpl(262)) - fc27ded4-a200-4b47-a78c-bc930dadcc21: GrpcService started, listening on 35905
2023-04-27 06:30:33,313 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis fc27ded4-a200-4b47-a78c-bc930dadcc21 is started using port 35905 for RATIS
2023-04-27 06:30:33,313 [JvmPauseMonitor56] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(105)) - JvmPauseMonitor-fc27ded4-a200-4b47-a78c-bc930dadcc21: Started
2023-04-27 06:30:33,313 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis fc27ded4-a200-4b47-a78c-bc930dadcc21 is started using port 35905 for RATIS_ADMIN
2023-04-27 06:30:33,313 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis fc27ded4-a200-4b47-a78c-bc930dadcc21 is started using port 35905 for RATIS_SERVER
2023-04-27 06:30:33,313 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:getRealPort(543)) - XceiverServerRatis fc27ded4-a200-4b47-a78c-bc930dadcc21 is started using port 35947 for RATIS_DATASTREAM
2023-04-27 06:30:33,314 [EndpointStateMachine task thread for /0.0.0.0:35697 - 0 ] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(180)) - XceiverServerGrpc fc27ded4-a200-4b47-a78c-bc930dadcc21 is started using port 46677
2023-04-27 06:30:33,315 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:33,388 [IPC Server handler 3 on default port 35697] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:33,389 [IPC Server handler 3 on default port 35697] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : 65d5be4b-faf3-45c4-8c44-989db5872d1e{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=45569, REPLICATION=39579, RATIS=43177, RATIS_ADMIN=43177, RATIS_SERVER=43177, RATIS_DATASTREAM=46609, STANDALONE=43595], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:33,399 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (DataNodeSafeModeRule.java:process(71)) - SCM in safe mode. 3 DataNodes registered, 3 required.
2023-04-27 06:30:33,399 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(200)) - DataNodeSafeModeRule rule is successfully validated
2023-04-27 06:30:33,399 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:completePreCheck(229)) - All SCM safe mode pre check rules have passed
2023-04-27 06:30:33,399 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  ha.SCMContext (SCMContext.java:updateSafeModeStatus(228)) - Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
2023-04-27 06:30:33,399 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:33,399 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=4ba5e2a4-1c2d-4d6b-9024-ffa4651c59e3 to datanode:65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:33,400 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 4ba5e2a4-1c2d-4d6b-9024-ffa4651c59e3, Nodes: 65d5be4b-faf3-45c4-8c44-989db5872d1e(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:33.399Z[Etc/UTC]].
2023-04-27 06:30:33,400 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=828ebe36-54ef-47fd-9d4d-4bcccffa137a to datanode:10617bfc-881b-4e3d-8722-e7f81aaf7e30
2023-04-27 06:30:33,400 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=828ebe36-54ef-47fd-9d4d-4bcccffa137a to datanode:65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:33,400 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=828ebe36-54ef-47fd-9d4d-4bcccffa137a to datanode:b85c0d2e-fc3b-4312-9b63-e609b6e2c228
2023-04-27 06:30:33,400 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 828ebe36-54ef-47fd-9d4d-4bcccffa137a, Nodes: 10617bfc-881b-4e3d-8722-e7f81aaf7e30(fv-az260-775/10.1.0.33)65d5be4b-faf3-45c4-8c44-989db5872d1e(fv-az260-775/10.1.0.33)b85c0d2e-fc3b-4312-9b63-e609b6e2c228(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:33.400Z[Etc/UTC]].
2023-04-27 06:30:33,401 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(160)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
2023-04-27 06:30:33,401 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:33,401 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(160)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
2023-04-27 06:30:33,753 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:33,753 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:33,753 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:33,753 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:33,757 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:33,759 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:33,761 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:33,763 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:33,763 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:33,888 [IPC Server handler 4 on default port 35697] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/ff409154-2e62-420c-a7a7-066f3a70d145
2023-04-27 06:30:33,888 [IPC Server handler 4 on default port 35697] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : ff409154-2e62-420c-a7a7-066f3a70d145{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=45617, REPLICATION=41211, RATIS=38661, RATIS_ADMIN=38661, RATIS_SERVER=38661, RATIS_DATASTREAM=34185, STANDALONE=35187], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:33,889 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:33,889 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=a0bc8da9-5bdc-49fa-a174-6a7d0f9095b0 to datanode:ff409154-2e62-420c-a7a7-066f3a70d145
2023-04-27 06:30:33,889 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: a0bc8da9-5bdc-49fa-a174-6a7d0f9095b0, Nodes: ff409154-2e62-420c-a7a7-066f3a70d145(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:33.889Z[Etc/UTC]].
2023-04-27 06:30:33,890 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(160)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 3.
2023-04-27 06:30:34,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:34,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:34,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:34,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:34,112 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:34,257 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Waiting for nodes to be ready. Got 4 of 7 DN Heartbeats.
2023-04-27 06:30:34,257 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:34,257 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:34,387 [IPC Server handler 3 on default port 35697] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/735446df-424c-4d38-a683-bd4ef5c8b9e6
2023-04-27 06:30:34,387 [IPC Server handler 3 on default port 35697] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : 735446df-424c-4d38-a683-bd4ef5c8b9e6{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=36753, REPLICATION=37653, RATIS=38151, RATIS_ADMIN=38151, RATIS_SERVER=38151, RATIS_DATASTREAM=41265, STANDALONE=36889], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:34,388 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:34,388 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=a8387d3e-c7a3-4c54-ae38-13438dc0d738 to datanode:735446df-424c-4d38-a683-bd4ef5c8b9e6
2023-04-27 06:30:34,388 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: a8387d3e-c7a3-4c54-ae38-13438dc0d738, Nodes: 735446df-424c-4d38-a683-bd4ef5c8b9e6(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:34.388Z[Etc/UTC]].
2023-04-27 06:30:34,389 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(160)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
2023-04-27 06:30:34,753 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:34,754 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:34,754 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:34,761 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:34,850 [IPC Server handler 15 on default port 35697] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/ec732e1e-81da-4c3b-ad71-f2fe790a57c7
2023-04-27 06:30:34,850 [IPC Server handler 15 on default port 35697] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : ec732e1e-81da-4c3b-ad71-f2fe790a57c7{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=36927, REPLICATION=34143, RATIS=41649, RATIS_ADMIN=41649, RATIS_SERVER=41649, RATIS_DATASTREAM=36675, STANDALONE=39751], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:34,850 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:34,851 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=5839be52-9155-436a-b7fa-f81ba65d63c6 to datanode:ec732e1e-81da-4c3b-ad71-f2fe790a57c7
2023-04-27 06:30:34,851 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 5839be52-9155-436a-b7fa-f81ba65d63c6, Nodes: ec732e1e-81da-4c3b-ad71-f2fe790a57c7(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:34.851Z[Etc/UTC]].
2023-04-27 06:30:34,852 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=8fc0afc7-f51d-447c-be01-a8bb40e16cad to datanode:735446df-424c-4d38-a683-bd4ef5c8b9e6
2023-04-27 06:30:34,852 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=8fc0afc7-f51d-447c-be01-a8bb40e16cad to datanode:ec732e1e-81da-4c3b-ad71-f2fe790a57c7
2023-04-27 06:30:34,852 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=8fc0afc7-f51d-447c-be01-a8bb40e16cad to datanode:ff409154-2e62-420c-a7a7-066f3a70d145
2023-04-27 06:30:34,852 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 8fc0afc7-f51d-447c-be01-a8bb40e16cad, Nodes: 735446df-424c-4d38-a683-bd4ef5c8b9e6(fv-az260-775/10.1.0.33)ec732e1e-81da-4c3b-ad71-f2fe790a57c7(fv-az260-775/10.1.0.33)ff409154-2e62-420c-a7a7-066f3a70d145(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:34.852Z[Etc/UTC]].
2023-04-27 06:30:34,853 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(160)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 6.
2023-04-27 06:30:34,892 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:35,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:35,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:35,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:35,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:35,257 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Waiting for nodes to be ready. Got 6 of 7 DN Heartbeats.
2023-04-27 06:30:35,257 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:35,257 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:35,258 [IPC Server handler 5 on default port 35697] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:add(112)) - Added a new node: /default-rack/fc27ded4-a200-4b47-a78c-bc930dadcc21
2023-04-27 06:30:35,258 [IPC Server handler 5 on default port 35697] INFO  node.SCMNodeManager (SCMNodeManager.java:register(404)) - Registered Data node : fc27ded4-a200-4b47-a78c-bc930dadcc21{ip: 10.1.0.33, host: fv-az260-775, ports: [HTTP=40463, REPLICATION=46611, RATIS=35905, RATIS_ADMIN=35905, RATIS_SERVER=35905, RATIS_DATASTREAM=35947, STANDALONE=46677], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
2023-04-27 06:30:35,264 [EventQueue-NewNodeForNewNodeHandler] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyEventTriggered(276)) - trigger a one-shot run on RatisPipelineUtilsThread.
2023-04-27 06:30:35,265 [RatisPipelineUtilsThread - 0] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$0(206)) - Sending CreatePipelineCommand for pipeline:PipelineID=22ba932c-7854-4cc0-beb2-1ff79dd9e52c to datanode:fc27ded4-a200-4b47-a78c-bc930dadcc21
2023-04-27 06:30:35,265 [RatisPipelineUtilsThread - 0] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:addPipeline(103)) - Created pipeline Pipeline[ Id: 22ba932c-7854-4cc0-beb2-1ff79dd9e52c, Nodes: fc27ded4-a200-4b47-a78c-bc930dadcc21(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-27T06:30:35.265Z[Etc/UTC]].
2023-04-27 06:30:35,266 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(160)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 6.
2023-04-27 06:30:35,442 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: addNew group-ED7ECFF91685:[b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:1|startupRole:FOLLOWER] returns group-ED7ECFF91685:java.util.concurrent.CompletableFuture@3cd4ac16[Not completed]
2023-04-27 06:30:35,443 [pool-2496-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: new RaftServerImpl for group-ED7ECFF91685:[b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:35,443 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:35,443 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:35,443 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:35,443 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:35,443 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:35,443 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:35,443 [pool-2496-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685: ConfigurationManager, init=-1: peers:[b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:35,443 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis] (custom)
2023-04-27 06:30:35,444 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:35,444 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:35,444 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:35,444 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:35,444 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:35,445 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:35,445 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:35,445 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:35,445 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:35,446 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:35,446 [pool-2496-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis/530b76c3-23b6-4f35-bf64-ed7ecff91685 does not exist. Creating ...
2023-04-27 06:30:35,447 [pool-2496-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis/530b76c3-23b6-4f35-bf64-ed7ecff91685/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:35,450 [pool-2496-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis/530b76c3-23b6-4f35-bf64-ed7ecff91685 has been successfully formatted.
2023-04-27 06:30:35,450 [pool-2496-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-ED7ECFF91685: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:35,450 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:35,450 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:35,450 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:35,451 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:35,451 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:35,451 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 530b76c3-23b6-4f35-bf64-ed7ecff91685, Nodes: b85c0d2e-fc3b-4312-9b63-e609b6e2c228(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b85c0d2e-fc3b-4312-9b63-e609b6e2c228, CreationTimestamp2023-04-27T06:30:32.463Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:35,451 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:35,458 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:35,459 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:35,459 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:35,459 [pool-2496-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis/530b76c3-23b6-4f35-bf64-ed7ecff91685
2023-04-27 06:30:35,459 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:35,459 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:35,459 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:35,459 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:35,459 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:35,459 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:35,459 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:35,460 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:35,461 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:35,461 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:35,468 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:35,468 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:35,468 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:35,468 [pool-2496-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:35,468 [pool-2496-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:35,468 [pool-2496-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685: start as a follower, conf=-1: peers:[b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:35,468 [pool-2496-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:35,469 [pool-2496-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: start b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-FollowerState
2023-04-27 06:30:35,469 [pool-2496-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-ED7ECFF91685,id=b85c0d2e-fc3b-4312-9b63-e609b6e2c228
2023-04-27 06:30:35,469 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:35,469 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:35,469 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:35,469 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:35,470 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:35,470 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:35,470 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=530b76c3-23b6-4f35-bf64-ed7ecff91685
2023-04-27 06:30:35,470 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=530b76c3-23b6-4f35-bf64-ed7ecff91685.
2023-04-27 06:30:35,470 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: addNew group-4BCCCFFA137A:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER] returns group-4BCCCFFA137A:java.util.concurrent.CompletableFuture@38eb2f54[Not completed]
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: new RaftServerImpl for group-4BCCCFFA137A:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A: ConfigurationManager, init=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis] (custom)
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:35,472 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:35,473 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:35,474 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:35,474 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:35,474 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:35,474 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:35,474 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:35,474 [pool-2496-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a does not exist. Creating ...
2023-04-27 06:30:35,476 [pool-2496-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:35,477 [pool-2496-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a has been successfully formatted.
2023-04-27 06:30:35,477 [pool-2496-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-4BCCCFFA137A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:35,477 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:35,477 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:35,478 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:35,478 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:35,478 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:35,478 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:35,478 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:35,479 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:35,479 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:35,479 [pool-2496-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a
2023-04-27 06:30:35,479 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:35,479 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:35,479 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:35,479 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:35,480 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:35,480 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:35,480 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:35,480 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:35,481 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:35,481 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:35,489 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:35,489 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:35,489 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:35,490 [pool-2496-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:35,490 [pool-2496-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:35,490 [pool-2496-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A: start as a follower, conf=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:35,490 [pool-2496-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:35,490 [pool-2496-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: start b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState
2023-04-27 06:30:35,491 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:35,491 [pool-2496-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4BCCCFFA137A,id=b85c0d2e-fc3b-4312-9b63-e609b6e2c228
2023-04-27 06:30:35,491 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:35,491 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:35,491 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:35,491 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:35,491 [pool-2496-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:35,492 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=828ebe36-54ef-47fd-9d4d-4bcccffa137a
2023-04-27 06:30:35,499 [grpc-default-executor-8] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: addNew group-4BCCCFFA137A:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER] returns group-4BCCCFFA137A:java.util.concurrent.CompletableFuture@183db4c3[Not completed]
2023-04-27 06:30:35,500 [pool-2518-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: new RaftServerImpl for group-4BCCCFFA137A:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:35,500 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:35,500 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:35,500 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:35,500 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:35,500 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:35,500 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:35,500 [pool-2518-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A: ConfigurationManager, init=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:35,500 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis] (custom)
2023-04-27 06:30:35,501 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:35,501 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:35,501 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:35,501 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:35,501 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:35,503 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:35,503 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:35,503 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:35,503 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:35,503 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:35,503 [pool-2518-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a does not exist. Creating ...
2023-04-27 06:30:35,504 [pool-2518-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:35,507 [pool-2518-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a has been successfully formatted.
2023-04-27 06:30:35,507 [pool-2518-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-4BCCCFFA137A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:35,507 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:35,507 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:35,508 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:35,508 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:35,508 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:35,508 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:35,509 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:35,509 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:35,509 [pool-2518-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a
2023-04-27 06:30:35,509 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:35,509 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:35,509 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:35,509 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:35,509 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:35,509 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:35,509 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:35,509 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:35,511 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:35,512 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:35,519 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:35,519 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:35,519 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:35,520 [pool-2518-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:35,520 [pool-2518-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:35,520 [pool-2518-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A: start as a follower, conf=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:35,520 [pool-2518-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:35,520 [pool-2518-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: start 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FollowerState
2023-04-27 06:30:35,521 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:35,523 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:35,525 [pool-2518-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4BCCCFFA137A,id=10617bfc-881b-4e3d-8722-e7f81aaf7e30
2023-04-27 06:30:35,525 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:35,525 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:35,525 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:35,526 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:35,536 [grpc-default-executor-8] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: addNew group-4BCCCFFA137A:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER] returns group-4BCCCFFA137A:java.util.concurrent.CompletableFuture@bd4030e[Not completed]
2023-04-27 06:30:35,537 [pool-2540-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: new RaftServerImpl for group-4BCCCFFA137A:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:35,537 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:35,537 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:35,538 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:35,538 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:35,538 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:35,538 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:35,538 [pool-2540-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A: ConfigurationManager, init=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:35,538 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis] (custom)
2023-04-27 06:30:35,538 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:35,538 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:35,538 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:35,538 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:35,538 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:35,540 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:35,540 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:35,540 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:35,540 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:35,540 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:35,541 [pool-2540-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a does not exist. Creating ...
2023-04-27 06:30:35,542 [pool-2540-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:35,544 [pool-2540-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a has been successfully formatted.
2023-04-27 06:30:35,544 [pool-2540-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-4BCCCFFA137A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:35,544 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:35,544 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:35,544 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:35,544 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:35,545 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:35,545 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:35,546 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:35,546 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:35,546 [pool-2540-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a
2023-04-27 06:30:35,546 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:35,546 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:35,546 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:35,547 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:35,547 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:35,547 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:35,547 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:35,547 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:35,548 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:35,549 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:35,557 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:35,557 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:35,557 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:35,557 [pool-2540-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:35,557 [pool-2540-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:35,558 [pool-2540-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A: start as a follower, conf=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:35,558 [pool-2540-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:35,558 [pool-2540-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: start 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-FollowerState
2023-04-27 06:30:35,558 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:35,558 [pool-2540-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4BCCCFFA137A,id=65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:35,558 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:35,558 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:35,559 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:35,559 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:35,559 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:35,570 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS THREE PipelineID=828ebe36-54ef-47fd-9d4d-4bcccffa137a.
2023-04-27 06:30:35,753 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:35,755 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:35,755 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:35,758 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:35,758 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:35,762 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:35,763 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:35,763 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:35,771 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:35,893 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: addNew group-85090F82D0E9:[10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:1|startupRole:FOLLOWER] returns group-85090F82D0E9:java.util.concurrent.CompletableFuture@65982e8c[Not completed]
2023-04-27 06:30:35,894 [pool-2518-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: new RaftServerImpl for group-85090F82D0E9:[10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:35,894 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:35,894 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:35,894 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:35,894 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:35,894 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:35,894 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:35,894 [pool-2518-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9: ConfigurationManager, init=-1: peers:[10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:35,894 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis] (custom)
2023-04-27 06:30:35,895 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:35,895 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:35,895 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:35,895 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:35,895 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:35,896 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:35,896 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:35,897 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:35,897 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:35,897 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:35,897 [pool-2518-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis/e9f594c3-20a8-4fb8-8977-85090f82d0e9 does not exist. Creating ...
2023-04-27 06:30:35,898 [pool-2518-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis/e9f594c3-20a8-4fb8-8977-85090f82d0e9/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:35,899 [pool-2518-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis/e9f594c3-20a8-4fb8-8977-85090f82d0e9 has been successfully formatted.
2023-04-27 06:30:35,900 [pool-2518-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-85090F82D0E9: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:35,900 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:35,900 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:35,900 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:35,900 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: e9f594c3-20a8-4fb8-8977-85090f82d0e9, Nodes: 10617bfc-881b-4e3d-8722-e7f81aaf7e30(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:10617bfc-881b-4e3d-8722-e7f81aaf7e30, CreationTimestamp2023-04-27T06:30:32.896Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:35,901 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:35,901 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:35,901 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:35,901 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:35,902 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:35,902 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:35,902 [pool-2518-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis/e9f594c3-20a8-4fb8-8977-85090f82d0e9
2023-04-27 06:30:35,902 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:35,902 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:35,902 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:35,902 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:35,902 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:35,902 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:35,902 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:35,902 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:35,903 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:35,904 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:35,910 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:35,911 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:35,911 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:35,911 [pool-2518-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:35,911 [pool-2518-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:35,911 [pool-2518-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9: start as a follower, conf=-1: peers:[10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:35,912 [pool-2518-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:35,912 [pool-2518-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: start 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-FollowerState
2023-04-27 06:30:35,912 [pool-2518-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-85090F82D0E9,id=10617bfc-881b-4e3d-8722-e7f81aaf7e30
2023-04-27 06:30:35,912 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:35,912 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:35,912 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:35,912 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:35,912 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:35,912 [pool-2518-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:35,916 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=e9f594c3-20a8-4fb8-8977-85090f82d0e9
2023-04-27 06:30:35,916 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=e9f594c3-20a8-4fb8-8977-85090f82d0e9.
2023-04-27 06:30:36,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:36,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:36,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:36,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:36,251 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:36,257 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Nodes are ready. Got 7 of 7 DN Heartbeats.
2023-04-27 06:30:36,257 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:36,258 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:36,400 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: addNew group-FFA4651C59E3:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER] returns group-FFA4651C59E3:java.util.concurrent.CompletableFuture@4504bebb[Not completed]
2023-04-27 06:30:36,401 [pool-2540-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: new RaftServerImpl for group-FFA4651C59E3:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:36,401 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:36,401 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:36,401 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:36,401 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:36,401 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:36,402 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:36,402 [pool-2540-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3: ConfigurationManager, init=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:36,402 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis] (custom)
2023-04-27 06:30:36,402 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:36,402 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:36,402 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:36,402 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:36,402 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:36,404 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:36,404 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:36,404 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:36,404 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:36,404 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:36,404 [pool-2540-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis/4ba5e2a4-1c2d-4d6b-9024-ffa4651c59e3 does not exist. Creating ...
2023-04-27 06:30:36,408 [pool-2540-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis/4ba5e2a4-1c2d-4d6b-9024-ffa4651c59e3/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:36,412 [pool-2540-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis/4ba5e2a4-1c2d-4d6b-9024-ffa4651c59e3 has been successfully formatted.
2023-04-27 06:30:36,412 [pool-2540-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-FFA4651C59E3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:36,413 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:36,413 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:36,413 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:36,413 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:36,413 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:36,413 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:36,414 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:36,415 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 4ba5e2a4-1c2d-4d6b-9024-ffa4651c59e3, Nodes: 65d5be4b-faf3-45c4-8c44-989db5872d1e(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:65d5be4b-faf3-45c4-8c44-989db5872d1e, CreationTimestamp2023-04-27T06:30:33.399Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:36,415 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:36,415 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:36,415 [pool-2540-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis/4ba5e2a4-1c2d-4d6b-9024-ffa4651c59e3
2023-04-27 06:30:36,415 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:36,415 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:36,415 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:36,415 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:36,416 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:36,416 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:36,416 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:36,416 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:36,417 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:36,417 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:36,429 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:36,429 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:36,429 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:36,430 [pool-2540-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:36,430 [pool-2540-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:36,430 [pool-2540-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3: start as a follower, conf=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:36,430 [pool-2540-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:36,430 [pool-2540-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: start 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-FollowerState
2023-04-27 06:30:36,433 [pool-2540-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FFA4651C59E3,id=65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:36,433 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:36,433 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:36,433 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:36,433 [pool-2540-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:36,434 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:36,434 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:36,437 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=4ba5e2a4-1c2d-4d6b-9024-ffa4651c59e3
2023-04-27 06:30:36,438 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=4ba5e2a4-1c2d-4d6b-9024-ffa4651c59e3.
2023-04-27 06:30:36,531 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:36,697 [BlockDeletingService#0] INFO  interfaces.ContainerDeletionChoosingPolicyTemplate (ContainerDeletionChoosingPolicyTemplate.java:chooseContainerForBlockDeletion(83)) - Chosen 0/5000 blocks from 0 candidate containers.
2023-04-27 06:30:36,754 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:36,756 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:36,756 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:36,762 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:36,888 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - ff409154-2e62-420c-a7a7-066f3a70d145: addNew group-6A7D0F9095B0:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER] returns group-6A7D0F9095B0:java.util.concurrent.CompletableFuture@ebdbe3[Not completed]
2023-04-27 06:30:36,890 [pool-2562-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - ff409154-2e62-420c-a7a7-066f3a70d145: new RaftServerImpl for group-6A7D0F9095B0:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:36,890 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:36,890 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:36,890 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:36,890 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:36,890 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:36,890 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:36,890 [pool-2562-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0: ConfigurationManager, init=-1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:36,890 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis] (custom)
2023-04-27 06:30:36,891 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:36,891 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:36,891 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:36,891 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:36,891 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:36,892 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:36,892 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:36,892 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:36,893 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:36,893 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:36,893 [pool-2562-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis/a0bc8da9-5bdc-49fa-a174-6a7d0f9095b0 does not exist. Creating ...
2023-04-27 06:30:36,894 [pool-2562-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis/a0bc8da9-5bdc-49fa-a174-6a7d0f9095b0/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:36,896 [pool-2562-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis/a0bc8da9-5bdc-49fa-a174-6a7d0f9095b0 has been successfully formatted.
2023-04-27 06:30:36,896 [pool-2562-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-6A7D0F9095B0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:36,896 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:36,896 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:36,896 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:36,897 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:36,897 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:36,897 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: a0bc8da9-5bdc-49fa-a174-6a7d0f9095b0, Nodes: ff409154-2e62-420c-a7a7-066f3a70d145(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:ff409154-2e62-420c-a7a7-066f3a70d145, CreationTimestamp2023-04-27T06:30:33.889Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:36,897 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:36,897 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:36,898 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:36,898 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:36,898 [pool-2562-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis/a0bc8da9-5bdc-49fa-a174-6a7d0f9095b0
2023-04-27 06:30:36,898 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:36,898 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:36,898 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:36,898 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:36,898 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:36,898 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:36,898 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:36,898 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:36,899 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:36,900 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:36,901 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:36,906 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:36,906 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:36,907 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:36,907 [pool-2562-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:36,907 [pool-2562-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:36,907 [pool-2562-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0: start as a follower, conf=-1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:36,907 [pool-2562-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:36,907 [pool-2562-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ff409154-2e62-420c-a7a7-066f3a70d145: start ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-FollowerState
2023-04-27 06:30:36,908 [pool-2562-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6A7D0F9095B0,id=ff409154-2e62-420c-a7a7-066f3a70d145
2023-04-27 06:30:36,908 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:36,908 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:36,908 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:36,908 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:36,908 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:36,909 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:36,910 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=a0bc8da9-5bdc-49fa-a174-6a7d0f9095b0
2023-04-27 06:30:36,910 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=a0bc8da9-5bdc-49fa-a174-6a7d0f9095b0.
2023-04-27 06:30:36,910 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - ff409154-2e62-420c-a7a7-066f3a70d145: addNew group-A8BB40E16CAD:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER] returns group-A8BB40E16CAD:java.util.concurrent.CompletableFuture@4d44f26[Not completed]
2023-04-27 06:30:36,912 [pool-2562-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - ff409154-2e62-420c-a7a7-066f3a70d145: new RaftServerImpl for group-A8BB40E16CAD:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:36,912 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:36,912 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:36,912 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:36,912 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:36,913 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:36,913 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:36,913 [pool-2562-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD: ConfigurationManager, init=-1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:36,913 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis] (custom)
2023-04-27 06:30:36,913 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:36,913 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:36,913 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:36,913 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:36,913 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:36,915 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:36,915 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:36,915 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:36,915 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:36,915 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:36,915 [pool-2562-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad does not exist. Creating ...
2023-04-27 06:30:36,917 [pool-2562-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:36,918 [pool-2562-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad has been successfully formatted.
2023-04-27 06:30:36,918 [pool-2562-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-A8BB40E16CAD: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:36,918 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:36,918 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:36,919 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:36,919 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:36,919 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:36,919 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:36,919 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:36,920 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:36,920 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:36,920 [pool-2562-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad
2023-04-27 06:30:36,920 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:36,920 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:36,920 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:36,920 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:36,920 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:36,920 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:36,920 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:36,920 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:36,921 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:36,922 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:36,929 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:36,929 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:36,929 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:36,930 [pool-2562-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:36,930 [pool-2562-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:36,930 [pool-2562-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD: start as a follower, conf=-1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:36,930 [pool-2562-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:36,930 [pool-2562-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ff409154-2e62-420c-a7a7-066f3a70d145: start ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-FollowerState
2023-04-27 06:30:36,931 [pool-2562-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A8BB40E16CAD,id=ff409154-2e62-420c-a7a7-066f3a70d145
2023-04-27 06:30:36,932 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:36,932 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:36,932 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:36,932 [pool-2562-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:36,933 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:36,933 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:36,933 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=8fc0afc7-f51d-447c-be01-a8bb40e16cad
2023-04-27 06:30:36,941 [grpc-default-executor-5] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: addNew group-A8BB40E16CAD:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER] returns group-A8BB40E16CAD:java.util.concurrent.CompletableFuture@4f59ba87[Not completed]
2023-04-27 06:30:36,943 [pool-2584-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: new RaftServerImpl for group-A8BB40E16CAD:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:36,943 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:36,943 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:36,943 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:36,943 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:36,943 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:36,943 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:36,943 [pool-2584-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD: ConfigurationManager, init=-1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:36,943 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis] (custom)
2023-04-27 06:30:36,944 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:36,944 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:36,944 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:36,944 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:36,944 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:36,945 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:36,945 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:36,945 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:36,945 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:36,945 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:36,946 [pool-2584-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad does not exist. Creating ...
2023-04-27 06:30:36,947 [pool-2584-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:36,949 [pool-2584-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad has been successfully formatted.
2023-04-27 06:30:36,949 [pool-2584-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-A8BB40E16CAD: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:36,949 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:36,950 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:36,950 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:36,950 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:36,950 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:36,950 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:36,951 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:36,951 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:36,951 [pool-2584-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad
2023-04-27 06:30:36,951 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:36,951 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:36,951 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:36,951 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:36,951 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:36,951 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:36,951 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:36,951 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:36,952 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:36,953 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:36,960 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:36,960 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:36,960 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:36,960 [pool-2584-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:36,960 [pool-2584-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:36,961 [pool-2584-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD: start as a follower, conf=-1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:36,961 [pool-2584-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:36,961 [pool-2584-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: start 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-FollowerState
2023-04-27 06:30:36,961 [pool-2584-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A8BB40E16CAD,id=735446df-424c-4d38-a683-bd4ef5c8b9e6
2023-04-27 06:30:36,961 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:36,961 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:36,961 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:36,961 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:36,961 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:36,961 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:36,971 [grpc-default-executor-5] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: addNew group-A8BB40E16CAD:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER] returns group-A8BB40E16CAD:java.util.concurrent.CompletableFuture@5ad53920[Not completed]
2023-04-27 06:30:36,972 [pool-2606-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: new RaftServerImpl for group-A8BB40E16CAD:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:36,972 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:36,972 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:36,972 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:36,973 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:36,973 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:36,973 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:36,973 [pool-2606-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD: ConfigurationManager, init=-1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:36,973 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis] (custom)
2023-04-27 06:30:36,973 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:36,973 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:36,973 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:36,973 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:36,973 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:36,975 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:36,975 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:36,975 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:36,975 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:36,975 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:36,975 [pool-2606-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad does not exist. Creating ...
2023-04-27 06:30:36,976 [pool-2606-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:36,978 [pool-2606-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad has been successfully formatted.
2023-04-27 06:30:36,978 [pool-2606-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-A8BB40E16CAD: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:36,978 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:36,978 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:36,979 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:36,979 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:36,979 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:36,979 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:36,980 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:36,980 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:36,980 [pool-2606-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad
2023-04-27 06:30:36,980 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:36,980 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:36,980 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:36,980 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:36,980 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:36,980 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:36,980 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:36,980 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:36,982 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:36,982 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:36,990 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:36,990 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:36,990 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:36,990 [pool-2606-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:36,990 [pool-2606-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:36,991 [pool-2606-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD: start as a follower, conf=-1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:36,991 [pool-2606-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:36,991 [pool-2606-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: start ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-FollowerState
2023-04-27 06:30:36,991 [pool-2606-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A8BB40E16CAD,id=ec732e1e-81da-4c3b-ad71-f2fe790a57c7
2023-04-27 06:30:36,991 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:36,991 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:36,991 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:36,992 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:36,992 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:36,992 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:36,995 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS THREE PipelineID=8fc0afc7-f51d-447c-be01-a8bb40e16cad.
2023-04-27 06:30:37,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:37,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:37,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:37,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:37,258 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Nodes are ready. Got 7 of 7 DN Heartbeats.
2023-04-27 06:30:37,258 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:37,258 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:37,386 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: addNew group-13438DC0D738:[735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:1|startupRole:FOLLOWER] returns group-13438DC0D738:java.util.concurrent.CompletableFuture@bac8623[Not completed]
2023-04-27 06:30:37,387 [pool-2584-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: new RaftServerImpl for group-13438DC0D738:[735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:37,387 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:37,387 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:37,388 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:37,388 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:37,388 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:37,388 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:37,388 [pool-2584-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738: ConfigurationManager, init=-1: peers:[735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:37,388 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis] (custom)
2023-04-27 06:30:37,388 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:37,388 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:37,388 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:37,388 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:37,388 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:37,390 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:37,390 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:37,390 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:37,390 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:37,390 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:37,391 [pool-2584-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis/a8387d3e-c7a3-4c54-ae38-13438dc0d738 does not exist. Creating ...
2023-04-27 06:30:37,393 [pool-2584-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis/a8387d3e-c7a3-4c54-ae38-13438dc0d738/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:37,395 [pool-2584-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis/a8387d3e-c7a3-4c54-ae38-13438dc0d738 has been successfully formatted.
2023-04-27 06:30:37,396 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: a8387d3e-c7a3-4c54-ae38-13438dc0d738, Nodes: 735446df-424c-4d38-a683-bd4ef5c8b9e6(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:735446df-424c-4d38-a683-bd4ef5c8b9e6, CreationTimestamp2023-04-27T06:30:34.388Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:37,396 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:37,396 [pool-2584-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-13438DC0D738: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:37,396 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:37,397 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:37,397 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:37,397 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:37,397 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:37,397 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:37,398 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:37,398 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:37,398 [pool-2584-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis/a8387d3e-c7a3-4c54-ae38-13438dc0d738
2023-04-27 06:30:37,398 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:37,398 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:37,398 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:37,398 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:37,398 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:37,398 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:37,398 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:37,398 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:37,400 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:37,401 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:37,412 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:37,412 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:37,412 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:37,413 [pool-2584-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:37,413 [pool-2584-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:37,415 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:37,423 [pool-2584-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738: start as a follower, conf=-1: peers:[735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:37,424 [pool-2584-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:37,424 [pool-2584-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: start 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-FollowerState
2023-04-27 06:30:37,424 [pool-2584-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-13438DC0D738,id=735446df-424c-4d38-a683-bd4ef5c8b9e6
2023-04-27 06:30:37,424 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:37,424 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:37,424 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:37,424 [pool-2584-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:37,425 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:37,425 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:37,427 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=a8387d3e-c7a3-4c54-ae38-13438dc0d738
2023-04-27 06:30:37,427 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=a8387d3e-c7a3-4c54-ae38-13438dc0d738.
2023-04-27 06:30:37,754 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:37,757 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:37,757 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:37,759 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:37,759 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:37,761 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:37,763 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:37,763 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:37,839 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: addNew group-F81BA65D63C6:[ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:1|startupRole:FOLLOWER] returns group-F81BA65D63C6:java.util.concurrent.CompletableFuture@48946365[Not completed]
2023-04-27 06:30:37,840 [pool-2606-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: new RaftServerImpl for group-F81BA65D63C6:[ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:37,840 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:37,840 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:37,840 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:37,840 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:37,840 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:37,840 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:37,841 [pool-2606-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6: ConfigurationManager, init=-1: peers:[ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:37,841 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis] (custom)
2023-04-27 06:30:37,841 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:37,841 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:37,841 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:37,841 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:37,841 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:37,843 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:37,843 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:37,843 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:37,843 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:37,843 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:37,843 [pool-2606-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis/5839be52-9155-436a-b7fa-f81ba65d63c6 does not exist. Creating ...
2023-04-27 06:30:37,845 [pool-2606-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis/5839be52-9155-436a-b7fa-f81ba65d63c6/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:37,846 [pool-2606-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis/5839be52-9155-436a-b7fa-f81ba65d63c6 has been successfully formatted.
2023-04-27 06:30:37,847 [pool-2606-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-F81BA65D63C6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:37,847 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:37,848 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:37,848 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:37,848 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:37,848 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:37,848 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:37,849 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 5839be52-9155-436a-b7fa-f81ba65d63c6, Nodes: ec732e1e-81da-4c3b-ad71-f2fe790a57c7(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:ec732e1e-81da-4c3b-ad71-f2fe790a57c7, CreationTimestamp2023-04-27T06:30:34.851Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:37,849 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:37,849 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:37,849 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:37,849 [pool-2606-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis/5839be52-9155-436a-b7fa-f81ba65d63c6
2023-04-27 06:30:37,849 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:37,849 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:37,849 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:37,849 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:37,849 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:37,849 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:37,849 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:37,849 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:37,853 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:37,854 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:37,867 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:37,867 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:37,867 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:37,868 [pool-2606-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:37,868 [pool-2606-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:37,868 [pool-2606-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6: start as a follower, conf=-1: peers:[ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:37,868 [pool-2606-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:37,868 [pool-2606-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: start ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-FollowerState
2023-04-27 06:30:37,868 [pool-2606-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F81BA65D63C6,id=ec732e1e-81da-4c3b-ad71-f2fe790a57c7
2023-04-27 06:30:37,868 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:37,869 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:37,869 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:37,869 [pool-2606-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:37,869 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:37,869 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:37,869 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=5839be52-9155-436a-b7fa-f81ba65d63c6
2023-04-27 06:30:37,869 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=5839be52-9155-436a-b7fa-f81ba65d63c6.
2023-04-27 06:30:37,919 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:38,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:38,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:38,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:38,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:38,258 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - fc27ded4-a200-4b47-a78c-bc930dadcc21: addNew group-1FF79DD9E52C:[fc27ded4-a200-4b47-a78c-bc930dadcc21|rpc:10.1.0.33:35905|dataStream:10.1.0.33:35947|priority:1|startupRole:FOLLOWER] returns group-1FF79DD9E52C:java.util.concurrent.CompletableFuture@b27b33a[Not completed]
2023-04-27 06:30:38,258 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Nodes are ready. Got 7 of 7 DN Heartbeats.
2023-04-27 06:30:38,258 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:38,258 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:38,259 [pool-2636-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(195)) - fc27ded4-a200-4b47-a78c-bc930dadcc21: new RaftServerImpl for group-1FF79DD9E52C:[fc27ded4-a200-4b47-a78c-bc930dadcc21|rpc:10.1.0.33:35905|dataStream:10.1.0.33:35947|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
2023-04-27 06:30:38,259 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2023-04-27 06:30:38,259 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2023-04-27 06:30:38,259 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2023-04-27 06:30:38,259 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2023-04-27 06:30:38,259 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2023-04-27 06:30:38,259 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2023-04-27 06:30:38,259 [pool-2636-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(118)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C: ConfigurationManager, init=-1: peers:[fc27ded4-a200-4b47-a78c-bc930dadcc21|rpc:10.1.0.33:35905|dataStream:10.1.0.33:35947|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
2023-04-27 06:30:38,259 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data/ratis] (custom)
2023-04-27 06:30:38,260 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2023-04-27 06:30:38,260 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2023-04-27 06:30:38,260 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2023-04-27 06:30:38,260 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2023-04-27 06:30:38,260 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2023-04-27 06:30:38,262 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:38,262 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2023-04-27 06:30:38,262 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2023-04-27 06:30:38,263 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2023-04-27 06:30:38,263 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2023-04-27 06:30:38,263 [pool-2636-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(137)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data/ratis/22ba932c-7854-4cc0-beb2-1ff79dd9e52c does not exist. Creating ...
2023-04-27 06:30:38,264 [pool-2636-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(231)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data/ratis/22ba932c-7854-4cc0-beb2-1ff79dd9e52c/in_use.lock acquired by nodename 72321@fv-az260-775
2023-04-27 06:30:38,266 [pool-2636-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(96)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data/ratis/22ba932c-7854-4cc0-beb2-1ff79dd9e52c has been successfully formatted.
2023-04-27 06:30:38,266 [pool-2636-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(262)) - group-1FF79DD9E52C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2023-04-27 06:30:38,266 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2023-04-27 06:30:38,267 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2023-04-27 06:30:38,267 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:38,267 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2023-04-27 06:30:38,267 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.preservation.log.num = 0 (default)
2023-04-27 06:30:38,267 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 22ba932c-7854-4cc0-beb2-1ff79dd9e52c, Nodes: fc27ded4-a200-4b47-a78c-bc930dadcc21(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:fc27ded4-a200-4b47-a78c-bc930dadcc21, CreationTimestamp2023-04-27T06:30:35.265Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:38,267 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:38,268 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:38,268 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2023-04-27 06:30:38,268 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2023-04-27 06:30:38,268 [pool-2636-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(189)) - new fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data/ratis/22ba932c-7854-4cc0-beb2-1ff79dd9e52c
2023-04-27 06:30:38,268 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2023-04-27 06:30:38,268 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2023-04-27 06:30:38,269 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2023-04-27 06:30:38,269 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2023-04-27 06:30:38,269 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2023-04-27 06:30:38,269 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2023-04-27 06:30:38,269 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2023-04-27 06:30:38,269 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2023-04-27 06:30:38,272 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2023-04-27 06:30:38,272 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:38,278 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2023-04-27 06:30:38,278 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.async-flush.enabled = false (default)
2023-04-27 06:30:38,278 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2023-04-27 06:30:38,279 [pool-2636-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:38,279 [pool-2636-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2023-04-27 06:30:38,279 [pool-2636-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(334)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C: start as a follower, conf=-1: peers:[fc27ded4-a200-4b47-a78c-bc930dadcc21|rpc:10.1.0.33:35905|dataStream:10.1.0.33:35947|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:38,279 [pool-2636-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C: changes role from      null to FOLLOWER at term 0 for startAsFollower
2023-04-27 06:30:38,279 [pool-2636-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - fc27ded4-a200-4b47-a78c-bc930dadcc21: start fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-FollowerState
2023-04-27 06:30:38,279 [pool-2636-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1FF79DD9E52C,id=fc27ded4-a200-4b47-a78c-bc930dadcc21
2023-04-27 06:30:38,279 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2023-04-27 06:30:38,279 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2023-04-27 06:30:38,279 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2023-04-27 06:30:38,280 [pool-2636-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2023-04-27 06:30:38,280 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:38,280 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:38,280 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(806)) - Created group PipelineID=22ba932c-7854-4cc0-beb2-1ff79dd9e52c
2023-04-27 06:30:38,280 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=22ba932c-7854-4cc0-beb2-1ff79dd9e52c.
2023-04-27 06:30:38,396 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:38,414 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:38,527 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:38,754 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:38,758 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:38,758 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:38,763 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:38,848 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:38,900 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:38,920 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:39,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:39,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:39,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:39,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:39,258 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Nodes are ready. Got 7 of 7 DN Heartbeats.
2023-04-27 06:30:39,259 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:39,259 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:39,396 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:39,754 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:39,758 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:39,758 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:39,763 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:39,764 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:39,765 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:39,768 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:39,769 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:39,849 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:40,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:40,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:40,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:40,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:40,259 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Nodes are ready. Got 7 of 7 DN Heartbeats.
2023-04-27 06:30:40,259 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Waiting for cluster to exit safe mode
2023-04-27 06:30:40,259 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:40,269 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:40,397 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:40,415 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:40,528 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:40,569 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5078932181ns, electionTimeout:5078ms
2023-04-27 06:30:40,569 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: shutdown b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState
2023-04-27 06:30:40,569 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:40,570 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:40,570 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: start b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87
2023-04-27 06:30:40,570 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,571 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87-1] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for 65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:40,571 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:40,571 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:40,571 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5102487749ns, electionTimeout:5101ms
2023-04-27 06:30:40,571 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: shutdown b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-FollowerState
2023-04-27 06:30:40,571 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:40,571 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:40,571 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: start b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88
2023-04-27 06:30:40,572 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87-2] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for 10617bfc-881b-4e3d-8722-e7f81aaf7e30
2023-04-27 06:30:40,573 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,573 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:40,575 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88 ELECTION round 0: submit vote requests at term 1 for -1: peers:[b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,575 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:40,575 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: shutdown b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88
2023-04-27 06:30:40,575 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:40,575 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-ED7ECFF91685 with new leaderId: b85c0d2e-fc3b-4312-9b63-e609b6e2c228
2023-04-27 06:30:40,575 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685: change Leader from null to b85c0d2e-fc3b-4312-9b63-e609b6e2c228 at term 1 for becomeLeader, leader elected after 5131ms
2023-04-27 06:30:40,576 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:40,576 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:40,576 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:40,576 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:40,579 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:40,579 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:40,579 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:40,579 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:40,579 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:40,579 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: start b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderStateImpl
2023-04-27 06:30:40,580 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:40,588 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-LeaderElection88] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685: set configuration 0: peers:[b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,588 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-ED7ECFF91685-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis/530b76c3-23b6-4f35-bf64-ed7ecff91685/current/log_inprogress_0
2023-04-27 06:30:40,591 [grpc-default-executor-8] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A: receive requestVote(PRE_VOTE, b85c0d2e-fc3b-4312-9b63-e609b6e2c228, group-4BCCCFFA137A, 0, (t:0, i:0))
2023-04-27 06:30:40,591 [grpc-default-executor-8] INFO  impl.VoteContext (VoteContext.java:log(49)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FOLLOWER: accept PRE_VOTE from b85c0d2e-fc3b-4312-9b63-e609b6e2c228: our priority 0 <= candidate's priority 0
2023-04-27 06:30:40,591 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A: receive requestVote(PRE_VOTE, b85c0d2e-fc3b-4312-9b63-e609b6e2c228, group-4BCCCFFA137A, 0, (t:0, i:0))
2023-04-27 06:30:40,592 [grpc-default-executor-5] INFO  impl.VoteContext (VoteContext.java:log(49)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-FOLLOWER: reject PRE_VOTE from b85c0d2e-fc3b-4312-9b63-e609b6e2c228: our priority 1 > candidate's priority 0
2023-04-27 06:30:40,592 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A replies to PRE_VOTE vote request: b85c0d2e-fc3b-4312-9b63-e609b6e2c228<-65d5be4b-faf3-45c4-8c44-989db5872d1e#0:FAIL-t0. Peer's state: 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A:t0, leader=null, voted=, raftlog=Memoized:65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,591 [grpc-default-executor-8] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A replies to PRE_VOTE vote request: b85c0d2e-fc3b-4312-9b63-e609b6e2c228<-10617bfc-881b-4e3d-8722-e7f81aaf7e30#0:OK-t0. Peer's state: 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A:t0, leader=null, voted=, raftlog=Memoized:10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,593 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
2023-04-27 06:30:40,593 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: b85c0d2e-fc3b-4312-9b63-e609b6e2c228<-65d5be4b-faf3-45c4-8c44-989db5872d1e#0:FAIL-t0
2023-04-27 06:30:40,593 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87 PRE_VOTE round 0: result REJECTED
2023-04-27 06:30:40,593 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
2023-04-27 06:30:40,593 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: shutdown b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87
2023-04-27 06:30:40,594 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-LeaderElection87] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: start b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState
2023-04-27 06:30:40,594 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:40,594 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:40,598 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:40,598 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:40,718 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5159801236ns, electionTimeout:5159ms
2023-04-27 06:30:40,718 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: shutdown 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-FollowerState
2023-04-27 06:30:40,718 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:40,718 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:40,718 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: start 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89
2023-04-27 06:30:40,722 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,724 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89-1] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for 10617bfc-881b-4e3d-8722-e7f81aaf7e30
2023-04-27 06:30:40,739 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:40,739 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:40,740 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89-2] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for b85c0d2e-fc3b-4312-9b63-e609b6e2c228
2023-04-27 06:30:40,741 [grpc-default-executor-8] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A: receive requestVote(PRE_VOTE, 65d5be4b-faf3-45c4-8c44-989db5872d1e, group-4BCCCFFA137A, 0, (t:0, i:0))
2023-04-27 06:30:40,741 [grpc-default-executor-8] INFO  impl.VoteContext (VoteContext.java:log(49)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FOLLOWER: accept PRE_VOTE from 65d5be4b-faf3-45c4-8c44-989db5872d1e: our priority 0 <= candidate's priority 1
2023-04-27 06:30:40,741 [grpc-default-executor-8] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A replies to PRE_VOTE vote request: 65d5be4b-faf3-45c4-8c44-989db5872d1e<-10617bfc-881b-4e3d-8722-e7f81aaf7e30#0:OK-t0. Peer's state: 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A:t0, leader=null, voted=, raftlog=Memoized:10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,741 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
2023-04-27 06:30:40,742 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: 65d5be4b-faf3-45c4-8c44-989db5872d1e<-10617bfc-881b-4e3d-8722-e7f81aaf7e30#0:OK-t0
2023-04-27 06:30:40,742 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89 PRE_VOTE round 0: result PASSED
2023-04-27 06:30:40,745 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89 ELECTION round 0: submit vote requests at term 1 for -1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,748 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A: receive requestVote(ELECTION, 65d5be4b-faf3-45c4-8c44-989db5872d1e, group-4BCCCFFA137A, 1, (t:0, i:0))
2023-04-27 06:30:40,748 [grpc-default-executor-5] INFO  impl.VoteContext (VoteContext.java:log(49)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FOLLOWER: accept ELECTION from 65d5be4b-faf3-45c4-8c44-989db5872d1e: our priority 0 <= candidate's priority 1
2023-04-27 06:30:40,748 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:40,748 [grpc-default-executor-5] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: shutdown 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FollowerState
2023-04-27 06:30:40,755 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:40,757 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:40,758 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:40,758 [grpc-default-executor-5] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: start 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FollowerState
2023-04-27 06:30:40,759 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FollowerState was interrupted
2023-04-27 06:30:40,759 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:40,759 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:40,764 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:40,765 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:40,765 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:40,765 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A replies to ELECTION vote request: 65d5be4b-faf3-45c4-8c44-989db5872d1e<-10617bfc-881b-4e3d-8722-e7f81aaf7e30#0:OK-t1. Peer's state: 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A:t1, leader=null, voted=65d5be4b-faf3-45c4-8c44-989db5872d1e, raftlog=Memoized:10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,768 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A: receive requestVote(PRE_VOTE, 65d5be4b-faf3-45c4-8c44-989db5872d1e, group-4BCCCFFA137A, 0, (t:0, i:0))
2023-04-27 06:30:40,768 [grpc-default-executor-5] INFO  impl.VoteContext (VoteContext.java:log(49)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FOLLOWER: accept PRE_VOTE from 65d5be4b-faf3-45c4-8c44-989db5872d1e: our priority 0 <= candidate's priority 1
2023-04-27 06:30:40,768 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89: ELECTION PASSED received 1 response(s) and 0 exception(s):
2023-04-27 06:30:40,769 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A replies to PRE_VOTE vote request: 65d5be4b-faf3-45c4-8c44-989db5872d1e<-b85c0d2e-fc3b-4312-9b63-e609b6e2c228#0:OK-t0. Peer's state: b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A:t0, leader=null, voted=, raftlog=Memoized:b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,769 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: 65d5be4b-faf3-45c4-8c44-989db5872d1e<-10617bfc-881b-4e3d-8722-e7f81aaf7e30#0:OK-t1
2023-04-27 06:30:40,769 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89 ELECTION round 0: result PASSED
2023-04-27 06:30:40,769 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: shutdown 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89
2023-04-27 06:30:40,769 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:40,769 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-4BCCCFFA137A with new leaderId: 65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:40,769 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A: change Leader from null to 65d5be4b-faf3-45c4-8c44-989db5872d1e at term 1 for becomeLeader, leader elected after 5230ms
2023-04-27 06:30:40,769 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:40,769 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:40,769 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:40,769 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A: receive requestVote(ELECTION, 65d5be4b-faf3-45c4-8c44-989db5872d1e, group-4BCCCFFA137A, 1, (t:0, i:0))
2023-04-27 06:30:40,770 [grpc-default-executor-5] INFO  impl.VoteContext (VoteContext.java:log(49)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FOLLOWER: accept ELECTION from 65d5be4b-faf3-45c4-8c44-989db5872d1e: our priority 0 <= candidate's priority 1
2023-04-27 06:30:40,770 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:40,770 [grpc-default-executor-5] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: shutdown b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState
2023-04-27 06:30:40,770 [grpc-default-executor-5] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228: start b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState
2023-04-27 06:30:40,770 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:40,770 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:40,770 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:40,770 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState was interrupted
2023-04-27 06:30:40,770 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:40,770 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:40,771 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2023-04-27 06:30:40,771 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:40,771 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2023-04-27 06:30:40,772 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 828ebe36-54ef-47fd-9d4d-4bcccffa137a, Nodes: 10617bfc-881b-4e3d-8722-e7f81aaf7e30(fv-az260-775/10.1.0.33)65d5be4b-faf3-45c4-8c44-989db5872d1e(fv-az260-775/10.1.0.33)b85c0d2e-fc3b-4312-9b63-e609b6e2c228(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:65d5be4b-faf3-45c4-8c44-989db5872d1e, CreationTimestamp2023-04-27T06:30:33.400Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:40,772 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
2023-04-27 06:30:40,773 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (HealthyPipelineSafeModeRule.java:process(137)) - SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
2023-04-27 06:30:40,773 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(200)) - HealthyPipelineSafeModeRule rule is successfully validated
2023-04-27 06:30:40,773 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(215)) - ScmSafeModeManager, all rules are successfully validated
2023-04-27 06:30:40,773 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:exitSafeMode(244)) - SCM exiting safe mode.
2023-04-27 06:30:40,773 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  ha.SCMContext (SCMContext.java:updateSafeModeStatus(228)) - Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
2023-04-27 06:30:40,773 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:notifyStatusChanged(254)) - Service BackgroundPipelineCreator transitions to RUNNING.
2023-04-27 06:30:40,773 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  BackgroundPipelineScrubber (BackgroundSCMService.java:notifyStatusChanged(82)) - Service BackgroundPipelineScrubber transitions to RUNNING.
2023-04-27 06:30:40,773 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  ExpiredContainerReplicaOpScrubber (BackgroundSCMService.java:notifyStatusChanged(82)) - Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
2023-04-27 06:30:40,773 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO  replication.ReplicationManager (ReplicationManager.java:notifyStatusChanged(1369)) - Service ReplicationManager transitions to RUNNING.
2023-04-27 06:30:40,773 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN  balancer.ContainerBalancer (ContainerBalancer.java:shouldRun(132)) - Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
2023-04-27 06:30:40,775 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2023-04-27 06:30:40,775 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:40,776 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:40,776 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:40,776 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.wait-time.min = 10ms (default)
2023-04-27 06:30:40,777 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A replies to ELECTION vote request: 65d5be4b-faf3-45c4-8c44-989db5872d1e<-b85c0d2e-fc3b-4312-9b63-e609b6e2c228#0:OK-t1. Peer's state: b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A:t1, leader=null, voted=65d5be4b-faf3-45c4-8c44-989db5872d1e, raftlog=Memoized:b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,777 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2023-04-27 06:30:40,777 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:40,777 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2023-04-27 06:30:40,777 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2023-04-27 06:30:40,777 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:40,777 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:40,777 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:40,777 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.wait-time.min = 10ms (default)
2023-04-27 06:30:40,777 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: start 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderStateImpl
2023-04-27 06:30:40,778 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:40,779 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:40,779 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:40,780 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a/current/log_inprogress_0
2023-04-27 06:30:40,798 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A-LeaderElection89] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-4BCCCFFA137A: set configuration 0: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,801 [10617bfc-881b-4e3d-8722-e7f81aaf7e30-server-thread1] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-4BCCCFFA137A with new leaderId: 65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:40,801 [10617bfc-881b-4e3d-8722-e7f81aaf7e30-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A: change Leader from null to 65d5be4b-faf3-45c4-8c44-989db5872d1e at term 1 for appendEntries, leader elected after 5300ms
2023-04-27 06:30:40,809 [10617bfc-881b-4e3d-8722-e7f81aaf7e30-server-thread2] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A: set configuration 0: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,810 [10617bfc-881b-4e3d-8722-e7f81aaf7e30-server-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:40,812 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228-server-thread1] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-4BCCCFFA137A with new leaderId: 65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:40,812 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A: change Leader from null to 65d5be4b-faf3-45c4-8c44-989db5872d1e at term 1 for appendEntries, leader elected after 5339ms
2023-04-27 06:30:40,814 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228-server-thread2] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A: set configuration 0: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER, 10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:0|startupRole:FOLLOWER, b85c0d2e-fc3b-4312-9b63-e609b6e2c228|rpc:10.1.0.33:41235|dataStream:10.1.0.33:40691|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:40,814 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228-server-thread2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:40,815 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-4BCCCFFA137A-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a/current/log_inprogress_0
2023-04-27 06:30:40,817 [b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - b85c0d2e-fc3b-4312-9b63-e609b6e2c228@group-4BCCCFFA137A-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-0/data/ratis/828ebe36-54ef-47fd-9d4d-4bcccffa137a/current/log_inprogress_0
2023-04-27 06:30:41,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:41,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:41,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:41,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:41,093 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5181766018ns, electionTimeout:5181ms
2023-04-27 06:30:41,094 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: shutdown 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-FollowerState
2023-04-27 06:30:41,094 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:41,094 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:41,094 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: start 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90
2023-04-27 06:30:41,095 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:41,095 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:41,096 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90 ELECTION round 0: submit vote requests at term 1 for -1: peers:[10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:41,097 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:41,097 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: shutdown 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90
2023-04-27 06:30:41,097 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:41,097 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-85090F82D0E9 with new leaderId: 10617bfc-881b-4e3d-8722-e7f81aaf7e30
2023-04-27 06:30:41,097 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9: change Leader from null to 10617bfc-881b-4e3d-8722-e7f81aaf7e30 at term 1 for becomeLeader, leader elected after 5202ms
2023-04-27 06:30:41,097 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:41,097 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:41,097 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:41,098 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:41,098 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:41,098 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:41,098 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:41,098 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:41,098 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30: start 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderStateImpl
2023-04-27 06:30:41,099 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:41,099 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-LeaderElection90] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9: set configuration 0: peers:[10617bfc-881b-4e3d-8722-e7f81aaf7e30|rpc:10.1.0.33:37309|dataStream:10.1.0.33:37703|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:41,100 [10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 10617bfc-881b-4e3d-8722-e7f81aaf7e30@group-85090F82D0E9-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-1/data/ratis/e9f594c3-20a8-4fb8-8977-85090f82d0e9/current/log_inprogress_0
2023-04-27 06:30:41,259 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(223)) - Nodes are ready. Got 7 of 7 DN Heartbeats.
2023-04-27 06:30:41,259 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(226)) - Cluster exits safe mode
2023-04-27 06:30:41,259 [Listener at 127.0.0.1/32931] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(228)) - SCM became leader
2023-04-27 06:30:41,593 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5163031872ns, electionTimeout:5159ms
2023-04-27 06:30:41,594 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: shutdown 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-FollowerState
2023-04-27 06:30:41,594 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:41,594 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:41,594 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: start 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91
2023-04-27 06:30:41,594 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:41,595 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:41,597 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91 ELECTION round 0: submit vote requests at term 1 for -1: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:41,597 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:41,597 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: shutdown 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91
2023-04-27 06:30:41,597 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:41,597 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-FFA4651C59E3 with new leaderId: 65d5be4b-faf3-45c4-8c44-989db5872d1e
2023-04-27 06:30:41,597 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3: change Leader from null to 65d5be4b-faf3-45c4-8c44-989db5872d1e at term 1 for becomeLeader, leader elected after 5195ms
2023-04-27 06:30:41,598 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:41,598 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:41,598 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:41,599 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:41,599 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:41,599 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:41,599 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:41,599 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:41,599 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e: start 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderStateImpl
2023-04-27 06:30:41,600 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:41,600 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-LeaderElection91] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3: set configuration 0: peers:[65d5be4b-faf3-45c4-8c44-989db5872d1e|rpc:10.1.0.33:43177|dataStream:10.1.0.33:46609|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:41,604 [65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 65d5be4b-faf3-45c4-8c44-989db5872d1e@group-FFA4651C59E3-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-2/data/ratis/4ba5e2a4-1c2d-4d6b-9024-ffa4651c59e3/current/log_inprogress_0
2023-04-27 06:30:41,755 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:41,759 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:41,760 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2023-04-27 06:30:41,764 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:41,984 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5053465521ns, electionTimeout:5051ms
2023-04-27 06:30:41,984 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - ff409154-2e62-420c-a7a7-066f3a70d145: shutdown ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-FollowerState
2023-04-27 06:30:41,984 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:41,984 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:41,984 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ff409154-2e62-420c-a7a7-066f3a70d145: start ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92
2023-04-27 06:30:41,985 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:41,985 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92-1] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for 735446df-424c-4d38-a683-bd4ef5c8b9e6
2023-04-27 06:30:41,987 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:41,987 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:41,988 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92-2] INFO  server.GrpcServerProtocolClient (GrpcServerProtocolClient.java:<init>(63)) - Build channel for ec732e1e-81da-4c3b-ad71-f2fe790a57c7
2023-04-27 06:30:41,996 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD: receive requestVote(PRE_VOTE, ff409154-2e62-420c-a7a7-066f3a70d145, group-A8BB40E16CAD, 0, (t:0, i:0))
2023-04-27 06:30:41,996 [grpc-default-executor-5] INFO  impl.VoteContext (VoteContext.java:log(49)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-FOLLOWER: accept PRE_VOTE from ff409154-2e62-420c-a7a7-066f3a70d145: our priority 0 <= candidate's priority 1
2023-04-27 06:30:41,996 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD replies to PRE_VOTE vote request: ff409154-2e62-420c-a7a7-066f3a70d145<-735446df-424c-4d38-a683-bd4ef5c8b9e6#0:OK-t0. Peer's state: 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD:t0, leader=null, voted=, raftlog=Memoized:735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:41,998 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
2023-04-27 06:30:41,998 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: ff409154-2e62-420c-a7a7-066f3a70d145<-735446df-424c-4d38-a683-bd4ef5c8b9e6#0:OK-t0
2023-04-27 06:30:41,998 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92 PRE_VOTE round 0: result PASSED
2023-04-27 06:30:42,000 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92 ELECTION round 0: submit vote requests at term 1 for -1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,004 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD: receive requestVote(PRE_VOTE, ff409154-2e62-420c-a7a7-066f3a70d145, group-A8BB40E16CAD, 0, (t:0, i:0))
2023-04-27 06:30:42,004 [grpc-default-executor-5] INFO  impl.VoteContext (VoteContext.java:log(49)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-FOLLOWER: accept PRE_VOTE from ff409154-2e62-420c-a7a7-066f3a70d145: our priority 0 <= candidate's priority 1
2023-04-27 06:30:42,004 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD replies to PRE_VOTE vote request: ff409154-2e62-420c-a7a7-066f3a70d145<-ec732e1e-81da-4c3b-ad71-f2fe790a57c7#0:OK-t0. Peer's state: ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD:t0, leader=null, voted=, raftlog=Memoized:ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,008 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD: receive requestVote(ELECTION, ff409154-2e62-420c-a7a7-066f3a70d145, group-A8BB40E16CAD, 1, (t:0, i:0))
2023-04-27 06:30:42,008 [grpc-default-executor-5] INFO  impl.VoteContext (VoteContext.java:log(49)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-FOLLOWER: accept ELECTION from ff409154-2e62-420c-a7a7-066f3a70d145: our priority 0 <= candidate's priority 1
2023-04-27 06:30:42,008 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:ff409154-2e62-420c-a7a7-066f3a70d145
2023-04-27 06:30:42,009 [grpc-default-executor-5] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: shutdown 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-FollowerState
2023-04-27 06:30:42,009 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:42,009 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:42,010 [grpc-default-executor-8] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1218)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD: receive requestVote(ELECTION, ff409154-2e62-420c-a7a7-066f3a70d145, group-A8BB40E16CAD, 1, (t:0, i:0))
2023-04-27 06:30:42,010 [grpc-default-executor-5] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: start 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-FollowerState
2023-04-27 06:30:42,010 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-FollowerState was interrupted
2023-04-27 06:30:42,010 [grpc-default-executor-8] INFO  impl.VoteContext (VoteContext.java:log(49)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-FOLLOWER: accept ELECTION from ff409154-2e62-420c-a7a7-066f3a70d145: our priority 0 <= candidate's priority 1
2023-04-27 06:30:42,010 [grpc-default-executor-8] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:ff409154-2e62-420c-a7a7-066f3a70d145
2023-04-27 06:30:42,010 [grpc-default-executor-8] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: shutdown ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-FollowerState
2023-04-27 06:30:42,010 [grpc-default-executor-8] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: start ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-FollowerState
2023-04-27 06:30:42,010 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-FollowerState was interrupted
2023-04-27 06:30:42,011 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:42,011 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:42,012 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2023-04-27 06:30:42,012 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logFallback(53)) - raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2023-04-27 06:30:42,012 [grpc-default-executor-5] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD replies to ELECTION vote request: ff409154-2e62-420c-a7a7-066f3a70d145<-735446df-424c-4d38-a683-bd4ef5c8b9e6#0:OK-t1. Peer's state: 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD:t1, leader=null, voted=ff409154-2e62-420c-a7a7-066f3a70d145, raftlog=Memoized:735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,012 [grpc-default-executor-8] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1251)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD replies to ELECTION vote request: ff409154-2e62-420c-a7a7-066f3a70d145<-ec732e1e-81da-4c3b-ad71-f2fe790a57c7#0:OK-t1. Peer's state: ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD:t1, leader=null, voted=ff409154-2e62-420c-a7a7-066f3a70d145, raftlog=Memoized:ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,013 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92: ELECTION PASSED received 1 response(s) and 0 exception(s):
2023-04-27 06:30:42,013 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: ff409154-2e62-420c-a7a7-066f3a70d145<-ec732e1e-81da-4c3b-ad71-f2fe790a57c7#0:OK-t1
2023-04-27 06:30:42,013 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92 ELECTION round 0: result PASSED
2023-04-27 06:30:42,013 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - ff409154-2e62-420c-a7a7-066f3a70d145: shutdown ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92
2023-04-27 06:30:42,013 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:42,013 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-A8BB40E16CAD with new leaderId: ff409154-2e62-420c-a7a7-066f3a70d145
2023-04-27 06:30:42,013 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD: change Leader from null to ff409154-2e62-420c-a7a7-066f3a70d145 at term 1 for becomeLeader, leader elected after 5099ms
2023-04-27 06:30:42,013 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:42,013 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:42,013 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:42,014 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:42,014 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:42,014 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:42,014 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:42,014 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:42,015 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2023-04-27 06:30:42,015 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:42,015 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2023-04-27 06:30:42,015 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2023-04-27 06:30:42,015 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:42,016 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:42,016 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:42,016 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.wait-time.min = 10ms (default)
2023-04-27 06:30:42,017 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: 8fc0afc7-f51d-447c-be01-a8bb40e16cad, Nodes: 735446df-424c-4d38-a683-bd4ef5c8b9e6(fv-az260-775/10.1.0.33)ec732e1e-81da-4c3b-ad71-f2fe790a57c7(fv-az260-775/10.1.0.33)ff409154-2e62-420c-a7a7-066f3a70d145(fv-az260-775/10.1.0.33), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:ff409154-2e62-420c-a7a7-066f3a70d145, CreationTimestamp2023-04-27T06:30:34.852Z[Etc/UTC]] moved to OPEN state
2023-04-27 06:30:42,018 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2023-04-27 06:30:42,018 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2023-04-27 06:30:42,018 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2023-04-27 06:30:42,018 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2023-04-27 06:30:42,018 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.request.timeout = 60s (custom)
2023-04-27 06:30:42,018 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2023-04-27 06:30:42,018 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  grpc.GrpcConfigKeys (ConfUtils.java:logGet(46)) - raft.grpc.server.heartbeat.channel = true (default)
2023-04-27 06:30:42,018 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.wait-time.min = 10ms (default)
2023-04-27 06:30:42,018 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ff409154-2e62-420c-a7a7-066f3a70d145: start ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderStateImpl
2023-04-27 06:30:42,019 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:42,021 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad/current/log_inprogress_0
2023-04-27 06:30:42,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:42,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:42,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:42,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:42,033 [ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD-LeaderElection92] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-A8BB40E16CAD: set configuration 0: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,046 [735446df-424c-4d38-a683-bd4ef5c8b9e6-server-thread1] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-A8BB40E16CAD with new leaderId: ff409154-2e62-420c-a7a7-066f3a70d145
2023-04-27 06:30:42,046 [735446df-424c-4d38-a683-bd4ef5c8b9e6-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD: change Leader from null to ff409154-2e62-420c-a7a7-066f3a70d145 at term 1 for appendEntries, leader elected after 5102ms
2023-04-27 06:30:42,057 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5149684393ns, electionTimeout:5149ms
2023-04-27 06:30:42,057 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - ff409154-2e62-420c-a7a7-066f3a70d145: shutdown ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-FollowerState
2023-04-27 06:30:42,057 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:42,058 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7-server-thread1] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-A8BB40E16CAD with new leaderId: ff409154-2e62-420c-a7a7-066f3a70d145
2023-04-27 06:30:42,058 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7-server-thread1] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD: change Leader from null to ff409154-2e62-420c-a7a7-066f3a70d145 at term 1 for appendEntries, leader elected after 5084ms
2023-04-27 06:30:42,062 [735446df-424c-4d38-a683-bd4ef5c8b9e6-server-thread3] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD: set configuration 0: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,063 [735446df-424c-4d38-a683-bd4ef5c8b9e6-server-thread3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:42,064 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7-server-thread3] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD: set configuration 0: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER, 735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:0|startupRole:FOLLOWER, ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,065 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7-server-thread3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:42,068 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:42,068 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ff409154-2e62-420c-a7a7-066f3a70d145: start ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93
2023-04-27 06:30:42,070 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-A8BB40E16CAD-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad/current/log_inprogress_0
2023-04-27 06:30:42,072 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-A8BB40E16CAD-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis/8fc0afc7-f51d-447c-be01-a8bb40e16cad/current/log_inprogress_0
2023-04-27 06:30:42,072 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,072 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:42,074 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93 ELECTION round 0: submit vote requests at term 1 for -1: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,074 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:42,074 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - ff409154-2e62-420c-a7a7-066f3a70d145: shutdown ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93
2023-04-27 06:30:42,074 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:42,074 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-6A7D0F9095B0 with new leaderId: ff409154-2e62-420c-a7a7-066f3a70d145
2023-04-27 06:30:42,076 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0: change Leader from null to ff409154-2e62-420c-a7a7-066f3a70d145 at term 1 for becomeLeader, leader elected after 5183ms
2023-04-27 06:30:42,076 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:42,076 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:42,076 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:42,077 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:42,077 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:42,077 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:42,080 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:42,080 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:42,080 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ff409154-2e62-420c-a7a7-066f3a70d145: start ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderStateImpl
2023-04-27 06:30:42,083 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:42,084 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-LeaderElection93] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0: set configuration 0: peers:[ff409154-2e62-420c-a7a7-066f3a70d145|rpc:10.1.0.33:38661|dataStream:10.1.0.33:34185|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,088 [ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - ff409154-2e62-420c-a7a7-066f3a70d145@group-6A7D0F9095B0-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-3/data/ratis/a0bc8da9-5bdc-49fa-a174-6a7d0f9095b0/current/log_inprogress_0
2023-04-27 06:30:42,528 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5104635605ns, electionTimeout:5103ms
2023-04-27 06:30:42,528 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: shutdown 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-FollowerState
2023-04-27 06:30:42,529 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:42,529 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:42,529 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: start 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94
2023-04-27 06:30:42,529 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,529 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:42,531 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94 ELECTION round 0: submit vote requests at term 1 for -1: peers:[735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,531 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:42,531 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: shutdown 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94
2023-04-27 06:30:42,532 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:42,532 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-13438DC0D738 with new leaderId: 735446df-424c-4d38-a683-bd4ef5c8b9e6
2023-04-27 06:30:42,532 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738: change Leader from null to 735446df-424c-4d38-a683-bd4ef5c8b9e6 at term 1 for becomeLeader, leader elected after 5143ms
2023-04-27 06:30:42,532 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:42,532 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:42,532 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:42,533 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:42,533 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:42,533 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:42,533 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:42,533 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:42,534 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6: start 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderStateImpl
2023-04-27 06:30:42,534 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:42,534 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-LeaderElection94] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738: set configuration 0: peers:[735446df-424c-4d38-a683-bd4ef5c8b9e6|rpc:10.1.0.33:38151|dataStream:10.1.0.33:41265|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,535 [735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - 735446df-424c-4d38-a683-bd4ef5c8b9e6@group-13438DC0D738-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-4/data/ratis/a8387d3e-c7a3-4c54-ae38-13438dc0d738/current/log_inprogress_0
2023-04-27 06:30:42,756 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 1 milliseconds for processing 0 containers.
2023-04-27 06:30:42,760 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:42,760 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:42,764 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:42,940 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5071539268ns, electionTimeout:5070ms
2023-04-27 06:30:42,940 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: shutdown ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-FollowerState
2023-04-27 06:30:42,940 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:42,940 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:42,940 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: start ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95
2023-04-27 06:30:42,941 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,941 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:42,943 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95 ELECTION round 0: submit vote requests at term 1 for -1: peers:[ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,943 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:42,943 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: shutdown ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95
2023-04-27 06:30:42,943 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:42,943 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-F81BA65D63C6 with new leaderId: ec732e1e-81da-4c3b-ad71-f2fe790a57c7
2023-04-27 06:30:42,944 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6: change Leader from null to ec732e1e-81da-4c3b-ad71-f2fe790a57c7 at term 1 for becomeLeader, leader elected after 5101ms
2023-04-27 06:30:42,944 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:42,944 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:42,945 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:42,945 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:42,945 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:42,945 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:42,945 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:42,945 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:42,946 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7: start ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderStateImpl
2023-04-27 06:30:42,946 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:42,946 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-LeaderElection95] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6: set configuration 0: peers:[ec732e1e-81da-4c3b-ad71-f2fe790a57c7|rpc:10.1.0.33:41649|dataStream:10.1.0.33:36675|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:42,947 [ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - ec732e1e-81da-4c3b-ad71-f2fe790a57c7@group-F81BA65D63C6-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-5/data/ratis/5839be52-9155-436a-b7fa-f81ba65d63c6/current/log_inprogress_0
2023-04-27 06:30:43,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:43,032 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:43,032 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:43,032 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:43,456 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5177463904ns, electionTimeout:5176ms
2023-04-27 06:30:43,457 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - fc27ded4-a200-4b47-a78c-bc930dadcc21: shutdown fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-FollowerState
2023-04-27 06:30:43,457 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2023-04-27 06:30:43,457 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = true (default)
2023-04-27 06:30:43,457 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - fc27ded4-a200-4b47-a78c-bc930dadcc21: start fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96
2023-04-27 06:30:43,457 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[fc27ded4-a200-4b47-a78c-bc930dadcc21|rpc:10.1.0.33:35905|dataStream:10.1.0.33:35947|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:43,458 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96 PRE_VOTE round 0: result PASSED (term=0)
2023-04-27 06:30:43,460 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96 ELECTION round 0: submit vote requests at term 1 for -1: peers:[fc27ded4-a200-4b47-a78c-bc930dadcc21|rpc:10.1.0.33:35905|dataStream:10.1.0.33:35947|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:43,460 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(314)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96 ELECTION round 0: result PASSED (term=1)
2023-04-27 06:30:43,460 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - fc27ded4-a200-4b47-a78c-bc930dadcc21: shutdown fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96
2023-04-27 06:30:43,460 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(321)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2023-04-27 06:30:43,461 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(904)) - Leader change notification received for group: group-1FF79DD9E52C with new leaderId: fc27ded4-a200-4b47-a78c-bc930dadcc21
2023-04-27 06:30:43,461 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  server.RaftServer$Division (ServerState.java:setLeader(313)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C: change Leader from null to fc27ded4-a200-4b47-a78c-bc930dadcc21 at term 1 for becomeLeader, leader elected after 5200ms
2023-04-27 06:30:43,461 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2023-04-27 06:30:43,462 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:43,462 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2023-04-27 06:30:43,463 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2023-04-27 06:30:43,463 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2023-04-27 06:30:43,463 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2023-04-27 06:30:43,463 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2023-04-27 06:30:43,463 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2023-04-27 06:30:43,463 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - fc27ded4-a200-4b47-a78c-bc930dadcc21: start fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderStateImpl
2023-04-27 06:30:43,464 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(452)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-SegmentedRaftLogWorker: Starting segment from index:0
2023-04-27 06:30:43,465 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-LeaderElection96] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(430)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C: set configuration 0: peers:[fc27ded4-a200-4b47-a78c-bc930dadcc21|rpc:10.1.0.33:35905|dataStream:10.1.0.33:35947|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
2023-04-27 06:30:43,465 [fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(656)) - fc27ded4-a200-4b47-a78c-bc930dadcc21@group-1FF79DD9E52C-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-4a2a4eb3-88e4-41b6-8ed0-f47d88de9ae0/datanode-6/data/ratis/22ba932c-7854-4cc0-beb2-1ff79dd9e52c/current/log_inprogress_0
2023-04-27 06:30:43,756 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:43,761 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:43,762 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:43,762 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:43,762 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:43,764 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:43,765 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(363)) - Replication Manager is not ready to run until 3000ms after safemode exit
2023-04-27 06:30:43,765 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:43,767 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:43,767 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:44,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:44,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:44,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:44,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:44,757 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:44,763 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:44,763 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:44,765 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:45,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:45,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:45,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:45,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
2023-04-27 06:30:45,757 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:45,761 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:45,762 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:45,763 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1403)) - Sending close container command for container #1 to datanode dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33).
2023-04-27 06:30:45,763 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:184)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:229)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: class org.apache.hadoop.hdds.utils.db.RocksDatabase: Failed to open /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db; status : IOError; message : lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.HddsServerUtil.toIOException(HddsServerUtil.java:588)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.toIOException(RocksDatabase.java:85)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:162)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:124)
	... 15 more
Caused by: org.rocksdb.RocksDBException: lock hold by current process, acquire time 1682577014 acquiring thread 140013547587328: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:307)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRocksDB.open(ManagedRocksDB.java:66)
	at org.apache.hadoop.hdds.utils.db.RocksDatabase.open(RocksDatabase.java:148)
	... 16 more
2023-04-27 06:30:45,763 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2023-04-27 06:30:45,764 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler (CloseContainerCommandHandler.java:handle(133)) - Can't close container #1
org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Error opening DB. Container:1 ContainerPath:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/datanode-1/data-0/containers/hdds/b26be3d8-5dc0-4f12-8d50-c5bb43fc0e01/DS-2ea384d3-d6dc-417f-9ae3-6c2eb9499a15/container.db
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:145)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CloseContainerCommandHandler.handle(CloseContainerCommandHandler.java:111)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
	at java.lang.Thread.run(Thread.java:750)
2023-04-27 06:30:45,765 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(396)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2023-04-27 06:30:46,027 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(357)) - Under Replicated Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:46,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(368)) - Unhealthy Container #1 Container State: CLOSED Replica Count: 3 Healthy Count: 2 Unhealthy Count: 0 Decommission Count: 0 Maintenance Count: 1 inFlightAdd Count: 0 inFightDel Count: 0 ReplicationFactor: 3 minMaintenance Count: 2; Replicas{ContainerReplica{containerID=#1, state=CLOSING, datanodeDetails=dbe8ec56-fdf9-4402-aab9-993d7a20391d(fv-az260-775/10.1.0.33), placeOfBirth=dbe8ec56-fdf9-4402-aab9-993d7a20391d, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33), placeOfBirth=70ce52ad-a2f4-4f2c-b96d-cd309621d39c, sequenceId=2, keyCount=1, bytesUsed=19},ContainerReplica{containerID=#1, state=CLOSED, datanodeDetails=ef6aab41-a1eb-41b3-a4a5-2458878a1611(fv-az260-775/10.1.0.33), placeOfBirth=ef6aab41-a1eb-41b3-a4a5-2458878a1611, sequenceId=2, keyCount=1, bytesUsed=19}}
2023-04-27 06:30:46,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:checkContainersReplicatedOnNode(378)) - 70ce52ad-a2f4-4f2c-b96d-cd309621d39c(fv-az260-775/10.1.0.33) has 2 sufficientlyReplicated, 1 underReplicated and 1 unhealthy containers
2023-04-27 06:30:46,028 [DatanodeAdminManager-0] INFO  node.DatanodeAdminMonitorImpl (DatanodeAdminMonitorImpl.java:run(170)) - There are 1 nodes tracked for decommission and maintenance.  0 pending nodes.
]]></system-out>
  </testcase>
  <testcase name="testEnteringMaintenanceNodeCompletesAfterSCMRestart" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="13.17"/>
  <testcase name="testDecommissioningNodesCompleteDecommissionOnSCMRestart" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="35.484"/>
</testsuite>