Attaching to ha_scm3_1, ha_scm1_1, ha_s3g_1, ha_dn5_1, ha_dn2_1, ha_dn4_1, ha_scm2_1, ha_dn1_1, ha_om1_1, ha_om2_1, ha_recon_1, ha_om3_1, ha_dn3_1
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-04-04 07:53:21,665 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = 1644e6d2c65a/10.9.0.17
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
dn1_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn1_1    | ************************************************************/
dn1_1    | 2023-04-04 07:53:21,697 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-04-04 07:53:22,126 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-04-04 07:53:22,782 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-04-04 07:53:23,964 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-04-04 07:53:23,964 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-04-04 07:53:24,999 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:1644e6d2c65a ip:10.9.0.17
dn1_1    | 2023-04-04 07:53:27,035 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn1_1    | 2023-04-04 07:53:28,567 [main] INFO reflections.Reflections: Reflections took 1243 ms to scan 2 urls, producing 103 keys and 227 values 
dn1_1    | 2023-04-04 07:53:29,293 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn1_1    | 2023-04-04 07:53:30,442 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8192 at 2023-04-04T07:52:55.378Z
dn1_1    | 2023-04-04 07:53:30,569 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-04-04 07:53:30,575 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-04-04 07:53:30,582 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-04-04 07:53:30,778 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-04-04 07:53:30,938 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-04-04 07:53:30,997 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-04-04T07:52:55.451Z
dn1_1    | 2023-04-04 07:53:31,010 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-04-04 07:53:31,014 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-04-04 07:53:31,015 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-04-04 07:53:31,332 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/DS-153a1766-4b5a-4659-9286-466380fd0009/container.db to cache
dn1_1    | 2023-04-04 07:53:31,332 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/DS-153a1766-4b5a-4659-9286-466380fd0009/container.db for volume DS-153a1766-4b5a-4659-9286-466380fd0009
dn1_1    | 2023-04-04 07:53:31,378 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn1_1    | 2023-04-04 07:53:31,386 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-04-04 07:53:31,389 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn1_1    | 2023-04-04 07:53:42,196 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-04-04 07:53:42,665 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-04-04 07:53:43,069 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-04-04 07:53:44,061 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-04-04 07:53:44,062 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-04-04 07:53:44,062 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-04-04 07:53:44,063 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-04-04 07:53:44,063 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-04-04 07:53:44,063 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-04-04 07:53:44,102 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-04-04 07:53:44,102 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-04-04 07:53:23,429 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = feac3239a846/10.9.0.18
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
dn2_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn2_1    | ************************************************************/
dn2_1    | 2023-04-04 07:53:23,489 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-04-04 07:53:23,895 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 2023-04-04 07:53:24,564 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn2_1    | 2023-04-04 07:53:25,897 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-04-04 07:53:25,917 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn2_1    | 2023-04-04 07:53:27,244 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:feac3239a846 ip:10.9.0.18
dn2_1    | 2023-04-04 07:53:29,016 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn2_1    | 2023-04-04 07:53:30,495 [main] INFO reflections.Reflections: Reflections took 1232 ms to scan 2 urls, producing 103 keys and 227 values 
dn2_1    | 2023-04-04 07:53:31,245 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn2_1    | 2023-04-04 07:53:32,599 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8747 at 2023-04-04T07:52:55.240Z
dn2_1    | 2023-04-04 07:53:32,711 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-04-04 07:53:32,727 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn2_1    | 2023-04-04 07:53:32,728 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn2_1    | 2023-04-04 07:53:32,836 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-04-04 07:53:33,070 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-04-04 07:53:33,091 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-04-04T07:52:55.237Z
dn2_1    | 2023-04-04 07:53:33,114 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-04-04 07:53:33,120 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn2_1    | 2023-04-04 07:53:33,127 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn2_1    | 2023-04-04 07:53:33,399 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/DS-ff2a8f33-86a7-4521-a785-2940a4f23ce7/container.db to cache
dn2_1    | 2023-04-04 07:53:33,405 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/DS-ff2a8f33-86a7-4521-a785-2940a4f23ce7/container.db for volume DS-ff2a8f33-86a7-4521-a785-2940a4f23ce7
dn2_1    | 2023-04-04 07:53:33,461 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn2_1    | 2023-04-04 07:53:33,985 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-04-04 07:53:34,033 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn2_1    | 2023-04-04 07:53:43,743 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn2_1    | 2023-04-04 07:53:44,131 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-04-04 07:53:44,635 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-04-04 07:53:45,652 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-04-04 07:53:45,668 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-04-04 07:53:45,672 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-04-04 07:53:45,673 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-04-04 07:53:45,673 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-04-04 07:53:45,676 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-04-04 07:53:45,684 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-04-04 07:53:45,712 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-04-04 07:53:44,104 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-04-04 07:53:44,104 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-04-04 07:53:44,224 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-04-04 07:53:44,227 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-04-04 07:53:44,227 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-04-04 07:53:46,514 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-04-04 07:53:46,516 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-04-04 07:53:46,522 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-04-04 07:53:46,530 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-04-04 07:53:46,537 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-04-04 07:53:46,590 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-04-04 07:53:46,631 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServer: b7b7229a-e604-4787-8543-e6d8cdebe63b: found a subdirectory /data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2
dn1_1    | 2023-04-04 07:53:46,693 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServer: b7b7229a-e604-4787-8543-e6d8cdebe63b: addNew group-FB0AD827E6E2:[] returns group-FB0AD827E6E2:java.util.concurrent.CompletableFuture@7df60c8b[Not completed]
dn1_1    | 2023-04-04 07:53:46,878 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-04-04 07:53:46,932 [pool-26-thread-1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b: new RaftServerImpl for group-FB0AD827E6E2:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-04-04 07:53:46,968 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-04-04 07:53:47,161 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-04-04 07:53:47,162 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-04-04 07:53:47,162 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-04-04 07:53:47,162 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-04-04 07:53:47,209 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-04-04 07:53:47,291 [pool-26-thread-1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-04-04 07:53:47,325 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-04-04 07:53:47,406 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-04-04 07:53:47,411 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-04-04 07:53:47,656 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-04-04 07:53:47,699 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-04-04 07:53:47,735 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-04-04 07:53:48,462 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-04-04 07:53:48,520 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-04-04 07:53:48,530 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-04-04 07:53:48,538 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-04-04 07:53:48,542 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-04-04 07:53:48,697 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn1_1    | 2023-04-04 07:53:48,850 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-04-04 07:53:49,075 [main] INFO util.log: Logging initialized @38485ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-04-04 07:53:49,902 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-04-04 07:53:49,942 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-04-04 07:53:49,995 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-04-04 07:53:49,997 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-04-04 07:53:49,997 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-04-04 07:53:49,997 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn1_1    | 2023-04-04 07:53:50,291 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn1_1    | 2023-04-04 07:53:50,354 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-04-04 07:53:50,355 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn1_1    | 2023-04-04 07:53:50,745 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-04-04 07:53:50,747 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-04-04 07:53:50,749 [main] INFO server.session: node0 Scavenging every 660000ms
dn1_1    | 2023-04-04 07:53:50,991 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@21edd891{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-04-04 07:53:51,056 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@31aab981{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-04-04 07:53:51,921 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@78d92eef{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-8891537307078189332/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn1_1    | 2023-04-04 07:53:51,980 [main] INFO server.AbstractConnector: Started ServerConnector@261f359f{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-04-04 07:53:51,981 [main] INFO server.Server: Started @41421ms
dn1_1    | 2023-04-04 07:53:52,026 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-04-04 07:53:52,026 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-04-04 07:53:52,030 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-04-04 07:53:52,086 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-04-04 07:53:52,272 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn1_1    | 2023-04-04 07:53:52,273 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn1_1    | 2023-04-04 07:53:52,344 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@73d24a03] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn1_1    | 2023-04-04 07:53:52,825 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn1_1    | 2023-04-04 07:53:52,926 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-04-04 07:53:55,570 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:55,571 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:55,571 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:56,572 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:56,573 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:56,574 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:57,573 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:57,574 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:57,575 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:58,574 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:58,575 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:58,575 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:59,574 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:59,575 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:53:59,661 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 1644e6d2c65a/10.9.0.17 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:33856 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-04-04 07:53:45,723 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn2_1    | 2023-04-04 07:53:45,738 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-04-04 07:53:45,875 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-04-04 07:53:45,899 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-04-04 07:53:45,910 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-04-04 07:53:48,033 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-04-04 07:53:48,095 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-04-04 07:53:48,097 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-04-04 07:53:48,100 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-04-04 07:53:48,100 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-04-04 07:53:48,140 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-04-04 07:53:48,143 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServer: e2957b23-687f-4626-af75-9b42f3a43b99: found a subdirectory /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b
dn2_1    | 2023-04-04 07:53:48,190 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServer: e2957b23-687f-4626-af75-9b42f3a43b99: addNew group-0EBF5B06395B:[] returns group-0EBF5B06395B:java.util.concurrent.CompletableFuture@586b73c9[Not completed]
dn2_1    | 2023-04-04 07:53:48,202 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServer: e2957b23-687f-4626-af75-9b42f3a43b99: found a subdirectory /data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210
dn2_1    | 2023-04-04 07:53:48,202 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServer: e2957b23-687f-4626-af75-9b42f3a43b99: addNew group-F3D51D6EC210:[] returns group-F3D51D6EC210:java.util.concurrent.CompletableFuture@2e6df5fc[Not completed]
dn2_1    | 2023-04-04 07:53:48,202 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServer: e2957b23-687f-4626-af75-9b42f3a43b99: found a subdirectory /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e
dn2_1    | 2023-04-04 07:53:48,204 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServer: e2957b23-687f-4626-af75-9b42f3a43b99: addNew group-EEB13EAC206E:[] returns group-EEB13EAC206E:java.util.concurrent.CompletableFuture@25819b26[Not completed]
dn2_1    | 2023-04-04 07:53:48,296 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-04-04 07:53:48,648 [pool-26-thread-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99: new RaftServerImpl for group-0EBF5B06395B:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-04-04 07:53:48,651 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-04-04 07:53:48,658 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-04-04 07:53:48,659 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-04-04 07:53:48,659 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-04-04 07:53:48,659 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-04-04 07:53:48,661 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-04-04 07:53:48,904 [pool-26-thread-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-04-04 07:53:48,904 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-04-04 07:53:49,005 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-04-04 07:53:49,034 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-04-04 07:53:49,311 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-04-04 07:53:49,411 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-04-04 07:53:49,411 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-04-04 07:53:50,299 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-04-04 07:53:50,299 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-04-04 07:53:50,300 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-04-04 07:53:50,301 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-04-04 07:53:50,329 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-04-04 07:53:50,358 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-04-04 07:53:50,359 [pool-26-thread-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99: new RaftServerImpl for group-F3D51D6EC210:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-04-04 07:53:50,399 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-04-04 07:53:50,404 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-04-04 07:53:50,405 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-04-04 07:53:50,405 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-04-04 07:53:50,405 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-04-04 07:53:50,407 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-04-04 07:53:50,415 [pool-26-thread-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-04-04 07:53:50,415 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-04-04 07:53:50,415 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-04-04 07:53:50,419 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-04-04 07:53:50,420 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-04-04 07:53:50,423 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-04-04 07:53:20,171 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = d6d8a15a26f6/10.9.0.19
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
dn3_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:33856 remote=recon/10.9.0.22:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn1_1    | 2023-04-04 07:54:00,576 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:00,577 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:01,577 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:01,578 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:02,578 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:02,579 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:03,579 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:03,579 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:03,587 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 1644e6d2c65a/10.9.0.17 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:53652 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:53652 remote=scm1/10.9.0.14:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn1_1    | 2023-04-04 07:54:04,580 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:04,580 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:05,580 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:05,581 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:06,583 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:07,584 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:08,585 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:09,586 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:09,587 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From 1644e6d2c65a/10.9.0.17 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn3_1    | ************************************************************/
dn3_1    | 2023-04-04 07:53:20,220 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-04-04 07:53:20,652 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-04-04 07:53:21,299 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-04-04 07:53:22,710 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-04-04 07:53:22,711 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-04-04 07:53:23,917 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:d6d8a15a26f6 ip:10.9.0.19
dn3_1    | 2023-04-04 07:53:25,936 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn3_1    | 2023-04-04 07:53:27,409 [main] INFO reflections.Reflections: Reflections took 1220 ms to scan 2 urls, producing 103 keys and 227 values 
dn3_1    | 2023-04-04 07:53:28,050 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn3_1    | 2023-04-04 07:53:29,244 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8747 at 2023-04-04T07:52:55.347Z
dn3_1    | 2023-04-04 07:53:29,376 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 2023-04-04 07:53:29,383 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-04-04 07:53:29,385 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-04-04 07:53:29,533 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-04-04 07:53:29,648 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-04-04 07:53:29,672 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-04-04T07:52:55.567Z
dn3_1    | 2023-04-04 07:53:29,705 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn3_1    | 2023-04-04 07:53:29,708 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn3_1    | 2023-04-04 07:53:29,709 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn3_1    | 2023-04-04 07:53:30,110 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/DS-64bbbab7-8cc0-4f24-986f-629a561e00a7/container.db to cache
dn3_1    | 2023-04-04 07:53:30,110 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/DS-64bbbab7-8cc0-4f24-986f-629a561e00a7/container.db for volume DS-64bbbab7-8cc0-4f24-986f-629a561e00a7
dn3_1    | 2023-04-04 07:53:30,230 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn3_1    | 2023-04-04 07:53:30,741 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-04-04 07:53:30,750 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn3_1    | 2023-04-04 07:53:41,522 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn3_1    | 2023-04-04 07:53:42,063 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-04-04 07:53:42,486 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-04-04 07:53:43,419 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-04-04 07:53:43,426 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn3_1    | 2023-04-04 07:53:43,429 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-04-04 07:53:43,430 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-04-04 07:53:43,443 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-04-04 07:53:43,443 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-04-04 07:53:43,447 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-04-04 07:53:43,448 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-04-04 07:53:50,428 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-04-04 07:53:50,430 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-04-04 07:53:50,436 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-04-04 07:53:50,438 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-04-04 07:53:50,445 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-04-04 07:53:50,445 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-04-04 07:53:50,477 [pool-26-thread-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99: new RaftServerImpl for group-EEB13EAC206E:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-04-04 07:53:50,482 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-04-04 07:53:50,483 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-04-04 07:53:50,484 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-04-04 07:53:50,520 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-04-04 07:53:50,525 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-04-04 07:53:50,529 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-04-04 07:53:50,550 [pool-26-thread-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-04-04 07:53:50,566 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-04-04 07:53:50,570 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-04-04 07:53:50,576 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-04-04 07:53:50,577 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-04-04 07:53:50,577 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-04-04 07:53:50,580 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-04-04 07:53:50,587 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-04-04 07:53:50,593 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-04-04 07:53:50,611 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-04-04 07:53:50,579 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-04-04 07:53:50,623 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-04-04 07:53:50,623 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-04-04 07:53:50,732 [main] INFO util.log: Logging initialized @37891ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-04-04 07:53:51,562 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn2_1    | 2023-04-04 07:53:51,591 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-04-04 07:53:51,631 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-04-04 07:53:51,664 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-04-04 07:53:51,665 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-04-04 07:53:51,676 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-04-04 07:53:51,985 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn2_1    | 2023-04-04 07:53:52,024 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn2_1    | 2023-04-04 07:53:52,033 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn2_1    | 2023-04-04 07:53:52,155 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-04-04 07:53:52,155 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-04-04 07:53:52,162 [main] INFO server.session: node0 Scavenging every 660000ms
dn2_1    | 2023-04-04 07:53:52,254 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a04f730{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-04-04 07:53:52,263 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a31ca20{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-04-04 07:53:53,194 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1c84d80a{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-13437063660815384181/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn2_1    | 2023-04-04 07:53:53,270 [main] INFO server.AbstractConnector: Started ServerConnector@281b2dfd{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn2_1    | 2023-04-04 07:53:53,273 [main] INFO server.Server: Started @40432ms
dn2_1    | 2023-04-04 07:53:53,274 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-04-04 07:53:53,286 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-04-04 07:53:53,296 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-04-04 07:53:53,321 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn2_1    | 2023-04-04 07:53:53,439 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn2_1    | 2023-04-04 07:53:53,455 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn2_1    | 2023-04-04 07:53:53,547 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@276535d3] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn2_1    | 2023-04-04 07:53:53,851 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn2_1    | 2023-04-04 07:53:53,886 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-04-04 07:53:43,459 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-04-04 07:53:43,462 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-04-04 07:53:43,529 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-04-04 07:53:43,532 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-04-04 07:53:43,557 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-04-04 07:53:45,553 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 2023-04-04 07:53:45,558 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-04-04 07:53:45,565 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-04-04 07:53:45,571 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-04-04 07:53:45,579 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-04-04 07:53:45,600 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-04-04 07:53:45,633 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: found a subdirectory /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b
dn3_1    | 2023-04-04 07:53:45,660 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: addNew group-0EBF5B06395B:[] returns group-0EBF5B06395B:java.util.concurrent.CompletableFuture@3c0ab1ac[Not completed]
dn3_1    | 2023-04-04 07:53:45,660 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: found a subdirectory /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e
dn3_1    | 2023-04-04 07:53:45,660 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: addNew group-EEB13EAC206E:[] returns group-EEB13EAC206E:java.util.concurrent.CompletableFuture@1a17f6bf[Not completed]
dn3_1    | 2023-04-04 07:53:45,660 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: found a subdirectory /data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2
dn3_1    | 2023-04-04 07:53:45,662 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: addNew group-8B1B4F9C62A2:[] returns group-8B1B4F9C62A2:java.util.concurrent.CompletableFuture@74ecd689[Not completed]
dn3_1    | 2023-04-04 07:53:45,767 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn3_1    | 2023-04-04 07:53:46,174 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94: new RaftServerImpl for group-0EBF5B06395B:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-04-04 07:53:46,288 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-04-04 07:53:46,356 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-04-04 07:53:46,356 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-04-04 07:53:46,356 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-04-04 07:53:46,356 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-04-04 07:53:46,377 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-04-04 07:53:46,452 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-04-04 07:53:46,466 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-04-04 07:53:46,562 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-04-04 07:53:46,566 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-04-04 07:53:46,781 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-04-04 07:53:46,868 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-04-04 07:53:46,889 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-04-04 07:53:47,594 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-04-04 07:53:47,630 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-04-04 07:53:47,636 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-04-04 07:53:47,646 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-04-04 07:53:47,646 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-04-04 07:53:47,649 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-04-04 07:53:47,723 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94: new RaftServerImpl for group-EEB13EAC206E:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-04-04 07:53:47,747 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-04-04 07:53:47,753 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-04-04 07:53:47,756 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-04-04 07:53:47,757 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-04-04 07:53:47,765 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-04-04 07:53:47,773 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-04-04 07:53:47,773 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-04-04 07:53:47,774 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-04-04 07:53:47,774 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-04-04 07:53:47,774 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-04-04 07:53:47,775 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-04-04 07:53:47,775 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-04-04 07:53:23,702 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn4_1    | /************************************************************
dn4_1    | STARTUP_MSG: Starting HddsDatanodeService
dn4_1    | STARTUP_MSG:   host = 0972042128cd/10.9.0.20
dn4_1    | STARTUP_MSG:   args = []
dn4_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn4_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
dn4_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | 2023-04-04 07:53:56,717 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:53:56,718 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:53:56,717 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:53:57,719 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:53:57,720 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:53:57,720 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:53:58,720 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:53:58,721 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:53:58,721 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:53:59,721 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:53:59,722 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:00,722 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:00,722 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:00,748 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From feac3239a846/10.9.0.18 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:48002 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:48002 remote=recon/10.9.0.22:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn2_1    | 2023-04-04 07:54:01,561 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn4_1    | ************************************************************/
dn4_1    | 2023-04-04 07:53:23,775 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 2023-04-04 07:53:24,132 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 2023-04-04 07:53:24,882 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 2023-04-04 07:53:26,069 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn4_1    | 2023-04-04 07:53:26,069 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn4_1    | 2023-04-04 07:53:27,179 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:0972042128cd ip:10.9.0.20
dn4_1    | 2023-04-04 07:53:29,313 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn4_1    | 2023-04-04 07:53:31,063 [main] INFO reflections.Reflections: Reflections took 1547 ms to scan 2 urls, producing 103 keys and 227 values 
dn4_1    | 2023-04-04 07:53:31,771 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn4_1    | 2023-04-04 07:53:32,944 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4096 at 2023-04-04T07:52:55.364Z
dn4_1    | 2023-04-04 07:53:33,025 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn4_1    | 2023-04-04 07:53:33,033 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn4_1    | 2023-04-04 07:53:33,050 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn4_1    | 2023-04-04 07:53:33,187 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn4_1    | 2023-04-04 07:53:33,343 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-04-04 07:53:33,367 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-04-04T07:52:55.348Z
dn4_1    | 2023-04-04 07:53:33,370 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn4_1    | 2023-04-04 07:53:33,370 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn4_1    | 2023-04-04 07:53:33,408 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn4_1    | 2023-04-04 07:53:33,695 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/DS-c7eb49e2-fbde-4bb5-a94d-a50ef260b2c1/container.db to cache
dn4_1    | 2023-04-04 07:53:33,709 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/DS-c7eb49e2-fbde-4bb5-a94d-a50ef260b2c1/container.db for volume DS-c7eb49e2-fbde-4bb5-a94d-a50ef260b2c1
dn4_1    | 2023-04-04 07:53:33,799 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn4_1    | 2023-04-04 07:53:33,802 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn4_1    | 2023-04-04 07:53:33,807 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn4_1    | 2023-04-04 07:53:44,725 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn4_1    | 2023-04-04 07:53:45,190 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-04-04 07:53:45,621 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 2023-04-04 07:53:46,498 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-04-04 07:53:46,505 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn4_1    | 2023-04-04 07:53:46,507 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-04-04 07:53:46,509 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn4_1    | 2023-04-04 07:53:46,511 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn4_1    | 2023-04-04 07:53:46,511 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn4_1    | 2023-04-04 07:53:46,517 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn4_1    | 2023-04-04 07:53:46,519 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-04-04 07:53:46,526 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn4_1    | 2023-04-04 07:53:46,526 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-04-04 07:53:46,610 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-04-04 07:53:46,669 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-04-04 07:53:46,711 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 2023-04-04 07:53:48,424 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-04-04 07:53:48,459 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-04-04 07:53:48,465 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-04-04 07:53:48,467 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-04-04 07:53:48,487 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-04-04 07:53:48,494 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-04-04 07:53:48,525 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServer: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: found a subdirectory /data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410
dn4_1    | 2023-04-04 07:53:48,612 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServer: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: addNew group-6617081AE410:[] returns group-6617081AE410:java.util.concurrent.CompletableFuture@585c54bb[Not completed]
dn4_1    | 2023-04-04 07:53:48,789 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn4_1    | 2023-04-04 07:53:49,004 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: new RaftServerImpl for group-6617081AE410:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-04-04 07:53:49,066 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-04-04 07:53:49,093 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-04-04 07:53:49,093 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-04-04 07:53:49,094 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-04-04 07:53:49,094 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-04-04 07:53:49,094 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-04-04 07:53:49,385 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-04-04 07:53:49,401 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-04-04 07:53:49,531 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-04-04 07:53:49,540 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-04-04 07:53:49,787 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-04-04 07:53:49,927 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-04-04 07:53:49,927 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-04-04 07:53:50,727 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-04-04 07:53:50,728 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-04-04 07:53:50,728 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-04-04 07:53:50,729 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-04-04 07:53:50,733 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-04-04 07:53:50,818 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn4_1    | 2023-04-04 07:53:50,962 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 2023-04-04 07:53:51,160 [main] INFO util.log: Logging initialized @38166ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 2023-04-04 07:53:52,036 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 2023-04-04 07:53:52,138 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn4_1    | 2023-04-04 07:53:52,172 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn4_1    | 2023-04-04 07:53:52,187 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn4_1    | 2023-04-04 07:53:52,201 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn4_1    | 2023-04-04 07:53:52,202 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-04-04 07:53:52,599 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn4_1    | 2023-04-04 07:53:52,729 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn4_1    | 2023-04-04 07:53:52,732 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn4_1    | 2023-04-04 07:53:52,991 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-04-04 07:53:52,991 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 2023-04-04 07:53:52,992 [main] INFO server.session: node0 Scavenging every 660000ms
dn4_1    | 2023-04-04 07:53:53,045 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@357f6391{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 2023-04-04 07:53:53,057 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@de579ff{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn4_1    | 2023-04-04 07:53:53,810 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@48499739{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-8745751312250589333/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn4_1    | 2023-04-04 07:53:53,834 [main] INFO server.AbstractConnector: Started ServerConnector@1118d539{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-04-04 07:53:47,775 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-04-04 07:53:47,782 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-04-04 07:53:47,787 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-04-04 07:53:47,795 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-04-04 07:53:47,795 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-04-04 07:53:47,796 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-04-04 07:53:47,797 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-04-04 07:53:47,799 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94: new RaftServerImpl for group-8B1B4F9C62A2:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-04-04 07:53:47,822 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-04-04 07:53:47,823 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-04-04 07:53:47,823 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-04-04 07:53:47,823 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-04-04 07:53:47,823 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-04-04 07:53:47,823 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-04-04 07:53:47,824 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-04-04 07:53:47,824 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-04-04 07:53:47,826 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-04-04 07:53:47,837 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-04-04 07:53:47,838 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-04-04 07:53:47,841 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-04-04 07:53:47,842 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-04-04 07:53:47,842 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-04-04 07:53:47,856 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-04-04 07:53:47,856 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-04-04 07:53:47,884 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-04-04 07:53:47,898 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-04-04 07:53:48,077 [main] INFO util.log: Logging initialized @38746ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-04-04 07:53:48,696 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-04-04 07:53:48,755 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-04-04 07:53:48,853 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-04-04 07:53:48,872 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-04-04 07:53:48,894 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 2023-04-04 07:53:48,895 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 2023-04-04 07:53:49,184 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn3_1    | 2023-04-04 07:53:49,270 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn3_1    | 2023-04-04 07:53:49,274 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn3_1    | 2023-04-04 07:53:49,764 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn3_1    | 2023-04-04 07:53:49,798 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-04-04 07:53:49,800 [main] INFO server.session: node0 Scavenging every 600000ms
dn3_1    | 2023-04-04 07:53:49,904 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5ec88f9e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-04-04 07:53:49,905 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@582e9152{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-04-04 07:53:50,797 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@561f9d92{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-4279687359892361449/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn3_1    | 2023-04-04 07:53:50,905 [main] INFO server.AbstractConnector: Started ServerConnector@52abed9d{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-04-04 07:53:50,905 [main] INFO server.Server: Started @41574ms
dn3_1    | 2023-04-04 07:53:50,939 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-04-04 07:53:50,939 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-04-04 07:53:50,940 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-04-04 07:53:50,991 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-04-04 07:53:51,111 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn3_1    | 2023-04-04 07:53:51,123 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn3_1    | 2023-04-04 07:53:51,233 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@64454b56] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn3_1    | 2023-04-04 07:53:51,812 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn3_1    | 2023-04-04 07:53:51,902 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn1_1    | 	... 12 more
dn1_1    | 2023-04-04 07:54:10,589 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:10,595 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 1644e6d2c65a/10.9.0.17 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:55996 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 2023-04-04 07:53:54,586 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:54,586 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:54,586 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:54,591 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:55,587 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:55,587 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:55,592 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:56,588 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:56,588 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:56,592 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:57,588 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:57,589 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:57,593 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:58,590 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:58,590 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:58,598 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:59,592 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:59,599 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:53:59,632 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From d6d8a15a26f6/10.9.0.19 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:56080 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:56080 remote=recon/10.9.0.22:9891]
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	... 1 more
dn2_1    | 2023-04-04 07:54:01,722 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:01,724 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:02,724 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:02,724 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:03,726 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:03,726 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:03,731 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From feac3239a846/10.9.0.18 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:60396 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:60396 remote=scm1/10.9.0.14:9861]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn2_1    | 2023-04-04 07:54:04,726 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:53:53,839 [main] INFO server.Server: Started @40845ms
dn4_1    | 2023-04-04 07:53:53,868 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn4_1    | 2023-04-04 07:53:53,868 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn4_1    | 2023-04-04 07:53:53,869 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn4_1    | 2023-04-04 07:53:53,893 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn4_1    | 2023-04-04 07:53:53,994 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn4_1    | 2023-04-04 07:53:53,999 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn4_1    | 2023-04-04 07:53:54,068 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2db1b504] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn4_1    | 2023-04-04 07:53:54,365 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn4_1    | 2023-04-04 07:53:54,453 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn4_1    | 2023-04-04 07:53:57,327 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:53:57,327 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:53:57,327 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:53:58,329 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:53:58,331 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:53:58,331 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:53:59,330 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:53:59,332 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:00,331 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:00,332 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:01,332 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:01,333 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:01,372 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From 0972042128cd/10.9.0.20 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:50006 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:50006 remote=recon/10.9.0.22:9891]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:55996 remote=scm2/10.9.0.15:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn1_1    | 2023-04-04 07:54:11,058 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn1_1    | 2023-04-04 07:54:11,075 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn1_1    | 2023-04-04 07:54:11,401 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn1_1    | 2023-04-04 07:54:11,404 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis b7b7229a-e604-4787-8543-e6d8cdebe63b
dn1_1    | 2023-04-04 07:54:11,613 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:11,619 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2/in_use.lock acquired by nodename 7@1644e6d2c65a
dn1_1    | 2023-04-04 07:54:11,633 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=b7b7229a-e604-4787-8543-e6d8cdebe63b} from /data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2/current/raft-meta
dn1_1    | 2023-04-04 07:54:11,800 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: set configuration 3: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-04-04 07:54:11,856 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO ratis.ContainerStateMachine: group-FB0AD827E6E2: Setting the last applied index to (t:3, i:4)
dn1_1    | 2023-04-04 07:54:12,479 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-04-04 07:54:12,547 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-04-04 07:54:12,548 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-04-04 07:54:12,573 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-04-04 07:54:12,582 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-04-04 07:54:12,612 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-04-04 07:54:12,618 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:12,684 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-04-04 07:54:12,698 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-04-04 07:54:12,749 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2
dn1_1    | 2023-04-04 07:54:12,767 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-04-04 07:54:12,769 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-04-04 07:54:12,780 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-04-04 07:54:12,785 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-04-04 07:54:12,797 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-04-04 07:54:12,814 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-04-04 07:54:12,815 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-04-04 07:54:12,818 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-04-04 07:54:12,923 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-04-04 07:54:12,928 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-04-04 07:54:13,041 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-04-04 07:54:13,049 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-04-04 07:54:13,054 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-04-04 07:54:13,353 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: set configuration 0: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-04-04 07:54:13,355 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2/current/log_0-0
dn1_1    | 2023-04-04 07:54:13,436 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: set configuration 1: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-04-04 07:54:13,483 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2/current/log_1-2
dn1_1    | 2023-04-04 07:54:13,522 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: set configuration 3: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-04-04 07:54:13,534 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2/current/log_inprogress_3
dn1_1    | 2023-04-04 07:54:13,559 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO segmented.SegmentedRaftLogWorker: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn1_1    | 2023-04-04 07:54:13,599 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO segmented.SegmentedRaftLogWorker: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn1_1    | 2023-04-04 07:54:13,619 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:14,624 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:15,163 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO raftlog.RaftLog: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn1_1    | 2023-04-04 07:54:15,163 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: start as a follower, conf=3: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-04-04 07:54:15,165 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn1_1    | 2023-04-04 07:54:15,180 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO impl.RoleInfo: b7b7229a-e604-4787-8543-e6d8cdebe63b: start b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-FollowerState
dn1_1    | 2023-04-04 07:54:15,231 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-04-04 07:54:15,246 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FB0AD827E6E2,id=b7b7229a-e604-4787-8543-e6d8cdebe63b
dn1_1    | 2023-04-04 07:54:15,249 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-04-04 07:54:15,250 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-04-04 07:54:15,254 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-04-04 07:54:15,256 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-04-04 07:54:15,257 [b7b7229a-e604-4787-8543-e6d8cdebe63b-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-04-04 07:54:15,288 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: b7b7229a-e604-4787-8543-e6d8cdebe63b: start RPC server
dn1_1    | 2023-04-04 07:54:15,317 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: b7b7229a-e604-4787-8543-e6d8cdebe63b: GrpcService started, listening on 9858
dn1_1    | 2023-04-04 07:54:15,336 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: b7b7229a-e604-4787-8543-e6d8cdebe63b: GrpcService started, listening on 9856
dn1_1    | 2023-04-04 07:54:15,389 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: b7b7229a-e604-4787-8543-e6d8cdebe63b: GrpcService started, listening on 9857
dn1_1    | 2023-04-04 07:54:15,439 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b7b7229a-e604-4787-8543-e6d8cdebe63b is started using port 9858 for RATIS
dn1_1    | 2023-04-04 07:54:15,439 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b7b7229a-e604-4787-8543-e6d8cdebe63b is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-04-04 07:54:15,439 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b7b7229a-e604-4787-8543-e6d8cdebe63b is started using port 9856 for RATIS_SERVER
dn1_1    | 2023-04-04 07:54:15,453 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-b7b7229a-e604-4787-8543-e6d8cdebe63b: Started
dn1_1    | 2023-04-04 07:54:15,535 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn1_1    | 2023-04-04 07:54:15,559 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-04-04 07:54:15,633 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:16,636 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:17,636 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:18,658 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:19,647 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-b7b7229a-e604-4787-8543-e6d8cdebe63b: Detected pause in JVM or host machine (eg GC): pause of approximately 158004453ns. No GCs detected.
dn1_1    | 2023-04-04 07:54:19,660 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:20,316 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-FollowerState] INFO impl.FollowerState: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5136596900ns, electionTimeout:5065ms
dn1_1    | 2023-04-04 07:54:20,317 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-FollowerState] INFO impl.RoleInfo: b7b7229a-e604-4787-8543-e6d8cdebe63b: shutdown b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-FollowerState
dn1_1    | 2023-04-04 07:54:20,317 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-FollowerState] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn1_1    | 2023-04-04 07:54:20,323 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-04-04 07:54:20,323 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-FollowerState] INFO impl.RoleInfo: b7b7229a-e604-4787-8543-e6d8cdebe63b: start b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1
dn1_1    | 2023-04-04 07:54:20,368 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO impl.LeaderElection: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-04-04 07:54:20,381 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO impl.LeaderElection: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
dn1_1    | 2023-04-04 07:54:20,497 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO impl.LeaderElection: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-04-04 07:54:20,498 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO impl.LeaderElection: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1 ELECTION round 0: result PASSED (term=4)
dn1_1    | 2023-04-04 07:54:20,498 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO impl.RoleInfo: b7b7229a-e604-4787-8543-e6d8cdebe63b: shutdown b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1
dn1_1    | 2023-04-04 07:54:20,499 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn1_1    | 2023-04-04 07:54:20,506 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-FB0AD827E6E2 with new leaderId: b7b7229a-e604-4787-8543-e6d8cdebe63b
dn1_1    | 2023-04-04 07:54:20,510 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: change Leader from null to b7b7229a-e604-4787-8543-e6d8cdebe63b at term 4 for becomeLeader, leader elected after 32892ms
dn1_1    | 2023-04-04 07:54:20,607 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-04-04 07:54:20,654 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-04-04 07:54:20,657 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-04-04 07:54:20,661 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:20,709 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-04-04 07:54:20,710 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-04-04 07:54:20,718 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-04-04 07:54:20,761 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-04-04 07:54:20,774 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-04-04 07:54:20,813 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO impl.RoleInfo: b7b7229a-e604-4787-8543-e6d8cdebe63b: start b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderStateImpl
dn1_1    | 2023-04-04 07:54:20,832 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn1_1    | 2023-04-04 07:54:20,861 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2/current/log_inprogress_3 to /data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2/current/log_3-4
dn1_1    | 2023-04-04 07:54:20,878 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderElection1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: set configuration 5: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-04-04 07:54:20,894 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2/current/log_inprogress_5
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn3_1    | 2023-04-04 07:54:00,592 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:00,599 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:01,593 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:01,600 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:02,594 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:02,601 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:03,595 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:03,601 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:03,603 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From d6d8a15a26f6/10.9.0.19 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:52370 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:52370 remote=scm1/10.9.0.14:9861]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn3_1    | 2023-04-04 07:54:04,596 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:04,602 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:05,597 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:05,603 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:06,597 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:07,598 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:08,599 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:08,600 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn3_1    | java.net.ConnectException: Call From d6d8a15a26f6/10.9.0.19 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 2023-04-04 07:54:21,665 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:54:33,765 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn1_1    | 2023-04-04 07:55:15,560 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-04-04 07:54:04,727 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 2023-04-04 07:54:05,727 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | Caused by: java.net.ConnectException: Connection refused
dn5_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 2023-04-04 07:55:51,511 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-04-04 07:55:51,513 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-04-04 07:54:05,728 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:06,729 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 2023-04-04 07:55:51,513 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn5_1    | 2023-04-04 07:53:23,051 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn5_1    | /************************************************************
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-04-04 07:53:23,141 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1    | /************************************************************
om1_1    | 2023-04-04 07:53:20,727 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn2_1    | 2023-04-04 07:54:07,730 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:08,731 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:09,563 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
om3_1    | 2023-04-04 07:53:21,750 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
dn1_1    | 2023-04-04 07:55:51,514 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn1_1    | 2023-04-04 07:55:51,515 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn1_1    | 2023-04-04 07:55:51,516 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn1_1    | 2023-04-04 07:55:51,516 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn1_1    | 2023-04-04 07:55:51,516 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om1_1    | /************************************************************
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
om2_1    | STARTUP_MSG: Starting OzoneManager
om2_1    | STARTUP_MSG:   host = 6734cfdf8d96/10.9.0.12
dn5_1    | STARTUP_MSG: Starting HddsDatanodeService
dn5_1    | STARTUP_MSG:   host = 7e8229df9409/10.9.0.21
dn5_1    | STARTUP_MSG:   args = []
dn5_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | 	... 12 more
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
om3_1    | /************************************************************
om3_1    | STARTUP_MSG: Starting OzoneManager
om3_1    | STARTUP_MSG:   host = 8b40267daa06/10.9.0.13
dn1_1    | 2023-04-04 07:55:51,516 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn1_1    | 2023-04-04 07:55:51,516 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-04-04 07:56:15,560 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-04-04 07:54:09,602 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | STARTUP_MSG:   args = [--upgrade]
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 2023-04-04 07:54:02,333 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:02,334 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-04-04 07:56:21,520 [Command processor thread] INFO server.RaftServer: b7b7229a-e604-4787-8543-e6d8cdebe63b: remove    LEADER b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2:t4, leader=b7b7229a-e604-4787-8543-e6d8cdebe63b, voted=b7b7229a-e604-4787-8543-e6d8cdebe63b, raftlog=Memoized:b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLog:OPENED:c6, conf=5: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn1_1    | 2023-04-04 07:56:21,522 [Command processor thread] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: shutdown
dn1_1    | 2023-04-04 07:56:21,522 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-FB0AD827E6E2,id=b7b7229a-e604-4787-8543-e6d8cdebe63b
dn1_1    | 2023-04-04 07:56:21,522 [Command processor thread] INFO impl.RoleInfo: b7b7229a-e604-4787-8543-e6d8cdebe63b: shutdown b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-LeaderStateImpl
dn5_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
recon_1  | 2023-04-04 07:53:22,338 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1  | /************************************************************
om2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = cb73d2d52b76/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--upgrade]
om1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
s3g_1    | 2023-04-04 07:53:22,353 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
dn1_1    | 2023-04-04 07:56:21,527 [Command processor thread] INFO impl.PendingRequests: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-PendingRequests: sendNotLeaderResponses
dn5_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
dn5_1    | STARTUP_MSG:   java = 11.0.14.1
dn3_1    | 2023-04-04 07:54:10,602 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:10,610 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
recon_1  | STARTUP_MSG: Starting ReconServer
recon_1  | STARTUP_MSG:   host = 0d461f16e2c4/10.9.0.22
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
om1_1    | STARTUP_MSG:   java = 11.0.14.1
om3_1    | STARTUP_MSG:   args = [--upgrade]
om3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
s3g_1    | 2023-04-04 07:53:22,373 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-04-04 07:56:21,529 [Command processor thread] INFO impl.StateMachineUpdater: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-StateMachineUpdater: set stopIndex = 6
dn1_1    | 2023-04-04 07:56:21,529 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-FB0AD827E6E2: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2/sm/snapshot.4_6
dn1_1    | 2023-04-04 07:56:21,531 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-FB0AD827E6E2: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2/sm/snapshot.4_6 took: 2 ms
dn1_1    | 2023-04-04 07:56:21,532 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-StateMachineUpdater] INFO impl.StateMachineUpdater: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-StateMachineUpdater: Took a snapshot at index 6
dn5_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1  | STARTUP_MSG:   args = []
recon_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
scm3_1   | Waiting for the service scm2:9894
s3g_1    | 2023-04-04 07:53:22,642 [main] INFO util.log: Logging initialized @10056ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn3_1    | java.net.SocketTimeoutException: Call From d6d8a15a26f6/10.9.0.19 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:43882 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | ************************************************************/
dn5_1    | 2023-04-04 07:53:23,110 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.23.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.3.23.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.23.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.23.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.23.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
dn4_1    | 2023-04-04 07:54:03,334 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-04-04 07:53:23,755 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om1_1    | ************************************************************/
om1_1    | 2023-04-04 07:53:20,813 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-04-04 07:53:29,103 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-04-04 07:53:32,132 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | 2023-04-04 07:53:23,369 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2_1   | Waiting for the service scm1:9894
om3_1    | STARTUP_MSG:   java = 11.0.14.1
recon_1  | STARTUP_MSG:   java = 11.0.14.1
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
s3g_1    | 2023-04-04 07:53:23,949 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
om1_1    | 2023-04-04 07:53:32,452 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-04-04 07:53:32,454 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-04-04 07:53:32,483 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-04-04 07:53:32,688 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | 2023-04-04 07:53:24,031 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=true, hdds.scm.replication.event.timeout=30m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1  | ************************************************************/
recon_1  | 2023-04-04 07:53:22,419 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn4_1    | 2023-04-04 07:54:03,335 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:03,362 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
s3g_1    | 2023-04-04 07:53:23,993 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1    | 2023-04-04 07:53:35,138 [main] INFO reflections.Reflections: Reflections took 2278 ms to scan 1 urls, producing 129 keys and 376 values [using 2 cores]
om2_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | 2023-04-04 07:56:21,534 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-StateMachineUpdater] INFO impl.StateMachineUpdater: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
scm1_1   | 2023-04-04 07:53:31,230 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
dn5_1    | 2023-04-04 07:53:24,989 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | 2023-04-04 07:53:26,466 [main] INFO reflections.Reflections: Reflections took 553 ms to scan 1 urls, producing 17 keys and 54 values 
recon_1  | 2023-04-04 07:53:29,428 [main] INFO recon.ReconServer: Initializing Recon server...
om3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1    | ************************************************************/
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | java.net.SocketTimeoutException: Call From 0972042128cd/10.9.0.20 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:50712 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
s3g_1    | 2023-04-04 07:53:24,030 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
om1_1    | 2023-04-04 07:53:35,389 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn1_1    | 2023-04-04 07:56:21,536 [Command processor thread] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: closes. applyIndex: 6
om2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
scm1_1   | /************************************************************
dn5_1    | 2023-04-04 07:53:24,989 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
recon_1  | 2023-04-04 07:53:29,457 [main] INFO impl.ReconDBProvider: Last known Recon DB : /data/metadata/recon/recon-container-key.db_1680594192298
recon_1  | 2023-04-04 07:53:31,195 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-04-04 07:53:36,188 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | WARNING: An illegal reflective access operation has occurred
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
s3g_1    | 2023-04-04 07:53:24,031 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om1_1    | 2023-04-04 07:53:37,844 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
dn1_1    | 2023-04-04 07:56:21,536 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
om2_1    | ************************************************************/
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
dn5_1    | 2023-04-04 07:53:26,239 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:7e8229df9409 ip:10.9.0.21
recon_1  | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
om3_1    | 2023-04-04 07:53:21,831 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1    | 2023-04-04 07:53:30,062 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1    | 2023-04-04 07:53:33,272 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-04-04 07:53:33,460 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
s3g_1    | 2023-04-04 07:53:24,039 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om1_1    | 2023-04-04 07:53:38,112 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
dn1_1    | 2023-04-04 07:56:21,538 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2-SegmentedRaftLogWorker close()
om2_1    | 2023-04-04 07:53:23,235 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-04-04 07:53:31,334 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-04-04 07:53:34,947 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-04-04 07:53:35,289 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-04-04 07:53:35,291 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1    | 2023-04-04 07:53:35,334 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-04-04 07:53:33,461 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-04-04 07:53:33,502 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-04-04 07:53:33,700 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
s3g_1    | 2023-04-04 07:53:24,427 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /data/metadata/webserver
om1_1    | 2023-04-04 07:53:44,627 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb73d2d52b76/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
dn1_1    | 2023-04-04 07:56:21,549 [Command processor thread] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-FB0AD827E6E2: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/84e9f90e-0ab8-4731-82e1-fb0ad827e6e2
om2_1    | 2023-04-04 07:53:35,727 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om2_1    | 2023-04-04 07:53:38,593 [main] INFO reflections.Reflections: Reflections took 1941 ms to scan 1 urls, producing 129 keys and 376 values [using 2 cores]
om2_1    | 2023-04-04 07:53:38,810 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-04-04 07:53:40,479 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | STARTUP_MSG:   host = 4129f13dc7cc/10.9.0.14
om3_1    | 2023-04-04 07:53:36,117 [main] INFO reflections.Reflections: Reflections took 2207 ms to scan 1 urls, producing 129 keys and 376 values [using 2 cores]
om3_1    | 2023-04-04 07:53:36,486 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-04-04 07:53:38,793 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
s3g_1    | 2023-04-04 07:53:25,428 [main] INFO s3.Gateway: STARTUP_MSG: 
om1_1    | 2023-04-04 07:53:46,628 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb73d2d52b76/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
dn1_1    | 2023-04-04 07:56:21,550 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=84e9f90e-0ab8-4731-82e1-fb0ad827e6e2 command on datanode b7b7229a-e604-4787-8543-e6d8cdebe63b.
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
scm1_1   | STARTUP_MSG:   args = []
om3_1    | 2023-04-04 07:53:39,301 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-04-04 07:53:45,895 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8b40267daa06/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om3_1    | 2023-04-04 07:53:47,896 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8b40267daa06/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
s3g_1    | /************************************************************
dn1_1    | 2023-04-04 07:56:51,615 [Command processor thread] INFO server.RaftServer: b7b7229a-e604-4787-8543-e6d8cdebe63b: addNew group-0FCEB345A748:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-0FCEB345A748:java.util.concurrent.CompletableFuture@18a45944[Not completed]
dn1_1    | 2023-04-04 07:56:51,618 [pool-26-thread-1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b: new RaftServerImpl for group-0FCEB345A748:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-04-04 07:56:51,627 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-04-04 07:56:51,627 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-04-04 07:56:51,627 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-04-04 07:56:51,628 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm1_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1    | 2023-04-04 07:53:40,843 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3_1   | 2023-04-04 07:54:08,297 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
s3g_1    | STARTUP_MSG: Starting Gateway
dn1_1    | 2023-04-04 07:56:51,628 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-04-04 07:56:51,628 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-04-04 07:56:51,628 [pool-26-thread-1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748: ConfigurationManager, init=-1: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om3_1    | 2023-04-04 07:53:49,899 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8b40267daa06/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om2_1    | 2023-04-04 07:53:47,372 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6734cfdf8d96/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om2_1    | 2023-04-04 07:53:49,374 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6734cfdf8d96/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om2_1    | 2023-04-04 07:53:51,380 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6734cfdf8d96/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om2_1    | 2023-04-04 07:53:53,381 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6734cfdf8d96/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
s3g_1    | STARTUP_MSG:   host = dfd4f94c4f26/10.9.0.23
dn5_1    | 2023-04-04 07:53:28,299 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn5_1    | 2023-04-04 07:53:29,601 [main] INFO reflections.Reflections: Reflections took 1199 ms to scan 2 urls, producing 103 keys and 227 values 
dn1_1    | 2023-04-04 07:56:51,629 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 2023-04-04 07:53:51,906 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8b40267daa06/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
recon_1  | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
scm1_1   | STARTUP_MSG:   java = 11.0.14.1
scm3_1   | /************************************************************
dn1_1    | 2023-04-04 07:56:51,636 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 	... 1 more
om3_1    | 2023-04-04 07:53:53,907 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8b40267daa06/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om3_1    | 2023-04-04 07:53:55,908 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8b40267daa06/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1  | WARNING: All illegal access operations will be denied in a future release
recon_1  | 2023-04-04 07:53:39,764 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-04-04 07:53:39,790 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-04-04 07:53:39,794 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1  | 2023-04-04 07:53:43,417 [main] INFO codegen.SqlDbUtils: GLOBAL_STATS table already exists, skipping creation.
s3g_1    | STARTUP_MSG:   args = []
scm1_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=true, hdds.scm.replication.event.timeout=30m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn1_1    | 2023-04-04 07:56:51,636 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-04-04 07:56:51,636 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-04-04 07:56:51,636 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-04-04 07:54:09,732 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-04-04 07:53:57,910 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8b40267daa06/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om3_1    | 2023-04-04 07:53:59,912 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8b40267daa06/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
recon_1  | 2023-04-04 07:53:43,535 [main] INFO codegen.SqlDbUtils: RECON_TASK_STATUS table already exists, skipping creation.
om2_1    | 2023-04-04 07:53:55,383 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6734cfdf8d96/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
s3g_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
dn2_1    | 2023-04-04 07:54:10,732 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:10,733 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn2_1    | java.net.ConnectException: Call From feac3239a846/10.9.0.18 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
om3_1    | 2023-04-04 07:54:01,913 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8b40267daa06/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om3_1    | 2023-04-04 07:54:03,954 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:861060d2-f30f-4260-bac0-b4e7af8aba9d is not the leader. Could not determine the leader node.
s3g_1    | STARTUP_MSG:   java = 11.0.14.1
s3g_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1    | ************************************************************/
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
s3g_1    | 2023-04-04 07:53:25,486 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | 2023-04-04 07:53:45,646 [main] INFO codegen.SqlDbUtils: FILE_COUNT_BY_SIZE table already exists, skipping creation.
om2_1    | 2023-04-04 07:53:57,384 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6734cfdf8d96/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om2_1    | 2023-04-04 07:54:00,918 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:861060d2-f30f-4260-bac0-b4e7af8aba9d is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 2023-04-04 07:53:48,630 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb73d2d52b76/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
s3g_1    | 2023-04-04 07:53:25,694 [main] INFO s3.Gateway: Starting Ozone S3 gateway
recon_1  | 2023-04-04 07:53:45,727 [main] INFO codegen.SqlDbUtils: CLUSTER_GROWTH_DAILY table already exists, skipping creation.
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 2023-04-04 07:53:50,633 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb73d2d52b76/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
recon_1  | 2023-04-04 07:53:45,879 [main] INFO codegen.SqlDbUtils: UNHEALTHY_CONTAINERS table already exists, skipping creation.
s3g_1    | 2023-04-04 07:53:26,377 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1    | 2023-04-04 07:53:27,813 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1  | 2023-04-04 07:53:46,214 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
s3g_1    | 2023-04-04 07:53:27,813 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1    | 2023-04-04 07:53:28,151 [main] INFO http.HttpServer2: Jetty bound to port 9878
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn5_1    | 2023-04-04 07:53:30,241 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn5_1    | 2023-04-04 07:53:31,146 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4651 at 2023-04-04T07:52:55.666Z
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm3_1   | STARTUP_MSG:   host = 2bce303d964c/10.9.0.16
dn5_1    | 2023-04-04 07:53:31,224 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
recon_1  | 2023-04-04 07:53:46,363 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1    | 2023-04-04 07:53:28,183 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
dn5_1    | 2023-04-04 07:53:31,262 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn5_1    | 2023-04-04 07:53:31,296 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn5_1    | 2023-04-04 07:53:31,436 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn5_1    | 2023-04-04 07:53:31,556 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-04-04 07:53:31,600 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-04-04T07:52:55.976Z
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
recon_1  | 2023-04-04 07:53:46,448 [main] INFO util.log: Logging initialized @35566ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1    | 2023-04-04 07:53:28,659 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:43882 remote=scm2/10.9.0.15:9861]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 2023-04-04 07:53:47,049 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1    | 2023-04-04 07:53:28,680 [main] INFO server.session: No SessionScavenger set, using defaults
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 2023-04-04 07:53:47,076 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
s3g_1    | 2023-04-04 07:53:28,686 [main] INFO server.session: node0 Scavenging every 600000ms
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-04-04 07:53:31,633 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn5_1    | 2023-04-04 07:53:31,637 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn5_1    | 2023-04-04 07:53:31,638 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
om1_1    | 2023-04-04 07:53:52,634 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb73d2d52b76/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om1_1    | 2023-04-04 07:53:54,636 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb73d2d52b76/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om1_1    | 2023-04-04 07:53:56,638 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb73d2d52b76/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om1_1    | 2023-04-04 07:53:58,642 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb73d2d52b76/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om1_1    | 2023-04-04 07:54:00,644 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb73d2d52b76/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om1_1    | 2023-04-04 07:54:02,693 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:861060d2-f30f-4260-bac0-b4e7af8aba9d is not the leader. Could not determine the leader node.
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 2023-04-04 07:53:47,111 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1    | 2023-04-04 07:53:28,773 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@176b75f7{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om3_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
dn5_1    | 2023-04-04 07:53:32,117 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/DS-7b74d109-f576-405c-b59a-8f26ca8c37a4/container.db to cache
dn5_1    | 2023-04-04 07:53:32,121 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/DS-7b74d109-f576-405c-b59a-8f26ca8c37a4/container.db for volume DS-7b74d109-f576-405c-b59a-8f26ca8c37a4
dn5_1    | 2023-04-04 07:53:32,202 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn5_1    | 2023-04-04 07:53:32,946 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn5_1    | 2023-04-04 07:53:32,946 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn5_1    | 2023-04-04 07:53:43,172 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn5_1    | 2023-04-04 07:53:43,746 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-04-04 07:53:44,140 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 2023-04-04 07:53:47,136 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
s3g_1    | 2023-04-04 07:53:28,774 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48793bef{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om3_1    | 2023-04-04 07:54:06,914 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:19de384c-7008-4d6c-8004-b0a99b5aa3c2 is not the leader. Could not determine the leader node.
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
dn1_1    | 2023-04-04 07:56:51,636 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-04-04 07:56:51,638 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-04-04 07:56:51,639 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-04-04 07:53:47,137 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1    | WARNING: An illegal reflective access operation has occurred
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 2023-04-04 07:56:51,640 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-04-04 07:56:51,640 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-04-04 07:56:51,640 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-04-04 07:56:51,641 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6d92c706-b857-41ae-a811-0fceb345a748 does not exist. Creating ...
dn1_1    | 2023-04-04 07:56:51,645 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6d92c706-b857-41ae-a811-0fceb345a748/in_use.lock acquired by nodename 7@1644e6d2c65a
dn1_1    | 2023-04-04 07:56:51,648 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6d92c706-b857-41ae-a811-0fceb345a748 has been successfully formatted.
dn1_1    | 2023-04-04 07:56:51,655 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-0FCEB345A748: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-04-04 07:56:51,655 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 2023-04-04 07:53:47,138 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1    | WARNING: Illegal reflective access by com.sun.xml.bind.v2.runtime.reflect.opt.Injector (file:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
om2_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om2_1    | 2023-04-04 07:54:02,919 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6734cfdf8d96/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om2_1    | 2023-04-04 07:54:04,920 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6734cfdf8d96/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
recon_1  | 2023-04-04 07:53:47,287 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
s3g_1    | WARNING: Please consider reporting this to the maintainers of com.sun.xml.bind.v2.runtime.reflect.opt.Injector
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:50712 remote=scm1/10.9.0.14:9861]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
om2_1    | 2023-04-04 07:54:06,924 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:861060d2-f30f-4260-bac0-b4e7af8aba9d is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 2023-04-04 07:53:47,783 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1  | 2023-04-04 07:53:48,060 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
recon_1  | 2023-04-04 07:53:48,095 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
s3g_1    | WARNING: All illegal access operations will be denied in a future release
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
recon_1  | 2023-04-04 07:53:48,102 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om1_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om1_1    | 2023-04-04 07:54:04,695 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb73d2d52b76/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om1_1    | 2023-04-04 07:54:06,697 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb73d2d52b76/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om1_1    | 2023-04-04 07:54:08,703 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:861060d2-f30f-4260-bac0-b4e7af8aba9d is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
recon_1  | 2023-04-04 07:53:48,217 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 2023-04-04 07:53:48,219 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om1_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om1_1    | 2023-04-04 07:54:10,755 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:19de384c-7008-4d6c-8004-b0a99b5aa3c2 is not the leader. Suggested leader is Server:scm1:9863.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 2023-04-04 07:53:50,548 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-04-04 07:53:51,445 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-04-04 07:53:52,223 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om2_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om2_1    | 2023-04-04 07:54:08,944 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:19de384c-7008-4d6c-8004-b0a99b5aa3c2 is not the leader. Could not determine the leader node.
dn5_1    | 2023-04-04 07:53:45,064 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-04-04 07:53:45,104 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn5_1    | 2023-04-04 07:53:45,105 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-04-04 07:53:45,105 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn5_1    | 2023-04-04 07:53:45,107 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn5_1    | 2023-04-04 07:53:45,107 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn5_1    | 2023-04-04 07:53:45,110 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn4_1    | 2023-04-04 07:54:04,335 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:04,336 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:05,336 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:05,337 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1  | 2023-04-04 07:53:52,244 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1  | 2023-04-04 07:53:52,652 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-04-04 07:53:53,048 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn1_1    | 2023-04-04 07:56:51,658 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-04-04 07:56:51,658 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-04-04 07:56:51,667 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-04-04 07:56:51,671 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-04-04 07:56:51,671 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-04-04 07:56:51,673 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-04-04 07:56:51,673 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-04-04 07:56:51,673 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6d92c706-b857-41ae-a811-0fceb345a748
dn1_1    | 2023-04-04 07:56:51,673 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-04-04 07:54:11,203 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1  | 2023-04-04 07:53:53,509 [main] INFO reflections.Reflections: Reflections took 400 ms to scan 3 urls, producing 128 keys and 283 values 
recon_1  | 2023-04-04 07:53:53,699 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-04-04 07:53:53,874 [main] INFO node.SCMNodeManager: Entering startup safe mode.
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
recon_1  | 2023-04-04 07:53:54,006 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/14df8ad4-ce2f-4483-aada-29ab5cae3ffb
recon_1  | 2023-04-04 07:53:54,016 [main] INFO node.SCMNodeManager: Registered Data node : 14df8ad4-ce2f-4483-aada-29ab5cae3ffb{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | ************************************************************/
scm1_1   | 2023-04-04 07:53:31,459 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn5_1    | 2023-04-04 07:53:45,122 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-04-04 07:53:45,170 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn5_1    | 2023-04-04 07:53:45,171 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-04-04 07:53:45,231 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-04-04 07:53:45,320 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
scm2_1   | 2023-04-04 07:54:01,777 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
recon_1  | 2023-04-04 07:53:54,019 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2ebde02c-a404-41d0-92a4-7b6da490547a
recon_1  | 2023-04-04 07:53:54,025 [main] INFO node.SCMNodeManager: Registered Data node : 2ebde02c-a404-41d0-92a4-7b6da490547a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn3_1    | 2023-04-04 07:54:11,221 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn3_1    | 2023-04-04 07:54:11,521 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn3_1    | 2023-04-04 07:54:11,525 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm1_1   | 2023-04-04 07:53:32,438 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-04-04 07:53:33,197 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1_1   | 2023-04-04 07:53:33,409 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
dn4_1    | 2023-04-04 07:54:06,338 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:07,339 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:08,340 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
dn1_1    | 2023-04-04 07:56:51,673 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn2_1    | 	... 12 more
dn3_1    | 2023-04-04 07:54:11,604 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:11,632 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2/in_use.lock acquired by nodename 8@d6d8a15a26f6
dn3_1    | 2023-04-04 07:54:11,637 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/in_use.lock acquired by nodename 8@d6d8a15a26f6
dn3_1    | 2023-04-04 07:54:11,664 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=c1077390-d65b-4523-9cd4-abe9e2c9eb94} from /data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2/current/raft-meta
dn3_1    | 2023-04-04 07:54:11,665 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/in_use.lock acquired by nodename 8@d6d8a15a26f6
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | /************************************************************
dn4_1    | 2023-04-04 07:54:09,340 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:56:51,674 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-04-04 07:54:10,738 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From feac3239a846/10.9.0.18 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:37356 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
scm1_1   | 2023-04-04 07:53:33,749 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
scm1_1   | 2023-04-04 07:53:33,771 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
scm1_1   | 2023-04-04 07:53:43,607 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-04-04 07:53:45,779 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
dn4_1    | 2023-04-04 07:54:10,341 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-04-04 07:56:51,674 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 2023-04-04 07:53:45,325 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn5_1    | 2023-04-04 07:53:47,399 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | STARTUP_MSG:   args = []
dn4_1    | 2023-04-04 07:54:10,346 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn1_1    | 2023-04-04 07:56:51,674 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn5_1    | 2023-04-04 07:53:47,419 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-04-04 07:53:47,437 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1_1   | 2023-04-04 07:53:47,658 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm1_1   | 2023-04-04 07:53:47,678 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
dn3_1    | 2023-04-04 07:54:11,667 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=14, votedFor=e2957b23-687f-4626-af75-9b42f3a43b99} from /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/raft-meta
dn3_1    | 2023-04-04 07:54:11,665 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=18, votedFor=2ebde02c-a404-41d0-92a4-7b6da490547a} from /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/raft-meta
scm2_1   | STARTUP_MSG:   host = f571653a518f/10.9.0.15
scm2_1   | STARTUP_MSG:   args = []
scm3_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | java.net.SocketTimeoutException: Call From 0972042128cd/10.9.0.20 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:33014 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 2023-04-04 07:56:51,674 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1  | 2023-04-04 07:53:54,026 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b7b7229a-e604-4787-8543-e6d8cdebe63b
recon_1  | 2023-04-04 07:53:54,030 [main] INFO node.SCMNodeManager: Registered Data node : b7b7229a-e604-4787-8543-e6d8cdebe63b{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-04-04 07:53:47,438 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm1_1   | 2023-04-04 07:53:48,615 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1_1   | 2023-04-04 07:53:48,797 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:861060d2-f30f-4260-bac0-b4e7af8aba9d
scm1_1   | 2023-04-04 07:53:49,759 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-04-04 07:53:50,718 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 2023-04-04 07:56:51,674 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1  | 2023-04-04 07:53:54,030 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c1077390-d65b-4523-9cd4-abe9e2c9eb94
recon_1  | 2023-04-04 07:53:54,030 [main] INFO node.SCMNodeManager: Registered Data node : c1077390-d65b-4523-9cd4-abe9e2c9eb94{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
dn5_1    | 2023-04-04 07:53:47,441 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-04-04 07:53:50,754 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1_1   | 2023-04-04 07:53:50,754 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-04-04 07:54:11,860 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: set configuration 15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:11,858 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: set configuration 26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 2023-04-04 07:56:51,674 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1  | 2023-04-04 07:53:54,032 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e2957b23-687f-4626-af75-9b42f3a43b99
recon_1  | 2023-04-04 07:53:54,035 [main] INFO node.SCMNodeManager: Registered Data node : e2957b23-687f-4626-af75-9b42f3a43b99{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-04-04 07:53:47,472 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-04-04 07:53:47,501 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: found a subdirectory /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b
dn5_1    | 2023-04-04 07:53:47,525 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: addNew group-0EBF5B06395B:[] returns group-0EBF5B06395B:java.util.concurrent.CompletableFuture@5781f22e[Not completed]
dn5_1    | 2023-04-04 07:53:47,525 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: found a subdirectory /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e
dn3_1    | 2023-04-04 07:54:11,882 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: set configuration 3: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:11,938 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO ratis.ContainerStateMachine: group-EEB13EAC206E: Setting the last applied index to (t:18, i:46)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
s3g_1    | 2023-04-04 07:53:52,534 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@61ab6521{s3gateway,/,file:///data/metadata/webserver/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-9344007477863703425/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
scm1_1   | 2023-04-04 07:53:50,781 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
dn5_1    | 2023-04-04 07:53:47,525 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: addNew group-EEB13EAC206E:[] returns group-EEB13EAC206E:java.util.concurrent.CompletableFuture@241495e5[Not completed]
dn5_1    | 2023-04-04 07:53:47,525 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: found a subdirectory /data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6
dn5_1    | 2023-04-04 07:53:47,525 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: addNew group-65BC10982AC6:[] returns group-65BC10982AC6:java.util.concurrent.CompletableFuture@204cee8d[Not completed]
dn5_1    | 2023-04-04 07:53:47,675 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn5_1    | 2023-04-04 07:53:48,179 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a: new RaftServerImpl for group-0EBF5B06395B:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-04-04 07:56:51,675 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-04-04 07:56:51,676 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 2023-04-04 07:53:54,035 [main] INFO scm.ReconNodeManager: Loaded 5 nodes from node DB.
recon_1  | 2023-04-04 07:53:54,059 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1  | 2023-04-04 07:53:54,179 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn5_1    | 2023-04-04 07:53:48,297 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-04-04 07:54:11,946 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO ratis.ContainerStateMachine: group-8B1B4F9C62A2: Setting the last applied index to (t:3, i:4)
dn3_1    | 2023-04-04 07:54:11,946 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO ratis.ContainerStateMachine: group-0EBF5B06395B: Setting the last applied index to (t:14, i:20)
dn3_1    | 2023-04-04 07:54:12,597 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-04-04 07:54:12,604 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-04-04 07:56:51,700 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-04-04 07:56:51,701 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1  | 2023-04-04 07:53:54,296 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-04-04 07:53:50,788 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1_1   | 2023-04-04 07:53:50,788 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
dn5_1    | 2023-04-04 07:53:48,312 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-04-04 07:54:12,628 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:12,627 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-04-04 07:54:12,649 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-04-04 07:54:12,650 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-04-04 07:56:51,701 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-04-04 07:56:51,702 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-04-04 07:53:50,788 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1_1   | 2023-04-04 07:53:50,810 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-04-04 07:53:50,816 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
dn5_1    | 2023-04-04 07:53:48,312 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-04-04 07:54:12,649 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-04-04 07:54:12,651 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-04-04 07:54:12,661 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-04-04 07:54:12,663 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn1_1    | 2023-04-04 07:56:51,702 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-04-04 07:56:51,702 [pool-26-thread-1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748: start as a follower, conf=-1: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:53:50,841 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-04-04 07:53:50,995 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-04-04 07:53:51,093 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-04-04 07:53:48,312 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-04-04 07:53:48,332 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-04-04 07:53:48,336 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 2023-04-04 07:54:12,679 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-04-04 07:54:12,682 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2_1   | STARTUP_MSG:   java = 11.0.14.1
om2_1    | , while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 2023-04-04 07:56:51,703 [pool-26-thread-1] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-04-04 07:56:51,704 [pool-26-thread-1] INFO impl.RoleInfo: b7b7229a-e604-4787-8543-e6d8cdebe63b: start b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-FollowerState
recon_1  | 2023-04-04 07:53:54,449 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
s3g_1    | 2023-04-04 07:53:52,568 [main] INFO server.AbstractConnector: Started ServerConnector@68df9280{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
dn5_1    | 2023-04-04 07:53:48,439 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-04-04 07:53:48,484 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/e22a8f610390df8589dbacbe6e6be2a64044e19f ; compiled by 'runner' on 2023-04-04T07:17Z
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 2023-04-04 07:54:12,688 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
scm2_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=true, hdds.scm.replication.event.timeout=30m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
scm1_1   | 2023-04-04 07:53:51,104 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
recon_1  | 2023-04-04 07:53:54,738 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
s3g_1    | 2023-04-04 07:53:52,568 [main] INFO server.Server: Started @39982ms
dn5_1    | 2023-04-04 07:53:48,530 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 2023-04-04 07:54:12,692 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-04-04 07:54:12,715 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
scm2_1   | ************************************************************/
scm1_1   | 2023-04-04 07:53:53,904 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1_1   | 2023-04-04 07:53:53,935 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
s3g_1    | 2023-04-04 07:53:52,570 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 2023-04-04 07:53:48,534 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm3_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=1440, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=true, hdds.scm.replication.event.timeout=30m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=false, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.maximum.response.length=134217728, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm3_1   | ************************************************************/
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm2_1   | 2023-04-04 07:54:01,792 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1_1   | 2023-04-04 07:53:53,990 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
recon_1  | 2023-04-04 07:53:54,738 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
s3g_1    | 2023-04-04 07:53:52,572 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 2023-04-04 07:53:48,681 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 2023-04-04 07:54:12,724 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-04-04 07:54:12,742 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-04-04 07:54:12,742 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-04-04 07:56:51,725 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0FCEB345A748,id=b7b7229a-e604-4787-8543-e6d8cdebe63b
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm2_1   | 2023-04-04 07:54:01,849 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-04-04 07:53:53,994 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
recon_1  | 2023-04-04 07:53:54,939 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1  | 2023-04-04 07:53:54,960 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 2023-04-04 07:53:48,871 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om2_1    | 2023-04-04 07:54:10,949 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 6734cfdf8d96/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
scm2_1   | 2023-04-04 07:54:01,888 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1_1   | 2023-04-04 07:53:53,994 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 2023-04-04 07:53:54,960 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
s3g_1    | 2023-04-04 07:53:52,574 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
dn5_1    | 2023-04-04 07:53:48,872 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-04-04 07:53:49,344 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn5_1    | 2023-04-04 07:53:49,596 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    | 2023-04-04 07:56:51,726 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 2023-04-04 07:53:54,046 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
recon_1  | 2023-04-04 07:53:55,656 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
dn5_1    | 2023-04-04 07:53:49,611 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-04-04 07:53:49,615 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-04-04 07:53:49,621 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-04-04 07:53:49,633 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om1_1    | , while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
dn3_1    | 2023-04-04 07:54:12,743 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-04-04 07:56:51,726 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 2023-04-04 07:54:17,066 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om2_1    | 2023-04-04 07:54:17,301 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-04-04 07:53:54,077 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServer: 861060d2-f30f-4260-bac0-b4e7af8aba9d: found a subdirectory /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7
recon_1  | 2023-04-04 07:53:55,666 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn5_1    | 2023-04-04 07:53:49,637 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-04-04 07:53:49,683 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a: new RaftServerImpl for group-EEB13EAC206E:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-04-04 07:53:49,689 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-04-04 07:53:49,690 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-04-04 07:54:18,150 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om1_1    | 2023-04-04 07:54:18,415 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn3_1    | 2023-04-04 07:54:12,784 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-04-04 07:56:51,726 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-04-04 07:56:51,726 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om2_1    | 2023-04-04 07:54:18,464 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm2_1   | 2023-04-04 07:54:01,898 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1_1   | 2023-04-04 07:53:54,198 [main] INFO server.RaftServer: 861060d2-f30f-4260-bac0-b4e7af8aba9d: addNew group-0BB16BE3F1B7:[] returns group-0BB16BE3F1B7:java.util.concurrent.CompletableFuture@6e1f8469[Not completed]
scm1_1   | 2023-04-04 07:53:54,362 [pool-16-thread-1] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 5#72 with transactionInfo term andIndex
scm1_1   | 2023-04-04 07:53:54,371 [pool-16-thread-1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d: new RaftServerImpl for group-0BB16BE3F1B7:[] with SCMStateMachine:uninitialized
scm1_1   | 2023-04-04 07:53:54,395 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1_1   | 2023-04-04 07:53:54,396 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 2023-04-04 07:53:55,788 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-04-04 07:53:55,788 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
om1_1    | 2023-04-04 07:54:19,831 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
dn3_1    | 2023-04-04 07:54:12,786 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-04-04 07:56:51,730 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-04-04 07:56:51,731 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-04-04 07:54:18,487 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
scm2_1   | 2023-04-04 07:54:01,951 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
scm1_1   | 2023-04-04 07:53:54,396 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1   | 2023-04-04 07:53:54,396 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1   | 2023-04-04 07:53:54,396 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-04-04 07:54:08,307 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | 2023-04-04 07:53:55,801 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
om1_1    | 2023-04-04 07:54:19,838 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
dn3_1    | 2023-04-04 07:54:12,814 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-04-04 07:56:51,743 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=6d92c706-b857-41ae-a811-0fceb345a748
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 2023-04-04 07:54:19,342 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
scm2_1   | 2023-04-04 07:54:01,952 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:37356 remote=scm2/10.9.0.15:9861]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
recon_1  | 2023-04-04 07:53:55,869 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@599a9cb2{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1    | 2023-04-04 07:54:20,838 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
dn3_1    | 2023-04-04 07:54:12,821 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-04-04 07:56:51,745 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=6d92c706-b857-41ae-a811-0fceb345a748.
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-04-04 07:54:19,471 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-04-04 07:54:19,619 [main] WARN om.OzoneManager: Prepare marker file index 106 does not match DB prepare index 105. Writing DB index to prepare file and maintaining prepared state.
scm3_1   | 2023-04-04 07:54:08,355 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-04-04 07:53:54,396 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1   | 2023-04-04 07:53:54,429 [pool-16-thread-1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1  | 2023-04-04 07:53:55,871 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bf03fee{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-04-04 07:54:12,831 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 2023-04-04 07:54:20,969 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-04-04 07:54:21,124 [main] WARN om.OzoneManager: Prepare marker file index 106 does not match DB prepare index 105. Writing DB index to prepare file and maintaining prepared state.
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:33014 remote=scm2/10.9.0.15:9861]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
scm3_1   | 2023-04-04 07:54:08,392 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3_1   | 2023-04-04 07:54:08,401 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3_1   | 2023-04-04 07:54:08,421 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
scm3_1   | 2023-04-04 07:54:08,422 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
recon_1  | 2023-04-04 07:53:59,156 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@34f32575{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-5909724953099747590/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
dn1_1    | 2023-04-04 07:56:56,762 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-FollowerState] INFO impl.FollowerState: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5058127374ns, electionTimeout:5031ms
dn1_1    | 2023-04-04 07:56:56,762 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-FollowerState] INFO impl.RoleInfo: b7b7229a-e604-4787-8543-e6d8cdebe63b: shutdown b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-FollowerState
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
scm2_1   | 2023-04-04 07:54:02,471 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-04-04 07:54:02,638 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
scm3_1   | 2023-04-04 07:54:08,892 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn3_1    | 2023-04-04 07:54:12,836 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-04-04 07:54:12,876 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e
dn3_1    | 2023-04-04 07:54:12,880 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-04-04 07:54:12,879 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b
dn1_1    | 2023-04-04 07:56:56,762 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-FollowerState] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-04-04 07:56:56,763 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-04-04 07:56:56,763 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-FollowerState] INFO impl.RoleInfo: b7b7229a-e604-4787-8543-e6d8cdebe63b: start b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
scm2_1   | 2023-04-04 07:54:02,787 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm2_1   | 2023-04-04 07:54:02,788 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm3_1   | 2023-04-04 07:54:09,142 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn3_1    | 2023-04-04 07:54:12,885 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-04-04 07:54:12,886 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-04-04 07:54:12,892 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-04-04 07:54:12,894 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-04-04 07:56:56,775 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO impl.LeaderElection: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-04-04 07:56:56,775 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO impl.LeaderElection: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
dn1_1    | 2023-04-04 07:56:56,778 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO impl.LeaderElection: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
om2_1    | 2023-04-04 07:54:19,631 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 105 to file /data/metadata/current/prepareMarker
om2_1    | 2023-04-04 07:54:19,847 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm3_1   | 2023-04-04 07:54:09,352 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
dn3_1    | 2023-04-04 07:54:12,896 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-04-04 07:54:12,898 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-04-04 07:54:12,905 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-04-04 07:54:12,883 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2
dn3_1    | 2023-04-04 07:54:12,910 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-04-04 07:56:56,779 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO impl.LeaderElection: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2 ELECTION round 0: result PASSED (term=1)
dn1_1    | 2023-04-04 07:56:56,779 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO impl.RoleInfo: b7b7229a-e604-4787-8543-e6d8cdebe63b: shutdown b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
scm2_1   | 2023-04-04 07:54:02,834 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
scm3_1   | 2023-04-04 07:54:09,354 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm3_1   | 2023-04-04 07:54:09,405 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-04-04 07:54:12,905 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-04-04 07:54:12,910 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-04-04 07:54:12,910 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-04-04 07:53:49,690 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-04-04 07:56:56,779 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn1_1    | 2023-04-04 07:56:56,780 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0FCEB345A748 with new leaderId: b7b7229a-e604-4787-8543-e6d8cdebe63b
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
om1_1    | 2023-04-04 07:54:21,125 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 105 to file /data/metadata/current/prepareMarker
scm3_1   | 2023-04-04 07:54:09,421 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:e0c91ed2-48f2-4467-9ffd-e28e64336c64
scm3_1   | 2023-04-04 07:54:09,476 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-04-04 07:54:12,910 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-04-04 07:53:49,692 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-04-04 07:53:49,698 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-04-04 07:56:56,780 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748: change Leader from null to b7b7229a-e604-4787-8543-e6d8cdebe63b at term 1 for becomeLeader, leader elected after 5143ms
dn1_1    | 2023-04-04 07:56:56,780 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 2023-04-04 07:54:11,020 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn2_1    | 2023-04-04 07:54:11,040 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn2_1    | 2023-04-04 07:54:11,273 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
om3_1    | , while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om1_1    | 2023-04-04 07:54:21,425 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3_1   | 2023-04-04 07:54:09,530 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-04-04 07:53:54,431 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
dn3_1    | 2023-04-04 07:54:12,911 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-04-04 07:53:49,703 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-04-04 07:53:49,708 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-04-04 07:56:56,781 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-04-04 07:56:56,781 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn2_1    | 2023-04-04 07:54:11,275 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis e2957b23-687f-4626-af75-9b42f3a43b99
dn2_1    | 2023-04-04 07:54:11,484 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/in_use.lock acquired by nodename 7@feac3239a846
dn2_1    | 2023-04-04 07:54:11,484 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210/in_use.lock acquired by nodename 7@feac3239a846
dn2_1    | 2023-04-04 07:54:11,508 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/in_use.lock acquired by nodename 7@feac3239a846
om3_1    | 2023-04-04 07:54:08,916 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8b40267daa06/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om1_1    | 2023-04-04 07:54:21,430 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
scm3_1   | 2023-04-04 07:54:09,531 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm3_1   | 2023-04-04 07:54:09,531 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-04-04 07:54:12,916 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-04-04 07:53:49,709 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-04-04 07:53:49,711 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-04-04 07:56:56,801 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-04-04 07:56:56,802 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
om3_1    | 2023-04-04 07:54:16,676 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om1_1    | 2023-04-04 07:54:21,621 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om1_1    | 2023-04-04 07:54:22,671 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1_1   | 2023-04-04 07:53:54,439 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-04-04 07:54:12,916 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-04-04 07:53:49,712 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-04-04 07:53:49,730 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-04-04 07:56:56,802 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
s3g_1    | 2023-04-04 07:57:07,997 [qtp1182908789-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
om3_1    | 2023-04-04 07:54:16,878 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-04-04 07:54:22,768 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3_1   | 2023-04-04 07:54:09,532 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm3_1   | 2023-04-04 07:54:09,532 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-04-04 07:54:12,917 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-04-04 07:54:12,918 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-04-04 07:54:12,920 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-04-04 07:53:49,733 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-04-04 07:53:49,735 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-04-04 07:53:49,762 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
om3_1    | 2023-04-04 07:54:18,077 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-04-04 07:54:23,128 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
scm3_1   | 2023-04-04 07:54:09,532 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3_1   | 2023-04-04 07:54:09,533 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
dn3_1    | 2023-04-04 07:54:12,921 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-04-04 07:54:12,921 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-04-04 07:54:12,918 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-04-04 07:53:49,767 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-04-04 07:54:11,512 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=14, votedFor=e2957b23-687f-4626-af75-9b42f3a43b99} from /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/raft-meta
dn2_1    | 2023-04-04 07:54:11,512 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=18, votedFor=2ebde02c-a404-41d0-92a4-7b6da490547a} from /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/raft-meta
dn4_1    | 2023-04-04 07:54:11,046 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
om3_1    | 2023-04-04 07:54:18,079 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-04-04 07:54:23,236 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:3, i:106)
scm3_1   | 2023-04-04 07:54:09,534 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-04-04 07:53:54,441 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-04-04 07:54:12,920 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-04-04 07:54:12,948 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-04-04 07:54:12,990 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-04-04 07:53:49,772 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-04-04 07:54:11,517 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=e2957b23-687f-4626-af75-9b42f3a43b99} from /data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210/current/raft-meta
s3g_1    | 2023-04-04 07:57:08,041 [qtp1182908789-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
dn4_1    | 2023-04-04 07:54:11,057 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn4_1    | 2023-04-04 07:54:11,316 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn4_1    | 2023-04-04 07:54:11,317 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn4_1    | 2023-04-04 07:54:11,343 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-04-04 07:54:18,865 [Thread-31] WARN rocksdiff.RocksDBCheckpointDiffer: Snapshot info table column family handle is not set!
om3_1    | 2023-04-04 07:54:18,878 [Thread-34] INFO rocksdiff.RocksDBCheckpointDiffer: Skipped the compaction entry. Compaction input files: [/data/metadata/om.db/000082.sst, /data/metadata/om.db/000073.sst, /data/metadata/om.db/000051.sst, /data/metadata/om.db/000047.sst] and output files: [/data/metadata/om.db/000082.sst, /data/metadata/om.db/000073.sst, /data/metadata/om.db/000051.sst, /data/metadata/om.db/000047.sst] are same.
om3_1    | 2023-04-04 07:54:18,879 [Thread-35] WARN rocksdiff.RocksDBCheckpointDiffer: Snapshot info table column family handle is not set!
om3_1    | 2023-04-04 07:54:19,354 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
dn3_1    | 2023-04-04 07:54:12,994 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-04-04 07:54:12,999 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-04-04 07:56:56,807 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-04-04 07:53:49,774 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-04-04 07:54:11,674 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: set configuration 15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | 2023-04-04 07:57:08,050 [qtp1182908789-20] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
dn4_1    | 2023-04-04 07:54:11,343 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn4_1    | java.net.ConnectException: Call From 0972042128cd/10.9.0.20 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om3_1    | 2023-04-04 07:54:19,513 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-04-04 07:54:23,668 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-04-04 07:53:54,530 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1_1   | 2023-04-04 07:53:54,547 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
recon_1  | 2023-04-04 07:53:59,174 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@7db40fd5{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1  | 2023-04-04 07:53:59,174 [Listener at 0.0.0.0/9891] INFO server.Server: Started @48292ms
dn1_1    | 2023-04-04 07:56:56,807 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-04-04 07:53:49,776 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-04-04 07:54:11,673 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: set configuration 26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | 2023-04-04 07:57:08,051 [qtp1182908789-20] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om3_1    | 2023-04-04 07:54:19,626 [main] WARN om.OzoneManager: Prepare marker file index 106 does not match DB prepare index 105. Writing DB index to prepare file and maintaining prepared state.
om1_1    | 2023-04-04 07:54:24,157 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-04-04 07:54:24,163 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-04-04 07:54:24,165 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
recon_1  | 2023-04-04 07:53:59,178 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 2023-04-04 07:53:59,178 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1  | 2023-04-04 07:53:59,179 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
dn5_1    | 2023-04-04 07:53:49,805 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a: new RaftServerImpl for group-65BC10982AC6:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-04-04 07:54:11,685 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: set configuration 3: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | 2023-04-04 07:57:09,387 [qtp1182908789-20] WARN impl.MetricsSystemImpl: S3Gateway metrics system already initialized!
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
scm2_1   | 2023-04-04 07:54:02,847 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:19de384c-7008-4d6c-8004-b0a99b5aa3c2
om2_1    | 2023-04-04 07:54:19,848 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-04-04 07:54:19,629 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 105 to file /data/metadata/current/prepareMarker
om1_1    | 2023-04-04 07:54:24,167 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-04-04 07:54:24,167 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1    | 2023-04-04 07:54:24,167 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
recon_1  | 2023-04-04 07:53:59,179 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
dn1_1    | 2023-04-04 07:56:56,807 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO impl.RoleInfo: b7b7229a-e604-4787-8543-e6d8cdebe63b: start b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderStateImpl
dn1_1    | 2023-04-04 07:56:56,808 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-04-04 07:53:49,812 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-04-04 07:54:11,734 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-04-04 07:58:05,575 [qtp1182908789-19] INFO rpc.RpcClient: Creating Bucket: s3v/new2-bucket, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false, Replication Type set to server-side default replication type, Namespace Quota set to -1, Space Quota set to -1 
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
scm2_1   | 2023-04-04 07:54:02,894 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1    | 2023-04-04 07:54:19,907 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om3_1    | 2023-04-04 07:54:19,804 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-04-04 07:54:24,171 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1    | 2023-04-04 07:54:24,180 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-04-04 07:54:24,181 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
recon_1  | 2023-04-04 07:53:59,212 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1  | 2023-04-04 07:53:59,213 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn5_1    | 2023-04-04 07:53:49,817 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-04-04 07:54:13,002 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1    | 2023-04-04 07:58:06,533 [qtp1182908789-19] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
scm2_1   | 2023-04-04 07:54:02,951 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-04-04 07:54:02,952 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-04-04 07:54:02,952 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-04-04 07:54:02,953 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-04-04 07:54:02,953 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1    | 2023-04-04 07:54:20,947 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-04-04 07:56:56,811 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6d92c706-b857-41ae-a811-0fceb345a748/current/log_inprogress_0
dn1_1    | 2023-04-04 07:56:56,832 [b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748-LeaderElection2] INFO server.RaftServer$Division: b7b7229a-e604-4787-8543-e6d8cdebe63b@group-0FCEB345A748: set configuration 0: peers:[b7b7229a-e604-4787-8543-e6d8cdebe63b|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:11,743 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO ratis.ContainerStateMachine: group-0EBF5B06395B: Setting the last applied index to (t:14, i:20)
dn2_1    | 2023-04-04 07:54:11,747 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO ratis.ContainerStateMachine: group-EEB13EAC206E: Setting the last applied index to (t:18, i:46)
dn1_1    | 2023-04-04 07:57:15,561 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm2_1   | 2023-04-04 07:54:02,953 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
om1_1    | 2023-04-04 07:54:24,182 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
scm1_1   | 2023-04-04 07:53:54,558 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-04-04 07:54:19,815 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-04-04 07:54:19,875 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om3_1    | 2023-04-04 07:54:20,992 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1    | 2023-04-04 07:54:21,039 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-04-04 07:54:13,007 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om1_1    | 2023-04-04 07:54:24,246 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-04-04 07:53:55,171 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-04-04 07:53:55,186 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1   | 2023-04-04 07:54:09,534 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3_1   | 2023-04-04 07:54:09,535 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3_1   | 2023-04-04 07:54:09,543 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-04-04 07:54:14,238 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-04-04 07:54:11,754 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO ratis.ContainerStateMachine: group-F3D51D6EC210: Setting the last applied index to (t:3, i:4)
dn2_1    | 2023-04-04 07:54:12,587 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-04-04 07:58:15,561 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 2023-04-04 07:53:59,213 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Last known snapshot for OM : /data/metadata/om.snapshot.db_1680594280718
recon_1  | 2023-04-04 07:53:59,227 [Listener at 0.0.0.0/9891] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-04-04 07:53:59,228 [Listener at 0.0.0.0/9891] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-04-04 07:53:55,187 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-04-04 07:53:55,187 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1    | 2023-04-04 07:54:21,584 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
dn3_1    | 2023-04-04 07:54:14,232 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 2023-04-04 07:54:09,546 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm3_1   | 2023-04-04 07:54:09,546 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1    | 2023-04-04 07:54:24,277 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
scm1_1   | 2023-04-04 07:53:55,191 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 2023-04-04 07:53:55,198 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
om3_1    | 2023-04-04 07:54:21,753 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:3, i:106)
dn3_1    | 2023-04-04 07:54:14,244 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-04-04 07:54:12,632 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-04-04 07:54:12,659 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-04-04 07:54:24,278 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
recon_1  | 2023-04-04 07:53:59,290 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1680594280718.
scm1_1   | 2023-04-04 07:53:55,201 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm1_1   | 2023-04-04 07:53:55,202 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-04-04 07:54:22,248 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-04-04 07:54:14,244 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-04-04 07:54:12,661 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-04-04 07:54:12,663 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-04-04 07:54:26,009 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1    | 2023-04-04 07:54:26,015 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1    | 2023-04-04 07:54:26,019 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-04-04 07:54:26,019 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-04-04 07:54:26,021 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-04-04 07:54:26,030 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm1_1   | 2023-04-04 07:53:55,247 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
scm1_1   | 2023-04-04 07:53:55,868 [main] INFO reflections.Reflections: Reflections took 474 ms to scan 3 urls, producing 128 keys and 283 values 
dn3_1    | 2023-04-04 07:54:14,225 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-04-04 07:54:22,842 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-04-04 07:54:12,655 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-04-04 07:54:12,667 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-04-04 07:54:26,067 [om1-impl-thread1] INFO server.RaftServer: om1: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-04-04 07:54:26,089 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@3dbd7107[Not completed]
scm1_1   | 2023-04-04 07:53:56,204 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
dn5_1    | 2023-04-04 07:53:49,817 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-04-04 07:53:49,817 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm2_1   | 2023-04-04 07:54:02,954 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2_1   | 2023-04-04 07:54:02,955 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-04-04 07:54:02,956 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
dn3_1    | 2023-04-04 07:54:14,369 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-04-04 07:54:22,843 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
dn2_1    | 2023-04-04 07:54:12,667 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-04-04 07:54:12,673 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-04-04 07:54:26,112 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1    | 2023-04-04 07:54:26,118 [main] INFO om.OzoneManager: Creating RPC Server
dn5_1    | 2023-04-04 07:53:49,823 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-04-04 07:54:21,015 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-04-04 07:54:21,226 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1    | 2023-04-04 07:54:21,632 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:3, i:106)
om2_1    | 2023-04-04 07:54:22,170 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-04-04 07:54:14,370 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1    | 2023-04-04 07:54:22,855 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-04-04 07:54:12,667 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-04-04 07:54:12,683 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-04-04 07:54:26,337 [pool-29-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
scm1_1   | 2023-04-04 07:53:56,381 [main] INFO node.SCMNodeManager: Entering startup safe mode.
dn5_1    | 2023-04-04 07:53:49,835 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1  | 2023-04-04 07:53:59,338 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
om2_1    | 2023-04-04 07:54:22,845 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-04-04 07:54:02,956 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-04-04 07:54:02,966 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-04-04 07:54:14,370 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
om3_1    | 2023-04-04 07:54:22,861 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
dn2_1    | 2023-04-04 07:54:12,691 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm3_1   | 2023-04-04 07:54:09,833 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1    | 2023-04-04 07:54:26,391 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-04-04 07:53:56,408 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
dn5_1    | 2023-04-04 07:53:49,844 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-04-04 07:53:49,859 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-04-04 07:53:49,883 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-04-04 07:54:22,877 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 2023-04-04 07:54:14,445 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1   | 2023-04-04 07:54:02,969 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm2_1   | 2023-04-04 07:54:02,969 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-04-04 07:54:12,692 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-04-04 07:54:12,696 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-04-04 07:54:26,408 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-04-04 07:53:56,409 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn5_1    | 2023-04-04 07:53:49,884 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
recon_1  | 2023-04-04 07:53:59,339 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
om2_1    | 2023-04-04 07:54:22,879 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-04-04 07:54:22,880 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-04-04 07:54:22,886 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-04-04 07:54:14,472 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 2023-04-04 07:54:03,153 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-04-04 07:54:12,701 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-04-04 07:54:12,705 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-04-04 07:54:26,409 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1   | 2023-04-04 07:53:56,569 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
dn5_1    | 2023-04-04 07:53:49,895 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
om2_1    | 2023-04-04 07:54:22,887 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
recon_1  | 2023-04-04 07:54:07,040 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:861060d2-f30f-4260-bac0-b4e7af8aba9d is not the leader. Could not determine the leader node.
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
dn3_1    | 2023-04-04 07:54:14,475 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm2_1   | 2023-04-04 07:54:03,156 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-04-04 07:54:12,707 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-04-04 07:54:12,716 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om1_1    | 2023-04-04 07:54:26,409 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
scm1_1   | 2023-04-04 07:53:56,572 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn5_1    | 2023-04-04 07:53:49,906 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om3_1    | 2023-04-04 07:54:22,868 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1    | 2023-04-04 07:54:22,871 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
dn3_1    | 2023-04-04 07:54:14,630 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:03,156 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-04-04 07:54:12,727 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1   | 2023-04-04 07:54:09,836 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1    | 2023-04-04 07:54:26,410 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-04-04 07:53:56,585 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
dn5_1    | 2023-04-04 07:53:49,908 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-04-04 07:54:22,872 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1    | 2023-04-04 07:54:22,886 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-04-04 07:54:09,837 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-04-04 07:54:14,635 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-04-04 07:54:26,410 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | 2023-04-04 07:54:09,837 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-04-04 07:54:09,837 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-04-04 07:54:09,840 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1   | 2023-04-04 07:54:09,847 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServer: e0c91ed2-48f2-4467-9ffd-e28e64336c64: found a subdirectory /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7
scm3_1   | 2023-04-04 07:54:09,857 [main] INFO server.RaftServer: e0c91ed2-48f2-4467-9ffd-e28e64336c64: addNew group-0BB16BE3F1B7:[] returns group-0BB16BE3F1B7:java.util.concurrent.CompletableFuture@2e380628[Not completed]
dn5_1    | 2023-04-04 07:53:49,909 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-04-04 07:53:49,952 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-04-04 07:53:49,952 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-04-04 07:53:49,954 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-04-04 07:54:14,634 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:14,705 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2/current/log_0-0
scm3_1   | 2023-04-04 07:54:09,888 [pool-16-thread-1] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 5#72 with transactionInfo term andIndex
scm3_1   | 2023-04-04 07:54:09,889 [pool-16-thread-1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64: new RaftServerImpl for group-0BB16BE3F1B7:[] with SCMStateMachine:uninitialized
scm3_1   | 2023-04-04 07:54:09,892 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3_1   | 2023-04-04 07:54:09,892 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm3_1   | 2023-04-04 07:54:09,892 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-04-04 07:53:49,958 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 2023-04-04 07:54:09,892 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-04-04 07:54:09,893 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-04-04 07:54:09,893 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | 2023-04-04 07:54:09,899 [pool-16-thread-1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3_1   | 2023-04-04 07:54:09,899 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1   | 2023-04-04 07:54:09,902 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 2023-04-04 07:54:12,727 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-04-04 07:54:12,727 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-04-04 07:54:12,732 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-04-04 07:54:26,604 [pool-29-thread-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1    | 2023-04-04 07:54:26,611 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-04-04 07:54:26,755 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1    | 2023-04-04 07:54:26,767 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1    | 2023-04-04 07:54:27,204 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
scm3_1   | 2023-04-04 07:54:09,902 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1   | 2023-04-04 07:54:09,926 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-04-04 07:54:12,740 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:12,745 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | 2023-04-04 07:54:22,896 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:197)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:64993)
om1_1    | 2023-04-04 07:54:27,310 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
scm3_1   | 2023-04-04 07:54:09,931 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1   | 2023-04-04 07:53:56,586 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | 2023-04-04 07:53:56,599 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm1_1   | 2023-04-04 07:53:56,602 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1_1   | 2023-04-04 07:53:56,610 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
scm3_1   | 2023-04-04 07:54:09,932 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1   | 2023-04-04 07:54:10,169 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-04-04 07:53:56,613 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1_1   | 2023-04-04 07:53:56,729 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1_1   | 2023-04-04 07:53:56,763 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
dn5_1    | 2023-04-04 07:53:49,935 [main] INFO util.log: Logging initialized @37033ms to org.eclipse.jetty.util.log.Slf4jLog
dn5_1    | 2023-04-04 07:53:50,845 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn5_1    | 2023-04-04 07:53:50,882 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn5_1    | 2023-04-04 07:53:50,948 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 2023-04-04 07:53:50,964 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn5_1    | 2023-04-04 07:53:50,969 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-04-04 07:54:12,752 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn5_1    | 2023-04-04 07:53:50,970 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn5_1    | 2023-04-04 07:53:51,295 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn5_1    | 2023-04-04 07:53:51,356 [main] INFO http.HttpServer2: Jetty bound to port 9882
om1_1    | 2023-04-04 07:54:27,314 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1    | 2023-04-04 07:54:28,380 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-04-04 07:54:12,782 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e
dn2_1    | 2023-04-04 07:54:12,784 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 2023-04-04 07:54:28,389 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1    | 2023-04-04 07:54:28,394 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-04-04 07:54:12,782 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b
dn2_1    | 2023-04-04 07:54:12,792 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
scm2_1   | 2023-04-04 07:54:03,156 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2_1   | 2023-04-04 07:54:03,156 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 2023-04-04 07:54:28,401 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-04-04 07:54:12,792 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-04-04 07:54:12,793 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn4_1    | 	... 12 more
scm2_1   | 2023-04-04 07:54:03,159 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 2023-04-04 07:54:28,405 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | 2023-04-04 07:54:10,170 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-04-04 07:53:51,365 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn5_1    | 2023-04-04 07:53:51,907 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm2_1   | 2023-04-04 07:54:03,162 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServer: 19de384c-7008-4d6c-8004-b0a99b5aa3c2: found a subdirectory /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7
scm2_1   | 2023-04-04 07:54:03,170 [main] INFO server.RaftServer: 19de384c-7008-4d6c-8004-b0a99b5aa3c2: addNew group-0BB16BE3F1B7:[] returns group-0BB16BE3F1B7:java.util.concurrent.CompletableFuture@2e380628[Not completed]
scm1_1   | 2023-04-04 07:53:56,870 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm1_1   | 2023-04-04 07:53:56,907 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1_1   | 2023-04-04 07:53:56,911 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
dn3_1    | 2023-04-04 07:54:14,757 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_0-4
dn3_1    | 2023-04-04 07:54:15,463 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: set configuration 1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:12,794 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-04-04 07:54:12,797 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om1_1    | 2023-04-04 07:54:29,737 [main] INFO reflections.Reflections: Reflections took 3198 ms to scan 8 urls, producing 23 keys and 589 values [using 2 cores]
om1_1    | 2023-04-04 07:54:30,264 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 2023-04-04 07:54:30,340 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1    | 2023-04-04 07:54:30,682 [Listener at om1/9862] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
om1_1    | 2023-04-04 07:54:31,064 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-04-04 07:54:15,464 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2/current/log_1-2
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 2023-04-04 07:54:12,805 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-04-04 07:54:12,806 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1   | 2023-04-04 07:54:03,189 [pool-16-thread-1] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 5#72 with transactionInfo term andIndex
om1_1    | 2023-04-04 07:54:31,114 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 2023-04-04 07:53:51,909 [main] INFO server.session: No SessionScavenger set, using defaults
scm1_1   | WARNING: An illegal reflective access operation has occurred
dn4_1    | 2023-04-04 07:54:11,548 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410/in_use.lock acquired by nodename 6@0972042128cd
dn4_1    | 2023-04-04 07:54:11,619 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=14df8ad4-ce2f-4483-aada-29ab5cae3ffb} from /data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410/current/raft-meta
om3_1    | 2023-04-04 07:54:22,902 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-04-04 07:54:23,014 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-04-04 07:54:15,464 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_0-7
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 2023-04-04 07:54:12,811 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-04-04 07:54:12,815 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2_1   | 2023-04-04 07:54:03,190 [pool-16-thread-1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2: new RaftServerImpl for group-0BB16BE3F1B7:[] with SCMStateMachine:uninitialized
om1_1    | 2023-04-04 07:54:31,118 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
dn5_1    | 2023-04-04 07:53:51,963 [main] INFO server.session: node0 Scavenging every 600000ms
scm1_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
om3_1    | 2023-04-04 07:54:23,039 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1    | 2023-04-04 07:54:23,044 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-04-04 07:54:15,479 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: set configuration 8: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:15,492 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: set configuration 3: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:12,806 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-04-04 07:54:12,815 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1   | 2023-04-04 07:54:03,192 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
om1_1    | 2023-04-04 07:54:31,364 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/10.9.0.11:9862
dn5_1    | 2023-04-04 07:53:52,078 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@bb3ecfe{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm1_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm1_1   | WARNING: All illegal access operations will be denied in a future release
om3_1    | 2023-04-04 07:54:24,924 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1    | 2023-04-04 07:54:24,964 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-04-04 07:54:15,494 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2/current/log_inprogress_3
dn3_1    | 2023-04-04 07:54:15,501 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO segmented.LogSegment: Successfully read 18 entries from segment file /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_8-25
dn2_1    | 2023-04-04 07:54:12,817 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-04-04 07:54:12,817 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2_1   | 2023-04-04 07:54:03,192 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-04-04 07:54:31,367 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
dn5_1    | 2023-04-04 07:53:52,086 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@689faf79{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn5_1    | 2023-04-04 07:53:52,871 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@46bb0bdf{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-8033967873428301834/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn5_1    | 2023-04-04 07:53:52,916 [main] INFO server.AbstractConnector: Started ServerConnector@6aa7e176{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
om3_1    | 2023-04-04 07:54:24,966 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1    | 2023-04-04 07:54:22,887 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
dn3_1    | 2023-04-04 07:54:15,527 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: set configuration 5: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:15,592 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO segmented.LogSegment: Successfully read 10 entries from segment file /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_5-14
dn2_1    | 2023-04-04 07:54:12,817 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-04-04 07:54:12,824 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-04-04 07:54:12,837 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210
scm2_1   | 2023-04-04 07:54:03,192 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2_1   | 2023-04-04 07:54:03,192 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om3_1    | 2023-04-04 07:54:24,972 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-04-04 07:54:24,973 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-04-04 07:54:15,514 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn3_1    | 2023-04-04 07:54:15,507 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:12,841 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-04-04 07:54:12,841 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-04-04 07:54:12,843 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-04-04 07:53:52,917 [main] INFO server.Server: Started @40014ms
dn5_1    | 2023-04-04 07:53:52,922 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 2023-04-04 07:53:52,926 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1    | 2023-04-04 07:54:22,916 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-04-04 07:54:25,013 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-04-04 07:54:25,039 [om3-impl-thread1] INFO server.RaftServer: om3: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-04-04 07:54:25,122 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@1129829c[Not completed]
dn2_1    | 2023-04-04 07:54:12,846 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-04-04 07:54:12,846 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-04-04 07:54:12,847 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2_1   | 2023-04-04 07:54:03,192 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-04-04 07:54:03,193 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2_1   | 2023-04-04 07:54:03,199 [pool-16-thread-1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-04-04 07:54:22,921 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1    | 2023-04-04 07:54:25,126 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn2_1    | 2023-04-04 07:54:12,848 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-04-04 07:54:12,850 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-04-04 07:54:12,890 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm2_1   | 2023-04-04 07:54:03,199 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1   | 2023-04-04 07:53:56,932 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
dn5_1    | 2023-04-04 07:53:52,934 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn5_1    | 2023-04-04 07:53:52,958 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn5_1    | 2023-04-04 07:53:53,100 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn5_1    | 2023-04-04 07:53:53,102 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om2_1    | 2023-04-04 07:54:22,930 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
recon_1  | , while invoking $Proxy45.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
recon_1  | 2023-04-04 07:54:09,230 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:19de384c-7008-4d6c-8004-b0a99b5aa3c2 is not the leader. Could not determine the leader node.
dn2_1    | 2023-04-04 07:54:12,895 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-04-04 07:54:12,896 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-04-04 07:54:12,903 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm2_1   | 2023-04-04 07:54:03,203 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1   | 2023-04-04 07:53:56,937 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
om3_1    | 2023-04-04 07:54:25,160 [main] INFO om.OzoneManager: Creating RPC Server
om3_1    | 2023-04-04 07:54:25,284 [pool-29-thread-1] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1    | 2023-04-04 07:54:25,327 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1    | 2023-04-04 07:54:25,327 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | 2023-04-04 07:54:23,093 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
dn2_1    | 2023-04-04 07:54:12,914 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-04-04 07:54:10,170 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 2023-04-04 07:54:10,171 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1   | 2023-04-04 07:54:03,203 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-04-04 07:53:56,940 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
om3_1    | 2023-04-04 07:54:25,332 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1    | 2023-04-04 07:54:25,346 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-04-04 07:54:25,350 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-04-04 07:54:25,353 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-04-04 07:54:23,149 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:197)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:64993)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
dn2_1    | 2023-04-04 07:54:14,381 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-04-04 07:54:03,215 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1_1   | 2023-04-04 07:53:57,028 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm1_1   | 2023-04-04 07:53:58,011 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-04-04 07:53:58,047 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-04-04 07:53:58,105 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1_1   | 2023-04-04 07:53:58,151 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om2_1    | 2023-04-04 07:54:23,160 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 2023-04-04 07:54:31,389 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 6@cb73d2d52b76
om1_1    | 2023-04-04 07:54:31,403 [om1-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=om1} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
dn3_1    | 2023-04-04 07:54:15,601 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: set configuration 15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:14,391 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1   | 2023-04-04 07:54:03,218 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2_1   | 2023-04-04 07:54:03,218 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1   | 2023-04-04 07:54:03,359 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-04-04 07:53:58,154 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-04-04 07:53:58,164 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm1_1   | 2023-04-04 07:53:58,197 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om2_1    | 2023-04-04 07:54:25,352 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om1_1    | 2023-04-04 07:54:31,538 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-04-04 07:54:31,544 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-04-04 07:54:15,612 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn2_1    | 2023-04-04 07:54:14,392 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 2023-04-04 07:54:03,360 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-04-04 07:54:11,788 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: set configuration 3: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:54:11,882 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO ratis.ContainerStateMachine: group-6617081AE410: Setting the last applied index to (t:3, i:4)
dn4_1    | 2023-04-04 07:54:12,353 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:53:53,146 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@64454b56] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1    | 2023-04-04 07:54:25,386 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om1_1    | 2023-04-04 07:54:31,619 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-04-04 07:54:31,619 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-04-04 07:54:15,613 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO segmented.LogSegment: Successfully read 6 entries from segment file /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_inprogress_15
dn2_1    | 2023-04-04 07:54:14,401 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-04-04 07:54:14,400 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-04-04 07:54:03,362 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1   | 2023-04-04 07:54:03,367 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-04-04 07:54:12,485 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-04-04 07:54:12,551 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-04-04 07:54:12,551 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-04-04 07:54:25,394 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 2023-04-04 07:54:10,171 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-04-04 07:54:15,632 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 20
om3_1    | 2023-04-04 07:54:25,468 [pool-29-thread-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 2023-04-04 07:54:25,483 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm2_1   | 2023-04-04 07:54:03,368 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1   | 2023-04-04 07:54:03,372 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
dn4_1    | 2023-04-04 07:54:12,554 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-04-04 07:54:12,560 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-04-04 07:53:53,565 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
om2_1    | 2023-04-04 07:54:25,401 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-04-04 07:54:25,402 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-04-04 07:54:25,445 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 2023-04-04 07:54:15,632 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 14
dn3_1    | 2023-04-04 07:54:15,543 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: set configuration 26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:03,372 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2_1   | 2023-04-04 07:54:03,372 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
dn5_1    | 2023-04-04 07:53:53,631 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
om2_1    | 2023-04-04 07:54:25,491 [om2-impl-thread1] INFO server.RaftServer: om2: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
scm3_1   | 2023-04-04 07:54:10,172 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3_1   | 2023-04-04 07:54:10,172 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3_1   | 2023-04-04 07:54:10,172 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3_1   | 2023-04-04 07:54:10,198 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
om1_1    | 2023-04-04 07:54:31,630 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-04-04 07:54:31,638 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-04-04 07:54:31,649 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-04-04 07:54:31,678 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 2023-04-04 07:54:31,681 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-04-04 07:54:31,718 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-04-04 07:54:31,723 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1    | 2023-04-04 07:54:31,728 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1    | 2023-04-04 07:54:31,735 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-04-04 07:54:31,737 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1    | 2023-04-04 07:54:31,740 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-04-04 07:54:31,744 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   | 2023-04-04 07:54:10,722 [main] INFO reflections.Reflections: Reflections took 440 ms to scan 3 urls, producing 128 keys and 283 values 
scm3_1   | 2023-04-04 07:54:10,885 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3_1   | 2023-04-04 07:54:11,182 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3_1   | 2023-04-04 07:54:11,281 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
om3_1    | 2023-04-04 07:54:25,562 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1   | 2023-04-04 07:53:58,217 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn2_1    | 2023-04-04 07:54:14,439 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@276535d3] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1312ms
dn2_1    | GC pool 'ParNew' had collection(s): count=1 time=84ms
dn2_1    | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1307ms
scm2_1   | 2023-04-04 07:54:03,393 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = WEBUI_PORTS_IN_DATANODEDETAILS (version = 6)
dn5_1    | 2023-04-04 07:53:56,385 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:53:56,385 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:53:56,385 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:53:57,386 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:53:57,387 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-04-04 07:54:25,563 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-04-04 07:53:58,220 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
recon_1  | , while invoking $Proxy45.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
dn3_1    | 2023-04-04 07:54:15,669 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO segmented.LogSegment: Successfully read 21 entries from segment file /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_inprogress_26
dn3_1    | 2023-04-04 07:54:15,705 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 46
dn3_1    | 2023-04-04 07:54:15,706 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 25
scm2_1   | 2023-04-04 07:54:03,577 [main] INFO reflections.Reflections: Reflections took 131 ms to scan 3 urls, producing 128 keys and 283 values 
dn5_1    | 2023-04-04 07:53:57,387 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-04-04 07:54:11,304 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3_1   | 2023-04-04 07:54:11,877 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm3_1   | 2023-04-04 07:54:11,883 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3_1   | 2023-04-04 07:54:11,928 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
om3_1    | 2023-04-04 07:54:25,878 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
scm1_1   | 2023-04-04 07:53:58,318 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
recon_1  | 2023-04-04 07:54:11,234 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 0d461f16e2c4/10.9.0.22 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy45.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
dn4_1    | 2023-04-04 07:54:12,582 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-04-04 07:54:14,467 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-04-04 07:54:14,480 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 2023-04-04 07:54:03,686 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3_1   | 2023-04-04 07:54:11,931 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3_1   | 2023-04-04 07:54:11,960 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3_1   | 2023-04-04 07:54:11,961 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm3_1   | 2023-04-04 07:54:11,974 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
dn5_1    | 2023-04-04 07:53:58,387 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:53:58,387 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-04-04 07:53:58,319 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm1_1   | Container Balancer status:
dn4_1    | 2023-04-04 07:54:12,639 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-04-04 07:54:12,651 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-04-04 07:54:12,711 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410
scm2_1   | 2023-04-04 07:54:03,729 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3_1   | 2023-04-04 07:54:11,980 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm3_1   | 2023-04-04 07:54:12,286 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3_1   | 2023-04-04 07:54:12,488 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3_1   | 2023-04-04 07:54:12,804 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
dn5_1    | 2023-04-04 07:53:58,388 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-04-04 07:54:25,985 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
recon_1  | 2023-04-04 07:54:17,008 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 7 pipelines from SCM.
scm1_1   | Key                            Value
scm1_1   | Running                        false
scm1_1   | Container Balancer Configuration values:
scm1_1   | Key                                                Value
scm2_1   | 2023-04-04 07:54:03,738 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm3_1   | 2023-04-04 07:54:13,014 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3_1   | 2023-04-04 07:54:13,017 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
om2_1    | 2023-04-04 07:54:25,568 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@101cf747[Not completed]
om2_1    | 2023-04-04 07:54:25,568 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
dn5_1    | 2023-04-04 07:53:59,387 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-04-04 07:54:25,994 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | Threshold                                          10
dn3_1    | 2023-04-04 07:54:16,613 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:16,830 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO raftlog.RaftLog: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLog: commitIndex: updateToMax old=46, new=45, updated? false
dn3_1    | 2023-04-04 07:54:16,843 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: start as a follower, conf=26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:16,848 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: changes role from      null to FOLLOWER at term 18 for startAsFollower
scm2_1   | 2023-04-04 07:54:03,740 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
om1_1    | 2023-04-04 07:54:31,746 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1    | 2023-04-04 07:54:31,748 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1    | 2023-04-04 07:54:25,575 [main] INFO om.OzoneManager: Creating RPC Server
om2_1    | 2023-04-04 07:54:25,958 [pool-29-thread-1] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
dn5_1    | 2023-04-04 07:53:59,389 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-04-04 07:54:27,157 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | Max Datanodes to Involve per Iteration(percent)    20
dn3_1    | 2023-04-04 07:54:16,856 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO raftlog.RaftLog: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLog: commitIndex: updateToMax old=20, new=19, updated? false
recon_1  | 2023-04-04 07:54:17,008 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
dn2_1    | 2023-04-04 07:54:14,481 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-04-04 07:54:14,547 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1   | 2023-04-04 07:54:03,800 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
om1_1    | 2023-04-04 07:54:31,828 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1    | 2023-04-04 07:54:31,831 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-04-04 07:54:25,972 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1    | 2023-04-04 07:54:26,000 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-04-04 07:54:00,389 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-04-04 07:54:27,176 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1   | Max Size to Move per Iteration                     500GB
dn3_1    | 2023-04-04 07:54:16,892 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: start as a follower, conf=15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-04-04 07:54:17,216 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
dn4_1    | 2023-04-04 07:54:12,716 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-04-04 07:54:12,720 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm2_1   | 2023-04-04 07:54:03,800 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
om1_1    | 2023-04-04 07:54:31,863 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-04-04 07:54:31,864 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-04-04 07:54:31,868 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1    | 2023-04-04 07:54:31,889 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 106
dn5_1    | 2023-04-04 07:54:00,389 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-04-04 07:54:27,177 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1    | 2023-04-04 07:54:27,188 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1    | 2023-04-04 07:54:27,204 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 2023-04-04 07:54:17,227 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
dn2_1    | 2023-04-04 07:54:14,549 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-04-04 07:54:12,730 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-04-04 07:54:12,731 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-04-04 07:54:12,736 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1    | 2023-04-04 07:54:26,000 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1    | 2023-04-04 07:54:26,000 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-04-04 07:54:26,000 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-04-04 07:54:00,424 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
om3_1    | 2023-04-04 07:54:29,095 [main] INFO reflections.Reflections: Reflections took 3599 ms to scan 8 urls, producing 23 keys and 589 values [using 2 cores]
scm1_1   | Max Size Entering Target per Iteration             26GB
dn3_1    | 2023-04-04 07:54:16,892 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: changes role from      null to FOLLOWER at term 14 for startAsFollower
recon_1  | 2023-04-04 07:54:17,231 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 2023-04-04 07:54:17,232 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
om1_1    | 2023-04-04 07:54:31,891 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-04-04 07:54:14,552 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-04-04 07:54:12,738 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1    | 2023-04-04 07:54:26,001 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-04-04 07:54:26,035 [pool-29-thread-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-04-04 07:54:26,050 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | java.net.SocketTimeoutException: Call From 7e8229df9409/10.9.0.21 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:47844 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
om3_1    | 2023-04-04 07:54:29,758 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | Max Size Leaving Source per Iteration              26GB
dn3_1    | 2023-04-04 07:54:16,893 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState
recon_1  | 2023-04-04 07:54:18,582 [IPC Server handler 13 on default port 9891] WARN ipc.Server: IPC Server handler 13 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:50008: output error
recon_1  | 2023-04-04 07:54:18,583 [IPC Server handler 13 on default port 9891] INFO ipc.Server: IPC Server handler 13 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn2_1    | 2023-04-04 07:54:15,499 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:12,738 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-04-04 07:54:26,146 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-04-04 07:54:26,149 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 2023-04-04 07:54:26,411 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om3_1    | 2023-04-04 07:54:29,813 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
scm1_1   | 
dn3_1    | 2023-04-04 07:54:16,918 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0EBF5B06395B,id=c1077390-d65b-4523-9cd4-abe9e2c9eb94
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn2_1    | 2023-04-04 07:54:15,528 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:54:12,745 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-04-04 07:54:12,838 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm3_1   | WARNING: An illegal reflective access operation has occurred
scm3_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om1_1    | 2023-04-04 07:54:31,896 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-04-04 07:54:31,897 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn2_1    | 2023-04-04 07:54:15,546 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: set configuration 0: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-04-04 07:54:26,432 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1    | 2023-04-04 07:54:26,440 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1    | 2023-04-04 07:54:27,672 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-04-04 07:54:30,109 [Listener at om3/9862] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
scm3_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm3_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 2023-04-04 07:53:58,320 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1_1   | 2023-04-04 07:53:58,330 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om1_1    | 2023-04-04 07:54:31,903 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-04-04 07:54:31,930 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
dn2_1    | 2023-04-04 07:54:15,546 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om2_1    | 2023-04-04 07:54:27,701 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1    | 2023-04-04 07:54:27,705 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1    | 2023-04-04 07:54:30,568 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3_1   | WARNING: All illegal access operations will be denied in a future release
scm3_1   | 2023-04-04 07:54:13,135 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 2023-04-04 07:53:58,333 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
scm1_1   | 2023-04-04 07:53:58,334 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
om1_1    | 2023-04-04 07:54:31,948 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-04-04 07:54:31,948 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-04-04 07:54:15,572 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210/current/log_0-0
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
om2_1    | 2023-04-04 07:54:27,708 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1    | 2023-04-04 07:54:27,721 [pool-29-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1    | 2023-04-04 07:54:30,633 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm3_1   | 2023-04-04 07:54:13,159 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
scm3_1   | 2023-04-04 07:54:13,178 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 2023-04-04 07:53:58,340 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1_1   | 2023-04-04 07:53:58,364 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/in_use.lock acquired by nodename 7@4129f13dc7cc
om1_1    | 2023-04-04 07:54:31,950 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1    | 2023-04-04 07:54:31,950 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-04-04 07:54:31,962 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1    | 2023-04-04 07:54:31,964 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1    | 2023-04-04 07:54:29,928 [main] INFO reflections.Reflections: Reflections took 3888 ms to scan 8 urls, producing 23 keys and 589 values [using 2 cores]
om2_1    | 2023-04-04 07:54:30,603 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-04-04 07:54:30,633 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
scm3_1   | 2023-04-04 07:54:13,850 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm1_1   | 2023-04-04 07:53:58,382 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=861060d2-f30f-4260-bac0-b4e7af8aba9d} from /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/raft-meta
scm1_1   | 2023-04-04 07:53:58,530 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: set configuration 59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:53:58,543 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-04-04 07:54:15,579 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: set configuration 1: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:15,633 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210/current/log_1-2
om2_1    | 2023-04-04 07:54:30,648 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1    | 2023-04-04 07:54:30,903 [Listener at om2/9862] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
om3_1    | 2023-04-04 07:54:30,993 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/10.9.0.13:9862
scm3_1   | 2023-04-04 07:54:20,011 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
om1_1    | 2023-04-04 07:54:31,992 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
scm1_1   | 2023-04-04 07:53:58,592 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om2_1    | 2023-04-04 07:54:31,323 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1    | 2023-04-04 07:54:31,389 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-04-04 07:54:15,643 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_0-4
om3_1    | 2023-04-04 07:54:30,993 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
scm3_1   | 2023-04-04 07:54:20,247 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-04-04 07:54:03,805 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2_1   | 2023-04-04 07:54:03,805 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2_1   | 2023-04-04 07:54:03,807 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm1_1   | 2023-04-04 07:53:58,593 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
om2_1    | 2023-04-04 07:54:31,390 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1    | 2023-04-04 07:54:31,644 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/10.9.0.12:9862
dn2_1    | 2023-04-04 07:54:15,668 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: set configuration 5: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:15,639 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_0-7
scm3_1   | 2023-04-04 07:54:20,895 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2_1   | 2023-04-04 07:54:03,809 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm2_1   | 2023-04-04 07:54:03,813 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm2_1   | 2023-04-04 07:54:03,814 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1_1   | 2023-04-04 07:53:58,602 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
om2_1    | 2023-04-04 07:54:31,650 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1    | 2023-04-04 07:54:31,681 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@6734cfdf8d96
dn2_1    | 2023-04-04 07:54:15,691 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: set configuration 8: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:21,623 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-04-04 07:54:21,708 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-04-04 07:54:03,844 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
om1_1    | 2023-04-04 07:54:32,239 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1    | 2023-04-04 07:54:32,247 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
scm1_1   | 2023-04-04 07:53:58,607 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
om2_1    | 2023-04-04 07:54:31,709 [om2-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=om2} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
dn2_1    | 2023-04-04 07:54:15,705 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO segmented.LogSegment: Successfully read 10 entries from segment file /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_5-14
scm3_1   | 2023-04-04 07:54:21,751 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
om3_1    | 2023-04-04 07:54:31,007 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@8b40267daa06
om1_1    | 2023-04-04 07:54:32,268 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1    | 2023-04-04 07:54:32,477 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
scm1_1   | 2023-04-04 07:53:58,620 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn4_1    | 2023-04-04 07:54:12,843 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
om2_1    | 2023-04-04 07:54:31,867 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:15,730 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: set configuration 15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:22,146 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om3_1    | 2023-04-04 07:54:31,038 [om3-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=om2} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om1_1    | 2023-04-04 07:54:32,478 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm3_1   | 2023-04-04 07:54:22,209 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn3_1    | 2023-04-04 07:54:16,933 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-04-04 07:54:16,936 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-04-04 07:54:16,937 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-04-04 07:54:16,938 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om1_1    | 2023-04-04 07:54:32,911 [Listener at om1/9862] INFO util.log: Logging initialized @83364ms to org.eclipse.jetty.util.log.Slf4jLog
scm2_1   | 2023-04-04 07:54:03,855 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2_1   | 2023-04-04 07:54:03,885 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm3_1   | 2023-04-04 07:54:22,220 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-04-04 07:54:16,950 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-04-04 07:54:16,959 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO raftlog.RaftLog: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn3_1    | 2023-04-04 07:54:17,023 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: start as a follower, conf=3: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:17,023 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: changes role from      null to FOLLOWER at term 3 for startAsFollower
om1_1    | 2023-04-04 07:54:33,630 [Listener at om1/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-04-04 07:54:17,031 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn4_1    | 2023-04-04 07:54:12,900 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-04-04 07:54:12,906 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-04-04 07:54:33,675 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1    | 2023-04-04 07:54:31,140 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-04-04 07:54:31,146 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   | 2023-04-04 07:54:22,485 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn5_1    | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
om2_1    | 2023-04-04 07:54:31,872 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | 2023-04-04 07:54:31,930 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn4_1    | 2023-04-04 07:54:12,907 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-04-04 07:54:13,183 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: set configuration 0: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-04-04 07:54:33,723 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1    | 2023-04-04 07:54:31,195 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | 2023-04-04 07:54:31,197 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-04-04 07:54:22,507 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm3_1   | Container Balancer status:
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 2023-04-04 07:54:13,193 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410/current/log_0-0
dn4_1    | 2023-04-04 07:54:13,216 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: set configuration 1: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-04-04 07:54:33,734 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om1_1    | 2023-04-04 07:54:33,735 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om1_1    | 2023-04-04 07:54:33,738 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-04-04 07:54:15,714 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO segmented.LogSegment: Successfully read 18 entries from segment file /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_8-25
scm3_1   | Key                            Value
scm3_1   | Running                        false
om2_1    | 2023-04-04 07:54:31,935 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-04-04 07:54:31,938 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-04-04 07:54:13,242 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410/current/log_1-2
dn4_1    | 2023-04-04 07:54:13,253 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: set configuration 3: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-04-04 07:54:34,019 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om1_1    | 2023-04-04 07:54:34,049 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1    | 2023-04-04 07:54:34,054 [Listener at om1/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn2_1    | 2023-04-04 07:54:15,734 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: set configuration 26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | Container Balancer Configuration values:
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
om2_1    | 2023-04-04 07:54:31,944 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1    | 2023-04-04 07:54:31,960 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 2023-04-04 07:54:13,257 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410/current/log_inprogress_3
scm1_1   | 2023-04-04 07:53:58,643 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-04-04 07:53:58,649 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-04-04 07:53:58,679 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7
scm1_1   | 2023-04-04 07:53:58,683 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
dn2_1    | 2023-04-04 07:54:15,732 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO segmented.LogSegment: Successfully read 6 entries from segment file /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_inprogress_15
scm3_1   | Key                                                Value
scm2_1   | 2023-04-04 07:54:03,896 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn4_1    | 2023-04-04 07:54:13,266 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
scm1_1   | 2023-04-04 07:53:58,706 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-04-04 07:53:58,715 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-04-04 07:54:31,203 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1    | 2023-04-04 07:54:31,213 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1    | 2023-04-04 07:54:31,220 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | Threshold                                          10
scm2_1   | 2023-04-04 07:54:03,899 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:47844 remote=recon/10.9.0.22:9891]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
recon_1  | 2023-04-04 07:54:18,600 [IPC Server handler 24 on default port 9891] WARN ipc.Server: IPC Server handler 24 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:35032: output error
dn4_1    | 2023-04-04 07:54:13,267 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn4_1    | 2023-04-04 07:54:13,361 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:15,770 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: set configuration 3: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-04-04 07:54:31,265 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | 2023-04-04 07:54:31,267 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1    | 2023-04-04 07:54:31,313 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
scm3_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2_1   | WARNING: An illegal reflective access operation has occurred
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
recon_1  | 2023-04-04 07:54:18,600 [IPC Server handler 24 on default port 9891] INFO ipc.Server: IPC Server handler 24 on default port 9891 caught an exception
scm1_1   | 2023-04-04 07:53:58,719 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-04-04 07:53:58,723 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-04-04 07:53:58,732 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1    | 2023-04-04 07:54:31,319 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1  | java.nio.channels.ClosedChannelException
dn2_1    | 2023-04-04 07:54:15,775 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210/current/log_inprogress_3
dn2_1    | 2023-04-04 07:54:15,776 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 20
om3_1    | 2023-04-04 07:54:31,321 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-04-04 07:53:58,732 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
om2_1    | 2023-04-04 07:54:31,983 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 2023-04-04 07:54:34,266 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1    | 2023-04-04 07:54:34,267 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-04-04 07:54:15,779 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn2_1    | 2023-04-04 07:54:15,779 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
om3_1    | 2023-04-04 07:54:31,326 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-04-04 07:53:58,733 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm3_1   | Max Size to Move per Iteration                     500GB
dn3_1    | 2023-04-04 07:54:17,032 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState
scm2_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm2_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm2_1   | WARNING: All illegal access operations will be denied in a future release
dn4_1    | 2023-04-04 07:54:14,366 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:14,709 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO raftlog.RaftLog: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
om3_1    | 2023-04-04 07:54:31,329 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-04-04 07:53:58,768 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1    | 2023-04-04 07:54:31,985 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn3_1    | 2023-04-04 07:54:17,069 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-04-04 07:54:03,911 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
om1_1    | 2023-04-04 07:54:34,278 [Listener at om1/9862] INFO server.session: node0 Scavenging every 660000ms
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn4_1    | 2023-04-04 07:54:14,709 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: start as a follower, conf=3: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:54:14,710 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: changes role from      null to FOLLOWER at term 3 for startAsFollower
om3_1    | 2023-04-04 07:54:31,329 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-04-04 07:53:58,769 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-04-04 07:53:58,978 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
scm2_1   | 2023-04-04 07:54:03,913 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
om1_1    | 2023-04-04 07:54:34,404 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5dfd31f4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn2_1    | 2023-04-04 07:54:15,788 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO segmented.LogSegment: Successfully read 21 entries from segment file /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_inprogress_26
dn2_1    | 2023-04-04 07:54:15,788 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 14
om3_1    | 2023-04-04 07:54:31,331 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1    | 2023-04-04 07:54:32,000 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
scm1_1   | 2023-04-04 07:53:58,979 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-04-04 07:54:01,390 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:17,089 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8B1B4F9C62A2,id=c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm3_1   | Max Size Entering Target per Iteration             26GB
scm2_1   | 2023-04-04 07:54:03,921 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
om1_1    | 2023-04-04 07:54:34,412 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@577bfadb{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn2_1    | 2023-04-04 07:54:15,802 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 46
dn2_1    | 2023-04-04 07:54:15,802 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 25
om3_1    | 2023-04-04 07:54:31,331 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-04-04 07:54:32,001 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-04-04 07:54:32,002 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-04-04 07:53:58,979 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
dn3_1    | 2023-04-04 07:54:17,093 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | Max Size Leaving Source per Iteration              26GB
scm2_1   | 2023-04-04 07:54:03,990 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
om1_1    | 2023-04-04 07:54:35,251 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7c4d1c7b{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-5235136285377890948/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn2_1    | 2023-04-04 07:54:16,501 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:14,747 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: start 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-FollowerState
om3_1    | 2023-04-04 07:54:31,332 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1    | 2023-04-04 07:54:32,003 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-04-04 07:54:32,004 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-04-04 07:53:59,040 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: set configuration 0: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:17,093 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-04-04 07:54:17,093 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm2_1   | 2023-04-04 07:54:04,843 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om1_1    | 2023-04-04 07:54:35,287 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@45f9d394{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1    | 2023-04-04 07:54:35,287 [Listener at om1/9862] INFO server.Server: Started @85750ms
dn2_1    | 2023-04-04 07:54:16,649 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO raftlog.RaftLog: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn4_1    | 2023-04-04 07:54:14,764 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-04-04 07:54:31,377 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1    | 2023-04-04 07:54:32,006 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1    | 2023-04-04 07:54:32,011 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-04-04 07:53:59,042 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_0-0
dn3_1    | 2023-04-04 07:54:17,093 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 
scm3_1   | 2023-04-04 07:54:22,510 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
om1_1    | 2023-04-04 07:54:35,298 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1    | 2023-04-04 07:54:35,299 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-04-04 07:54:16,649 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: start as a follower, conf=3: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:54:14,764 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-04-04 07:54:31,380 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-04-04 07:54:32,012 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-04-04 07:54:32,015 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-04-04 07:53:59,049 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: set configuration 1: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:17,097 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-04-04 07:54:22,585 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
scm3_1   | 2023-04-04 07:54:22,601 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om1_1    | 2023-04-04 07:54:35,301 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
om3_1    | 2023-04-04 07:54:31,766 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1    | 2023-04-04 07:54:32,040 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1    | 2023-04-04 07:54:32,041 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-04-04 07:53:59,065 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: set configuration 17: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn3_1    | 2023-04-04 07:54:17,097 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EEB13EAC206E,id=c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm2_1   | 2023-04-04 07:54:04,885 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-04-04 07:54:22,606 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
om1_1    | 2023-04-04 07:54:35,302 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-04-04 07:54:35,336 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1    | 2023-04-04 07:54:31,771 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1    | 2023-04-04 07:54:32,063 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1    | 2023-04-04 07:54:32,069 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-04-04 07:53:59,066 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: set configuration 19: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:17,098 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 2023-04-04 07:54:04,955 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3_1   | 2023-04-04 07:54:22,612 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
om1_1    | 2023-04-04 07:54:35,556 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om1_1    | 2023-04-04 07:54:35,599 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2d6a0766] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1    | 2023-04-04 07:54:31,772 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1    | 2023-04-04 07:54:32,069 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1    | 2023-04-04 07:54:32,111 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 106
om2_1    | 2023-04-04 07:54:32,111 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1    | 2023-04-04 07:54:32,119 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:05,078 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-04-04 07:54:22,667 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/in_use.lock acquired by nodename 7@2bce303d964c
om1_1    | 2023-04-04 07:54:37,123 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5220685191ns, electionTimeout:5163ms
om1_1    | 2023-04-04 07:54:37,124 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
om3_1    | 2023-04-04 07:54:31,785 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 106
om2_1    | 2023-04-04 07:54:32,121 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn3_1    | 2023-04-04 07:54:17,098 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-04-04 07:54:17,098 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 2023-04-04 07:53:59,074 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: set configuration 31: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2_1   | 2023-04-04 07:54:05,088 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-04-04 07:54:22,695 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=} from /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/raft-meta
om1_1    | 2023-04-04 07:54:37,125 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
om1_1    | 2023-04-04 07:54:37,132 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1    | 2023-04-04 07:54:31,785 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1    | 2023-04-04 07:54:32,125 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-04-04 07:54:32,153 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-04-04 07:54:32,165 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-04-04 07:54:32,173 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
scm2_1   | 2023-04-04 07:54:05,096 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2_1   | 2023-04-04 07:54:05,147 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-04-04 07:54:05,153 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-04-04 07:54:05,154 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2_1   | 2023-04-04 07:54:05,214 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
dn4_1    | 2023-04-04 07:54:14,804 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6617081AE410,id=14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn4_1    | 2023-04-04 07:54:14,841 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1    | 2023-04-04 07:54:31,786 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-04-04 07:54:32,177 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-04-04 07:54:32,178 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1    | 2023-04-04 07:54:32,180 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1_1   | 2023-04-04 07:53:59,076 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: set configuration 33: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:05,215 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
om1_1    | 2023-04-04 07:54:37,132 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-04-04 07:54:37,155 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-04-04 07:54:37,232 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
dn4_1    | 2023-04-04 07:54:14,863 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-04-04 07:54:14,868 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om3_1    | 2023-04-04 07:54:31,787 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 3 for startAsFollower
om2_1    | 2023-04-04 07:54:32,190 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1    | 2023-04-04 07:54:32,220 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
om2_1    | 2023-04-04 07:54:32,647 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
scm1_1   | 2023-04-04 07:53:59,083 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_1-48
scm1_1   | 2023-04-04 07:53:59,086 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: set configuration 49: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:53:59,089 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO segmented.LogSegment: Successfully read 10 entries from segment file /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_49-58
scm1_1   | 2023-04-04 07:53:59,090 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: set configuration 59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:53:59,168 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_inprogress_59
dn4_1    | 2023-04-04 07:54:14,871 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
om2_1    | 2023-04-04 07:54:32,703 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1    | 2023-04-04 07:54:32,744 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1    | 2023-04-04 07:54:33,231 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
scm1_1   | 2023-04-04 07:53:59,172 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 72
scm2_1   | Container Balancer status:
scm2_1   | Key                            Value
scm2_1   | Running                        false
dn4_1    | 2023-04-04 07:54:14,948 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: start RPC server
dn4_1    | 2023-04-04 07:54:15,002 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: GrpcService started, listening on 9858
dn3_1    | 2023-04-04 07:54:17,098 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-04-04 07:53:59,172 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 58
scm2_1   | Container Balancer Configuration values:
om1_1    | 2023-04-04 07:54:37,234 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-04-04 07:54:37,235 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-04-04 07:54:16,649 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn2_1    | 2023-04-04 07:54:16,661 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO raftlog.RaftLog: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLog: commitIndex: updateToMax old=46, new=45, updated? false
dn2_1    | 2023-04-04 07:54:16,684 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: start as a follower, conf=26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:16,684 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: changes role from      null to FOLLOWER at term 18 for startAsFollower
dn3_1    | 2023-04-04 07:54:17,099 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-04-04 07:54:17,099 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-04-04 07:54:17,099 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start RPC server
scm1_1   | 2023-04-04 07:53:59,281 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO raftlog.RaftLog: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-SegmentedRaftLog: commitIndex: updateToMax old=72, new=71, updated? false
dn5_1    | 2023-04-04 07:54:01,393 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:54:02,391 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:54:02,394 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:16,661 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO raftlog.RaftLog: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLog: commitIndex: updateToMax old=20, new=19, updated? false
dn3_1    | 2023-04-04 07:54:17,128 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: c1077390-d65b-4523-9cd4-abe9e2c9eb94: GrpcService started, listening on 9858
dn3_1    | 2023-04-04 07:54:17,129 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: c1077390-d65b-4523-9cd4-abe9e2c9eb94: GrpcService started, listening on 9856
dn3_1    | 2023-04-04 07:54:17,130 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: c1077390-d65b-4523-9cd4-abe9e2c9eb94: GrpcService started, listening on 9857
scm1_1   | 2023-04-04 07:53:59,283 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: start as a follower, conf=59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:54:15,004 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: GrpcService started, listening on 9856
dn4_1    | 2023-04-04 07:54:15,005 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: GrpcService started, listening on 9857
dn2_1    | 2023-04-04 07:54:16,685 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: start as a follower, conf=15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:16,685 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: changes role from      null to FOLLOWER at term 14 for startAsFollower
dn3_1    | 2023-04-04 07:54:17,134 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c1077390-d65b-4523-9cd4-abe9e2c9eb94 is started using port 9858 for RATIS
dn3_1    | 2023-04-04 07:54:17,134 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c1077390-d65b-4523-9cd4-abe9e2c9eb94 is started using port 9857 for RATIS_ADMIN
dn3_1    | 2023-04-04 07:54:17,135 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c1077390-d65b-4523-9cd4-abe9e2c9eb94 is started using port 9856 for RATIS_SERVER
scm1_1   | 2023-04-04 07:53:59,283 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: changes role from      null to FOLLOWER at term 5 for startAsFollower
dn4_1    | 2023-04-04 07:54:15,076 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 14df8ad4-ce2f-4483-aada-29ab5cae3ffb is started using port 9858 for RATIS
dn4_1    | 2023-04-04 07:54:15,077 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 14df8ad4-ce2f-4483-aada-29ab5cae3ffb is started using port 9857 for RATIS_ADMIN
dn4_1    | 2023-04-04 07:54:15,077 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 14df8ad4-ce2f-4483-aada-29ab5cae3ffb is started using port 9856 for RATIS_SERVER
dn4_1    | 2023-04-04 07:54:15,107 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-14df8ad4-ce2f-4483-aada-29ab5cae3ffb: Started
dn3_1    | 2023-04-04 07:54:17,136 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-c1077390-d65b-4523-9cd4-abe9e2c9eb94: Started
dn3_1    | 2023-04-04 07:54:17,097 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-04-04 07:54:17,301 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
scm1_1   | 2023-04-04 07:53:59,287 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO impl.RoleInfo: 861060d2-f30f-4260-bac0-b4e7af8aba9d: start 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState
dn4_1    | 2023-04-04 07:54:15,219 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-04-04 07:54:03,392 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:54:03,394 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:54:03,405 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn3_1    | 2023-04-04 07:54:17,346 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
om3_1    | 2023-04-04 07:54:31,790 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
scm3_1   | 2023-04-04 07:54:23,328 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: set configuration 59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:23,392 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1    | 2023-04-04 07:54:37,236 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
dn4_1    | 2023-04-04 07:54:15,275 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-04-04 07:54:15,368 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:16,371 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:17,373 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:17,880 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | 2023-04-04 07:54:17,665 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
om1_1    | 2023-04-04 07:54:38,691 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn2_1    | 2023-04-04 07:54:16,686 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-FollowerState
dn2_1    | 2023-04-04 07:54:16,693 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0EBF5B06395B,id=e2957b23-687f-4626-af75-9b42f3a43b99
dn2_1    | 2023-04-04 07:54:16,708 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-04-04 07:54:16,715 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-04-04 07:54:16,723 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-04-04 07:54:18,666 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:19,666 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:20,667 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:21,674 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:22,107 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO impl.FollowerState: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5214545073ns, electionTimeout:5037ms
dn3_1    | 2023-04-04 07:54:22,108 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState
dn3_1    | 2023-04-04 07:54:22,108 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: changes role from  FOLLOWER to CANDIDATE at term 14 for changeToCandidate
dn5_1    | java.net.SocketTimeoutException: Call From 7e8229df9409/10.9.0.21 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:55874 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
scm2_1   | Key                                                Value
scm2_1   | Threshold                                          10
scm3_1   | 2023-04-04 07:54:23,552 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | 2023-04-04 07:54:31,792 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
scm2_1   | Max Datanodes to Involve per Iteration(percent)    20
om1_1    | 2023-04-04 07:54:38,694 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om3#0:OK-t3
om1_1    | 2023-04-04 07:54:38,694 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
om1_1    | 2023-04-04 07:54:38,704 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-04-04 07:54:38,725 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-04-04 07:54:22,115 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
scm2_1   | Max Size to Move per Iteration                     500GB
scm3_1   | 2023-04-04 07:54:23,560 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-04-04 07:54:23,588 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1   | 2023-04-04 07:54:23,595 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | 2023-04-04 07:54:23,636 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn3_1    | 2023-04-04 07:54:22,115 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 2023-04-04 07:54:16,723 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-04-04 07:54:16,720 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState
dn2_1    | 2023-04-04 07:54:16,753 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-04-04 07:54:16,721 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-FollowerState
scm2_1   | Max Size Entering Target per Iteration             26GB
scm2_1   | Max Size Leaving Source per Iteration              26GB
scm2_1   | 
scm2_1   | 2023-04-04 07:54:05,216 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2_1   | 2023-04-04 07:54:05,220 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn3_1    | 2023-04-04 07:54:22,135 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 14 for 15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 2023-04-04 07:54:16,762 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-04-04 07:54:16,816 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-04-04 07:54:16,816 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-04-04 07:54:16,818 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EEB13EAC206E,id=e2957b23-687f-4626-af75-9b42f3a43b99
scm2_1   | 2023-04-04 07:54:05,222 [Listener at 0.0.0.0/9860] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om3_1    | 2023-04-04 07:54:31,800 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1    | 2023-04-04 07:54:31,801 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1    | 2023-04-04 07:54:31,802 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1    | 2023-04-04 07:54:31,803 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1    | 2023-04-04 07:54:31,804 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 2023-04-04 07:54:16,820 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-04-04 07:54:33,231 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om2_1    | 2023-04-04 07:54:33,417 [Listener at om2/9862] INFO util.log: Logging initialized @80582ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1    | 2023-04-04 07:54:33,952 [Listener at om2/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm2_1   | 2023-04-04 07:54:05,222 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2_1   | 2023-04-04 07:54:05,227 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3_1   | 2023-04-04 07:54:23,765 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1   | 2023-04-04 07:54:23,770 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   | 2023-04-04 07:54:23,832 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7
scm3_1   | 2023-04-04 07:54:23,840 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 2023-04-04 07:54:16,820 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-04-04 07:54:16,820 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-04-04 07:54:16,820 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-04-04 07:54:16,818 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F3D51D6EC210,id=e2957b23-687f-4626-af75-9b42f3a43b99
om3_1    | 2023-04-04 07:54:31,805 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-04-04 07:54:31,808 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
om3_1    | 2023-04-04 07:54:31,876 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1    | 2023-04-04 07:54:31,883 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1    | 2023-04-04 07:54:31,900 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1    | 2023-04-04 07:54:32,157 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:55874 remote=scm1/10.9.0.14:9861]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
om1_1    | 2023-04-04 07:54:38,725 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-04-04 07:54:38,733 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 3, (t:3, i:106))
om1_1    | 2023-04-04 07:54:38,739 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 3, (t:3, i:106))
om2_1    | 2023-04-04 07:54:34,008 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-04-04 07:54:23,868 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1    | 2023-04-04 07:54:38,761 [grpc-default-executor-1] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
om1_1    | 2023-04-04 07:54:38,763 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om1#0:OK-t4. Peer's state: om1@group-D66704EFC61C:t4, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c106, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:22,162 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO impl.FollowerState: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5130125023ns, electionTimeout:5063ms
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-04-04 07:54:18,636 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:50006: output error
scm3_1   | 2023-04-04 07:54:23,891 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-04-04 07:54:23,900 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3_1   | 2023-04-04 07:54:23,903 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1   | 2023-04-04 07:54:05,232 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/in_use.lock acquired by nodename 7@f571653a518f
dn3_1    | 2023-04-04 07:54:22,205 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState
om2_1    | 2023-04-04 07:54:34,076 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
recon_1  | 2023-04-04 07:54:18,636 [IPC Server handler 23 on default port 9891] WARN ipc.Server: IPC Server handler 23 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:56566: output error
scm3_1   | 2023-04-04 07:54:23,920 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   | 2023-04-04 07:54:23,928 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | 2023-04-04 07:54:23,933 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 2023-04-04 07:54:24,119 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3_1   | 2023-04-04 07:54:24,119 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-04-04 07:54:25,019 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 2023-04-04 07:54:25,027 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1   | 2023-04-04 07:54:25,030 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3_1   | 2023-04-04 07:54:25,238 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: set configuration 0: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:25,250 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_0-0
om1_1    | 2023-04-04 07:54:38,769 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
om1_1    | 2023-04-04 07:54:38,770 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om1#0:OK-t4. Peer's state: om1@group-D66704EFC61C:t4, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c106, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-04-04 07:54:38,855 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
om1_1    | 2023-04-04 07:54:38,855 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om3#0:OK-t4
om1_1    | 2023-04-04 07:54:38,855 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result PASSED
scm2_1   | 2023-04-04 07:54:05,237 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=861060d2-f30f-4260-bac0-b4e7af8aba9d} from /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/raft-meta
scm2_1   | 2023-04-04 07:54:05,264 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: set configuration 59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:05,267 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1   | 2023-04-04 07:54:05,274 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2_1   | 2023-04-04 07:54:05,274 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-04-04 07:54:38,855 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-LeaderElection1
recon_1  | 2023-04-04 07:54:18,636 [IPC Server handler 22 on default port 9891] WARN ipc.Server: IPC Server handler 22 on default port 9891, call Call#10 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:32810: output error
recon_1  | 2023-04-04 07:54:18,617 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:56080: output error
scm2_1   | 2023-04-04 07:54:05,275 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-04-04 07:54:38,871 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
om1_1    | 2023-04-04 07:54:38,871 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: change Leader from null to om1 at term 4 for becomeLeader, leader elected after 11666ms
om1_1    | 2023-04-04 07:54:38,926 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om1_1    | 2023-04-04 07:54:38,929 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 4, (t:3, i:106))
om1_1    | 2023-04-04 07:54:38,950 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om3_1    | 2023-04-04 07:54:32,160 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om3_1    | 2023-04-04 07:54:32,285 [Listener at om3/9862] INFO util.log: Logging initialized @81641ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1    | 2023-04-04 07:54:33,236 [Listener at om3/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om3_1    | 2023-04-04 07:54:33,257 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1    | 2023-04-04 07:54:33,291 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1    | 2023-04-04 07:54:33,303 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
scm2_1   | 2023-04-04 07:54:05,276 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-04-04 07:54:38,951 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-04-04 07:54:34,086 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om2_1    | 2023-04-04 07:54:34,086 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om2_1    | 2023-04-04 07:54:34,087 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1    | 2023-04-04 07:54:34,342 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
scm2_1   | 2023-04-04 07:54:05,279 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-04-04 07:54:05,304 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 2023-04-04 07:54:05,305 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 2023-04-04 07:54:05,310 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7
om1_1    | 2023-04-04 07:54:38,956 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om2_1    | 2023-04-04 07:54:34,353 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
scm1_1   | 2023-04-04 07:53:59,289 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 2023-04-04 07:54:16,821 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-04-04 07:54:16,827 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om1_1    | 2023-04-04 07:54:38,956 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om2_1    | 2023-04-04 07:54:34,362 [Listener at om2/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm1_1   | 2023-04-04 07:53:59,291 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-04-04 07:53:59,291 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0BB16BE3F1B7,id=861060d2-f30f-4260-bac0-b4e7af8aba9d
scm1_1   | 2023-04-04 07:53:59,296 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-04-04 07:53:59,296 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | 2023-04-04 07:54:38,957 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om2_1    | 2023-04-04 07:54:34,553 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-04-04 07:54:18,617 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:48006: output error
recon_1  | 2023-04-04 07:54:18,652 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn2_1    | 2023-04-04 07:54:16,827 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-04-04 07:54:16,827 [e2957b23-687f-4626-af75-9b42f3a43b99-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-04-04 07:54:16,880 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-04-04 07:54:16,880 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-04-04 07:54:38,966 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om2_1    | 2023-04-04 07:54:34,558 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-04-04 07:54:16,916 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: e2957b23-687f-4626-af75-9b42f3a43b99: start RPC server
dn2_1    | 2023-04-04 07:54:16,952 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: e2957b23-687f-4626-af75-9b42f3a43b99: GrpcService started, listening on 9858
dn2_1    | 2023-04-04 07:54:16,989 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: e2957b23-687f-4626-af75-9b42f3a43b99: GrpcService started, listening on 9856
dn2_1    | 2023-04-04 07:54:17,044 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: e2957b23-687f-4626-af75-9b42f3a43b99: GrpcService started, listening on 9857
om1_1    | 2023-04-04 07:54:38,971 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om2_1    | 2023-04-04 07:54:34,569 [Listener at om2/9862] INFO server.session: node0 Scavenging every 660000ms
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
om3_1    | 2023-04-04 07:54:33,303 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om3_1    | 2023-04-04 07:54:33,305 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om3_1    | 2023-04-04 07:54:33,547 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om3_1    | 2023-04-04 07:54:33,561 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
scm1_1   | 2023-04-04 07:53:59,297 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1    | 2023-04-04 07:54:38,996 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om2_1    | 2023-04-04 07:54:34,654 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@37d0d373{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
om3_1    | 2023-04-04 07:54:33,565 [Listener at om3/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om3_1    | 2023-04-04 07:54:33,744 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 2023-04-04 07:54:33,744 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
om3_1    | 2023-04-04 07:54:33,763 [Listener at om3/9862] INFO server.session: node0 Scavenging every 660000ms
om3_1    | 2023-04-04 07:54:33,842 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@44ccd75c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2_1   | 2023-04-04 07:54:05,311 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2_1   | 2023-04-04 07:54:05,314 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-04-04 07:53:59,298 [861060d2-f30f-4260-bac0-b4e7af8aba9d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-04-04 07:53:59,309 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 861060d2-f30f-4260-bac0-b4e7af8aba9d: start RPC server
om1_1    | 2023-04-04 07:54:38,996 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-04-04 07:54:34,657 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@36463b09{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm2_1   | 2023-04-04 07:54:05,320 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-04-04 07:53:59,422 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 861060d2-f30f-4260-bac0-b4e7af8aba9d: GrpcService started, listening on 9894
scm1_1   | 2023-04-04 07:53:59,427 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-861060d2-f30f-4260-bac0-b4e7af8aba9d: Started
om1_1    | 2023-04-04 07:54:39,000 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om2_1    | 2023-04-04 07:54:35,350 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6ce9771c{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-12752863843081751528/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
scm3_1   | 2023-04-04 07:54:25,302 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: set configuration 1: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:25,397 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: set configuration 17: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1   | 2023-04-04 07:54:25,402 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: set configuration 19: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:25,426 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: set configuration 31: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1   | 2023-04-04 07:54:25,438 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: set configuration 33: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm2_1   | 2023-04-04 07:54:05,320 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2_1   | 2023-04-04 07:54:05,321 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1   | 2023-04-04 07:54:05,322 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1    | 2023-04-04 07:54:39,004 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1    | 2023-04-04 07:54:39,005 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
dn3_1    | 2023-04-04 07:54:22,206 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: changes role from  FOLLOWER to CANDIDATE at term 18 for changeToCandidate
dn3_1    | 2023-04-04 07:54:22,206 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-04-04 07:54:22,206 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2
dn3_1    | 2023-04-04 07:54:22,233 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 18 for 26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:22,300 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 2ebde02c-a404-41d0-92a4-7b6da490547a
dn3_1    | 2023-04-04 07:54:22,311 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-04-04 07:54:22,347 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for e2957b23-687f-4626-af75-9b42f3a43b99
dn3_1    | 2023-04-04 07:54:22,379 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-FollowerState] INFO impl.FollowerState: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5347307980ns, electionTimeout:5156ms
dn3_1    | 2023-04-04 07:54:22,346 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-04-04 07:54:22,391 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-FollowerState] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-FollowerState
dn3_1    | 2023-04-04 07:54:22,391 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 2023-04-04 07:54:22,401 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-FollowerState] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
scm1_1   | 2023-04-04 07:53:59,438 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1_1   | 2023-04-04 07:53:59,438 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1_1   | 2023-04-04 07:53:59,595 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-04-04 07:54:05,322 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-04-04 07:54:22,401 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-04-04 07:53:59,619 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1_1   | 2023-04-04 07:53:59,619 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
dn2_1    | 2023-04-04 07:54:17,073 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e2957b23-687f-4626-af75-9b42f3a43b99 is started using port 9858 for RATIS
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 2023-04-04 07:54:39,005 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-04-04 07:54:35,390 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@559d19c{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1    | 2023-04-04 07:54:35,391 [Listener at om2/9862] INFO server.Server: Started @82556ms
om2_1    | 2023-04-04 07:54:35,408 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-04-04 07:54:17,073 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e2957b23-687f-4626-af75-9b42f3a43b99 is started using port 9857 for RATIS_ADMIN
dn2_1    | 2023-04-04 07:54:17,073 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e2957b23-687f-4626-af75-9b42f3a43b99 is started using port 9856 for RATIS_SERVER
scm3_1   | 2023-04-04 07:54:25,465 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_1-48
om3_1    | 2023-04-04 07:54:33,845 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3f3c8b60{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om3_1    | 2023-04-04 07:54:34,541 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@27d73d22{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-1253827433060909996/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	... 1 more
dn3_1    | 2023-04-04 07:54:22,410 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om1_1    | 2023-04-04 07:54:39,005 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-04-04 07:54:05,322 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1   | 2023-04-04 07:54:05,333 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2_1   | 2023-04-04 07:54:05,334 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-04-04 07:54:17,074 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e2957b23-687f-4626-af75-9b42f3a43b99: Started
scm3_1   | 2023-04-04 07:54:25,494 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: set configuration 49: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:25,515 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO segmented.LogSegment: Successfully read 10 entries from segment file /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_49-58
scm3_1   | 2023-04-04 07:54:25,525 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: set configuration 59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:25,992 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_inprogress_59
dn4_1    | 2023-04-04 07:54:18,373 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:19,416 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:19,891 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-FollowerState] INFO impl.FollowerState: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5163781109ns, electionTimeout:5119ms
dn4_1    | 2023-04-04 07:54:19,892 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-FollowerState] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: shutdown 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-FollowerState
dn4_1    | 2023-04-04 07:54:19,892 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-FollowerState] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn4_1    | 2023-04-04 07:54:19,894 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-04-04 07:54:00,149 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3_1   | 2023-04-04 07:54:26,156 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 72
scm3_1   | 2023-04-04 07:54:26,164 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO segmented.SegmentedRaftLogWorker: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 58
scm3_1   | 2023-04-04 07:54:26,801 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO raftlog.RaftLog: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-SegmentedRaftLog: commitIndex: updateToMax old=72, new=71, updated? false
scm3_1   | 2023-04-04 07:54:26,802 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: start as a follower, conf=59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:17,171 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-04-04 07:54:17,188 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-04-04 07:54:22,410 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-FollowerState] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3
dn3_1    | 2023-04-04 07:54:22,464 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:22,470 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3 PRE_VOTE round 0: result PASSED (term=3)
dn4_1    | 2023-04-04 07:54:19,895 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-FollowerState] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: start 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1
scm1_1   | 2023-04-04 07:54:00,156 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-04-04 07:54:26,802 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: changes role from      null to FOLLOWER at term 5 for startAsFollower
scm3_1   | 2023-04-04 07:54:26,813 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO impl.RoleInfo: e0c91ed2-48f2-4467-9ffd-e28e64336c64: start e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-FollowerState
scm3_1   | 2023-04-04 07:54:26,835 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0BB16BE3F1B7,id=e0c91ed2-48f2-4467-9ffd-e28e64336c64
scm3_1   | 2023-04-04 07:54:26,839 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-04-04 07:54:34,594 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@6c9bf3b5{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
dn2_1    | 2023-04-04 07:54:17,503 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:22,618 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for 3: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:22,619 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3 ELECTION round 0: result PASSED (term=4)
dn3_1    | 2023-04-04 07:54:22,619 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3
dn3_1    | 2023-04-04 07:54:22,622 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
scm1_1   | 2023-04-04 07:54:00,160 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1_1   | 2023-04-04 07:54:00,269 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
om2_1    | 2023-04-04 07:54:35,408 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm3_1   | 2023-04-04 07:54:26,842 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-04-04 07:54:26,843 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-04-04 07:54:18,504 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-04-04 07:54:22,627 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8B1B4F9C62A2 with new leaderId: c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm2_1   | 2023-04-04 07:54:05,346 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1   | 2023-04-04 07:54:05,347 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 2023-04-04 07:54:05,347 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1   | 2023-04-04 07:54:00,274 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om2_1    | 2023-04-04 07:54:35,411 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
scm3_1   | 2023-04-04 07:54:26,849 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3_1   | 2023-04-04 07:54:26,851 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
dn2_1    | 2023-04-04 07:54:19,505 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:20,507 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:21,510 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-04-04 07:54:21,794 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-FollowerState] INFO impl.FollowerState: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5107944188ns, electionTimeout:5026ms
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
scm1_1   | 2023-04-04 07:54:00,297 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-04-04 07:54:00,298 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 2023-04-04 07:54:35,419 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-04-04 07:54:26,859 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 2023-04-04 07:54:26,889 [Listener at 0.0.0.0/9860] INFO server.RaftServer: e0c91ed2-48f2-4467-9ffd-e28e64336c64: start RPC server
dn2_1    | 2023-04-04 07:54:21,801 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-FollowerState] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: shutdown e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-FollowerState
dn2_1    | 2023-04-04 07:54:21,831 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-FollowerState] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: changes role from  FOLLOWER to CANDIDATE at term 14 for changeToCandidate
dn2_1    | 2023-04-04 07:54:21,882 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-04-04 07:54:21,893 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-FollowerState] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
scm1_1   | 2023-04-04 07:54:00,425 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1578fa9] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm1_1   | 2023-04-04 07:54:00,445 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 2023-04-04 07:54:35,464 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1    | 2023-04-04 07:54:34,594 [Listener at om3/9862] INFO server.Server: Started @83950ms
om3_1    | 2023-04-04 07:54:34,615 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1    | 2023-04-04 07:54:39,006 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om1_1    | 2023-04-04 07:54:39,017 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1    | 2023-04-04 07:54:39,021 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-04-04 07:54:39,021 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 2023-04-04 07:54:21,895 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-FollowerState] INFO impl.FollowerState: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5211138983ns, electionTimeout:5013ms
scm1_1   | 2023-04-04 07:54:00,445 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-04-04 07:54:22,637 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: change Leader from null to c1077390-d65b-4523-9cd4-abe9e2c9eb94 at term 4 for becomeLeader, leader elected after 34788ms
dn3_1    | 2023-04-04 07:54:22,797 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om2_1    | 2023-04-04 07:54:35,555 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om2_1    | 2023-04-04 07:54:35,605 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@22ca1242] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1    | 2023-04-04 07:54:34,616 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-04-04 07:54:39,021 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm2_1   | 2023-04-04 07:54:05,409 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: set configuration 0: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:05,411 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_0-0
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn2_1    | 2023-04-04 07:54:21,999 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-FollowerState] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: shutdown e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-FollowerState
scm1_1   | 2023-04-04 07:54:00,484 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @48001ms to org.eclipse.jetty.util.log.Slf4jLog
scm1_1   | 2023-04-04 07:54:00,937 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om2_1    | 2023-04-04 07:54:37,358 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5232909355ns, electionTimeout:5191ms
om3_1    | 2023-04-04 07:54:34,621 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1    | 2023-04-04 07:54:34,624 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 2023-04-04 07:54:39,021 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
scm2_1   | 2023-04-04 07:54:05,414 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: set configuration 1: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:04,392 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:19,896 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:54:19,898 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
om2_1    | 2023-04-04 07:54:37,359 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
om3_1    | 2023-04-04 07:54:34,679 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-04-04 07:54:18,617 [IPC Server handler 15 on default port 9891] WARN ipc.Server: IPC Server handler 15 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:56550: output error
recon_1  | 2023-04-04 07:54:18,617 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:47844: output error
scm2_1   | 2023-04-04 07:54:05,424 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: set configuration 17: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn5_1    | 2023-04-04 07:54:04,395 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:54:05,393 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:19,921 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-04-04 07:54:37,359 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
om3_1    | 2023-04-04 07:54:34,626 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-04-04 07:54:39,021 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-04-04 07:54:39,022 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-04-04 07:54:05,431 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: set configuration 19: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:05,435 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: set configuration 31: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn5_1    | 2023-04-04 07:54:05,398 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:19,921 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1 ELECTION round 0: result PASSED (term=4)
dn4_1    | 2023-04-04 07:54:19,923 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: shutdown 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1
om2_1    | 2023-04-04 07:54:37,363 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1    | 2023-04-04 07:54:34,923 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@674184d] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1    | 2023-04-04 07:54:39,023 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm2_1   | 2023-04-04 07:54:05,438 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: set configuration 33: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:05,445 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_1-48
scm2_1   | 2023-04-04 07:54:05,447 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: set configuration 49: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:06,399 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:54:07,400 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:54:19,926 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
om2_1    | 2023-04-04 07:54:37,364 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
om3_1    | 2023-04-04 07:54:36,949 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5160013872ns, electionTimeout:5143ms
om1_1    | 2023-04-04 07:54:39,025 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderStateImpl
scm2_1   | 2023-04-04 07:54:05,450 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO segmented.LogSegment: Successfully read 10 entries from segment file /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_49-58
recon_1  | 2023-04-04 07:54:18,617 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:33864: output error
recon_1  | 2023-04-04 07:54:18,792 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn4_1    | 2023-04-04 07:54:19,926 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6617081AE410 with new leaderId: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn4_1    | 2023-04-04 07:54:19,928 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: change Leader from null to 14df8ad4-ce2f-4483-aada-29ab5cae3ffb at term 4 for becomeLeader, leader elected after 30138ms
om2_1    | 2023-04-04 07:54:37,376 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-04-04 07:54:36,950 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-04-04 07:54:36,951 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
scm2_1   | 2023-04-04 07:54:05,451 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: set configuration 59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:05,514 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_inprogress_59
scm3_1   | 2023-04-04 07:54:27,140 [Listener at 0.0.0.0/9860] INFO server.GrpcService: e0c91ed2-48f2-4467-9ffd-e28e64336c64: GrpcService started, listening on 9894
scm3_1   | 2023-04-04 07:54:27,178 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-e0c91ed2-48f2-4467-9ffd-e28e64336c64: Started
dn4_1    | 2023-04-04 07:54:19,980 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-04-04 07:54:21,999 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-FollowerState] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn3_1    | 2023-04-04 07:54:22,881 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-04-04 07:54:00,963 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
om1_1    | 2023-04-04 07:54:39,038 [om1@group-D66704EFC61C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:107
om1_1    | 2023-04-04 07:54:39,087 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 107: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:05,519 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 72
scm3_1   | 2023-04-04 07:54:27,198 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3_1   | 2023-04-04 07:54:27,198 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
dn4_1    | 2023-04-04 07:54:20,019 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-04-04 07:54:21,999 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-04-04 07:54:22,886 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-04-04 07:54:22,976 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
om2_1    | 2023-04-04 07:54:37,463 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-04-04 07:54:00,993 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1_1   | 2023-04-04 07:54:01,005 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
om1_1    | 2023-04-04 07:54:39,098 [grpc-default-executor-1] INFO impl.VoteContext: om1@group-D66704EFC61C-LEADER: reject ELECTION from om2: already has voted for om1 at current term 4
dn5_1    | 2023-04-04 07:54:08,400 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-04-04 07:54:05,519 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 58
scm3_1   | 2023-04-04 07:54:28,897 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3_1   | 2023-04-04 07:54:29,087 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn4_1    | 2023-04-04 07:54:20,025 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-04-04 07:54:21,999 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-FollowerState] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2
dn3_1    | 2023-04-04 07:54:22,977 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-04-04 07:54:22,985 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-04-04 07:54:23,141 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-04-04 07:54:01,005 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn5_1    | 2023-04-04 07:54:09,401 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:54:10,402 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-04-04 07:54:05,582 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO raftlog.RaftLog: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-SegmentedRaftLog: commitIndex: updateToMax old=72, new=71, updated? false
om1_1    | 2023-04-04 07:54:39,098 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om2<-om1#0:FAIL-t4. Peer's state: om1@group-D66704EFC61C:t4, leader=om1, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c106, conf=107: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:29,087 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
dn4_1    | 2023-04-04 07:54:20,049 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-04-04 07:54:22,001 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO impl.FollowerState: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5316730171ns, electionTimeout:5182ms
om3_1    | 2023-04-04 07:54:36,954 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1    | 2023-04-04 07:54:36,954 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderElection1
om2_1    | 2023-04-04 07:54:37,464 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-04-04 07:54:01,005 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-04-04 07:54:10,403 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn5_1    | java.net.ConnectException: Call From 7e8229df9409/10.9.0.21 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
scm3_1   | 2023-04-04 07:54:29,707 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-server-thread1] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
dn4_1    | 2023-04-04 07:54:20,049 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-04-04 07:54:22,002 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: shutdown e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState
om3_1    | 2023-04-04 07:54:36,967 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:23,170 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om2_1    | 2023-04-04 07:54:37,467 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
scm1_1   | 2023-04-04 07:54:01,086 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
om1_1    | 2023-04-04 07:54:39,404 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_107
scm3_1   | 2023-04-04 07:54:29,709 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-server-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: change Leader from null to 861060d2-f30f-4260-bac0-b4e7af8aba9d at term 6 for appendEntries, leader elected after 19782ms
dn4_1    | 2023-04-04 07:54:20,066 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-04-04 07:54:22,002 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: changes role from  FOLLOWER to CANDIDATE at term 18 for changeToCandidate
om3_1    | 2023-04-04 07:54:36,993 [om3@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om3_1    | 2023-04-04 07:54:36,997 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-04-04 07:54:36,997 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-04-04 07:54:37,480 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 2023-04-04 07:54:05,582 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: start as a follower, conf=59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-04-04 07:54:40,663 [om1@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
scm3_1   | 2023-04-04 07:54:29,775 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-server-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: Failed appendEntries as previous log entry ((t:6, i:74)) is not found
dn4_1    | 2023-04-04 07:54:20,103 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-04-04 07:54:20,114 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om3_1    | 2023-04-04 07:54:36,998 [om3@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om3_1    | 2023-04-04 07:54:38,607 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 3, (t:3, i:106))
om3_1    | 2023-04-04 07:54:38,615 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
om2_1    | 2023-04-04 07:54:38,684 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 3, (t:3, i:106))
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 2023-04-04 07:54:05,583 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: changes role from      null to FOLLOWER at term 5 for startAsFollower
om1_1    | [id: "om1"
scm3_1   | 2023-04-04 07:54:30,051 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-server-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: inconsistency entries. Reply:861060d2-f30f-4260-bac0-b4e7af8aba9d<-e0c91ed2-48f2-4467-9ffd-e28e64336c64#861719:FAIL-t5,INCONSISTENCY,nextIndex=73,followerCommit=72,matchIndex=-1
dn2_1    | 2023-04-04 07:54:22,004 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-04-04 07:54:20,148 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: start 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderStateImpl
om3_1    | 2023-04-04 07:54:38,635 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om3#0:OK-t3. Peer's state: om3@group-D66704EFC61C:t3, leader=null, voted=om2, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c106, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:23,192 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderStateImpl
dn3_1    | 2023-04-04 07:54:23,283 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
om2_1    | 2023-04-04 07:54:38,698 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 2023-04-04 07:54:05,587 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO impl.RoleInfo: 19de384c-7008-4d6c-8004-b0a99b5aa3c2: start 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-FollowerState
om1_1    | address: "om1:9872"
scm3_1   | 2023-04-04 07:54:30,126 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-server-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: Failed appendEntries as previous log entry ((t:6, i:74)) is not found
dn2_1    | 2023-04-04 07:54:22,004 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3
dn2_1    | 2023-04-04 07:54:22,033 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 14 for 15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-04-04 07:54:38,735 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 3, (t:3, i:106))
om3_1    | 2023-04-04 07:54:38,736 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
om3_1    | 2023-04-04 07:54:38,736 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om3#0:OK-t3. Peer's state: om3@group-D66704EFC61C:t3, leader=null, voted=om2, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c106, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-04-04 07:54:38,738 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om2#0:OK-t3. Peer's state: om2@group-D66704EFC61C:t3, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c106, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 2023-04-04 07:54:05,592 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0BB16BE3F1B7,id=19de384c-7008-4d6c-8004-b0a99b5aa3c2
scm2_1   | 2023-04-04 07:54:05,594 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 2023-04-04 07:54:05,594 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
dn2_1    | 2023-04-04 07:54:22,048 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:22,076 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2 PRE_VOTE round 0: result PASSED (term=3)
dn2_1    | 2023-04-04 07:54:22,098 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 18 for 26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:22,209 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2 ELECTION round 0: submit vote requests at term 4 for 3: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-04-04 07:54:38,766 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 3, (t:3, i:106))
dn3_1    | 2023-04-04 07:54:23,320 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2/current/log_inprogress_3 to /data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2/current/log_3-4
dn3_1    | 2023-04-04 07:54:23,359 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2/current/log_inprogress_5
dn3_1    | 2023-04-04 07:54:23,407 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderElection3] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: set configuration 5: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om3"
scm3_1   | 2023-04-04 07:54:30,128 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-server-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: inconsistency entries. Reply:861060d2-f30f-4260-bac0-b4e7af8aba9d<-e0c91ed2-48f2-4467-9ffd-e28e64336c64#861831:FAIL-t6,INCONSISTENCY,nextIndex=73,followerCommit=72,matchIndex=-1
scm3_1   | 2023-04-04 07:54:30,139 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-server-thread1] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: set configuration 73: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
om2_1    | 2023-04-04 07:54:38,766 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
dn3_1    | 2023-04-04 07:54:25,536 [grpc-default-executor-0] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: receive requestVote(PRE_VOTE, e2957b23-687f-4626-af75-9b42f3a43b99, group-EEB13EAC206E, 18, (t:18, i:46))
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 2023-04-04 07:54:05,595 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
dn4_1    | 2023-04-04 07:54:20,174 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn4_1    | 2023-04-04 07:54:20,192 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410/current/log_inprogress_3 to /data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410/current/log_3-4
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
om2_1    | 2023-04-04 07:54:38,768 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om2#0:OK-t3. Peer's state: om2@group-D66704EFC61C:t3, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c106, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:25,548 [grpc-default-executor-0] INFO impl.VoteContext: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-CANDIDATE: accept PRE_VOTE from e2957b23-687f-4626-af75-9b42f3a43b99: our priority 0 <= candidate's priority 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
om1_1    | address: "om3:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om2"
scm2_1   | 2023-04-04 07:54:05,595 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-04-04 07:54:20,203 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410/current/log_inprogress_5
scm3_1   | 2023-04-04 07:54:30,152 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-server-thread1] INFO segmented.SegmentedRaftLogWorker: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-SegmentedRaftLogWorker: Rolling segment log-59_72 to index:72
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
scm1_1   | 2023-04-04 07:54:01,087 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm1_1   | 2023-04-04 07:54:01,090 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om2_1    | 2023-04-04 07:54:38,843 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-04-04 07:54:25,583 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: receive requestVote(PRE_VOTE, e2957b23-687f-4626-af75-9b42f3a43b99, group-0EBF5B06395B, 14, (t:14, i:20))
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 2023-04-04 07:54:05,597 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-04-04 07:54:20,250 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderElection1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: set configuration 5: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:30,187 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_inprogress_59 to /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_59-72
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
scm1_1   | 2023-04-04 07:54:01,184 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 2023-04-04 07:54:38,793 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 4, (t:3, i:106))
om2_1    | 2023-04-04 07:54:38,858 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om3#0:OK-t3
dn3_1    | 2023-04-04 07:54:25,583 [grpc-default-executor-1] INFO impl.VoteContext: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-CANDIDATE: accept PRE_VOTE from e2957b23-687f-4626-af75-9b42f3a43b99: our priority 0 <= candidate's priority 1
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-04-04 07:54:05,600 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-04-04 07:54:20,418 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm1_1   | 2023-04-04 07:54:01,185 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm1_1   | 2023-04-04 07:54:01,188 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
om2_1    | 2023-04-04 07:54:38,859 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
dn3_1    | 2023-04-04 07:54:25,625 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B replies to PRE_VOTE vote request: e2957b23-687f-4626-af75-9b42f3a43b99<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t14. Peer's state: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B:t14, leader=null, voted=e2957b23-687f-4626-af75-9b42f3a43b99, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLog:OPENED:c20, conf=15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | address: "om2:9872"
om1_1    | startupRole: FOLLOWER
scm3_1   | 2023-04-04 07:54:30,329 [e0c91ed2-48f2-4467-9ffd-e28e64336c64-server-thread2] INFO server.RaftServer$Division: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7: set configuration 73: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:05,601 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 19de384c-7008-4d6c-8004-b0a99b5aa3c2: start RPC server
dn4_1    | 2023-04-04 07:54:21,425 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-04-04 07:54:38,863 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | ]
om1_1    | 2023-04-04 07:55:32,620 [KeyDeletingService#0] INFO service.KeyDeletingService: Number of keys deleted: 3, elapsed time: 235ms
scm3_1   | 2023-04-04 07:54:30,395 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_inprogress_73
scm2_1   | 2023-04-04 07:54:05,650 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 19de384c-7008-4d6c-8004-b0a99b5aa3c2: GrpcService started, listening on 9894
dn4_1    | 2023-04-04 07:54:33,754 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
om3_1    | 2023-04-04 07:54:38,793 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept ELECTION from om1: our priority 0 <= candidate's priority 0
om3_1    | 2023-04-04 07:54:38,794 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 4 for candidate:om1
om3_1    | 2023-04-04 07:54:38,794 [grpc-default-executor-0] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-LeaderElection1
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om1_1    | 2023-04-04 07:56:42,439 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 3
om1_1    | 2023-04-04 07:57:43,942 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
scm3_1   | 2023-04-04 07:54:30,440 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm2_1   | 2023-04-04 07:54:05,669 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-19de384c-7008-4d6c-8004-b0a99b5aa3c2: Started
dn4_1    | 2023-04-04 07:55:15,277 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-04-04 07:54:01,223 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@c0c8f96{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-04-04 07:54:25,644 [grpc-default-executor-0] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E replies to PRE_VOTE vote request: e2957b23-687f-4626-af75-9b42f3a43b99<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t18. Peer's state: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E:t18, leader=null, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLog:OPENED:c46, conf=26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:25,844 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.ConnectException: Connection refused
recon_1  | 2023-04-04 07:54:18,617 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:56088: output error
recon_1  | 2023-04-04 07:54:18,617 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:48002: output error
om1_1    | 2023-04-04 07:57:47,063 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om1_1    | 2023-04-04 07:57:57,148 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om1_1    | 2023-04-04 07:58:05,597 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
dn4_1    | 2023-04-04 07:55:50,927 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-04-04 07:54:01,225 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@14d1737a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1_1   | 2023-04-04 07:54:01,542 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@52ca0ad4{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-16480003608994488629/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
om3_1    | 2023-04-04 07:54:38,795 [grpc-default-executor-0] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
om2_1    | 2023-04-04 07:54:38,882 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-04-04 07:54:22,210 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2 ELECTION round 0: result PASSED (term=4)
dn2_1    | 2023-04-04 07:54:22,218 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: shutdown e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2
scm2_1   | 2023-04-04 07:54:05,679 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
om1_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
scm3_1   | 2023-04-04 07:54:30,476 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3_1   | 2023-04-04 07:54:30,477 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm3_1   | 2023-04-04 07:54:30,477 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
dn3_1    | 2023-04-04 07:54:25,845 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1] INFO impl.LeaderElection:   Response 0: c1077390-d65b-4523-9cd4-abe9e2c9eb94<-e2957b23-687f-4626-af75-9b42f3a43b99#0:FAIL-t14
dn3_1    | 2023-04-04 07:54:25,845 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1 PRE_VOTE round 0: result REJECTED
dn3_1    | 2023-04-04 07:54:25,846 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: changes role from CANDIDATE to FOLLOWER at term 14 for REJECTED
om3_1    | 2023-04-04 07:54:38,799 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-04-04 07:54:38,882 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-04-04 07:54:22,219 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn2_1    | 2023-04-04 07:54:22,222 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm2_1   | 2023-04-04 07:54:05,679 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
om1_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:213)
scm3_1   | 2023-04-04 07:54:31,420 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-04-04 07:54:31,488 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3_1   | 2023-04-04 07:54:31,652 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-04-04 07:54:01,570 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@7d8b66d9{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1_1   | 2023-04-04 07:54:01,570 [Listener at 0.0.0.0/9860] INFO server.Server: Started @49088ms
scm1_1   | 2023-04-04 07:54:01,581 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1    | 2023-04-04 07:54:38,883 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 4, (t:3, i:106))
dn2_1    | 2023-04-04 07:54:22,258 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-04-04 07:54:22,237 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-04-04 07:54:22,278 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-04-04 07:54:05,836 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1  | 2023-04-04 07:54:18,793 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
scm3_1   | 2023-04-04 07:54:31,665 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
dn3_1    | 2023-04-04 07:54:25,846 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1
dn3_1    | 2023-04-04 07:54:25,848 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-LeaderElection1] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState
dn3_1    | 2023-04-04 07:54:25,848 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-04-04 07:54:38,884 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: reject ELECTION from om1: already has voted for om2 at current term 4
om3_1    | 2023-04-04 07:54:38,805 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om1<-om3#0:OK-t4. Peer's state: om3@group-D66704EFC61C:t4, leader=null, voted=om1, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c106, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-04-04 07:54:38,829 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-04-04 07:54:38,836 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
scm2_1   | 2023-04-04 07:54:05,867 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-04-04 07:54:01,581 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1    | 2023-04-04 07:54:38,884 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om1<-om2#0:FAIL-t4. Peer's state: om2@group-D66704EFC61C:t4, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c106, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-04-04 07:54:39,101 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
dn3_1    | 2023-04-04 07:54:25,858 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-04-04 07:54:38,838 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om2#0:OK-t3
om3_1    | 2023-04-04 07:54:38,838 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
om3_1    | 2023-04-04 07:54:38,911 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 4, (t:3, i:106))
scm3_1   | 2023-04-04 07:54:31,938 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm3_1   | 2023-04-04 07:54:33,051 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-04-04 07:54:01,582 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1_1   | 2023-04-04 07:54:04,435 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO impl.FollowerState: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5149490444ns, electionTimeout:5143ms
scm1_1   | 2023-04-04 07:54:04,436 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO impl.RoleInfo: 861060d2-f30f-4260-bac0-b4e7af8aba9d: shutdown 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState
scm1_1   | 2023-04-04 07:54:04,437 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
scm1_1   | 2023-04-04 07:54:04,440 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1    | 2023-04-04 07:54:38,911 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-FOLLOWER: reject ELECTION from om2: already has voted for om1 at current term 4
om3_1    | 2023-04-04 07:54:38,913 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om2<-om3#0:FAIL-t4. Peer's state: om3@group-D66704EFC61C:t4, leader=null, voted=om1, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c106, conf=67: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-04-04 07:54:39,356 [om3-server-thread2] INFO server.RaftServer$Division: om3@group-D66704EFC61C: change Leader from null to om1 at term 4 for appendEntries, leader elected after 13478ms
scm3_1   | 2023-04-04 07:54:33,056 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn3_1    | 2023-04-04 07:54:25,953 [grpc-default-executor-0] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: receive requestVote(PRE_VOTE, 2ebde02c-a404-41d0-92a4-7b6da490547a, group-0EBF5B06395B, 14, (t:14, i:20))
om2_1    | 2023-04-04 07:54:39,101 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:FAIL-t4
scm1_1   | 2023-04-04 07:54:04,440 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO impl.RoleInfo: 861060d2-f30f-4260-bac0-b4e7af8aba9d: start 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1
scm2_1   | 2023-04-04 07:54:05,867 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
dn2_1    | 2023-04-04 07:54:22,279 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-04-04 07:54:22,225 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F3D51D6EC210 with new leaderId: e2957b23-687f-4626-af75-9b42f3a43b99
scm3_1   | 2023-04-04 07:54:33,058 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 2023-04-04 07:54:25,953 [grpc-default-executor-0] INFO impl.VoteContext: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FOLLOWER: accept PRE_VOTE from 2ebde02c-a404-41d0-92a4-7b6da490547a: our priority 0 <= candidate's priority 0
om2_1    | 2023-04-04 07:54:39,102 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om2<-om3#0:FAIL-t4
om2_1    | 2023-04-04 07:54:39,102 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result REJECTED
scm2_1   | 2023-04-04 07:54:06,347 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
dn2_1    | 2023-04-04 07:54:22,286 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: change Leader from null to e2957b23-687f-4626-af75-9b42f3a43b99 at term 4 for becomeLeader, leader elected after 31805ms
dn2_1    | 2023-04-04 07:54:22,238 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 2ebde02c-a404-41d0-92a4-7b6da490547a
scm3_1   | 2023-04-04 07:54:33,071 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn3_1    | 2023-04-04 07:54:25,961 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: receive requestVote(PRE_VOTE, 2ebde02c-a404-41d0-92a4-7b6da490547a, group-EEB13EAC206E, 18, (t:18, i:46))
om2_1    | 2023-04-04 07:54:39,102 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 4 for REJECTED
scm1_1   | 2023-04-04 07:54:04,443 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 5 for 59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:06,349 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-04-04 07:54:06,350 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2_1   | 2023-04-04 07:54:06,387 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3_1   | 2023-04-04 07:54:33,591 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@569348e1] INFO util.JvmPauseMonitor: Starting JVM pause monitor
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | 2023-04-04 07:55:50,929 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
om1_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn3_1    | 2023-04-04 07:54:25,961 [grpc-default-executor-1] INFO impl.VoteContext: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-CANDIDATE: accept PRE_VOTE from 2ebde02c-a404-41d0-92a4-7b6da490547a: our priority 0 <= candidate's priority 1
om2_1    | 2023-04-04 07:54:39,103 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-LeaderElection1
scm1_1   | 2023-04-04 07:54:04,468 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-04-04 07:54:06,388 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2_1   | 2023-04-04 07:54:06,391 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1    | 2023-04-04 07:54:39,374 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 107: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:33,745 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#7 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:50886: output error
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn3_1    | 2023-04-04 07:54:25,961 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E replies to PRE_VOTE vote request: 2ebde02c-a404-41d0-92a4-7b6da490547a<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t18. Peer's state: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E:t18, leader=null, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLog:OPENED:c46, conf=26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-04-04 07:54:39,104 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-04-04 07:54:39,115 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-04-04 07:54:22,308 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-04-04 07:54:22,455 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-04-04 07:54:22,476 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-04-04 07:54:22,522 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-04-04 07:55:50,930 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn3_1    | 2023-04-04 07:54:25,993 [grpc-default-executor-0] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B replies to PRE_VOTE vote request: 2ebde02c-a404-41d0-92a4-7b6da490547a<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t14. Peer's state: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B:t14, leader=null, voted=e2957b23-687f-4626-af75-9b42f3a43b99, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLog:OPENED:c20, conf=15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:26,067 [grpc-default-executor-0] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: receive requestVote(ELECTION, e2957b23-687f-4626-af75-9b42f3a43b99, group-0EBF5B06395B, 15, (t:14, i:20))
scm1_1   | 2023-04-04 07:54:04,469 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-04-04 07:54:22,546 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-04-04 07:54:22,549 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om3_1    | 2023-04-04 07:54:39,478 [om3-server-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:107
scm3_1   | 2023-04-04 07:54:33,752 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | 2023-04-04 07:55:50,930 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn5_1    | 	... 12 more
scm2_1   | 2023-04-04 07:54:06,392 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2_1   | 2023-04-04 07:54:06,486 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@414b2df5] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm2_1   | 2023-04-04 07:54:06,535 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3_1   | java.nio.channels.ClosedChannelException
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 2023-04-04 07:55:50,932 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn4_1    | 2023-04-04 07:55:50,932 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
scm1_1   | 2023-04-04 07:54:04,470 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 19de384c-7008-4d6c-8004-b0a99b5aa3c2
scm1_1   | 2023-04-04 07:54:04,472 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for e0c91ed2-48f2-4467-9ffd-e28e64336c64
scm2_1   | 2023-04-04 07:54:06,536 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om3_1    | 2023-04-04 07:54:40,286 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_107
om3_1    | 2023-04-04 07:54:40,934 [om3@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn4_1    | 2023-04-04 07:55:50,933 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn4_1    | 2023-04-04 07:55:50,933 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om2_1    | 2023-04-04 07:54:39,116 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-04-04 07:54:04,782 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1    | [id: "om1"
om3_1    | address: "om1:9872"
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 2023-04-04 07:54:26,067 [grpc-default-executor-0] INFO impl.VoteContext: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FOLLOWER: accept ELECTION from e2957b23-687f-4626-af75-9b42f3a43b99: our priority 0 <= candidate's priority 1
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn4_1    | 2023-04-04 07:55:50,933 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn4_1    | 2023-04-04 07:56:15,278 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-04-04 07:54:39,304 [om2-server-thread2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: change Leader from null to om1 at term 4 for appendEntries, leader elected after 12898ms
scm2_1   | 2023-04-04 07:54:06,567 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @6659ms to org.eclipse.jetty.util.log.Slf4jLog
scm2_1   | 2023-04-04 07:54:06,801 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om3"
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
om3_1    | address: "om3:9872"
dn3_1    | 2023-04-04 07:54:26,067 [grpc-default-executor-0] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: changes role from  FOLLOWER to FOLLOWER at term 15 for candidate:e2957b23-687f-4626-af75-9b42f3a43b99
dn3_1    | 2023-04-04 07:54:26,068 [grpc-default-executor-0] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState
om2_1    | 2023-04-04 07:54:39,324 [om2-server-thread2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 107: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-04-04 07:54:39,406 [om2-server-thread2] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:107
dn4_1    | 2023-04-04 07:56:20,928 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-04-04 07:56:20,936 [Command processor thread] INFO server.RaftServer: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: remove    LEADER 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410:t4, leader=14df8ad4-ce2f-4483-aada-29ab5cae3ffb, voted=14df8ad4-ce2f-4483-aada-29ab5cae3ffb, raftlog=Memoized:14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLog:OPENED:c6, conf=5: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
om3_1    | startupRole: FOLLOWER
dn2_1    | 2023-04-04 07:54:22,569 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-04-04 07:54:22,663 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-04-04 07:54:22,721 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderStateImpl
om3_1    | , id: "om2"
om2_1    | 2023-04-04 07:54:40,105 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_107
scm1_1   | 2023-04-04 07:54:04,787 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:04,788 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
om3_1    | address: "om2:9872"
om3_1    | startupRole: FOLLOWER
om2_1    | 2023-04-04 07:54:40,404 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
scm1_1   | 2023-04-04 07:54:04,789 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:04,789 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-04-04 07:56:20,938 [Command processor thread] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: shutdown
dn4_1    | 2023-04-04 07:56:20,938 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-6617081AE410,id=14df8ad4-ce2f-4483-aada-29ab5cae3ffb
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 2023-04-04 07:54:26,068 [grpc-default-executor-0] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
om3_1    | ]
dn2_1    | 2023-04-04 07:54:22,764 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
om2_1    | [id: "om1"
scm1_1   | 2023-04-04 07:54:04,789 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1 PRE_VOTE round 0: result REJECTED
scm2_1   | 2023-04-04 07:54:06,821 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
dn4_1    | 2023-04-04 07:56:20,938 [Command processor thread] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: shutdown 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-LeaderStateImpl
dn5_1    | 2023-04-04 07:54:10,406 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn3_1    | 2023-04-04 07:54:26,068 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO impl.FollowerState: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState was interrupted
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
om3_1    | 2023-04-04 07:56:42,451 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 3
dn2_1    | 2023-04-04 07:54:22,827 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210/current/log_inprogress_3 to /data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210/current/log_3-4
om2_1    | address: "om1:9872"
scm1_1   | 2023-04-04 07:54:04,791 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: changes role from CANDIDATE to FOLLOWER at term 5 for REJECTED
scm2_1   | 2023-04-04 07:54:06,857 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn4_1    | 2023-04-04 07:56:20,943 [Command processor thread] INFO impl.PendingRequests: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-PendingRequests: sendNotLeaderResponses
dn5_1    | java.net.SocketTimeoutException: Call From 7e8229df9409/10.9.0.21 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:39196 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 2023-04-04 07:54:26,070 [grpc-default-executor-0] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B replies to ELECTION vote request: e2957b23-687f-4626-af75-9b42f3a43b99<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t15. Peer's state: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B:t15, leader=null, voted=e2957b23-687f-4626-af75-9b42f3a43b99, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLog:OPENED:c20, conf=15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:54:26,069 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-04-04 07:57:43,998 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
dn2_1    | 2023-04-04 07:54:22,888 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210/current/log_inprogress_5
om2_1    | startupRole: FOLLOWER
scm1_1   | 2023-04-04 07:54:04,791 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1] INFO impl.RoleInfo: 861060d2-f30f-4260-bac0-b4e7af8aba9d: shutdown 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1
scm2_1   | 2023-04-04 07:54:06,868 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
dn4_1    | 2023-04-04 07:56:20,945 [Command processor thread] INFO impl.StateMachineUpdater: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-StateMachineUpdater: set stopIndex = 6
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn3_1    | 2023-04-04 07:54:26,083 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
om3_1    | 2023-04-04 07:57:47,083 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om3_1    | 2023-04-04 07:57:57,155 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om2_1    | , id: "om3"
scm1_1   | 2023-04-04 07:54:04,792 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection1] INFO impl.RoleInfo: 861060d2-f30f-4260-bac0-b4e7af8aba9d: start 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState
scm2_1   | 2023-04-04 07:54:06,870 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1   | 2023-04-04 07:54:06,870 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 2023-04-04 07:54:26,125 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
om3_1    | 2023-04-04 07:58:05,617 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
dn2_1    | 2023-04-04 07:54:22,969 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderElection2] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: set configuration 5: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | address: "om3:9872"
scm1_1   | 2023-04-04 07:54:04,796 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-04-04 07:54:06,972 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
dn4_1    | 2023-04-04 07:56:20,945 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-6617081AE410: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410/sm/snapshot.4_6
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 2023-04-04 07:54:26,125 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2] INFO impl.LeaderElection:   Response 0: c1077390-d65b-4523-9cd4-abe9e2c9eb94<-2ebde02c-a404-41d0-92a4-7b6da490547a#0:FAIL-t18
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
om3_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
dn2_1    | 2023-04-04 07:54:25,665 [grpc-default-executor-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: receive requestVote(PRE_VOTE, c1077390-d65b-4523-9cd4-abe9e2c9eb94, group-EEB13EAC206E, 18, (t:18, i:46))
om2_1    | startupRole: FOLLOWER
om2_1    | , id: "om2"
scm2_1   | 2023-04-04 07:54:06,975 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
dn4_1    | 2023-04-04 07:56:20,947 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-6617081AE410: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410/sm/snapshot.4_6 took: 1 ms
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
recon_1  | 2023-04-04 07:54:18,612 [IPC Server handler 11 on default port 9891] WARN ipc.Server: IPC Server handler 11 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:55424: output error
dn3_1    | 2023-04-04 07:54:26,125 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2] INFO impl.LeaderElection:   Response 1: c1077390-d65b-4523-9cd4-abe9e2c9eb94<-e2957b23-687f-4626-af75-9b42f3a43b99#0:OK-t18
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:213)
dn2_1    | 2023-04-04 07:54:25,667 [grpc-default-executor-0] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: receive requestVote(PRE_VOTE, c1077390-d65b-4523-9cd4-abe9e2c9eb94, group-0EBF5B06395B, 14, (t:14, i:20))
om2_1    | address: "om2:9872"
scm1_1   | 2023-04-04 07:54:04,796 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-04-04 07:54:06,977 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn4_1    | 2023-04-04 07:56:20,948 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-StateMachineUpdater] INFO impl.StateMachineUpdater: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-StateMachineUpdater: Took a snapshot at index 6
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
recon_1  | 2023-04-04 07:54:18,611 [IPC Server handler 14 on default port 9891] WARN ipc.Server: IPC Server handler 14 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:32798: output error
dn3_1    | 2023-04-04 07:54:26,125 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2 PRE_VOTE round 0: result REJECTED
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 2023-04-04 07:54:25,679 [grpc-default-executor-0] INFO impl.VoteContext: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-CANDIDATE: reject PRE_VOTE from c1077390-d65b-4523-9cd4-abe9e2c9eb94: our priority 1 > candidate's priority 0
om2_1    | startupRole: FOLLOWER
scm1_1   | 2023-04-04 07:54:09,811 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO impl.FollowerState: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5018829637ns, electionTimeout:5014ms
scm2_1   | 2023-04-04 07:54:07,046 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-04-04 07:56:20,949 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-StateMachineUpdater] INFO impl.StateMachineUpdater: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
recon_1  | 2023-04-04 07:54:18,794 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891 caught an exception
dn3_1    | 2023-04-04 07:54:26,125 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: changes role from CANDIDATE to FOLLOWER at term 18 for REJECTED
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
dn2_1    | 2023-04-04 07:54:25,714 [grpc-default-executor-0] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B replies to PRE_VOTE vote request: c1077390-d65b-4523-9cd4-abe9e2c9eb94<-e2957b23-687f-4626-af75-9b42f3a43b99#0:FAIL-t14. Peer's state: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B:t14, leader=null, voted=e2957b23-687f-4626-af75-9b42f3a43b99, raftlog=Memoized:e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLog:OPENED:c20, conf=15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | ]
scm1_1   | 2023-04-04 07:54:09,811 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO impl.RoleInfo: 861060d2-f30f-4260-bac0-b4e7af8aba9d: shutdown 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState
scm2_1   | 2023-04-04 07:54:07,048 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 2023-04-04 07:56:20,952 [Command processor thread] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: closes. applyIndex: 6
dn4_1    | 2023-04-04 07:56:20,952 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
recon_1  | java.nio.channels.ClosedChannelException
dn3_1    | 2023-04-04 07:54:26,125 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
dn2_1    | 2023-04-04 07:54:25,718 [grpc-default-executor-1] INFO impl.VoteContext: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-CANDIDATE: accept PRE_VOTE from c1077390-d65b-4523-9cd4-abe9e2c9eb94: our priority 0 <= candidate's priority 0
om2_1    | 2023-04-04 07:56:42,455 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 3
scm1_1   | 2023-04-04 07:54:09,812 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
scm2_1   | 2023-04-04 07:54:07,049 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
dn4_1    | 2023-04-04 07:56:20,953 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410-SegmentedRaftLogWorker close()
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-04-04 07:54:26,126 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-LeaderElection2] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState
dn3_1    | 2023-04-04 07:54:26,137 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
dn2_1    | 2023-04-04 07:54:25,743 [grpc-default-executor-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E replies to PRE_VOTE vote request: c1077390-d65b-4523-9cd4-abe9e2c9eb94<-e2957b23-687f-4626-af75-9b42f3a43b99#0:OK-t18. Peer's state: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E:t18, leader=null, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLog:OPENED:c46, conf=26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-04-04 07:57:44,038 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
scm1_1   | 2023-04-04 07:54:09,812 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm2_1   | 2023-04-04 07:54:07,074 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3bd08435{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 2023-04-04 07:56:20,963 [Command processor thread] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-6617081AE410: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/685cb9fd-d40d-4ecb-9ff2-6617081ae410
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 2023-04-04 07:54:26,137 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-04-04 07:54:33,747 [IPC Server handler 8 on default port 9861] WARN ipc.Server: IPC Server handler 8 on default port 9861, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:36490: output error
om3_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn2_1    | 2023-04-04 07:54:25,764 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
om2_1    | 2023-04-04 07:57:47,090 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
scm1_1   | 2023-04-04 07:54:09,812 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-FollowerState] INFO impl.RoleInfo: 861060d2-f30f-4260-bac0-b4e7af8aba9d: start 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2
scm2_1   | 2023-04-04 07:54:07,084 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@482a58c7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn4_1    | 2023-04-04 07:56:20,964 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=685cb9fd-d40d-4ecb-9ff2-6617081ae410 command on datanode 14df8ad4-ce2f-4483-aada-29ab5cae3ffb.
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 2023-04-04 07:54:33,790 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861 caught an exception
dn3_1    | 2023-04-04 07:54:26,361 [grpc-default-executor-0] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: receive requestVote(ELECTION, 2ebde02c-a404-41d0-92a4-7b6da490547a, group-EEB13EAC206E, 19, (t:18, i:46))
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 2023-04-04 07:57:57,157 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
scm1_1   | 2023-04-04 07:54:09,818 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 5 for 59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:07,341 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4536a715{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-3316381579591179356/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
dn4_1    | 2023-04-04 07:56:30,525 [grpc-default-executor-0] INFO server.RaftServer: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: addNew group-4E27919647DF:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns group-4E27919647DF:java.util.concurrent.CompletableFuture@3027c3a6[Not completed]
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | java.nio.channels.ClosedChannelException
dn3_1    | 2023-04-04 07:54:26,363 [grpc-default-executor-0] INFO impl.VoteContext: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FOLLOWER: accept ELECTION from 2ebde02c-a404-41d0-92a4-7b6da490547a: our priority 0 <= candidate's priority 1
om3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-04-04 07:54:25,766 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO impl.LeaderElection:   Response 0: e2957b23-687f-4626-af75-9b42f3a43b99<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t14
om2_1    | 2023-04-04 07:58:05,609 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
scm1_1   | 2023-04-04 07:54:09,819 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-04-04 07:54:07,352 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@5599b5bb{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
dn4_1    | 2023-04-04 07:56:30,529 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: new RaftServerImpl for group-4E27919647DF:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-04-04 07:54:26,363 [grpc-default-executor-0] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: changes role from  FOLLOWER to FOLLOWER at term 19 for candidate:2ebde02c-a404-41d0-92a4-7b6da490547a
dn2_1    | 2023-04-04 07:54:25,766 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1 PRE_VOTE round 0: result PASSED
dn2_1    | 2023-04-04 07:54:25,779 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1 ELECTION round 0: submit vote requests at term 15 for 15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
scm2_1   | 2023-04-04 07:54:07,352 [Listener at 0.0.0.0/9860] INFO server.Server: Started @7444ms
scm2_1   | 2023-04-04 07:54:07,355 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 2023-04-04 07:54:26,363 [grpc-default-executor-0] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState
dn2_1    | 2023-04-04 07:54:25,852 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-04-04 07:54:09,819 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:213)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:337)
scm2_1   | 2023-04-04 07:54:07,355 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2_1   | 2023-04-04 07:54:07,357 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn3_1    | 2023-04-04 07:54:26,363 [grpc-default-executor-0] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState
dn2_1    | 2023-04-04 07:54:25,894 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-04-04 07:54:26,019 [grpc-default-executor-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: receive requestVote(PRE_VOTE, 2ebde02c-a404-41d0-92a4-7b6da490547a, group-0EBF5B06395B, 14, (t:14, i:20))
dn4_1    | 2023-04-04 07:56:30,530 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-04-04 07:54:09,820 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm2_1   | 2023-04-04 07:54:09,985 [grpc-default-executor-0] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: receive requestVote(PRE_VOTE, 861060d2-f30f-4260-bac0-b4e7af8aba9d, group-0BB16BE3F1B7, 5, (t:5, i:72))
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-04-04 07:54:26,363 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO impl.FollowerState: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState was interrupted
dn3_1    | 2023-04-04 07:54:26,367 [grpc-default-executor-0] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E replies to ELECTION vote request: 2ebde02c-a404-41d0-92a4-7b6da490547a<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t19. Peer's state: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E:t19, leader=null, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLog:OPENED:c46, conf=26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:26,026 [grpc-default-executor-1] INFO impl.VoteContext: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-CANDIDATE: reject PRE_VOTE from 2ebde02c-a404-41d0-92a4-7b6da490547a: our priority 1 > candidate's priority 0
dn4_1    | 2023-04-04 07:56:30,530 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-04-04 07:54:10,048 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 1 exception(s):
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
scm2_1   | 2023-04-04 07:54:09,988 [grpc-default-executor-0] INFO impl.VoteContext: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-FOLLOWER: accept PRE_VOTE from 861060d2-f30f-4260-bac0-b4e7af8aba9d: our priority 0 <= candidate's priority 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-04-04 07:54:26,374 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-04-04 07:54:26,374 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-04-04 07:54:26,028 [grpc-default-executor-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B replies to PRE_VOTE vote request: 2ebde02c-a404-41d0-92a4-7b6da490547a<-e2957b23-687f-4626-af75-9b42f3a43b99#0:FAIL-t15. Peer's state: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B:t15, leader=null, voted=e2957b23-687f-4626-af75-9b42f3a43b99, raftlog=Memoized:e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLog:OPENED:c20, conf=15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:56:30,530 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1   | 2023-04-04 07:54:10,049 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.LeaderElection:   Response 0: 861060d2-f30f-4260-bac0-b4e7af8aba9d<-19de384c-7008-4d6c-8004-b0a99b5aa3c2#0:OK-t5
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
scm2_1   | 2023-04-04 07:54:10,012 [grpc-default-executor-0] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7 replies to PRE_VOTE vote request: 861060d2-f30f-4260-bac0-b4e7af8aba9d<-19de384c-7008-4d6c-8004-b0a99b5aa3c2#0:OK-t5. Peer's state: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7:t5, leader=null, voted=861060d2-f30f-4260-bac0-b4e7af8aba9d, raftlog=Memoized:19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-SegmentedRaftLog:OPENED:c72, conf=59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn2_1    | 2023-04-04 07:54:26,023 [grpc-default-executor-0] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: receive requestVote(PRE_VOTE, 2ebde02c-a404-41d0-92a4-7b6da490547a, group-EEB13EAC206E, 18, (t:18, i:46))
scm1_1   | 2023-04-04 07:54:10,049 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:10,049 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2 PRE_VOTE round 0: result PASSED
scm1_1   | 2023-04-04 07:54:10,078 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2 ELECTION round 0: submit vote requests at term 6 for 59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:10,111 [grpc-default-executor-0] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: receive requestVote(ELECTION, 861060d2-f30f-4260-bac0-b4e7af8aba9d, group-0BB16BE3F1B7, 6, (t:5, i:72))
scm2_1   | 2023-04-04 07:54:10,112 [grpc-default-executor-0] INFO impl.VoteContext: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-FOLLOWER: accept ELECTION from 861060d2-f30f-4260-bac0-b4e7af8aba9d: our priority 0 <= candidate's priority 0
dn3_1    | 2023-04-04 07:54:26,781 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0EBF5B06395B with new leaderId: e2957b23-687f-4626-af75-9b42f3a43b99
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
dn2_1    | 2023-04-04 07:54:26,028 [grpc-default-executor-0] INFO impl.VoteContext: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-CANDIDATE: accept PRE_VOTE from 2ebde02c-a404-41d0-92a4-7b6da490547a: our priority 0 <= candidate's priority 1
dn2_1    | 2023-04-04 07:54:26,029 [grpc-default-executor-0] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E replies to PRE_VOTE vote request: 2ebde02c-a404-41d0-92a4-7b6da490547a<-e2957b23-687f-4626-af75-9b42f3a43b99#0:OK-t18. Peer's state: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E:t18, leader=null, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLog:OPENED:c46, conf=26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
scm1_1   | 2023-04-04 07:54:10,083 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-04-04 07:54:10,112 [grpc-default-executor-0] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: changes role from  FOLLOWER to FOLLOWER at term 6 for candidate:861060d2-f30f-4260-bac0-b4e7af8aba9d
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn3_1    | 2023-04-04 07:54:26,798 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread2] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: change Leader from null to e2957b23-687f-4626-af75-9b42f3a43b99 at term 15 for appendEntries, leader elected after 40001ms
dn3_1    | 2023-04-04 07:54:26,822 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread2] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: set configuration 21: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 2023-04-04 07:54:26,109 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 2023-04-04 07:54:10,084 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-04-04 07:54:10,113 [grpc-default-executor-0] INFO impl.RoleInfo: 19de384c-7008-4d6c-8004-b0a99b5aa3c2: shutdown 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-FollowerState
scm2_1   | 2023-04-04 07:54:10,114 [grpc-default-executor-0] INFO impl.RoleInfo: 19de384c-7008-4d6c-8004-b0a99b5aa3c2: start 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-FollowerState
dn3_1    | 2023-04-04 07:54:26,848 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread2] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLogWorker: Rolling segment log-15_20 to index:20
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 2023-04-04 07:54:26,119 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO impl.LeaderElection:   Response 0: e2957b23-687f-4626-af75-9b42f3a43b99<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t15
dn2_1    | 2023-04-04 07:54:26,120 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1 ELECTION round 0: result PASSED
scm1_1   | 2023-04-04 07:54:10,084 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-04-04 07:54:10,114 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-FollowerState] INFO impl.FollowerState: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-FollowerState was interrupted
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-04-04 07:54:26,864 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_inprogress_15 to /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_15-20
dn4_1    | 2023-04-04 07:56:30,530 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-04-04 07:54:26,120 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: shutdown e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1
dn2_1    | 2023-04-04 07:54:26,120 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: changes role from CANDIDATE to LEADER at term 15 for changeToLeader
scm1_1   | 2023-04-04 07:54:10,150 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2: ELECTION PASSED received 1 response(s) and 1 exception(s):
scm2_1   | 2023-04-04 07:54:10,119 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-04-04 07:54:10,121 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-04-04 07:54:26,878 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_inprogress_21
dn4_1    | 2023-04-04 07:56:30,530 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-04-04 07:54:26,121 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0EBF5B06395B with new leaderId: e2957b23-687f-4626-af75-9b42f3a43b99
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 2023-04-04 07:54:10,150 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.LeaderElection:   Response 0: 861060d2-f30f-4260-bac0-b4e7af8aba9d<-19de384c-7008-4d6c-8004-b0a99b5aa3c2#0:OK-t6
scm2_1   | 2023-04-04 07:54:10,139 [grpc-default-executor-0] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7 replies to ELECTION vote request: 861060d2-f30f-4260-bac0-b4e7af8aba9d<-19de384c-7008-4d6c-8004-b0a99b5aa3c2#0:OK-t6. Peer's state: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7:t6, leader=null, voted=861060d2-f30f-4260-bac0-b4e7af8aba9d, raftlog=Memoized:19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-SegmentedRaftLog:OPENED:c72, conf=59: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn3_1    | 2023-04-04 07:54:27,085 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EEB13EAC206E with new leaderId: 2ebde02c-a404-41d0-92a4-7b6da490547a
dn4_1    | 2023-04-04 07:56:30,530 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 2023-04-04 07:54:10,150 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-04-04 07:54:10,496 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-server-thread1] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm2_1   | 2023-04-04 07:54:10,500 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-server-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: change Leader from null to 861060d2-f30f-4260-bac0-b4e7af8aba9d at term 6 for appendEntries, leader elected after 7281ms
dn3_1    | 2023-04-04 07:54:27,086 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: change Leader from null to 2ebde02c-a404-41d0-92a4-7b6da490547a at term 19 for appendEntries, leader elected after 39305ms
dn4_1    | 2023-04-04 07:56:30,530 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF: ConfigurationManager, init=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 2023-04-04 07:54:10,155 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.LeaderElection: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2 ELECTION round 0: result PASSED
scm1_1   | 2023-04-04 07:54:10,173 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.RoleInfo: 861060d2-f30f-4260-bac0-b4e7af8aba9d: shutdown 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 2023-04-04 07:54:27,213 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread2] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: set configuration 47: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:56:30,530 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-04-04 07:56:30,531 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 2023-04-04 07:54:10,504 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-server-thread1] INFO server.RaftServer$Division: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7: set configuration 73: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:54:10,174 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: changes role from CANDIDATE to LEADER at term 6 for changeToLeader
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-04-04 07:56:30,531 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-04-04 07:54:26,121 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: change Leader from null to e2957b23-687f-4626-af75-9b42f3a43b99 at term 15 for becomeLeader, leader elected after 36831ms
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-04-04 07:54:33,746 [IPC Server handler 10 on default port 9861] WARN ipc.Server: IPC Server handler 10 on default port 9861, call Call#19 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:40042: output error
scm2_1   | 2023-04-04 07:54:10,520 [19de384c-7008-4d6c-8004-b0a99b5aa3c2-server-thread1] INFO segmented.SegmentedRaftLogWorker: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-SegmentedRaftLogWorker: Rolling segment log-59_72 to index:72
scm1_1   | 2023-04-04 07:54:10,174 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO ha.SCMStateMachine: current SCM becomes leader of term 6.
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 2023-04-04 07:54:27,218 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread2] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLogWorker: Rolling segment log-26_46 to index:46
dn4_1    | 2023-04-04 07:56:30,532 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-04-04 07:54:26,122 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:39196 remote=scm2/10.9.0.15:9861]
scm3_1   | 2023-04-04 07:54:33,794 [IPC Server handler 10 on default port 9861] INFO ipc.Server: IPC Server handler 10 on default port 9861 caught an exception
scm2_1   | 2023-04-04 07:54:10,531 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_inprogress_59 to /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_59-72
scm1_1   | 2023-04-04 07:54:10,174 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,6>
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 2023-04-04 07:54:27,219 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_inprogress_26 to /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_26-46
dn4_1    | 2023-04-04 07:56:30,532 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-04-04 07:54:26,122 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
scm3_1   | java.nio.channels.ClosedChannelException
scm2_1   | 2023-04-04 07:54:10,609 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_inprogress_73
scm1_1   | 2023-04-04 07:54:10,178 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: change Leader from null to 861060d2-f30f-4260-bac0-b4e7af8aba9d at term 6 for becomeLeader, leader elected after 15643ms
recon_1  | 2023-04-04 07:54:18,611 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:47852: output error
dn3_1    | 2023-04-04 07:54:27,234 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_inprogress_47
dn4_1    | 2023-04-04 07:56:30,532 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-04-04 07:54:26,127 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
scm2_1   | 2023-04-04 07:54:10,647 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm1_1   | 2023-04-04 07:54:10,201 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1  | 2023-04-04 07:54:18,794 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
dn3_1    | 2023-04-04 07:54:33,760 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-04-04 07:56:30,535 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn2_1    | 2023-04-04 07:54:26,133 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm2_1   | 2023-04-04 07:54:10,654 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1_1   | 2023-04-04 07:54:10,214 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1  | java.nio.channels.ClosedChannelException
dn3_1    | 2023-04-04 07:55:17,346 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-04-04 07:56:30,535 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn2_1    | 2023-04-04 07:54:26,141 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm2_1   | 2023-04-04 07:54:10,655 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm1_1   | 2023-04-04 07:54:10,218 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-04-04 07:55:23,799 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-04-04 07:55:23,801 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn2_1    | 2023-04-04 07:54:26,141 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm2_1   | 2023-04-04 07:54:10,657 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm1_1   | 2023-04-04 07:54:10,244 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 2023-04-04 07:56:30,535 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-04-04 07:55:23,802 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn2_1    | 2023-04-04 07:54:26,141 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm2_1   | 2023-04-04 07:54:10,666 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1_1   | 2023-04-04 07:54:10,244 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 2023-04-04 07:55:23,802 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn2_1    | 2023-04-04 07:54:26,142 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm2_1   | 2023-04-04 07:54:10,669 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-04-04 07:54:10,247 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-04-04 07:56:30,535 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-04-04 07:56:30,535 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn2_1    | 2023-04-04 07:54:26,213 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
scm2_1   | 2023-04-04 07:54:10,923 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:37356: output error
scm1_1   | 2023-04-04 07:54:10,258 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
dn4_1    | 2023-04-04 07:56:30,537 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df does not exist. Creating ...
dn4_1    | 2023-04-04 07:56:30,545 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df/in_use.lock acquired by nodename 6@0972042128cd
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 2023-04-04 07:54:26,222 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3] INFO impl.LeaderElection:   Response 0: e2957b23-687f-4626-af75-9b42f3a43b99<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t18
scm2_1   | 2023-04-04 07:54:10,933 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm1_1   | 2023-04-04 07:54:10,262 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-04-04 07:56:30,546 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df has been successfully formatted.
dn4_1    | 2023-04-04 07:56:30,560 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-4E27919647DF: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn2_1    | 2023-04-04 07:54:26,223 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3] INFO impl.LeaderElection:   Response 1: e2957b23-687f-4626-af75-9b42f3a43b99<-2ebde02c-a404-41d0-92a4-7b6da490547a#0:FAIL-t18
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 2023-04-04 07:56:30,560 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 2023-04-04 07:54:26,223 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3 PRE_VOTE round 0: result REJECTED
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 2023-04-04 07:54:10,286 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1_1   | 2023-04-04 07:54:10,286 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-04-04 07:56:30,561 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-04-04 07:56:30,561 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn5_1    | 2023-04-04 07:54:11,080 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn5_1    | 2023-04-04 07:54:11,095 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn2_1    | 2023-04-04 07:54:26,225 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: changes role from CANDIDATE to FOLLOWER at term 18 for REJECTED
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 2023-04-04 07:54:10,287 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn4_1    | 2023-04-04 07:56:30,561 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-04-04 07:56:30,561 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-04-04 07:54:11,276 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn2_1    | 2023-04-04 07:54:26,225 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: shutdown e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3
dn2_1    | 2023-04-04 07:54:26,226 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-LeaderElection3] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState
scm1_1   | 2023-04-04 07:54:10,291 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | 2023-04-04 07:56:30,562 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn2_1    | 2023-04-04 07:54:26,255 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 2023-04-04 07:54:10,291 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-04-04 07:54:10,291 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 2023-04-04 07:55:23,802 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn2_1    | 2023-04-04 07:54:26,255 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | 2023-04-04 07:56:30,568 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-04-04 07:54:10,291 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-04-04 07:54:10,292 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 2023-04-04 07:54:26,285 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 2023-04-04 07:56:30,568 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 2023-04-04 07:54:10,295 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1_1   | 2023-04-04 07:54:10,295 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-04-04 07:54:26,288 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-04-04 07:56:30,568 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df
dn4_1    | 2023-04-04 07:56:30,569 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-04-04 07:56:30,569 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-04-04 07:54:10,296 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
dn2_1    | 2023-04-04 07:54:26,288 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:321)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-04-04 07:54:10,296 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-04-04 07:54:26,310 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-04-04 07:55:23,803 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn4_1    | 2023-04-04 07:56:30,571 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-04-04 07:56:30,571 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:518)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-04-04 07:54:10,296 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn2_1    | 2023-04-04 07:54:26,312 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
scm1_1   | 2023-04-04 07:54:10,296 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-04-04 07:54:26,312 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-04-04 07:54:26,312 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
scm1_1   | 2023-04-04 07:54:10,296 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-04-04 07:54:10,296 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-04-04 07:56:30,571 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 2023-04-04 07:56:30,571 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 2023-04-04 07:56:30,579 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-04-04 07:56:30,584 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 2023-04-04 07:54:26,312 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-04-04 07:54:33,746 [IPC Server handler 9 on default port 9861] WARN ipc.Server: IPC Server handler 9 on default port 9861, call Call#8 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:45218: output error
scm1_1   | 2023-04-04 07:54:10,300 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO impl.RoleInfo: 861060d2-f30f-4260-bac0-b4e7af8aba9d: start 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderStateImpl
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn5_1    | 	... 1 more
dn5_1    | 2023-04-04 07:54:11,404 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-04-04 07:54:33,754 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#20 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:47978: output error
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn4_1    | 2023-04-04 07:56:30,585 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-04-04 07:56:30,588 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-04-04 07:55:23,803 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-04-04 07:54:11,545 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn5_1    | 2023-04-04 07:54:11,546 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 2ebde02c-a404-41d0-92a4-7b6da490547a
dn2_1    | 2023-04-04 07:54:26,315 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm3_1   | 2023-04-04 07:54:33,763 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#23 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:40672: output error
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-04-04 07:54:11,640 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/in_use.lock acquired by nodename 7@7e8229df9409
dn5_1    | 2023-04-04 07:54:11,671 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=18, votedFor=2ebde02c-a404-41d0-92a4-7b6da490547a} from /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/raft-meta
dn2_1    | 2023-04-04 07:54:26,316 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 2023-04-04 07:54:10,956 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:33014: output error
dn5_1    | 2023-04-04 07:54:11,650 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/in_use.lock acquired by nodename 7@7e8229df9409
dn4_1    | 2023-04-04 07:56:30,607 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-04-04 07:56:30,608 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
scm2_1   | 2023-04-04 07:54:10,956 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm1_1   | 2023-04-04 07:54:10,308 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-SegmentedRaftLogWorker: Rolling segment log-59_72 to index:72
dn2_1    | 2023-04-04 07:54:26,316 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-04-04 07:54:26,316 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 2023-04-04 07:54:11,684 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=14, votedFor=2ebde02c-a404-41d0-92a4-7b6da490547a} from /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/raft-meta
dn2_1    | 2023-04-04 07:54:26,317 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-04-04 07:56:30,608 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm3_1   | 2023-04-04 07:54:33,802 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 2023-04-04 07:54:10,311 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_inprogress_59 to /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_59-72
dn5_1    | 2023-04-04 07:54:11,693 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6/in_use.lock acquired by nodename 7@7e8229df9409
dn2_1    | 2023-04-04 07:54:26,317 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-04-04 07:56:30,608 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
recon_1  | 2023-04-04 07:54:18,609 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:33856: output error
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn5_1    | 2023-04-04 07:54:11,705 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=2ebde02c-a404-41d0-92a4-7b6da490547a} from /data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6/current/raft-meta
dn2_1    | 2023-04-04 07:54:26,317 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-04-04 07:56:30,609 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 2023-04-04 07:54:18,606 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:42050: output error
scm1_1   | 2023-04-04 07:54:10,326 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/log_inprogress_73
scm1_1   | 2023-04-04 07:54:10,354 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-LeaderElection2] INFO server.RaftServer$Division: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7: set configuration 73: peers:[861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, e0c91ed2-48f2-4467-9ffd-e28e64336c64|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 19de384c-7008-4d6c-8004-b0a99b5aa3c2|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:11,835 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: set configuration 26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:26,317 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-04-04 07:56:30,613 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF: start as a follower, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 2023-04-04 07:54:18,796 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
dn2_1    | 2023-04-04 07:54:26,323 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderStateImpl
dn2_1    | 2023-04-04 07:54:26,326 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLogWorker: Rolling segment log-15_20 to index:20
dn5_1    | 2023-04-04 07:54:11,863 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: set configuration 15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-04-04 07:54:10,427 [grpc-default-executor-1] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn4_1    | 2023-04-04 07:56:30,614 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn5_1    | 2023-04-04 07:54:11,865 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: set configuration 3: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-04-04 07:54:10,440 [grpc-default-executor-1] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n73, attendVote=true, lastRpcSendTime=5375, lastRpcResponseTime=5375) yet, just keep nextIndex unchanged and retry.
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-04-04 07:56:30,614 [pool-26-thread-1] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: start 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-FollowerState
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn5_1    | 2023-04-04 07:54:11,932 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO ratis.ContainerStateMachine: group-EEB13EAC206E: Setting the last applied index to (t:18, i:46)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-04-04 07:54:10,469 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:10,485 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n74, attendVote=true, lastRpcSendTime=25, lastRpcResponseTime=5419) yet, just keep nextIndex unchanged and retry.
dn4_1    | 2023-04-04 07:56:30,615 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4E27919647DF,id=14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn4_1    | 2023-04-04 07:56:30,616 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn5_1    | 2023-04-04 07:54:11,935 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO ratis.ContainerStateMachine: group-65BC10982AC6: Setting the last applied index to (t:3, i:4)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 2023-04-04 07:54:10,477 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-04-04 07:55:23,803 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-04-04 07:54:26,333 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_inprogress_15 to /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_15-20
dn4_1    | 2023-04-04 07:56:30,616 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn5_1    | 2023-04-04 07:54:11,940 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO ratis.ContainerStateMachine: group-0EBF5B06395B: Setting the last applied index to (t:14, i:20)
scm1_1   | 2023-04-04 07:54:10,492 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n74, attendVote=true, lastRpcSendTime=33, lastRpcResponseTime=5427) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:55:23,803 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-04-04 07:54:26,338 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_inprogress_21
dn4_1    | 2023-04-04 07:56:30,616 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn5_1    | 2023-04-04 07:54:12,409 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-04-04 07:54:10,504 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-04-04 07:55:23,804 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-04-04 07:55:23,804 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
dn2_1    | 2023-04-04 07:54:26,345 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderElection1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: set configuration 21: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:54:26,398 [grpc-default-executor-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: receive requestVote(ELECTION, 2ebde02c-a404-41d0-92a4-7b6da490547a, group-EEB13EAC206E, 19, (t:18, i:46))
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 2023-04-04 07:54:10,525 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n74, attendVote=true, lastRpcSendTime=66, lastRpcResponseTime=5459) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:55:23,804 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-04-04 07:56:30,616 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn2_1    | 2023-04-04 07:54:26,398 [grpc-default-executor-1] INFO impl.VoteContext: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FOLLOWER: accept ELECTION from 2ebde02c-a404-41d0-92a4-7b6da490547a: our priority 0 <= candidate's priority 1
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn5_1    | 2023-04-04 07:54:12,740 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 2023-04-04 07:54:26,399 [grpc-default-executor-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: changes role from  FOLLOWER to FOLLOWER at term 19 for candidate:2ebde02c-a404-41d0-92a4-7b6da490547a
scm1_1   | 2023-04-04 07:54:10,642 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:10,642 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn5_1    | 2023-04-04 07:54:12,748 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-04-04 07:54:12,798 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-04-04 07:54:10,644 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=184, lastRpcResponseTime=5578) yet, just keep nextIndex unchanged and retry.
scm2_1   | 2023-04-04 07:54:10,981 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:39196: output error
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn5_1    | 2023-04-04 07:54:12,834 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-04-04 07:54:12,834 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-04-04 07:55:23,804 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-04-04 07:56:30,617 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-04-04 07:54:10,644 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=184, lastRpcResponseTime=5578) yet, just keep nextIndex unchanged and retry.
scm2_1   | 2023-04-04 07:54:10,983 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:43882: output error
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn2_1    | 2023-04-04 07:54:26,399 [grpc-default-executor-1] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: shutdown e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState
dn2_1    | 2023-04-04 07:54:26,399 [grpc-default-executor-1] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState
dn2_1    | 2023-04-04 07:54:26,399 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO impl.FollowerState: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState was interrupted
dn2_1    | 2023-04-04 07:54:26,445 [grpc-default-executor-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E replies to ELECTION vote request: 2ebde02c-a404-41d0-92a4-7b6da490547a<-e2957b23-687f-4626-af75-9b42f3a43b99#0:OK-t19. Peer's state: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E:t19, leader=null, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLog:OPENED:c46, conf=26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:12,838 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-04-04 07:54:26,458 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn5_1    | 2023-04-04 07:54:12,839 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-04-04 07:54:12,839 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-04-04 07:54:10,646 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
dn4_1    | 2023-04-04 07:56:30,625 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-04-04 07:56:31,528 [grpc-default-executor-1] INFO server.RaftServer: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: addNew group-1D7D5DE977D3:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-1D7D5DE977D3:java.util.concurrent.CompletableFuture@7728433[Not completed]
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 2023-04-04 07:54:10,993 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn5_1    | 2023-04-04 07:54:12,840 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn2_1    | 2023-04-04 07:54:26,458 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-04-04 07:54:27,204 [e2957b23-687f-4626-af75-9b42f3a43b99-server-thread2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EEB13EAC206E with new leaderId: 2ebde02c-a404-41d0-92a4-7b6da490547a
dn4_1    | 2023-04-04 07:56:31,532 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: new RaftServerImpl for group-1D7D5DE977D3:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn5_1    | 2023-04-04 07:54:12,849 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn5_1    | 2023-04-04 07:54:12,871 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-04-04 07:54:10,658 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 2023-04-04 07:56:31,535 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-04-04 07:54:27,250 [e2957b23-687f-4626-af75-9b42f3a43b99-server-thread2] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: change Leader from null to 2ebde02c-a404-41d0-92a4-7b6da490547a at term 19 for appendEntries, leader elected after 36626ms
dn2_1    | 2023-04-04 07:54:27,281 [e2957b23-687f-4626-af75-9b42f3a43b99-server-thread2] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: set configuration 47: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:12,871 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1   | 2023-04-04 07:54:33,759 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#20 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:48224: output error
scm3_1   | 2023-04-04 07:54:33,759 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#8 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:57220: output error
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn4_1    | 2023-04-04 07:56:31,536 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-04-04 07:56:31,537 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn5_1    | 2023-04-04 07:54:12,871 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | 2023-04-04 07:54:33,807 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm1_1   | 2023-04-04 07:54:10,670 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm1_1   | 2023-04-04 07:54:10,671 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
dn4_1    | 2023-04-04 07:56:31,537 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-04-04 07:56:31,537 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn2_1    | 2023-04-04 07:54:27,297 [e2957b23-687f-4626-af75-9b42f3a43b99-server-thread2] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLogWorker: Rolling segment log-26_46 to index:46
dn5_1    | 2023-04-04 07:54:12,874 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-04-04 07:54:12,884 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-04-04 07:54:10,671 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
recon_1  | 2023-04-04 07:54:18,606 [IPC Server handler 19 on default port 9891] WARN ipc.Server: IPC Server handler 19 on default port 9891, call Call#10 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:42054: output error
dn4_1    | 2023-04-04 07:56:31,538 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-04-04 07:56:31,538 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3: ConfigurationManager, init=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | java.nio.channels.ClosedChannelException
dn5_1    | 2023-04-04 07:54:12,885 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-04-04 07:54:10,671 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
recon_1  | 2023-04-04 07:54:18,797 [IPC Server handler 19 on default port 9891] INFO ipc.Server: IPC Server handler 19 on default port 9891 caught an exception
dn4_1    | 2023-04-04 07:56:31,538 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-04-04 07:56:31,551 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-04-04 07:54:27,362 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_inprogress_26 to /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_26-46
dn2_1    | 2023-04-04 07:54:27,366 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_inprogress_47
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-04-04 07:54:10,679 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn4_1    | 2023-04-04 07:56:31,551 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 2023-04-04 07:54:33,755 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-04-04 07:55:17,189 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-04-04 07:54:12,892 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-04-04 07:54:10,680 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | 2023-04-04 07:56:31,552 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 2023-04-04 07:55:27,380 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 13.
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn5_1    | 2023-04-04 07:54:12,893 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm1_1   | 2023-04-04 07:54:11,726 [IPC Server handler 9 on default port 9861] WARN ipc.Server: IPC Server handler 9 on default port 9861, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:54238: output error
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | 2023-04-04 07:56:31,552 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-04-04 07:55:27,380 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 13.
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn5_1    | 2023-04-04 07:54:12,920 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-04-04 07:54:11,732 [IPC Server handler 10 on default port 9861] WARN ipc.Server: IPC Server handler 10 on default port 9861, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:44198: output error
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-04-04 07:55:23,804 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn2_1    | 2023-04-04 07:55:27,398 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-0] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 13.
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn5_1    | 2023-04-04 07:54:12,924 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-04-04 07:54:11,741 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:60396: output error
scm1_1   | 2023-04-04 07:54:11,741 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:55874: output error
dn4_1    | 2023-04-04 07:56:31,553 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 2023-04-04 07:55:27,677 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-1] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 19.
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn5_1    | 2023-04-04 07:54:12,929 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-04-04 07:54:12,932 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-04-04 07:54:11,741 [IPC Server handler 11 on default port 9861] WARN ipc.Server: IPC Server handler 11 on default port 9861, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:43678: output error
dn4_1    | 2023-04-04 07:56:31,554 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn5_1    | 2023-04-04 07:54:12,944 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-04-04 07:54:11,736 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:53652: output error
scm1_1   | 2023-04-04 07:54:11,736 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:52370: output error
dn4_1    | 2023-04-04 07:56:31,555 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-04-04 07:56:31,556 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-04-04 07:56:31,557 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn5_1    | 2023-04-04 07:54:12,975 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-04-04 07:54:11,736 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:50712: output error
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn5_1    | 2023-04-04 07:54:13,038 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-04-04 07:54:11,735 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:60316: output error
scm1_1   | 2023-04-04 07:54:11,733 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:44826: output error
scm1_1   | 2023-04-04 07:54:11,752 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm2_1   | 2023-04-04 07:54:10,983 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:55996: output error
scm2_1   | 2023-04-04 07:54:10,997 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
dn5_1    | 2023-04-04 07:54:13,045 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn2_1    | 2023-04-04 07:55:27,691 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-1] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 19.
dn2_1    | 2023-04-04 07:55:27,738 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-1] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 19.
scm1_1   | java.nio.channels.ClosedChannelException
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-04-04 07:54:13,046 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn5_1    | 2023-04-04 07:54:13,052 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn5_1    | 2023-04-04 07:54:13,052 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn2_1    | 2023-04-04 07:55:27,750 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn5_1    | 2023-04-04 07:54:13,054 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 2023-04-04 07:56:31,557 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-04-04 07:55:23,805 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn5_1    | 2023-04-04 07:54:13,057 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn4_1    | 2023-04-04 07:56:31,557 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 does not exist. Creating ...
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-04-04 07:55:23,805 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn5_1    | 2023-04-04 07:54:13,069 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm3_1   | 2023-04-04 07:54:33,753 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#19 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:42780: output error
dn4_1    | 2023-04-04 07:56:31,559 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3/in_use.lock acquired by nodename 6@0972042128cd
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-04-04 07:55:23,805 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn5_1    | 2023-04-04 07:54:13,070 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-04-04 07:54:13,071 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-04-04 07:56:31,563 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 has been successfully formatted.
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn2_1    | 2023-04-04 07:55:27,790 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-04-04 07:55:23,805 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 2023-04-04 07:54:33,809 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
dn5_1    | 2023-04-04 07:54:13,072 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-04-04 07:54:13,074 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-04-04 07:55:23,805 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 2023-04-04 07:54:33,753 [IPC Server handler 13 on default port 9861] WARN ipc.Server: IPC Server handler 13 on default port 9861, call Call#8 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:51024: output error
dn4_1    | 2023-04-04 07:56:31,564 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-1D7D5DE977D3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-04-04 07:54:13,079 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-04-04 07:55:27,792 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn5_1    | 2023-04-04 07:54:13,052 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 2023-04-04 07:54:33,812 [IPC Server handler 13 on default port 9861] INFO ipc.Server: IPC Server handler 13 on default port 9861 caught an exception
dn4_1    | 2023-04-04 07:56:31,586 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-04-04 07:55:27,799 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
dn5_1    | 2023-04-04 07:54:13,095 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | java.nio.channels.ClosedChannelException
dn4_1    | 2023-04-04 07:56:31,587 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-04-04 07:56:31,587 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn2_1    | 2023-04-04 07:55:27,800 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn5_1    | 2023-04-04 07:54:13,095 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn4_1    | 2023-04-04 07:56:31,587 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-04-04 07:55:23,805 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn2_1    | 2023-04-04 07:55:27,800 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn5_1    | 2023-04-04 07:54:13,098 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 2023-04-04 07:56:31,587 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-04-04 07:56:31,588 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn5_1    | 2023-04-04 07:54:13,102 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-04-04 07:54:13,102 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-04-04 07:54:13,088 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-04-04 07:54:13,104 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-04-04 07:54:13,077 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn5_1    | 2023-04-04 07:54:13,169 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-04-04 07:54:13,170 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-04-04 07:54:13,174 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 2023-04-04 07:54:18,605 [IPC Server handler 20 on default port 9891] WARN ipc.Server: IPC Server handler 20 on default port 9891, call Call#10 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:55430: output error
recon_1  | 2023-04-04 07:54:18,797 [IPC Server handler 20 on default port 9891] INFO ipc.Server: IPC Server handler 20 on default port 9891 caught an exception
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 2023-04-04 07:54:10,997 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | java.nio.channels.ClosedChannelException
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-04-04 07:54:13,201 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm1_1   | 2023-04-04 07:54:11,795 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-04-04 07:56:31,588 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
scm1_1   | 2023-04-04 07:54:11,793 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-04-04 07:56:31,588 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-04-04 07:55:27,803 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
scm1_1   | 2023-04-04 07:54:11,752 [IPC Server handler 9 on default port 9861] INFO ipc.Server: IPC Server handler 9 on default port 9861 caught an exception
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-04-04 07:55:23,805 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 2023-04-04 07:56:31,588 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3
dn4_1    | 2023-04-04 07:56:31,595 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-04-04 07:56:31,595 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn4_1    | 2023-04-04 07:56:31,595 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-04-04 07:56:31,595 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-04-04 07:56:31,595 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-04-04 07:56:31,595 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-04-04 07:56:31,596 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 2023-04-04 07:56:31,596 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-04-04 07:56:31,596 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-04-04 07:56:31,606 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-04-04 07:56:31,691 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn5_1    | 2023-04-04 07:54:13,201 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-04-04 07:56:31,691 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn5_1    | 2023-04-04 07:54:13,218 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-04-04 07:56:31,691 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-04-04 07:56:31,691 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
scm1_1   | java.nio.channels.ClosedChannelException
dn5_1    | 2023-04-04 07:54:13,231 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-04-04 07:55:27,803 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-04-04 07:55:27,805 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 2023-04-04 07:54:33,807 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
dn4_1    | 2023-04-04 07:56:31,691 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-04-04 07:56:31,692 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3: start as a follower, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:13,239 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-04-04 07:54:13,234 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | java.nio.channels.ClosedChannelException
dn4_1    | 2023-04-04 07:56:31,692 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-04-04 07:54:13,234 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-04-04 07:55:27,805 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-04-04 07:55:27,805 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn4_1    | 2023-04-04 07:56:31,692 [pool-26-thread-1] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: start 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 2023-04-04 07:54:18,605 [IPC Server handler 21 on default port 9891] WARN ipc.Server: IPC Server handler 21 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:35042: output error
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 2023-04-04 07:55:27,805 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn2_1    | 2023-04-04 07:55:27,805 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-04-04 07:56:31,692 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1D7D5DE977D3,id=14df8ad4-ce2f-4483-aada-29ab5cae3ffb
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 2023-04-04 07:56:31,692 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 2023-04-04 07:56:31,693 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn4_1    | 2023-04-04 07:56:31,693 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn5_1    | 2023-04-04 07:54:14,335 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 2023-04-04 07:56:31,693 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-04-04 07:55:23,805 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 2023-04-04 07:56:31,693 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn5_1    | 2023-04-04 07:54:14,336 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-04-04 07:55:23,805 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-04-04 07:55:23,805 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn3_1    | 2023-04-04 07:55:23,806 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
dn3_1    | 2023-04-04 07:55:23,806 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn2_1    | 2023-04-04 07:55:27,805 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 2023-04-04 07:54:18,801 [IPC Server handler 21 on default port 9891] INFO ipc.Server: IPC Server handler 21 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn5_1    | 2023-04-04 07:54:14,338 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-04-04 07:55:23,806 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn5_1    | 2023-04-04 07:54:14,339 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 2023-04-04 07:56:31,694 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn5_1    | 2023-04-04 07:54:14,343 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-04-04 07:54:14,427 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-04-04 07:54:14,428 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn5_1    | 2023-04-04 07:54:14,437 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-04-04 07:54:14,568 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-04-04 07:54:14,629 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-04-04 07:55:27,805 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-04-04 07:55:27,806 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn5_1    | 2023-04-04 07:54:14,631 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-04-04 07:54:14,744 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:14,760 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 2023-04-04 07:54:16,411 [IPC Server handler 72 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/14df8ad4-ce2f-4483-aada-29ab5cae3ffb
scm2_1   | 2023-04-04 07:54:16,483 [IPC Server handler 71 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b7b7229a-e604-4787-8543-e6d8cdebe63b
scm2_1   | 2023-04-04 07:54:16,509 [IPC Server handler 71 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b7b7229a-e604-4787-8543-e6d8cdebe63b{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-04-04 07:54:11,752 [IPC Server handler 10 on default port 9861] INFO ipc.Server: IPC Server handler 10 on default port 9861 caught an exception
scm2_1   | 2023-04-04 07:54:16,569 [IPC Server handler 72 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 14df8ad4-ce2f-4483-aada-29ab5cae3ffb{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-04-04 07:54:14,761 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: set configuration 0: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:14,798 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6/current/log_0-0
scm2_1   | 2023-04-04 07:54:16,585 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2_1   | 2023-04-04 07:54:16,694 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2_1   | 2023-04-04 07:54:16,693 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn5_1    | 2023-04-04 07:54:14,843 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_0-4
dn5_1    | 2023-04-04 07:54:14,867 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: set configuration 5: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-04-04 07:54:16,742 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-04-04 07:54:16,742 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn5_1    | 2023-04-04 07:54:14,877 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO segmented.LogSegment: Successfully read 10 entries from segment file /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_5-14
scm2_1   | 2023-04-04 07:54:16,741 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-04-04 07:54:16,746 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-04-04 07:54:16,762 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn5_1    | 2023-04-04 07:54:14,820 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_0-7
dn5_1    | 2023-04-04 07:54:14,932 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: set configuration 8: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:14,935 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO segmented.LogSegment: Successfully read 18 entries from segment file /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_8-25
dn5_1    | 2023-04-04 07:54:14,951 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: set configuration 1: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:33,802 [IPC Server handler 9 on default port 9861] INFO ipc.Server: IPC Server handler 9 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
dn4_1    | 2023-04-04 07:56:35,021 [grpc-default-executor-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF: receive requestVote(PRE_VOTE, 2ebde02c-a404-41d0-92a4-7b6da490547a, group-4E27919647DF, 0, (t:0, i:0))
dn4_1    | 2023-04-04 07:56:35,023 [grpc-default-executor-1] INFO impl.VoteContext: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-FOLLOWER: reject PRE_VOTE from 2ebde02c-a404-41d0-92a4-7b6da490547a: our priority 1 > candidate's priority 0
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 2023-04-04 07:54:16,765 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-04-04 07:54:18,185 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn2_1    | 2023-04-04 07:55:27,806 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-04-04 07:55:27,806 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
dn2_1    | 2023-04-04 07:55:27,806 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn4_1    | 2023-04-04 07:56:35,028 [grpc-default-executor-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF replies to PRE_VOTE vote request: 2ebde02c-a404-41d0-92a4-7b6da490547a<-14df8ad4-ce2f-4483-aada-29ab5cae3ffb#0:FAIL-t0. Peer's state: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF:t0, leader=null, voted=, raftlog=Memoized:14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn2_1    | 2023-04-04 07:55:27,806 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | 2023-04-04 07:56:35,661 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-FollowerState] INFO impl.FollowerState: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5047390687ns, electionTimeout:5035ms
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn5_1    | 2023-04-04 07:54:14,960 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6/current/log_1-2
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | 2023-04-04 07:56:35,661 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-FollowerState] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: shutdown 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-FollowerState
dn4_1    | 2023-04-04 07:56:35,662 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-FollowerState] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-04-04 07:54:18,190 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn5_1    | 2023-04-04 07:54:14,962 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: set configuration 3: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-04-04 07:54:14,976 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6/current/log_inprogress_3
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-04-04 07:55:23,806 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
scm2_1   | 2023-04-04 07:54:18,219 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn5_1    | 2023-04-04 07:54:14,944 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: set configuration 15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 2023-04-04 07:56:35,662 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-04-04 07:54:15,002 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO segmented.LogSegment: Successfully read 6 entries from segment file /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_inprogress_15
scm2_1   | 2023-04-04 07:54:18,219 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-04-04 07:54:18,448 [IPC Server handler 65 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c1077390-d65b-4523-9cd4-abe9e2c9eb94
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-04-04 07:54:18,457 [IPC Server handler 65 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c1077390-d65b-4523-9cd4-abe9e2c9eb94{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-04-04 07:54:11,752 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
dn5_1    | 2023-04-04 07:54:14,981 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: set configuration 26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:15,731 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
scm2_1   | 2023-04-04 07:54:18,458 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-04-04 07:54:18,476 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn5_1    | 2023-04-04 07:54:15,801 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn5_1    | 2023-04-04 07:54:15,730 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:54:15,730 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 20
dn5_1    | 2023-04-04 07:54:15,919 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 14
dn5_1    | 2023-04-04 07:54:15,823 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO segmented.LogSegment: Successfully read 21 entries from segment file /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_inprogress_26
dn5_1    | 2023-04-04 07:54:15,924 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 46
dn5_1    | 2023-04-04 07:54:15,925 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 25
scm2_1   | 2023-04-04 07:54:18,493 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2_1   | 2023-04-04 07:54:18,504 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2_1   | 2023-04-04 07:54:18,504 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2_1   | 2023-04-04 07:54:18,504 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 2023-04-04 07:54:18,505 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-04-04 07:54:18,505 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-04-04 07:54:18,505 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2_1   | 2023-04-04 07:54:18,506 [IPC Server handler 66 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e2957b23-687f-4626-af75-9b42f3a43b99
scm2_1   | 2023-04-04 07:54:18,511 [IPC Server handler 66 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e2957b23-687f-4626-af75-9b42f3a43b99{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-04-04 07:54:18,512 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn4_1    | 2023-04-04 07:56:35,662 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-FollowerState] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: start 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-04-04 07:54:33,802 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
recon_1  | 2023-04-04 07:54:18,794 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891 caught an exception
scm2_1   | 2023-04-04 07:54:18,549 [IPC Server handler 64 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2ebde02c-a404-41d0-92a4-7b6da490547a
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-04-04 07:54:16,894 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
scm2_1   | 2023-04-04 07:54:18,549 [IPC Server handler 64 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 2ebde02c-a404-41d0-92a4-7b6da490547a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-04-04 07:54:18,559 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 2023-04-04 07:54:17,199 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO raftlog.RaftLog: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLog: commitIndex: updateToMax old=20, new=19, updated? false
dn5_1    | 2023-04-04 07:54:17,215 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: start as a follower, conf=15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm2_1   | 2023-04-04 07:54:18,559 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | 2023-04-04 07:56:35,667 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | java.nio.channels.ClosedChannelException
dn5_1    | 2023-04-04 07:54:17,220 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: changes role from      null to FOLLOWER at term 14 for startAsFollower
dn5_1    | 2023-04-04 07:54:17,263 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO raftlog.RaftLog: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLog: commitIndex: updateToMax old=46, new=45, updated? false
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm2_1   | 2023-04-04 07:54:18,561 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm2_1   | 2023-04-04 07:54:18,568 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
dn5_1    | 2023-04-04 07:54:17,287 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: start as a follower, conf=26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:55:27,806 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn4_1    | 2023-04-04 07:56:35,673 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for c1077390-d65b-4523-9cd4-abe9e2c9eb94
dn4_1    | 2023-04-04 07:56:35,676 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-04-04 07:54:17,263 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO raftlog.RaftLog: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLog: commitIndex: updateToMax old=4, new=3, updated? false
dn5_1    | 2023-04-04 07:54:17,288 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: start as a follower, conf=3: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:17,288 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
scm2_1   | 2023-04-04 07:54:18,568 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-04-04 07:54:18,607 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | 2023-04-04 07:56:35,676 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 2ebde02c-a404-41d0-92a4-7b6da490547a
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 2023-04-04 07:55:23,806 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm2_1   | 2023-04-04 07:54:18,610 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-04-04 07:56:35,678 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 2023-04-04 07:55:23,806 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm2_1   | 2023-04-04 07:54:57,488 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 2023-04-04 07:56:35,873 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-04-04 07:56:35,874 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO impl.LeaderElection:   Response 0: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t0
scm2_1   | 2023-04-04 07:54:57,492 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 2023-04-04 07:54:57,909 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm2_1   | 2023-04-04 07:54:57,921 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-04-04 07:55:27,808 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-04-04 07:55:27,808 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm2_1   | 2023-04-04 07:54:57,928 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn2_1    | 2023-04-04 07:55:27,808 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-04-04 07:55:27,808 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 2023-04-04 07:54:57,931 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn2_1    | 2023-04-04 07:55:27,809 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn5_1    | 2023-04-04 07:54:17,287 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState
dn4_1    | 2023-04-04 07:56:35,874 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2 PRE_VOTE round 0: result PASSED
dn4_1    | 2023-04-04 07:56:35,880 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 2023-04-04 07:55:27,809 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn4_1    | 2023-04-04 07:56:35,883 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-04-04 07:56:35,884 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 2023-04-04 07:54:57,931 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
scm2_1   | 2023-04-04 07:54:57,944 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2_1   | 2023-04-04 07:54:57,944 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY READONLY state.
dn5_1    | 2023-04-04 07:54:17,306 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: changes role from      null to FOLLOWER at term 18 for startAsFollower
dn5_1    | 2023-04-04 07:54:17,303 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-FollowerState
dn3_1    | 2023-04-04 07:55:23,806 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-04-04 07:55:23,806 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
scm1_1   | 2023-04-04 07:54:11,752 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-04-04 07:54:17,307 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0EBF5B06395B,id=2ebde02c-a404-41d0-92a4-7b6da490547a
dn5_1    | 2023-04-04 07:54:17,377 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-65BC10982AC6,id=2ebde02c-a404-41d0-92a4-7b6da490547a
dn3_1    | 2023-04-04 07:55:23,806 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-04-04 07:55:23,806 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn5_1    | 2023-04-04 07:54:17,391 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-04-04 07:54:17,392 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn4_1    | 2023-04-04 07:56:35,929 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn5_1    | 2023-04-04 07:54:17,393 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn4_1    | 2023-04-04 07:56:35,929 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO impl.LeaderElection:   Response 0: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t1
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn5_1    | 2023-04-04 07:54:17,400 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 2023-04-04 07:56:35,929 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2 ELECTION round 0: result PASSED
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm2_1   | 2023-04-04 07:54:57,944 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=685cb9fd-d40d-4ecb-9ff2-6617081ae410 in state CLOSED which uses HEALTHY_READONLY datanode 14df8ad4-ce2f-4483-aada-29ab5cae3ffb. This will send close commands for its containers.
scm2_1   | 2023-04-04 07:54:57,945 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
scm2_1   | 2023-04-04 07:54:57,945 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=84e9f90e-0ab8-4731-82e1-fb0ad827e6e2 in state CLOSED which uses HEALTHY_READONLY datanode b7b7229a-e604-4787-8543-e6d8cdebe63b. This will send close commands for its containers.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn4_1    | 2023-04-04 07:56:35,929 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: shutdown 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2
scm3_1   | 2023-04-04 07:54:33,817 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm3_1   | 2023-04-04 07:54:33,813 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn4_1    | 2023-04-04 07:56:35,930 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn4_1    | 2023-04-04 07:56:35,930 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4E27919647DF with new leaderId: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn4_1    | 2023-04-04 07:56:35,930 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF: change Leader from null to 14df8ad4-ce2f-4483-aada-29ab5cae3ffb at term 1 for becomeLeader, leader elected after 5398ms
dn4_1    | 2023-04-04 07:56:35,930 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn5_1    | 2023-04-04 07:54:17,377 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-04-04 07:54:17,340 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-FollowerState
dn4_1    | 2023-04-04 07:56:35,932 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-04-04 07:56:35,932 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 2023-04-04 07:54:18,797 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn5_1    | 2023-04-04 07:54:17,392 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 2023-04-04 07:54:57,945 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-04-04 07:55:27,809 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn5_1    | 2023-04-04 07:54:17,415 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm2_1   | 2023-04-04 07:54:57,945 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e in state CLOSED which uses HEALTHY_READONLY datanode e2957b23-687f-4626-af75-9b42f3a43b99. This will send close commands for its containers.
dn4_1    | 2023-04-04 07:56:35,932 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-04-04 07:56:35,933 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn5_1    | 2023-04-04 07:54:17,415 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm2_1   | 2023-04-04 07:54:57,945 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=0a9acdcd-53b0-4fd8-86de-f3d51d6ec210 in state CLOSED which uses HEALTHY_READONLY datanode e2957b23-687f-4626-af75-9b42f3a43b99. This will send close commands for its containers.
scm2_1   | 2023-04-04 07:54:57,945 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b in state CLOSED which uses HEALTHY_READONLY datanode e2957b23-687f-4626-af75-9b42f3a43b99. This will send close commands for its containers.
scm2_1   | 2023-04-04 07:54:57,945 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY READONLY state.
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn5_1    | 2023-04-04 07:54:17,417 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-04-04 07:56:35,933 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-04-04 07:56:35,933 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm2_1   | 2023-04-04 07:54:57,945 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e in state CLOSED which uses HEALTHY_READONLY datanode 2ebde02c-a404-41d0-92a4-7b6da490547a. This will send close commands for its containers.
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 2023-04-04 07:54:57,946 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b in state CLOSED which uses HEALTHY_READONLY datanode 2ebde02c-a404-41d0-92a4-7b6da490547a. This will send close commands for its containers.
scm2_1   | 2023-04-04 07:54:57,946 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=35a434de-4469-4302-9ebd-65bc10982ac6 in state CLOSED which uses HEALTHY_READONLY datanode 2ebde02c-a404-41d0-92a4-7b6da490547a. This will send close commands for its containers.
scm2_1   | 2023-04-04 07:54:57,946 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn5_1    | 2023-04-04 07:54:17,418 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 2023-04-04 07:54:57,946 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e in state CLOSED which uses HEALTHY_READONLY datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94. This will send close commands for its containers.
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-04-04 07:54:11,752 [IPC Server handler 11 on default port 9861] INFO ipc.Server: IPC Server handler 11 on default port 9861 caught an exception
scm2_1   | 2023-04-04 07:54:57,949 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d0f51999-1853-4262-b31a-8b1b4f9c62a2 in state CLOSED which uses HEALTHY_READONLY datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94. This will send close commands for its containers.
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn5_1    | 2023-04-04 07:54:17,420 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-04-04 07:55:23,806 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn4_1    | 2023-04-04 07:56:35,933 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-04-04 07:54:57,949 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b in state CLOSED which uses HEALTHY_READONLY datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94. This will send close commands for its containers.
dn5_1    | 2023-04-04 07:54:17,420 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-04-04 07:56:36,012 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-04-04 07:56:36,012 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 2023-04-04 07:55:19,960 [IPC Server handler 6 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:54:17,474 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EEB13EAC206E,id=2ebde02c-a404-41d0-92a4-7b6da490547a
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-04-04 07:55:20,554 [IPC Server handler 63 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:54:17,474 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-04-04 07:55:22,649 [IPC Server handler 87 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:54:17,474 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 2023-04-04 07:54:33,990 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @27453ms to org.eclipse.jetty.util.log.Slf4jLog
scm2_1   | 2023-04-04 07:55:23,657 [IPC Server handler 80 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm2_1   | 2023-04-04 07:55:23,745 [IPC Server handler 72 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 2023-04-04 07:55:23,766 [IPC Server handler 99 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:54:17,475 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn2_1    | 2023-04-04 07:55:27,810 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-04-04 07:55:27,810 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-04-04 07:55:27,810 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
scm3_1   | 2023-04-04 07:54:34,497 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm3_1   | 2023-04-04 07:54:34,525 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn5_1    | 2023-04-04 07:54:17,475 [2ebde02c-a404-41d0-92a4-7b6da490547a-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-04-04 07:54:17,483 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-04-04 07:55:27,810 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
dn2_1    | 2023-04-04 07:55:27,810 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn2_1    | 2023-04-04 07:55:27,810 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn5_1    | 2023-04-04 07:54:17,483 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-04-04 07:56:36,012 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-04-04 07:56:36,015 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-04-04 07:56:36,015 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm3_1   | 2023-04-04 07:54:34,561 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3_1   | 2023-04-04 07:54:34,573 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm3_1   | 2023-04-04 07:54:34,574 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-04-04 07:54:18,796 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn5_1    | 2023-04-04 07:54:17,494 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: start RPC server
dn5_1    | 2023-04-04 07:54:17,506 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 2ebde02c-a404-41d0-92a4-7b6da490547a: GrpcService started, listening on 9858
dn4_1    | 2023-04-04 07:56:36,015 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-04-04 07:56:36,015 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-04-04 07:55:23,786 [IPC Server handler 6 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:54:34,574 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm3_1   | 2023-04-04 07:54:34,740 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
dn5_1    | 2023-04-04 07:54:17,507 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 2ebde02c-a404-41d0-92a4-7b6da490547a: GrpcService started, listening on 9856
dn5_1    | 2023-04-04 07:54:17,516 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 2ebde02c-a404-41d0-92a4-7b6da490547a: GrpcService started, listening on 9857
dn5_1    | 2023-04-04 07:54:17,528 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2ebde02c-a404-41d0-92a4-7b6da490547a is started using port 9858 for RATIS
dn4_1    | 2023-04-04 07:56:36,015 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-04-04 07:56:36,017 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm2_1   | 2023-04-04 07:55:26,160 [IPC Server handler 68 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm2_1   | 2023-04-04 07:55:26,494 [IPC Server handler 64 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 2023-04-04 07:54:34,741 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
dn5_1    | 2023-04-04 07:54:17,531 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2ebde02c-a404-41d0-92a4-7b6da490547a is started using port 9857 for RATIS_ADMIN
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 2023-04-04 07:55:27,152 [IPC Server handler 68 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:54:34,743 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn5_1    | 2023-04-04 07:54:17,531 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2ebde02c-a404-41d0-92a4-7b6da490547a is started using port 9856 for RATIS_SERVER
dn5_1    | 2023-04-04 07:54:17,531 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-2ebde02c-a404-41d0-92a4-7b6da490547a: Started
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm3_1   | 2023-04-04 07:54:34,883 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm3_1   | 2023-04-04 07:54:34,883 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm3_1   | 2023-04-04 07:54:34,886 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm3_1   | 2023-04-04 07:54:34,904 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@66b0e207{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3_1   | 2023-04-04 07:54:34,908 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5170bc02{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm2_1   | 2023-04-04 07:55:27,227 [IPC Server handler 71 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm2_1   | 2023-04-04 07:55:27,248 [IPC Server handler 66 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 2023-04-04 07:55:27,433 [IPC Server handler 64 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm2_1   | 2023-04-04 07:55:27,435 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) reported CLOSED replica.
scm2_1   | 2023-04-04 07:55:27,522 [IPC Server handler 63 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm2_1   | 2023-04-04 07:55:27,646 [IPC Server handler 67 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm2_1   | 2023-04-04 07:55:27,737 [FixedThreadPoolWithAffinityExecutor-8-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #2 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7 is not the leader 861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn5_1    | 2023-04-04 07:54:17,668 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm2_1   | 2023-04-04 07:55:27,768 [IPC Server handler 99 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-04-04 07:54:35,415 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3375b118{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-10546349443443963632/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm2_1   | 2023-04-04 07:55:27,825 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-04-04 07:54:17,716 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-04-04 07:54:17,895 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 2023-04-04 07:56:36,017 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-04-04 07:54:35,453 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@4f235e8e{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm2_1   | 2023-04-04 07:55:27,842 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-04-04 07:54:18,895 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-04-04 07:54:35,456 [Listener at 0.0.0.0/9860] INFO server.Server: Started @28919ms
scm2_1   | 2023-04-04 07:55:27,872 [IPC Server handler 0 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn5_1    | 2023-04-04 07:54:19,897 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-04-04 07:54:20,897 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-04-04 07:56:36,017 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-04-04 07:55:27,810 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn5_1    | 2023-04-04 07:54:22,537 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-FollowerState] INFO impl.FollowerState: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5197079425ns, electionTimeout:5038ms
dn5_1    | 2023-04-04 07:54:22,538 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-FollowerState] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-FollowerState
dn4_1    | 2023-04-04 07:56:36,017 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm1_1   | 2023-04-04 07:54:11,752 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
dn5_1    | 2023-04-04 07:54:22,538 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-FollowerState] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: changes role from  FOLLOWER to CANDIDATE at term 18 for changeToCandidate
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn4_1    | 2023-04-04 07:56:36,017 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
scm3_1   | 2023-04-04 07:54:35,463 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm3_1   | 2023-04-04 07:54:35,463 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-04-04 07:55:27,411 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 13.
dn3_1    | 2023-04-04 07:55:27,411 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 13.
dn5_1    | 2023-04-04 07:54:22,546 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-04-04 07:54:22,546 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-FollowerState] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn4_1    | 2023-04-04 07:56:36,017 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn3_1    | 2023-04-04 07:55:27,496 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-0] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 13.
dn3_1    | 2023-04-04 07:55:27,721 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-1] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 19.
dn5_1    | 2023-04-04 07:54:22,556 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 18 for 26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:22,570 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO impl.FollowerState: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5283403369ns, electionTimeout:5150ms
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn4_1    | 2023-04-04 07:56:36,017 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3_1   | 2023-04-04 07:54:35,467 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm3_1   | 2023-04-04 07:54:49,969 [IPC Server handler 23 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn3_1    | 2023-04-04 07:55:27,742 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-1] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 19.
dn3_1    | 2023-04-04 07:55:27,761 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-1] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 19.
scm2_1   | 2023-04-04 07:55:27,903 [IPC Server handler 10 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:54:22,572 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState
dn5_1    | 2023-04-04 07:54:22,573 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: changes role from  FOLLOWER to CANDIDATE at term 14 for changeToCandidate
dn4_1    | 2023-04-04 07:56:36,017 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 2023-04-04 07:55:28,090 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 24.
scm2_1   | 2023-04-04 07:55:28,123 [IPC Server handler 68 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:54:49,973 [IPC Server handler 23 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 14df8ad4-ce2f-4483-aada-29ab5cae3ffb{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-04-04 07:54:49,982 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn5_1    | 2023-04-04 07:54:22,573 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-04-04 07:56:36,019 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: start 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderStateImpl
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
scm2_1   | 2023-04-04 07:55:28,124 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21) reported CLOSED replica.
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn5_1    | 2023-04-04 07:54:22,574 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2
dn5_1    | 2023-04-04 07:54:22,617 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-FollowerState] INFO impl.FollowerState: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5314283279ns, electionTimeout:5187ms
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 2023-04-04 07:55:28,090 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 24.
scm3_1   | 2023-04-04 07:54:49,989 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2_1   | 2023-04-04 07:55:28,130 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7 is not the leader 861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 2023-04-04 07:56:36,019 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-04-04 07:56:36,021 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df/current/log_inprogress_0
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 2023-04-04 07:55:28,108 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-0] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 24.
scm3_1   | 2023-04-04 07:54:50,005 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2_1   | 2023-04-04 07:55:28,161 [IPC Server handler 71 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 2023-04-04 07:55:28,250 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 45.
scm3_1   | 2023-04-04 07:54:50,010 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-04-04 07:55:28,173 [IPC Server handler 66 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-04-04 07:56:36,047 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF-LeaderElection2] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-4E27919647DF: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:56:36,549 [grpc-default-executor-0] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3: receive requestVote(PRE_VOTE, c1077390-d65b-4523-9cd4-abe9e2c9eb94, group-1D7D5DE977D3, 0, (t:0, i:0))
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-04-04 07:55:28,251 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 45.
scm3_1   | 2023-04-04 07:54:50,014 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-04-04 07:55:28,215 [IPC Server handler 65 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm2_1   | 2023-04-04 07:55:28,307 [IPC Server handler 64 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-04-04 07:54:22,618 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-FollowerState] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-FollowerState
dn2_1    | 2023-04-04 07:55:27,811 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-04-04 07:55:28,288 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-1] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 45.
scm3_1   | 2023-04-04 07:54:50,545 [IPC Server handler 15 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b7b7229a-e604-4787-8543-e6d8cdebe63b
scm3_1   | 2023-04-04 07:54:50,546 [IPC Server handler 15 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b7b7229a-e604-4787-8543-e6d8cdebe63b{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn5_1    | 2023-04-04 07:54:22,618 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-FollowerState] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn2_1    | 2023-04-04 07:55:27,811 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-04-04 07:55:54,777 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm3_1   | 2023-04-04 07:54:50,548 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-04-04 07:55:28,318 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19) reported CLOSED replica.
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 2023-04-04 07:54:18,793 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
dn2_1    | 2023-04-04 07:55:27,811 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-04-04 07:55:54,777 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm3_1   | 2023-04-04 07:54:50,549 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-04-04 07:55:28,332 [IPC Server handler 63 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
recon_1  | java.nio.channels.ClosedChannelException
dn2_1    | 2023-04-04 07:55:27,811 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: CLOSING
dn3_1    | 2023-04-04 07:55:54,777 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
scm3_1   | 2023-04-04 07:54:50,557 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm3_1   | 2023-04-04 07:54:50,557 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn2_1    | 2023-04-04 07:55:27,811 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-04-04 07:55:54,777 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
scm2_1   | 2023-04-04 07:55:28,336 [FixedThreadPoolWithAffinityExecutor-8-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1001 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7 is not the leader 861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 2023-04-04 07:54:52,702 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn2_1    | 2023-04-04 07:55:27,811 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn3_1    | 2023-04-04 07:55:54,779 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
scm2_1   | 2023-04-04 07:55:28,338 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21) reported CLOSED replica.
scm3_1   | 2023-04-04 07:54:52,702 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c1077390-d65b-4523-9cd4-abe9e2c9eb94{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-04-04 07:55:54,779 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
scm2_1   | 2023-04-04 07:55:28,338 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1001 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7 is not the leader 861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 2023-04-04 07:54:52,702 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-04-04 07:54:11,752 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
dn3_1    | 2023-04-04 07:55:54,779 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
scm2_1   | 2023-04-04 07:55:28,350 [IPC Server handler 67 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:54:52,702 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1_1   | java.nio.channels.ClosedChannelException
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
dn5_1    | 2023-04-04 07:54:22,647 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm2_1   | 2023-04-04 07:55:28,352 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) reported CLOSED replica.
scm3_1   | 2023-04-04 07:54:52,703 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | 2023-04-04 07:56:36,549 [grpc-default-executor-0] INFO impl.VoteContext: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-FOLLOWER: accept PRE_VOTE from c1077390-d65b-4523-9cd4-abe9e2c9eb94: our priority 0 <= candidate's priority 0
dn4_1    | 2023-04-04 07:56:36,551 [grpc-default-executor-0] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3 replies to PRE_VOTE vote request: c1077390-d65b-4523-9cd4-abe9e2c9eb94<-14df8ad4-ce2f-4483-aada-29ab5cae3ffb#0:OK-t0. Peer's state: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3:t0, leader=null, voted=, raftlog=Memoized:14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-04-04 07:55:54,780 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm2_1   | 2023-04-04 07:55:28,358 [FixedThreadPoolWithAffinityExecutor-8-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1001 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7 is not the leader 861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 2023-04-04 07:54:22,647 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-FollowerState] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | 2023-04-04 07:56:36,659 [grpc-default-executor-0] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3: receive requestVote(PRE_VOTE, 2ebde02c-a404-41d0-92a4-7b6da490547a, group-1D7D5DE977D3, 0, (t:0, i:0))
dn3_1    | 2023-04-04 07:55:54,780 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn3_1    | 2023-04-04 07:55:54,780 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-04-04 07:55:54,780 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm2_1   | 2023-04-04 07:55:32,409 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for delTxnId, expected lastId is 0, actual lastId is 1000.
dn5_1    | 2023-04-04 07:54:22,648 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 14 for 15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
dn3_1    | 2023-04-04 07:55:58,786 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: c1077390-d65b-4523-9cd4-abe9e2c9eb94: Completed APPEND_ENTRIES, lastRequest: e2957b23-687f-4626-af75-9b42f3a43b99->c1077390-d65b-4523-9cd4-abe9e2c9eb94#289-t15,previous=(t:15, i:25),leaderCommit=25,initializing? true,entries: size=1, first=(t:15, i:26), METADATAENTRY(c:25)
scm2_1   | 2023-04-04 07:55:49,963 [IPC Server handler 5 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:54:22,684 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | 2023-04-04 07:56:36,660 [grpc-default-executor-0] INFO impl.VoteContext: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-FOLLOWER: accept PRE_VOTE from 2ebde02c-a404-41d0-92a4-7b6da490547a: our priority 0 <= candidate's priority 1
scm3_1   | 2023-04-04 07:54:52,703 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 2023-04-04 07:55:58,793 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: c1077390-d65b-4523-9cd4-abe9e2c9eb94: Completed APPEND_ENTRIES, lastRequest: null
scm2_1   | 2023-04-04 07:55:50,535 [IPC Server handler 87 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn5_1    | 2023-04-04 07:54:22,704 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3 PRE_VOTE round 0: result PASSED (term=3)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
scm2_1   | 2023-04-04 07:55:56,626 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 84e9f90e-0ab8-4731-82e1-fb0ad827e6e2, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:b7b7229a-e604-4787-8543-e6d8cdebe63b, CreationTimestamp2023-04-04T07:54:03.781707Z[UTC]] removed.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 2023-04-04 07:54:52,703 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-04-04 07:55:59,280 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: c1077390-d65b-4523-9cd4-abe9e2c9eb94: Completed APPEND_ENTRIES, lastRequest: 2ebde02c-a404-41d0-92a4-7b6da490547a->c1077390-d65b-4523-9cd4-abe9e2c9eb94#282-t19,previous=(t:19, i:51),leaderCommit=51,initializing? true,entries: size=1, first=(t:19, i:52), METADATAENTRY(c:51)
scm2_1   | 2023-04-04 07:55:56,642 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: a013e403-c1b0-4f4f-851c-eeb13eac206e, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:54:03.782068Z[UTC]] removed.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn4_1    | 2023-04-04 07:56:36,660 [grpc-default-executor-0] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3 replies to PRE_VOTE vote request: 2ebde02c-a404-41d0-92a4-7b6da490547a<-14df8ad4-ce2f-4483-aada-29ab5cae3ffb#0:OK-t0. Peer's state: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3:t0, leader=null, voted=, raftlog=Memoized:14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:56:36,684 [grpc-default-executor-0] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3: receive requestVote(ELECTION, 2ebde02c-a404-41d0-92a4-7b6da490547a, group-1D7D5DE977D3, 1, (t:0, i:0))
scm3_1   | 2023-04-04 07:54:52,713 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-04-04 07:54:52,714 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn2_1    | 2023-04-04 07:55:27,818 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn3_1    | 2023-04-04 07:55:59,282 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: c1077390-d65b-4523-9cd4-abe9e2c9eb94: Completed APPEND_ENTRIES, lastRequest: null
scm2_1   | 2023-04-04 07:55:56,655 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d0f51999-1853-4262-b31a-8b1b4f9c62a2, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:c1077390-d65b-4523-9cd4-abe9e2c9eb94, CreationTimestamp2023-04-04T07:54:03.782237Z[UTC]] removed.
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-04-04 07:56:36,685 [grpc-default-executor-0] INFO impl.VoteContext: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-FOLLOWER: accept ELECTION from 2ebde02c-a404-41d0-92a4-7b6da490547a: our priority 0 <= candidate's priority 1
dn5_1    | 2023-04-04 07:54:22,793 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm3_1   | 2023-04-04 07:54:52,714 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
dn2_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:57)
scm2_1   | 2023-04-04 07:55:56,676 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 685cb9fd-d40d-4ecb-9ff2-6617081ae410, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:54:03.781543Z[UTC]] removed.
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn4_1    | 2023-04-04 07:56:36,685 [grpc-default-executor-0] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:2ebde02c-a404-41d0-92a4-7b6da490547a
dn4_1    | 2023-04-04 07:56:36,688 [grpc-default-executor-0] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: shutdown 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-FollowerState
scm3_1   | 2023-04-04 07:54:52,714 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3_1   | 2023-04-04 07:54:56,211 [IPC Server handler 66 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e2957b23-687f-4626-af75-9b42f3a43b99
dn2_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:41)
scm2_1   | 2023-04-04 07:55:56,687 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 0a9acdcd-53b0-4fd8-86de-f3d51d6ec210, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:e2957b23-687f-4626-af75-9b42f3a43b99, CreationTimestamp2023-04-04T07:54:03.780018Z[UTC]] removed.
recon_1  | 2023-04-04 07:54:18,793 [IPC Server handler 15 on default port 9891] INFO ipc.Server: IPC Server handler 15 on default port 9891 caught an exception
dn4_1    | 2023-04-04 07:56:36,688 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-FollowerState] INFO impl.FollowerState: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-FollowerState was interrupted
dn5_1    | 2023-04-04 07:54:22,833 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-04-04 07:54:22,805 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-04-04 07:54:56,212 [IPC Server handler 66 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e2957b23-687f-4626-af75-9b42f3a43b99{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn3_1    | 2023-04-04 07:55:59,290 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
recon_1  | java.nio.channels.ClosedChannelException
dn4_1    | 2023-04-04 07:56:36,688 [grpc-default-executor-0] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: start 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-FollowerState
dn5_1    | 2023-04-04 07:54:22,834 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-04-04 07:54:22,838 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-04-04 07:54:56,212 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn3_1    | 2023-04-04 07:55:59,291 [Command processor thread] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: remove  FOLLOWER c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E:t19, leader=2ebde02c-a404-41d0-92a4-7b6da490547a, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLog:OPENED:c52, conf=47: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn2_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn5_1    | 2023-04-04 07:54:22,806 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for e2957b23-687f-4626-af75-9b42f3a43b99
scm3_1   | 2023-04-04 07:54:56,212 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn3_1    | 2023-04-04 07:55:59,293 [Command processor thread] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: shutdown
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:713)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn4_1    | 2023-04-04 07:56:36,693 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-04-04 07:55:56,696 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 3a70a706-d27b-47ff-9aed-0ebf5b06395b, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:e2957b23-687f-4626-af75-9b42f3a43b99, CreationTimestamp2023-04-04T07:54:03.781371Z[UTC]] removed.
dn5_1    | 2023-04-04 07:54:22,918 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for 3: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:22,920 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3 ELECTION round 0: result PASSED (term=4)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn3_1    | 2023-04-04 07:55:59,294 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-EEB13EAC206E,id=c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 2023-04-04 07:55:56,707 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 35a434de-4469-4302-9ebd-65bc10982ac6, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:54:03.781041Z[UTC]] removed.
dn5_1    | 2023-04-04 07:54:22,920 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3
dn5_1    | 2023-04-04 07:54:22,924 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn3_1    | 2023-04-04 07:55:59,294 [Command processor thread] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn4_1    | 2023-04-04 07:56:36,693 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-04-04 07:54:56,519 [IPC Server handler 15 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2ebde02c-a404-41d0-92a4-7b6da490547a
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:642)
dn3_1    | 2023-04-04 07:55:59,294 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState] INFO impl.FollowerState: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-FollowerState was interrupted
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-04-04 07:54:22,924 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-65BC10982AC6 with new leaderId: 2ebde02c-a404-41d0-92a4-7b6da490547a
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | 2023-04-04 07:56:36,694 [grpc-default-executor-0] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3 replies to ELECTION vote request: 2ebde02c-a404-41d0-92a4-7b6da490547a<-14df8ad4-ce2f-4483-aada-29ab5cae3ffb#0:OK-t1. Peer's state: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3:t1, leader=null, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:56:36,798 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1D7D5DE977D3 with new leaderId: 2ebde02c-a404-41d0-92a4-7b6da490547a
dn3_1    | 2023-04-04 07:55:59,295 [Command processor thread] INFO impl.StateMachineUpdater: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-StateMachineUpdater: set stopIndex = 52
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-04-04 07:55:59,294 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-EEB13EAC206E: Taking a snapshot at:(t:19, i:52) file /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/sm/snapshot.19_52
scm2_1   | 2023-04-04 07:55:58,353 [IPC Server handler 67 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 2023-04-04 07:54:11,752 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
dn4_1    | 2023-04-04 07:56:36,798 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-server-thread1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3: change Leader from null to 2ebde02c-a404-41d0-92a4-7b6da490547a at term 1 for appendEntries, leader elected after 5245ms
dn4_1    | 2023-04-04 07:56:36,809 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-server-thread2] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:56:36,810 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb-server-thread2] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-04-04 07:56:36,812 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-1D7D5DE977D3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3/current/log_inprogress_0
dn3_1    | 2023-04-04 07:55:59,299 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-EEB13EAC206E: Finished taking a snapshot at:(t:19, i:52) file:/data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/sm/snapshot.19_52 took: 5 ms
scm2_1   | 2023-04-04 07:55:59,506 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:55:59.480Z[UTC]].
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 2023-04-04 07:56:50,977 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: new RaftServerImpl for group-EBA4A65DC133:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-04-04 07:56:50,977 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-04-04 07:56:50,977 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-04-04 07:55:59,300 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-StateMachineUpdater] INFO impl.StateMachineUpdater: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-StateMachineUpdater: Took a snapshot at index 52
scm2_1   | 2023-04-04 07:55:59,534 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f3dc0283-6172-4aac-8038-9f3ca91aaa35, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:55:59.519Z[UTC]].
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn5_1    | 2023-04-04 07:54:22,955 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: change Leader from null to 2ebde02c-a404-41d0-92a4-7b6da490547a at term 4 for becomeLeader, leader elected after 33029ms
dn2_1    | 2023-04-04 07:55:28,108 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 24.
dn3_1    | 2023-04-04 07:55:59,301 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-StateMachineUpdater] INFO impl.StateMachineUpdater: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-StateMachineUpdater: snapshotIndex: updateIncreasingly 46 -> 52
scm2_1   | 2023-04-04 07:56:00,750 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY state.
dn4_1    | 2023-04-04 07:56:50,977 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-04-04 07:56:50,978 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-04-04 07:56:50,978 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-04-04 07:56:50,978 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-04-04 07:54:23,061 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-04-04 07:54:23,111 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm2_1   | 2023-04-04 07:56:00,750 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn4_1    | 2023-04-04 07:56:50,978 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133: ConfigurationManager, init=-1: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-04-04 07:56:50,978 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-04-04 07:56:50,978 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-04-04 07:56:50,978 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-04-04 07:54:23,112 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-04-04 07:55:59,304 [Command processor thread] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: closes. applyIndex: 52
scm2_1   | 2023-04-04 07:56:00,750 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY state.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-04-04 07:54:23,167 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-04-04 07:55:28,110 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 24.
dn3_1    | 2023-04-04 07:55:59,305 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-04-04 07:54:56,519 [IPC Server handler 15 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 2ebde02c-a404-41d0-92a4-7b6da490547a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-04-04 07:54:56,519 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-04-04 07:56:00,750 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn3_1    | 2023-04-04 07:55:59,306 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E-SegmentedRaftLogWorker close()
dn3_1    | 2023-04-04 07:55:59,316 [Command processor thread] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-EEB13EAC206E: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e
dn4_1    | 2023-04-04 07:56:50,979 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm3_1   | 2023-04-04 07:54:56,521 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm3_1   | 2023-04-04 07:54:56,521 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm2_1   | 2023-04-04 07:56:20,493 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 2b2f258a-8c66-4c27-9e64-eba4a65dc133, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.479Z[UTC]].
scm2_1   | 2023-04-04 07:56:20,528 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8037e277-427a-4034-9f8a-4e27919647df, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.502Z[UTC]].
dn5_1    | 2023-04-04 07:54:23,207 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-04-04 07:54:23,218 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-04-04 07:54:23,289 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-04-04 07:56:50,979 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-04-04 07:56:50,979 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-04-04 07:56:50,981 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-04-04 07:55:28,139 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-0] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 24.
dn2_1    | 2023-04-04 07:55:28,244 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 45.
dn2_1    | 2023-04-04 07:55:28,251 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 45.
recon_1  | 2023-04-04 07:54:18,652 [IPC Server handler 23 on default port 9891] INFO ipc.Server: IPC Server handler 23 on default port 9891 caught an exception
dn5_1    | 2023-04-04 07:54:23,312 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-04-04 07:56:50,981 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-04-04 07:56:50,981 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-04-04 07:56:50,981 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3_1   | 2023-04-04 07:54:56,522 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
recon_1  | java.nio.channels.ClosedChannelException
dn5_1    | 2023-04-04 07:54:23,358 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderStateImpl
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 2023-04-04 07:56:20,545 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.531Z[UTC]].
scm2_1   | 2023-04-04 07:56:21,754 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY state.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-04-04 07:54:23,414 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn5_1    | 2023-04-04 07:54:23,459 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6/current/log_inprogress_3 to /data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6/current/log_3-4
scm3_1   | 2023-04-04 07:54:56,522 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm3_1   | 2023-04-04 07:54:56,522 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm3_1   | 2023-04-04 07:54:57,480 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
scm2_1   | 2023-04-04 07:56:21,755 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn4_1    | 2023-04-04 07:56:50,981 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 2023-04-04 07:54:23,498 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6/current/log_inprogress_5
dn5_1    | 2023-04-04 07:54:23,527 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderElection3] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: set configuration 5: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 2023-04-04 07:56:21,755 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY state.
dn2_1    | 2023-04-04 07:55:28,325 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-1] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 45.
dn4_1    | 2023-04-04 07:56:50,984 [Command processor thread] INFO server.RaftServer: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: addNew group-EBA4A65DC133:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns      null 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn4_1    | 2023-04-04 07:56:50,985 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/2b2f258a-8c66-4c27-9e64-eba4a65dc133 does not exist. Creating ...
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn5_1    | 2023-04-04 07:54:25,677 [grpc-default-executor-2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: receive requestVote(PRE_VOTE, c1077390-d65b-4523-9cd4-abe9e2c9eb94, group-0EBF5B06395B, 14, (t:14, i:20))
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 2023-04-04 07:54:57,480 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
scm3_1   | 2023-04-04 07:54:57,915 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
scm3_1   | 2023-04-04 07:54:57,917 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
scm3_1   | 2023-04-04 07:54:57,936 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn5_1    | 2023-04-04 07:54:25,686 [grpc-default-executor-3] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: receive requestVote(PRE_VOTE, c1077390-d65b-4523-9cd4-abe9e2c9eb94, group-EEB13EAC206E, 18, (t:18, i:46))
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 2023-04-04 07:56:21,755 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-04-04 07:56:23,491 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6d92c706-b857-41ae-a811-0fceb345a748, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:23.479Z[UTC]].
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-04-04 07:55:59,316 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e command on datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94.
dn3_1    | 2023-04-04 07:55:59,317 [Command processor thread] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: remove    LEADER c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2:t4, leader=c1077390-d65b-4523-9cd4-abe9e2c9eb94, voted=c1077390-d65b-4523-9cd4-abe9e2c9eb94, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLog:OPENED:c6, conf=5: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn5_1    | 2023-04-04 07:54:25,694 [grpc-default-executor-3] INFO impl.VoteContext: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-CANDIDATE: reject PRE_VOTE from c1077390-d65b-4523-9cd4-abe9e2c9eb94: our priority 1 > candidate's priority 0
dn2_1    | 2023-04-04 07:55:58,741 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-04-04 07:55:58,741 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn4_1    | 2023-04-04 07:56:50,986 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2b2f258a-8c66-4c27-9e64-eba4a65dc133/in_use.lock acquired by nodename 6@0972042128cd
dn4_1    | 2023-04-04 07:56:50,988 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/2b2f258a-8c66-4c27-9e64-eba4a65dc133 has been successfully formatted.
dn4_1    | 2023-04-04 07:56:51,007 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-EBA4A65DC133: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 2023-04-04 07:54:57,940 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn5_1    | 2023-04-04 07:54:25,700 [grpc-default-executor-2] INFO impl.VoteContext: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-CANDIDATE: accept PRE_VOTE from c1077390-d65b-4523-9cd4-abe9e2c9eb94: our priority 0 <= candidate's priority 0
dn2_1    | 2023-04-04 07:55:58,742 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-04-04 07:55:58,742 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn2_1    | 2023-04-04 07:55:58,743 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn3_1    | 2023-04-04 07:55:59,317 [Command processor thread] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: shutdown
dn3_1    | 2023-04-04 07:55:59,317 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8B1B4F9C62A2,id=c1077390-d65b-4523-9cd4-abe9e2c9eb94
dn3_1    | 2023-04-04 07:55:59,318 [Command processor thread] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-LeaderStateImpl
dn3_1    | 2023-04-04 07:55:59,318 [Command processor thread] INFO impl.PendingRequests: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-PendingRequests: sendNotLeaderResponses
scm3_1   | 2023-04-04 07:54:57,940 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn2_1    | 2023-04-04 07:55:58,743 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn2_1    | 2023-04-04 07:55:58,744 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
scm2_1   | 2023-04-04 07:56:29,348 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:55:59.480Z[UTC]] moved to OPEN state
dn3_1    | 2023-04-04 07:55:59,320 [Command processor thread] INFO impl.StateMachineUpdater: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-StateMachineUpdater: set stopIndex = 6
dn3_1    | 2023-04-04 07:55:59,321 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8B1B4F9C62A2: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2/sm/snapshot.4_6
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn5_1    | 2023-04-04 07:54:25,737 [grpc-default-executor-2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B replies to PRE_VOTE vote request: c1077390-d65b-4523-9cd4-abe9e2c9eb94<-2ebde02c-a404-41d0-92a4-7b6da490547a#0:OK-t14. Peer's state: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B:t14, leader=null, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLog:OPENED:c20, conf=15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:25,755 [grpc-default-executor-3] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E replies to PRE_VOTE vote request: c1077390-d65b-4523-9cd4-abe9e2c9eb94<-2ebde02c-a404-41d0-92a4-7b6da490547a#0:FAIL-t18. Peer's state: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E:t18, leader=null, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLog:OPENED:c46, conf=26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-04-04 07:56:29,373 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f3dc0283-6172-4aac-8038-9f3ca91aaa35, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:c1077390-d65b-4523-9cd4-abe9e2c9eb94, CreationTimestamp2023-04-04T07:55:59.519Z[UTC]] moved to OPEN state
dn4_1    | 2023-04-04 07:56:51,008 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-04-04 07:56:51,008 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-04-04 07:56:51,008 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-04-04 07:54:25,783 [grpc-default-executor-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: receive requestVote(PRE_VOTE, e2957b23-687f-4626-af75-9b42f3a43b99, group-EEB13EAC206E, 18, (t:18, i:46))
scm3_1   | 2023-04-04 07:54:57,953 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 2023-04-04 07:56:29,507 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a21f8cfe-4543-468b-bd60-588a1a0d07ff, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:29.481Z[UTC]].
scm2_1   | 2023-04-04 07:56:30,757 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY state.
scm2_1   | 2023-04-04 07:56:30,757 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 2023-04-04 07:56:51,009 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-04-04 07:55:58,744 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn2_1    | 2023-04-04 07:55:58,744 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn5_1    | 2023-04-04 07:54:25,798 [grpc-default-executor-0] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: receive requestVote(PRE_VOTE, e2957b23-687f-4626-af75-9b42f3a43b99, group-0EBF5B06395B, 14, (t:14, i:20))
scm3_1   | 2023-04-04 07:54:57,956 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY READONLY state.
dn4_1    | 2023-04-04 07:56:51,009 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-04-04 07:56:51,009 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-04-04 07:56:51,009 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn5_1    | 2023-04-04 07:54:25,816 [grpc-default-executor-0] INFO impl.VoteContext: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-CANDIDATE: accept PRE_VOTE from e2957b23-687f-4626-af75-9b42f3a43b99: our priority 0 <= candidate's priority 1
dn5_1    | 2023-04-04 07:54:25,816 [grpc-default-executor-0] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B replies to PRE_VOTE vote request: e2957b23-687f-4626-af75-9b42f3a43b99<-2ebde02c-a404-41d0-92a4-7b6da490547a#0:OK-t14. Peer's state: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B:t14, leader=null, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLog:OPENED:c20, conf=15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-04-04 07:54:57,958 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=685cb9fd-d40d-4ecb-9ff2-6617081ae410 in state CLOSED which uses HEALTHY_READONLY datanode 14df8ad4-ce2f-4483-aada-29ab5cae3ffb. This will send close commands for its containers.
dn4_1    | 2023-04-04 07:56:51,009 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-04-04 07:56:51,009 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2b2f258a-8c66-4c27-9e64-eba4a65dc133
dn4_1    | 2023-04-04 07:56:51,010 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 2023-04-04 07:55:59,322 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8B1B4F9C62A2: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2/sm/snapshot.4_6 took: 2 ms
dn5_1    | 2023-04-04 07:54:25,813 [grpc-default-executor-1] INFO impl.VoteContext: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-CANDIDATE: reject PRE_VOTE from e2957b23-687f-4626-af75-9b42f3a43b99: our priority 1 > candidate's priority 0
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-04-04 07:54:57,958 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
scm3_1   | 2023-04-04 07:54:57,959 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=84e9f90e-0ab8-4731-82e1-fb0ad827e6e2 in state CLOSED which uses HEALTHY_READONLY datanode b7b7229a-e604-4787-8543-e6d8cdebe63b. This will send close commands for its containers.
scm2_1   | 2023-04-04 07:56:35,976 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8037e277-427a-4034-9f8a-4e27919647df, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:56:20.502Z[UTC]] moved to OPEN state
scm2_1   | 2023-04-04 07:56:36,702 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:56:20.531Z[UTC]] moved to OPEN state
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-04-04 07:54:25,826 [grpc-default-executor-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E replies to PRE_VOTE vote request: e2957b23-687f-4626-af75-9b42f3a43b99<-2ebde02c-a404-41d0-92a4-7b6da490547a#0:FAIL-t18. Peer's state: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E:t18, leader=null, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLog:OPENED:c46, conf=26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:26,246 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-04-04 07:54:26,261 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO impl.LeaderElection:   Response 0: 2ebde02c-a404-41d0-92a4-7b6da490547a<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t18
dn4_1    | 2023-04-04 07:56:51,010 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm2_1   | 2023-04-04 07:56:51,050 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 2b2f258a-8c66-4c27-9e64-eba4a65dc133, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:56:20.479Z[UTC]] moved to OPEN state
dn2_1    | 2023-04-04 07:55:58,744 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-04-04 07:55:59,323 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-StateMachineUpdater] INFO impl.StateMachineUpdater: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-StateMachineUpdater: Took a snapshot at index 6
dn3_1    | 2023-04-04 07:55:59,323 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-StateMachineUpdater] INFO impl.StateMachineUpdater: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn5_1    | 2023-04-04 07:54:26,261 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1 PRE_VOTE round 0: result PASSED
dn4_1    | 2023-04-04 07:56:51,010 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-04-04 07:56:51,010 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-04-04 07:56:51,010 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-04-04 07:55:59,323 [Command processor thread] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: closes. applyIndex: 6
dn3_1    | 2023-04-04 07:55:59,323 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-04-04 07:54:57,960 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
scm3_1   | 2023-04-04 07:54:57,962 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e in state CLOSED which uses HEALTHY_READONLY datanode e2957b23-687f-4626-af75-9b42f3a43b99. This will send close commands for its containers.
dn2_1    | 2023-04-04 07:55:58,744 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-04-04 07:55:58,745 [Command processor thread] INFO server.RaftServer: e2957b23-687f-4626-af75-9b42f3a43b99: remove  FOLLOWER e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E:t19, leader=2ebde02c-a404-41d0-92a4-7b6da490547a, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLog:OPENED:c52, conf=47: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-04-04 07:56:51,010 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-04-04 07:54:26,260 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2: PRE_VOTE DISCOVERED_A_NEW_TERM (term=15) received 1 response(s) and 0 exception(s):
dn3_1    | 2023-04-04 07:55:59,324 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2-SegmentedRaftLogWorker close()
dn3_1    | 2023-04-04 07:55:59,327 [Command processor thread] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-8B1B4F9C62A2: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/d0f51999-1853-4262-b31a-8b1b4f9c62a2
dn2_1    | 2023-04-04 07:55:58,749 [Command processor thread] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: shutdown
dn2_1    | 2023-04-04 07:55:58,749 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-EEB13EAC206E,id=e2957b23-687f-4626-af75-9b42f3a43b99
dn4_1    | 2023-04-04 07:56:51,010 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-04-04 07:54:26,269 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2] INFO impl.LeaderElection:   Response 0: 2ebde02c-a404-41d0-92a4-7b6da490547a<-e2957b23-687f-4626-af75-9b42f3a43b99#0:FAIL-t15
recon_1  | 2023-04-04 07:54:18,652 [IPC Server handler 22 on default port 9891] INFO ipc.Server: IPC Server handler 22 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn2_1    | 2023-04-04 07:55:58,749 [Command processor thread] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: shutdown e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState
dn2_1    | 2023-04-04 07:55:58,749 [Command processor thread] INFO impl.StateMachineUpdater: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-StateMachineUpdater: set stopIndex = 52
dn4_1    | 2023-04-04 07:56:51,010 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-04-04 07:54:26,269 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=15)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-04-04 07:54:11,752 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-04-04 07:54:57,964 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=0a9acdcd-53b0-4fd8-86de-f3d51d6ec210 in state CLOSED which uses HEALTHY_READONLY datanode e2957b23-687f-4626-af75-9b42f3a43b99. This will send close commands for its containers.
dn2_1    | 2023-04-04 07:55:58,749 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState] INFO impl.FollowerState: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-FollowerState was interrupted
dn4_1    | 2023-04-04 07:56:51,011 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-04-04 07:54:26,270 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: changes role from CANDIDATE to FOLLOWER at term 15 for DISCOVERED_A_NEW_TERM (term=15)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 2023-04-04 07:54:57,964 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b in state CLOSED which uses HEALTHY_READONLY datanode e2957b23-687f-4626-af75-9b42f3a43b99. This will send close commands for its containers.
scm3_1   | 2023-04-04 07:54:57,964 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY READONLY state.
dn2_1    | 2023-04-04 07:55:58,749 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-EEB13EAC206E: Taking a snapshot at:(t:19, i:52) file /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/sm/snapshot.19_52
dn4_1    | 2023-04-04 07:56:51,012 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-04-04 07:54:26,275 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 2023-04-04 07:56:51,672 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6d92c706-b857-41ae-a811-0fceb345a748, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b7b7229a-e604-4787-8543-e6d8cdebe63b, CreationTimestamp2023-04-04T07:56:23.479Z[UTC]] moved to OPEN state
scm3_1   | 2023-04-04 07:54:57,965 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e in state CLOSED which uses HEALTHY_READONLY datanode 2ebde02c-a404-41d0-92a4-7b6da490547a. This will send close commands for its containers.
scm3_1   | 2023-04-04 07:54:57,965 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b in state CLOSED which uses HEALTHY_READONLY datanode 2ebde02c-a404-41d0-92a4-7b6da490547a. This will send close commands for its containers.
dn2_1    | 2023-04-04 07:55:58,754 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-EEB13EAC206E: Finished taking a snapshot at:(t:19, i:52) file:/data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/sm/snapshot.19_52 took: 4 ms
dn2_1    | 2023-04-04 07:55:58,755 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-StateMachineUpdater] INFO impl.StateMachineUpdater: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-StateMachineUpdater: Took a snapshot at index 52
dn5_1    | 2023-04-04 07:54:26,276 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-LeaderElection2] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn2_1    | 2023-04-04 07:55:58,755 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-StateMachineUpdater] INFO impl.StateMachineUpdater: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-StateMachineUpdater: snapshotIndex: updateIncreasingly 46 -> 52
dn4_1    | 2023-04-04 07:56:51,015 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-04-04 07:54:26,277 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1 ELECTION round 0: submit vote requests at term 19 for 26: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm2_1   | 2023-04-04 07:56:59,457 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a21f8cfe-4543-468b-bd60-588a1a0d07ff, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e2957b23-687f-4626-af75-9b42f3a43b99, CreationTimestamp2023-04-04T07:56:29.481Z[UTC]] moved to OPEN state
scm2_1   | 2023-04-04 07:57:04,697 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn3_1    | 2023-04-04 07:55:59,327 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=d0f51999-1853-4262-b31a-8b1b4f9c62a2 command on datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94.
dn3_1    | 2023-04-04 07:55:59,328 [Command processor thread] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: remove  FOLLOWER c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B:t15, leader=e2957b23-687f-4626-af75-9b42f3a43b99, voted=e2957b23-687f-4626-af75-9b42f3a43b99, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLog:OPENED:c26, conf=21: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn2_1    | 2023-04-04 07:55:58,764 [Command processor thread] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: closes. applyIndex: 52
dn4_1    | 2023-04-04 07:56:51,015 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-04-04 07:54:26,299 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-04-04 07:55:58,764 [e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm3_1   | 2023-04-04 07:54:57,965 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=35a434de-4469-4302-9ebd-65bc10982ac6 in state CLOSED which uses HEALTHY_READONLY datanode 2ebde02c-a404-41d0-92a4-7b6da490547a. This will send close commands for its containers.
dn5_1    | 2023-04-04 07:54:26,305 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-04-04 07:57:06,720 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn3_1    | 2023-04-04 07:55:59,329 [Command processor thread] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: shutdown
dn3_1    | 2023-04-04 07:55:59,329 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-0EBF5B06395B,id=c1077390-d65b-4523-9cd4-abe9e2c9eb94
dn3_1    | 2023-04-04 07:55:59,329 [Command processor thread] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState
dn2_1    | 2023-04-04 07:55:58,765 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E-SegmentedRaftLogWorker close()
dn2_1    | 2023-04-04 07:55:58,774 [Command processor thread] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-EEB13EAC206E: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 2023-04-04 07:54:57,965 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
dn5_1    | 2023-04-04 07:54:26,310 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-04-04 07:57:34,631 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm2_1   | 2023-04-04 07:57:36,723 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm2_1   | 2023-04-04 07:57:50,369 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 2000.
scm2_1   | 2023-04-04 07:57:50,427 [19de384c-7008-4d6c-8004-b0a99b5aa3c2@group-0BB16BE3F1B7-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | 2023-04-04 07:56:51,016 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-04-04 07:54:26,336 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-04-04 07:55:59,329 [Command processor thread] INFO impl.StateMachineUpdater: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-StateMachineUpdater: set stopIndex = 26
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn4_1    | 2023-04-04 07:56:51,016 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-04-04 07:54:26,347 [grpc-default-executor-2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: receive requestVote(ELECTION, e2957b23-687f-4626-af75-9b42f3a43b99, group-0EBF5B06395B, 15, (t:14, i:20))
dn3_1    | 2023-04-04 07:55:59,329 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState] INFO impl.FollowerState: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-FollowerState was interrupted
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-04-04 07:56:51,016 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm2_1   | 2023-04-04 07:58:04,681 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn5_1    | 2023-04-04 07:54:26,354 [grpc-default-executor-2] INFO impl.VoteContext: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FOLLOWER: accept ELECTION from e2957b23-687f-4626-af75-9b42f3a43b99: our priority 0 <= candidate's priority 1
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 2023-04-04 07:55:58,775 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e command on datanode e2957b23-687f-4626-af75-9b42f3a43b99.
dn4_1    | 2023-04-04 07:56:51,032 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133: start as a follower, conf=-1: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-04-04 07:54:57,966 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e in state CLOSED which uses HEALTHY_READONLY datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94. This will send close commands for its containers.
scm3_1   | 2023-04-04 07:54:57,966 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d0f51999-1853-4262-b31a-8b1b4f9c62a2 in state CLOSED which uses HEALTHY_READONLY datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94. This will send close commands for its containers.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn5_1    | 2023-04-04 07:54:26,354 [grpc-default-executor-2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: changes role from  FOLLOWER to FOLLOWER at term 15 for candidate:e2957b23-687f-4626-af75-9b42f3a43b99
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn2_1    | 2023-04-04 07:55:58,775 [Command processor thread] INFO server.RaftServer: e2957b23-687f-4626-af75-9b42f3a43b99: remove    LEADER e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210:t4, leader=e2957b23-687f-4626-af75-9b42f3a43b99, voted=e2957b23-687f-4626-af75-9b42f3a43b99, raftlog=Memoized:e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLog:OPENED:c6, conf=5: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-04-04 07:56:51,032 [pool-26-thread-1] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm3_1   | 2023-04-04 07:54:57,967 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b in state CLOSED which uses HEALTHY_READONLY datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94. This will send close commands for its containers.
scm3_1   | 2023-04-04 07:55:19,953 [IPC Server handler 24 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn5_1    | 2023-04-04 07:54:26,355 [grpc-default-executor-2] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState
dn4_1    | 2023-04-04 07:56:51,033 [pool-26-thread-1] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: start 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-FollowerState
dn2_1    | 2023-04-04 07:55:58,775 [Command processor thread] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: shutdown
dn2_1    | 2023-04-04 07:55:58,775 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-F3D51D6EC210,id=e2957b23-687f-4626-af75-9b42f3a43b99
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn5_1    | 2023-04-04 07:54:26,355 [grpc-default-executor-2] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState
dn5_1    | 2023-04-04 07:54:26,356 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO impl.FollowerState: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState was interrupted
scm3_1   | 2023-04-04 07:55:20,546 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn4_1    | 2023-04-04 07:56:51,049 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EBA4A65DC133,id=14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn4_1    | 2023-04-04 07:56:51,049 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-04-04 07:56:51,054 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-04-04 07:56:51,065 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn5_1    | 2023-04-04 07:54:26,429 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
scm1_1   | 2023-04-04 07:54:11,822 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1363, lastRpcResponseTime=6757) yet, just keep nextIndex unchanged and retry.
scm3_1   | 2023-04-04 07:55:22,655 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn4_1    | 2023-04-04 07:56:51,068 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-04-04 07:56:51,068 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-04-04 07:56:51,070 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn5_1    | 2023-04-04 07:54:26,429 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO impl.LeaderElection:   Response 0: 2ebde02c-a404-41d0-92a4-7b6da490547a<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t19
dn5_1    | 2023-04-04 07:54:26,429 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1 ELECTION round 0: result PASSED
scm3_1   | 2023-04-04 07:55:23,683 [IPC Server handler 11 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn3_1    | 2023-04-04 07:55:59,330 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-0EBF5B06395B: Taking a snapshot at:(t:15, i:26) file /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/sm/snapshot.15_26
dn2_1    | 2023-04-04 07:55:58,775 [Command processor thread] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: shutdown e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-LeaderStateImpl
dn4_1    | 2023-04-04 07:56:51,071 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=2b2f258a-8c66-4c27-9e64-eba4a65dc133
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 2023-04-04 07:55:59,331 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-0EBF5B06395B: Finished taking a snapshot at:(t:15, i:26) file:/data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/sm/snapshot.15_26 took: 2 ms
dn4_1    | 2023-04-04 07:56:51,074 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=2b2f258a-8c66-4c27-9e64-eba4a65dc133.
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-04-04 07:54:11,860 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=23, lastRpcResponseTime=6794) yet, just keep nextIndex unchanged and retry.
scm1_1   | 2023-04-04 07:54:13,106 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-04-04 07:55:59,332 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-StateMachineUpdater] INFO impl.StateMachineUpdater: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-StateMachineUpdater: Took a snapshot at index 26
scm3_1   | 2023-04-04 07:55:23,737 [IPC Server handler 8 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:55:23,769 [IPC Server handler 3 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn2_1    | 2023-04-04 07:55:58,776 [Command processor thread] INFO impl.PendingRequests: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-PendingRequests: sendNotLeaderResponses
scm1_1   | 2023-04-04 07:54:13,108 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:13,116 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1279, lastRpcResponseTime=8051) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:55:59,332 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-StateMachineUpdater] INFO impl.StateMachineUpdater: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-StateMachineUpdater: snapshotIndex: updateIncreasingly 20 -> 26
scm3_1   | 2023-04-04 07:55:23,790 [IPC Server handler 10 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:55:26,159 [IPC Server handler 62 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:55:26,490 [IPC Server handler 15 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 2023-04-04 07:54:13,118 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1281, lastRpcResponseTime=8052) yet, just keep nextIndex unchanged and retry.
scm1_1   | 2023-04-04 07:54:13,196 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-04-04 07:55:59,333 [Command processor thread] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: closes. applyIndex: 26
scm3_1   | 2023-04-04 07:55:27,151 [IPC Server handler 62 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:55:27,223 [IPC Server handler 69 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn2_1    | 2023-04-04 07:55:58,777 [Command processor thread] INFO impl.StateMachineUpdater: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-StateMachineUpdater: set stopIndex = 6
dn5_1    | 2023-04-04 07:54:26,430 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1
dn3_1    | 2023-04-04 07:55:59,334 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm1_1   | 2023-04-04 07:54:13,205 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:13,207 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=69, lastRpcResponseTime=8141) yet, just keep nextIndex unchanged and retry.
scm1_1   | 2023-04-04 07:54:13,216 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=77, lastRpcResponseTime=8150) yet, just keep nextIndex unchanged and retry.
scm3_1   | 2023-04-04 07:55:27,254 [IPC Server handler 68 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:55:27,431 [IPC Server handler 15 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn2_1    | 2023-04-04 07:55:58,778 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-F3D51D6EC210: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210/sm/snapshot.4_6
dn5_1    | 2023-04-04 07:54:26,430 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: changes role from CANDIDATE to LEADER at term 19 for changeToLeader
dn3_1    | 2023-04-04 07:55:59,335 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B-SegmentedRaftLogWorker close()
scm3_1   | 2023-04-04 07:55:27,432 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) reported CLOSED replica.
scm3_1   | 2023-04-04 07:55:27,515 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:55:27,644 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:54:26,430 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EEB13EAC206E with new leaderId: 2ebde02c-a404-41d0-92a4-7b6da490547a
dn3_1    | 2023-04-04 07:55:59,339 [Command processor thread] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-0EBF5B06395B: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b
scm3_1   | 2023-04-04 07:55:27,687 [FixedThreadPoolWithAffinityExecutor-8-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #2 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7 is not the leader 861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 2023-04-04 07:55:27,771 [IPC Server handler 3 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:55:27,789 [IPC Server handler 10 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:54:26,464 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: change Leader from null to 2ebde02c-a404-41d0-92a4-7b6da490547a at term 19 for becomeLeader, leader elected after 36699ms
dn3_1    | 2023-04-04 07:55:59,339 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b command on datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94.
scm3_1   | 2023-04-04 07:55:27,836 [IPC Server handler 9 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:55:27,853 [IPC Server handler 16 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:55:27,907 [IPC Server handler 18 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn2_1    | 2023-04-04 07:55:58,779 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-F3D51D6EC210: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210/sm/snapshot.4_6 took: 1 ms
dn4_1    | 2023-04-04 07:56:56,265 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-FollowerState] INFO impl.FollowerState: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5232126089ns, electionTimeout:5194ms
dn4_1    | 2023-04-04 07:56:56,265 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-FollowerState] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: shutdown 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-FollowerState
recon_1  | 2023-04-04 07:54:18,652 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn5_1    | 2023-04-04 07:54:26,464 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-04-04 07:55:58,779 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-StateMachineUpdater] INFO impl.StateMachineUpdater: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-StateMachineUpdater: Took a snapshot at index 6
scm1_1   | 2023-04-04 07:54:14,391 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-04-04 07:56:56,265 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-FollowerState] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-04-04 07:56:17,347 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 2023-04-04 07:54:26,466 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-04-04 07:55:58,779 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-StateMachineUpdater] INFO impl.StateMachineUpdater: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn2_1    | 2023-04-04 07:55:58,780 [Command processor thread] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: closes. applyIndex: 6
dn4_1    | 2023-04-04 07:56:56,265 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-04-04 07:56:29,321 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94: new RaftServerImpl for group-9F3CA91AAA35:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 2023-04-04 07:55:28,124 [IPC Server handler 60 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:54:26,466 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-04-04 07:55:58,780 [e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm1_1   | 2023-04-04 07:54:14,422 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-04-04 07:56:56,265 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-FollowerState] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: start 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3
dn3_1    | 2023-04-04 07:56:29,322 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 2023-04-04 07:55:28,126 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21) reported CLOSED replica.
dn5_1    | 2023-04-04 07:54:26,467 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-04-04 07:55:58,781 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210-SegmentedRaftLogWorker close()
scm1_1   | 2023-04-04 07:54:14,426 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1288, lastRpcResponseTime=9360) yet, just keep nextIndex unchanged and retry.
dn4_1    | 2023-04-04 07:56:56,266 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:56:29,322 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 2023-04-04 07:55:28,131 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7 is not the leader 861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 2023-04-04 07:54:14,429 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1290, lastRpcResponseTime=9363) yet, just keep nextIndex unchanged and retry.
dn4_1    | 2023-04-04 07:56:56,266 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3 PRE_VOTE round 0: result PASSED (term=0)
dn3_1    | 2023-04-04 07:56:29,322 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 2023-04-04 07:55:28,177 [IPC Server handler 64 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn2_1    | 2023-04-04 07:55:58,783 [Command processor thread] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-F3D51D6EC210: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/0a9acdcd-53b0-4fd8-86de-f3d51d6ec210
dn2_1    | 2023-04-04 07:55:58,783 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=0a9acdcd-53b0-4fd8-86de-f3d51d6ec210 command on datanode e2957b23-687f-4626-af75-9b42f3a43b99.
scm1_1   | 2023-04-04 07:54:14,483 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-04-04 07:54:26,468 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-04-04 07:56:29,322 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 2023-04-04 07:55:28,177 [IPC Server handler 63 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn4_1    | 2023-04-04 07:56:56,268 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-04-04 07:56:56,268 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO impl.LeaderElection: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3 ELECTION round 0: result PASSED (term=1)
dn2_1    | 2023-04-04 07:55:58,783 [Command processor thread] INFO server.RaftServer: e2957b23-687f-4626-af75-9b42f3a43b99: remove    LEADER e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B:t15, leader=e2957b23-687f-4626-af75-9b42f3a43b99, voted=e2957b23-687f-4626-af75-9b42f3a43b99, raftlog=Memoized:e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLog:OPENED:c26, conf=21: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn5_1    | 2023-04-04 07:54:26,468 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-04-04 07:56:29,322 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 2023-04-04 07:55:28,193 [IPC Server handler 66 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 2023-04-04 07:54:14,489 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:14,501 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1363, lastRpcResponseTime=9435) yet, just keep nextIndex unchanged and retry.
dn2_1    | 2023-04-04 07:55:58,783 [Command processor thread] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: shutdown
dn5_1    | 2023-04-04 07:54:26,469 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-04-04 07:56:29,322 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 2023-04-04 07:55:28,279 [IPC Server handler 71 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn4_1    | 2023-04-04 07:56:56,269 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: shutdown 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3
scm1_1   | 2023-04-04 07:54:14,504 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1365, lastRpcResponseTime=9438) yet, just keep nextIndex unchanged and retry.
dn2_1    | 2023-04-04 07:55:58,783 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-0EBF5B06395B,id=e2957b23-687f-4626-af75-9b42f3a43b99
dn5_1    | 2023-04-04 07:54:26,469 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 2023-04-04 07:55:28,280 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21) reported CLOSED replica.
dn4_1    | 2023-04-04 07:56:56,271 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1_1   | 2023-04-04 07:54:14,552 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-04-04 07:55:58,784 [Command processor thread] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: shutdown e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-LeaderStateImpl
dn3_1    | 2023-04-04 07:56:29,322 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35: ConfigurationManager, init=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-04-04 07:54:26,529 [grpc-default-executor-2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B replies to ELECTION vote request: e2957b23-687f-4626-af75-9b42f3a43b99<-2ebde02c-a404-41d0-92a4-7b6da490547a#0:OK-t15. Peer's state: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B:t15, leader=null, voted=e2957b23-687f-4626-af75-9b42f3a43b99, raftlog=Memoized:2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLog:OPENED:c20, conf=15: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:54:26,573 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-04-04 07:55:28,281 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1001 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7 is not the leader 861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn4_1    | 2023-04-04 07:56:56,271 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EBA4A65DC133 with new leaderId: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb
scm1_1   | 2023-04-04 07:54:14,556 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=45, lastRpcResponseTime=9490) yet, just keep nextIndex unchanged and retry.
dn2_1    | 2023-04-04 07:55:58,784 [Command processor thread] INFO impl.PendingRequests: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-PendingRequests: sendNotLeaderResponses
dn3_1    | 2023-04-04 07:56:29,323 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn5_1    | 2023-04-04 07:54:26,573 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-04-04 07:55:28,295 [IPC Server handler 76 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn4_1    | 2023-04-04 07:56:56,272 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133: change Leader from null to 14df8ad4-ce2f-4483-aada-29ab5cae3ffb at term 1 for becomeLeader, leader elected after 5291ms
dn4_1    | 2023-04-04 07:56:56,272 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-04-04 07:55:58,784 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B->c1077390-d65b-4523-9cd4-abe9e2c9eb94-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B->c1077390-d65b-4523-9cd4-abe9e2c9eb94-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn3_1    | 2023-04-04 07:56:29,327 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-04-04 07:56:29,327 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-04-04 07:56:29,327 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-04-04 07:54:26,626 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm3_1   | 2023-04-04 07:55:28,296 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19) reported CLOSED replica.
dn4_1    | 2023-04-04 07:56:56,274 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-04-04 07:54:14,564 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-04-04 07:55:58,784 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B->2ebde02c-a404-41d0-92a4-7b6da490547a-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B->2ebde02c-a404-41d0-92a4-7b6da490547a-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn3_1    | 2023-04-04 07:56:29,331 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn5_1    | 2023-04-04 07:54:26,630 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-04-04 07:55:28,297 [FixedThreadPoolWithAffinityExecutor-8-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1001 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7 is not the leader 861060d2-f30f-4260-bac0-b4e7af8aba9d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn4_1    | 2023-04-04 07:56:56,274 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm1_1   | 2023-04-04 07:54:14,565 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=54, lastRpcResponseTime=9499) yet, just keep nextIndex unchanged and retry.
scm1_1   | 2023-04-04 07:54:15,738 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-861060d2-f30f-4260-bac0-b4e7af8aba9d: Detected pause in JVM or host machine (eg GC): pause of approximately 106282502ns.
dn3_1    | 2023-04-04 07:56:29,331 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn5_1    | 2023-04-04 07:54:26,631 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
scm3_1   | 2023-04-04 07:55:28,343 [IPC Server handler 99 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn4_1    | 2023-04-04 07:56:56,281 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-04-04 07:55:58,791 [grpc-default-executor-2] INFO server.GrpcLogAppender: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B->c1077390-d65b-4523-9cd4-abe9e2c9eb94-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm1_1   | GC pool 'ParNew' had collection(s): count=1 time=67ms
dn3_1    | 2023-04-04 07:56:29,345 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-04-04 07:56:29,345 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1   | 2023-04-04 07:55:32,414 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for delTxnId, expected lastId is 0, actual lastId is 1000.
dn4_1    | 2023-04-04 07:56:56,281 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-04-04 07:55:58,793 [grpc-default-executor-0] INFO server.GrpcLogAppender: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B->2ebde02c-a404-41d0-92a4-7b6da490547a-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm1_1   | 2023-04-04 07:54:15,781 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-04-04 07:56:29,345 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-04-04 07:55:58,795 [grpc-default-executor-0] INFO leader.FollowerInfo: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B->2ebde02c-a404-41d0-92a4-7b6da490547a: nextIndex: updateUnconditionally 27 -> 26
scm3_1   | 2023-04-04 07:55:49,965 [IPC Server handler 24 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn3_1    | 2023-04-04 07:56:29,345 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-04-04 07:55:58,796 [grpc-default-executor-0] INFO server.GrpcLogAppender: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B->2ebde02c-a404-41d0-92a4-7b6da490547a-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm1_1   | 2023-04-04 07:54:15,796 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=18, lastRpcResponseTime=10731) yet, just keep nextIndex unchanged and retry.
scm3_1   | 2023-04-04 07:55:50,533 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn3_1    | 2023-04-04 07:56:29,345 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 2023-04-04 07:55:58,796 [grpc-default-executor-0] INFO leader.FollowerInfo: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B->2ebde02c-a404-41d0-92a4-7b6da490547a: nextIndex: updateUnconditionally 26 -> 25
scm1_1   | 2023-04-04 07:54:15,913 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-04-04 07:55:56,627 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 84e9f90e-0ab8-4731-82e1-fb0ad827e6e2, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:b7b7229a-e604-4787-8543-e6d8cdebe63b, CreationTimestamp2023-04-04T07:54:11.795697Z[UTC]] removed.
dn3_1    | 2023-04-04 07:56:29,345 [Command processor thread] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: addNew group-9F3CA91AAA35:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns      null c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn3_1    | 2023-04-04 07:56:29,346 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/f3dc0283-6172-4aac-8038-9f3ca91aaa35 does not exist. Creating ...
dn3_1    | 2023-04-04 07:56:29,350 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/f3dc0283-6172-4aac-8038-9f3ca91aaa35/in_use.lock acquired by nodename 8@d6d8a15a26f6
dn4_1    | 2023-04-04 07:56:56,281 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1_1   | 2023-04-04 07:54:15,915 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=136, lastRpcResponseTime=10849) yet, just keep nextIndex unchanged and retry.
scm3_1   | 2023-04-04 07:55:56,639 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: a013e403-c1b0-4f4f-851c-eeb13eac206e, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:54:11.796602Z[UTC]] removed.
dn5_1    | 2023-04-04 07:54:26,703 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-04-04 07:54:26,706 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-04-04 07:56:29,354 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/f3dc0283-6172-4aac-8038-9f3ca91aaa35 has been successfully formatted.
dn4_1    | 2023-04-04 07:56:56,281 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-04-04 07:54:16,108 [IPC Server handler 69 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn2_1    | 2023-04-04 07:55:58,797 [grpc-default-executor-0] INFO server.GrpcLogAppender: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B->c1077390-d65b-4523-9cd4-abe9e2c9eb94-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn5_1    | 2023-04-04 07:54:26,709 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 2023-04-04 07:56:29,356 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-9F3CA91AAA35: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-04-04 07:56:56,281 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm3_1   | 2023-04-04 07:55:56,656 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d0f51999-1853-4262-b31a-8b1b4f9c62a2, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:c1077390-d65b-4523-9cd4-abe9e2c9eb94, CreationTimestamp2023-04-04T07:54:11.800072Z[UTC]] removed.
scm1_1   | 2023-04-04 07:54:16,141 [IPC Server handler 69 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 14df8ad4-ce2f-4483-aada-29ab5cae3ffb{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn2_1    | 2023-04-04 07:55:58,796 [grpc-default-executor-2] INFO leader.FollowerInfo: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B->c1077390-d65b-4523-9cd4-abe9e2c9eb94: nextIndex: updateUnconditionally 27 -> 26
dn4_1    | 2023-04-04 07:56:56,282 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO impl.RoleInfo: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: start 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderStateImpl
scm3_1   | 2023-04-04 07:55:56,668 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 685cb9fd-d40d-4ecb-9ff2-6617081ae410, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:54:11.790125Z[UTC]] removed.
recon_1  | 2023-04-04 07:54:18,833 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
dn5_1    | 2023-04-04 07:54:26,710 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-04-04 07:54:26,710 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1_1   | 2023-04-04 07:54:16,146 [IPC Server handler 71 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b7b7229a-e604-4787-8543-e6d8cdebe63b
dn4_1    | 2023-04-04 07:56:56,283 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-SegmentedRaftLogWorker: Starting segment from index:0
scm3_1   | 2023-04-04 07:55:56,680 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 0a9acdcd-53b0-4fd8-86de-f3d51d6ec210, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:e2957b23-687f-4626-af75-9b42f3a43b99, CreationTimestamp2023-04-04T07:54:11.760772Z[UTC]] removed.
recon_1  | java.nio.channels.ClosedChannelException
dn5_1    | 2023-04-04 07:54:26,719 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-04-04 07:54:26,730 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-04-04 07:54:16,364 [IPC Server handler 71 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b7b7229a-e604-4787-8543-e6d8cdebe63b{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn4_1    | 2023-04-04 07:56:56,284 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2b2f258a-8c66-4c27-9e64-eba4a65dc133/current/log_inprogress_0
dn2_1    | 2023-04-04 07:55:58,798 [grpc-default-executor-0] INFO leader.FollowerInfo: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B->c1077390-d65b-4523-9cd4-abe9e2c9eb94: nextIndex: updateUnconditionally 26 -> 25
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-04-04 07:54:26,731 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-04-04 07:54:26,733 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1_1   | 2023-04-04 07:54:16,514 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1_1   | 2023-04-04 07:54:16,488 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn4_1    | 2023-04-04 07:56:56,324 [14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133-LeaderElection3] INFO server.RaftServer$Division: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb@group-EBA4A65DC133: set configuration 0: peers:[14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 2023-04-04 07:54:26,733 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-04-04 07:54:26,735 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 2023-04-04 07:55:56,694 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 3a70a706-d27b-47ff-9aed-0ebf5b06395b, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:e2957b23-687f-4626-af75-9b42f3a43b99, CreationTimestamp2023-04-04T07:54:11.787140Z[UTC]] removed.
dn3_1    | 2023-04-04 07:56:29,356 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-04-04 07:57:15,278 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-04-04 07:56:29,356 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-04-04 07:58:15,279 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm3_1   | 2023-04-04 07:55:56,707 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 35a434de-4469-4302-9ebd-65bc10982ac6, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:54:11.776866Z[UTC]] removed.
scm3_1   | 2023-04-04 07:55:58,341 [IPC Server handler 99 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm3_1   | 2023-04-04 07:55:59,289 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY state.
dn3_1    | 2023-04-04 07:56:29,356 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-04-04 07:55:59,289 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-04-04 07:55:59,289 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY state.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-04-04 07:56:29,356 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-04-04 07:54:16,784 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-04-04 07:54:16,779 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-04-04 07:56:29,356 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-04-04 07:54:16,897 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-861060d2-f30f-4260-bac0-b4e7af8aba9d: Detected pause in JVM or host machine (eg GC): pause of approximately 142927098ns. No GCs detected.
dn5_1    | 2023-04-04 07:54:26,735 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3_1   | 2023-04-04 07:55:59,289 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn3_1    | 2023-04-04 07:56:29,358 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-04-04 07:56:29,359 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn2_1    | 2023-04-04 07:55:58,798 [Command processor thread] INFO impl.StateMachineUpdater: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-StateMachineUpdater: set stopIndex = 26
scm1_1   | 2023-04-04 07:54:16,779 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
dn5_1    | 2023-04-04 07:54:26,735 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm3_1   | 2023-04-04 07:55:59,504 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:55:59.480Z[UTC]].
dn3_1    | 2023-04-04 07:56:29,359 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-04-04 07:55:58,799 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-0EBF5B06395B: Taking a snapshot at:(t:15, i:26) file /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/sm/snapshot.15_26
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn5_1    | 2023-04-04 07:54:26,747 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderStateImpl
scm3_1   | 2023-04-04 07:55:59,535 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f3dc0283-6172-4aac-8038-9f3ca91aaa35, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:55:59.519Z[UTC]].
dn3_1    | 2023-04-04 07:56:29,359 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/f3dc0283-6172-4aac-8038-9f3ca91aaa35
dn3_1    | 2023-04-04 07:56:29,359 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn5_1    | 2023-04-04 07:54:26,756 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLogWorker: Rolling segment log-26_46 to index:46
scm3_1   | 2023-04-04 07:56:20,288 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY state.
dn2_1    | 2023-04-04 07:55:58,801 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-0EBF5B06395B: Finished taking a snapshot at:(t:15, i:26) file:/data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/sm/snapshot.15_26 took: 2 ms
dn3_1    | 2023-04-04 07:56:29,359 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-04-04 07:55:58,801 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-StateMachineUpdater] INFO impl.StateMachineUpdater: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-StateMachineUpdater: Took a snapshot at index 26
dn2_1    | 2023-04-04 07:55:58,801 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-StateMachineUpdater] INFO impl.StateMachineUpdater: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-StateMachineUpdater: snapshotIndex: updateIncreasingly 20 -> 26
scm1_1   | 2023-04-04 07:54:17,033 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:17,040 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm3_1   | 2023-04-04 07:56:20,289 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-04-04 07:56:20,498 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 2b2f258a-8c66-4c27-9e64-eba4a65dc133, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.479Z[UTC]].
dn3_1    | 2023-04-04 07:56:29,359 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-04-04 07:56:29,360 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm1_1   | 2023-04-04 07:54:17,033 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn5_1    | 2023-04-04 07:54:26,820 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_inprogress_26 to /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_26-46
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 2023-04-04 07:56:29,360 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-04-04 07:54:17,050 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-04-04 07:54:17,051 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 2023-04-04 07:55:58,802 [Command processor thread] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: closes. applyIndex: 26
dn2_1    | 2023-04-04 07:55:58,802 [e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn3_1    | 2023-04-04 07:56:29,362 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-04-04 07:54:26,826 [2ebde02c-a404-41d0-92a4-7b6da490547a-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0EBF5B06395B with new leaderId: e2957b23-687f-4626-af75-9b42f3a43b99
scm1_1   | 2023-04-04 07:54:17,054 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:17,089 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1310, lastRpcResponseTime=12023) yet, just keep nextIndex unchanged and retry.
scm1_1   | 2023-04-04 07:54:17,130 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1351, lastRpcResponseTime=12064) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:56:29,362 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | 2023-04-04 07:56:20,522 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8037e277-427a-4034-9f8a-4e27919647df, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.502Z[UTC]].
dn2_1    | 2023-04-04 07:55:58,803 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B-SegmentedRaftLogWorker close()
scm1_1   | 2023-04-04 07:54:17,221 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-04-04 07:54:26,826 [2ebde02c-a404-41d0-92a4-7b6da490547a-server-thread1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: change Leader from null to e2957b23-687f-4626-af75-9b42f3a43b99 at term 15 for appendEntries, leader elected after 38145ms
dn3_1    | 2023-04-04 07:56:29,362 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 2023-04-04 07:56:20,546 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.531Z[UTC]].
dn2_1    | 2023-04-04 07:55:58,806 [Command processor thread] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-0EBF5B06395B: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b
dn2_1    | 2023-04-04 07:55:58,806 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b command on datanode e2957b23-687f-4626-af75-9b42f3a43b99.
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-04-04 07:54:26,839 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/current/log_inprogress_47
dn3_1    | 2023-04-04 07:56:29,362 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-04-04 07:56:29,364 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-04-04 07:54:17,235 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=68, lastRpcResponseTime=12170) yet, just keep nextIndex unchanged and retry.
dn2_1    | 2023-04-04 07:55:59,275 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: e2957b23-687f-4626-af75-9b42f3a43b99: Completed APPEND_ENTRIES, lastRequest: null
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn5_1    | 2023-04-04 07:54:26,843 [2ebde02c-a404-41d0-92a4-7b6da490547a-server-thread1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: set configuration 21: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:56:29,375 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 2023-04-04 07:56:23,289 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY state.
scm1_1   | 2023-04-04 07:54:17,266 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-04-04 07:55:59,275 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e2957b23-687f-4626-af75-9b42f3a43b99: Completed APPEND_ENTRIES, lastRequest: 2ebde02c-a404-41d0-92a4-7b6da490547a->e2957b23-687f-4626-af75-9b42f3a43b99#52-t19,previous=(t:19, i:51),leaderCommit=51,initializing? true,entries: size=1, first=(t:19, i:52), METADATAENTRY(c:51)
recon_1  | 2023-04-04 07:54:19,961 [IPC Server handler 25 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn4_1.ha_net
dn5_1    | 2023-04-04 07:54:26,845 [2ebde02c-a404-41d0-92a4-7b6da490547a-server-thread1] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLogWorker: Rolling segment log-15_20 to index:20
dn3_1    | 2023-04-04 07:56:29,375 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1   | 2023-04-04 07:56:23,290 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-04-04 07:54:17,330 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=162, lastRpcResponseTime=12264) yet, just keep nextIndex unchanged and retry.
scm1_1   | 2023-04-04 07:54:18,093 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm1_1   | 2023-04-04 07:54:18,094 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
dn5_1    | 2023-04-04 07:54:26,881 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderElection1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: set configuration 47: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:56:29,392 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm1_1   | 2023-04-04 07:54:18,094 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-04-04 07:54:18,095 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 2023-04-04 07:54:20,262 [IPC Server handler 26 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn3_1.ha_net
scm3_1   | 2023-04-04 07:56:23,493 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6d92c706-b857-41ae-a811-0fceb345a748, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:23.479Z[UTC]].
dn5_1    | 2023-04-04 07:54:26,903 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_inprogress_15 to /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_15-20
dn3_1    | 2023-04-04 07:56:29,392 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-04-04 07:54:18,433 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:18,433 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-04-04 07:54:20,334 [IPC Server handler 99 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn5_1.ha_net
scm3_1   | 2023-04-04 07:56:29,292 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY state.
dn5_1    | 2023-04-04 07:54:26,919 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/current/log_inprogress_21
dn3_1    | 2023-04-04 07:56:29,393 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-04-04 07:56:29,395 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35: start as a follower, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:56:17,189 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 2023-04-04 07:54:20,390 [IPC Server handler 13 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn2_1.ha_net
scm3_1   | 2023-04-04 07:56:29,292 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn5_1    | 2023-04-04 07:54:33,747 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 2023-04-04 07:56:29,398 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 2023-04-04 07:54:18,435 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=4, lastRpcResponseTime=13369) yet, just keep nextIndex unchanged and retry.
dn2_1    | 2023-04-04 07:56:29,333 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-04-04 07:56:59,403 [pool-26-thread-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99: new RaftServerImpl for group-588A1A0D07FF:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-04-04 07:55:17,717 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-04-04 07:56:29,398 [pool-26-thread-1] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-FollowerState
dn2_1    | 2023-04-04 07:56:59,403 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
recon_1  | 2023-04-04 07:54:20,520 [IPC Server handler 18 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn1_1.ha_net
scm1_1   | 2023-04-04 07:54:18,590 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=159, lastRpcResponseTime=13524) yet, just keep nextIndex unchanged and retry.
scm3_1   | 2023-04-04 07:56:29,338 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:55:59.480Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:55:27,688 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 13.
dn3_1    | 2023-04-04 07:56:29,398 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9F3CA91AAA35,id=c1077390-d65b-4523-9cd4-abe9e2c9eb94
dn3_1    | 2023-04-04 07:56:29,399 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1  | 2023-04-04 07:54:47,333 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
scm1_1   | 2023-04-04 07:54:18,519 [IPC Server handler 84 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm3_1   | 2023-04-04 07:56:29,367 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f3dc0283-6172-4aac-8038-9f3ca91aaa35, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:c1077390-d65b-4523-9cd4-abe9e2c9eb94, CreationTimestamp2023-04-04T07:55:59.519Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:55:27,689 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 13.
dn2_1    | 2023-04-04 07:56:59,403 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-04-04 07:56:29,399 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
recon_1  | 2023-04-04 07:54:47,360 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
scm1_1   | 2023-04-04 07:54:18,655 [IPC Server handler 84 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c1077390-d65b-4523-9cd4-abe9e2c9eb94{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-04-04 07:56:29,524 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a21f8cfe-4543-468b-bd60-588a1a0d07ff, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:29.481Z[UTC]].
dn5_1    | 2023-04-04 07:55:27,720 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-0] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 13.
dn2_1    | 2023-04-04 07:56:59,403 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-04-04 07:56:29,399 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
recon_1  | 2023-04-04 07:54:47,406 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
scm1_1   | 2023-04-04 07:54:18,498 [IPC Server handler 83 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e2957b23-687f-4626-af75-9b42f3a43b99
scm3_1   | 2023-04-04 07:56:35,944 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8037e277-427a-4034-9f8a-4e27919647df, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:56:20.502Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:55:27,739 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-1] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 19.
dn2_1    | 2023-04-04 07:56:59,405 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-04-04 07:56:29,400 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1  | 2023-04-04 07:54:47,415 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 79 milliseconds.
scm1_1   | 2023-04-04 07:54:18,683 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-04-04 07:56:36,706 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:56:20.531Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:55:27,855 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-1] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 19.
dn2_1    | 2023-04-04 07:56:59,405 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-04-04 07:56:29,400 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-04-04 07:54:47,663 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 271 milliseconds to process 0 existing database records.
scm1_1   | 2023-04-04 07:54:18,684 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm3_1   | 2023-04-04 07:56:51,051 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 2b2f258a-8c66-4c27-9e64-eba4a65dc133, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:56:20.479Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:55:27,888 [ContainerOp-3a70a706-d27b-47ff-9aed-0ebf5b06395b-1] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 19.
dn5_1    | 2023-04-04 07:55:28,097 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 24.
dn3_1    | 2023-04-04 07:56:29,414 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=f3dc0283-6172-4aac-8038-9f3ca91aaa35
recon_1  | 2023-04-04 07:54:47,768 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 105 milliseconds for processing 4 containers.
scm1_1   | 2023-04-04 07:54:18,694 [IPC Server handler 83 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e2957b23-687f-4626-af75-9b42f3a43b99{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-04-04 07:56:51,666 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6d92c706-b857-41ae-a811-0fceb345a748, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b7b7229a-e604-4787-8543-e6d8cdebe63b, CreationTimestamp2023-04-04T07:56:23.479Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:55:28,097 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 24.
dn2_1    | 2023-04-04 07:56:59,405 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-04-04 07:56:29,416 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=f3dc0283-6172-4aac-8038-9f3ca91aaa35.
recon_1  | 2023-04-04 07:54:59,339 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
scm1_1   | 2023-04-04 07:54:18,695 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm3_1   | 2023-04-04 07:56:59,450 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a21f8cfe-4543-468b-bd60-588a1a0d07ff, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e2957b23-687f-4626-af75-9b42f3a43b99, CreationTimestamp2023-04-04T07:56:29.481Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:55:28,110 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-0] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 24.
dn3_1    | 2023-04-04 07:56:29,416 [Command processor thread] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: addNew group-4E27919647DF:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns group-4E27919647DF:java.util.concurrent.CompletableFuture@57be69d[Not completed]
dn2_1    | 2023-04-04 07:56:59,405 [pool-26-thread-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF: ConfigurationManager, init=-1: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1  | 2023-04-04 07:54:59,342 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-04-04 07:54:18,695 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-04-04 07:57:04,678 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm3_1   | 2023-04-04 07:57:06,718 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn3_1    | 2023-04-04 07:56:29,421 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94: new RaftServerImpl for group-4E27919647DF:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-04-04 07:56:59,406 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
recon_1  | 2023-04-04 07:54:59,342 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-04-04 07:54:18,701 [IPC Server handler 94 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2ebde02c-a404-41d0-92a4-7b6da490547a
scm3_1   | 2023-04-04 07:57:34,681 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn5_1    | 2023-04-04 07:55:28,232 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 45.
dn3_1    | 2023-04-04 07:56:29,421 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-04-04 07:56:59,416 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1  | 2023-04-04 07:54:59,342 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-04-04 07:54:18,702 [IPC Server handler 94 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 2ebde02c-a404-41d0-92a4-7b6da490547a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-04-04 07:57:36,722 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn5_1    | 2023-04-04 07:55:28,234 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-1] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 45.
dn5_1    | 2023-04-04 07:55:28,265 [ContainerOp-a013e403-c1b0-4f4f-851c-eeb13eac206e-1] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 45.
dn2_1    | 2023-04-04 07:56:59,418 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
recon_1  | 2023-04-04 07:54:59,342 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-04-04 07:54:18,702 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3_1   | 2023-04-04 07:57:50,372 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 2000.
dn5_1    | 2023-04-04 07:55:28,331 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-04-04 07:56:29,430 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-04-04 07:56:29,430 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1  | 2023-04-04 07:54:59,342 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-04-04 07:54:18,702 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm3_1   | 2023-04-04 07:57:50,433 [e0c91ed2-48f2-4467-9ffd-e28e64336c64@group-0BB16BE3F1B7-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
dn2_1    | 2023-04-04 07:56:59,419 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-04-04 07:56:29,430 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-04-04 07:56:29,430 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-04-04 07:54:18,710 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
scm3_1   | 2023-04-04 07:58:04,660 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn5_1    | 2023-04-04 07:55:28,339 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
recon_1  | 2023-04-04 07:54:59,342 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
dn3_1    | 2023-04-04 07:56:29,430 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1   | 2023-04-04 07:54:18,748 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1_1   | 2023-04-04 07:54:18,748 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
dn5_1    | 2023-04-04 07:55:28,344 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
recon_1  | 2023-04-04 07:54:59,342 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
dn2_1    | 2023-04-04 07:56:59,422 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm1_1   | 2023-04-04 07:54:18,749 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
dn3_1    | 2023-04-04 07:56:29,430 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF: ConfigurationManager, init=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-04-04 07:55:28,347 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
recon_1  | 2023-04-04 07:54:59,342 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 132 
dn2_1    | 2023-04-04 07:56:59,422 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-04-04 07:54:18,765 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
dn3_1    | 2023-04-04 07:56:29,438 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-04-04 07:55:28,353 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
recon_1  | 2023-04-04 07:54:59,456 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 7, SequenceNumber diff: 18, SequenceNumber Lag from OM 0.
dn2_1    | 2023-04-04 07:56:59,424 [Command processor thread] INFO server.RaftServer: e2957b23-687f-4626-af75-9b42f3a43b99: addNew group-588A1A0D07FF:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns group-588A1A0D07FF:java.util.concurrent.CompletableFuture@18487237[Not completed]
scm1_1   | 2023-04-04 07:54:18,807 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn3_1    | 2023-04-04 07:56:29,441 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-04-04 07:55:28,353 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
recon_1  | 2023-04-04 07:54:59,456 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 18 records
dn2_1    | 2023-04-04 07:56:59,425 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-04-04 07:56:29,441 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-04-04 07:55:28,354 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
recon_1  | 2023-04-04 07:54:59,461 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
dn2_1    | 2023-04-04 07:56:59,429 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-04-04 07:56:29,441 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-04-04 07:55:28,355 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
recon_1  | 2023-04-04 07:54:59,461 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
dn2_1    | 2023-04-04 07:56:59,429 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-04-04 07:54:18,807 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
dn3_1    | 2023-04-04 07:56:29,442 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-04-04 07:55:28,355 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
recon_1  | 2023-04-04 07:54:59,597 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
dn2_1    | 2023-04-04 07:56:59,429 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-04-04 07:54:18,807 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
dn3_1    | 2023-04-04 07:56:29,442 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-04-04 07:55:28,355 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-04-04 07:54:59,616 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 2 OM DB update event(s).
dn2_1    | 2023-04-04 07:56:59,429 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 2023-04-04 07:54:18,807 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
dn3_1    | 2023-04-04 07:56:29,442 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-04-04 07:55:28,357 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-04-04 07:54:59,719 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
dn2_1    | 2023-04-04 07:56:59,430 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a21f8cfe-4543-468b-bd60-588a1a0d07ff does not exist. Creating ...
scm1_1   | 2023-04-04 07:54:18,808 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
dn3_1    | 2023-04-04 07:56:29,442 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1  | 2023-04-04 07:55:23,666 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Container #1 has state OPEN, but given state is CLOSING.
dn2_1    | 2023-04-04 07:56:59,441 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a21f8cfe-4543-468b-bd60-588a1a0d07ff/in_use.lock acquired by nodename 7@feac3239a846
scm1_1   | 2023-04-04 07:54:18,809 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
dn3_1    | 2023-04-04 07:56:29,443 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-04-04 07:55:28,357 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-04-04 07:55:23,735 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Container #1001 has state OPEN, but given state is CLOSING.
dn2_1    | 2023-04-04 07:56:59,446 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a21f8cfe-4543-468b-bd60-588a1a0d07ff has been successfully formatted.
dn3_1    | 2023-04-04 07:56:29,443 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-04-04 07:55:28,357 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-04-04 07:55:23,775 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Container #2 has state OPEN, but given state is CLOSING.
scm1_1   | 2023-04-04 07:54:18,810 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
dn2_1    | 2023-04-04 07:56:59,459 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-588A1A0D07FF: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-04-04 07:56:29,443 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-04-04 07:55:28,357 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-04-04 07:55:23,793 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Container #1002 has state OPEN, but given state is CLOSING.
scm1_1   | 2023-04-04 07:54:18,811 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
dn2_1    | 2023-04-04 07:56:59,459 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-04-04 07:56:29,443 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df does not exist. Creating ...
dn5_1    | 2023-04-04 07:55:58,787 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: 2ebde02c-a404-41d0-92a4-7b6da490547a: Completed APPEND_ENTRIES, lastRequest: e2957b23-687f-4626-af75-9b42f3a43b99->2ebde02c-a404-41d0-92a4-7b6da490547a#63-t15,previous=(t:15, i:25),leaderCommit=25,initializing? true,entries: size=1, first=(t:15, i:26), METADATAENTRY(c:25)
recon_1  | 2023-04-04 07:55:27,430 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) reported CLOSED replica.
recon_1  | 2023-04-04 07:55:27,784 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) reported CLOSED replica.
dn2_1    | 2023-04-04 07:56:59,460 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-04-04 07:56:29,441 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-04-04 07:55:58,791 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: 2ebde02c-a404-41d0-92a4-7b6da490547a: Completed APPEND_ENTRIES, lastRequest: null
recon_1  | 2023-04-04 07:55:28,124 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21) reported CLOSED replica.
scm1_1   | 2023-04-04 07:54:18,811 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
dn2_1    | 2023-04-04 07:56:59,462 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-04-04 07:55:59,267 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-04-04 07:55:28,295 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19) reported CLOSED replica.
scm1_1   | 2023-04-04 07:54:18,919 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
dn2_1    | 2023-04-04 07:56:59,462 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-04-04 07:56:59,462 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-04-04 07:56:59,462 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 2023-04-04 07:55:59,731 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
scm1_1   | 2023-04-04 07:54:18,948 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn5_1    | 2023-04-04 07:55:59,269 [Command processor thread] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: remove    LEADER 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E:t19, leader=2ebde02c-a404-41d0-92a4-7b6da490547a, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLog:OPENED:c52, conf=47: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn2_1    | 2023-04-04 07:56:59,462 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-04-04 07:56:29,450 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df/in_use.lock acquired by nodename 8@d6d8a15a26f6
recon_1  | 2023-04-04 07:55:59,731 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
scm1_1   | 2023-04-04 07:54:19,682 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-04-04 07:55:59,270 [Command processor thread] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: shutdown
dn2_1    | 2023-04-04 07:56:59,463 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-04-04 07:56:29,452 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df has been successfully formatted.
recon_1  | 2023-04-04 07:55:59,731 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 150 
scm1_1   | 2023-04-04 07:54:19,685 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-04-04 07:55:59,270 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-EEB13EAC206E,id=2ebde02c-a404-41d0-92a4-7b6da490547a
dn2_1    | 2023-04-04 07:56:59,464 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a21f8cfe-4543-468b-bd60-588a1a0d07ff
dn2_1    | 2023-04-04 07:56:59,464 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1  | 2023-04-04 07:55:59,743 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 2, SequenceNumber diff: 5, SequenceNumber Lag from OM 0.
scm1_1   | 2023-04-04 07:54:19,689 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1258, lastRpcResponseTime=14623) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:56:29,453 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-4E27919647DF: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-04-04 07:56:59,464 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
recon_1  | 2023-04-04 07:55:59,743 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 5 records
dn3_1    | 2023-04-04 07:56:29,455 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | 2023-04-04 07:54:19,702 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1271, lastRpcResponseTime=14637) yet, just keep nextIndex unchanged and retry.
dn2_1    | 2023-04-04 07:56:59,464 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 2023-04-04 07:55:59,747 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
dn5_1    | 2023-04-04 07:55:59,271 [Command processor thread] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-LeaderStateImpl
dn3_1    | 2023-04-04 07:56:29,455 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-04-04 07:54:19,718 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-04-04 07:56:59,464 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1  | 2023-04-04 07:55:59,747 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
dn5_1    | 2023-04-04 07:55:59,271 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E->e2957b23-687f-4626-af75-9b42f3a43b99-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E->e2957b23-687f-4626-af75-9b42f3a43b99-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn3_1    | 2023-04-04 07:56:29,455 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-04-04 07:54:19,718 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-04-04 07:55:59,838 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
dn5_1    | 2023-04-04 07:55:59,271 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E->c1077390-d65b-4523-9cd4-abe9e2c9eb94-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E->c1077390-d65b-4523-9cd4-abe9e2c9eb94-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn3_1    | 2023-04-04 07:56:29,455 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-04-04 07:56:59,465 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-04-04 07:54:19,725 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=2, lastRpcResponseTime=14659) yet, just keep nextIndex unchanged and retry.
recon_1  | 2023-04-04 07:55:59,838 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
scm1_1   | 2023-04-04 07:54:19,743 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=20, lastRpcResponseTime=14677) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:56:29,455 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-04-04 07:55:59,273 [Command processor thread] INFO impl.PendingRequests: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-PendingRequests: sendNotLeaderResponses
dn5_1    | 2023-04-04 07:55:59,278 [grpc-default-executor-2] INFO server.GrpcLogAppender: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E->e2957b23-687f-4626-af75-9b42f3a43b99-AppendLogResponseHandler: follower responses appendEntries COMPLETED
recon_1  | 2023-04-04 07:55:59,838 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
scm1_1   | 2023-04-04 07:54:20,981 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-04-04 07:56:29,455 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-04-04 07:56:59,466 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-04-04 07:56:59,466 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1  | 2023-04-04 07:56:29,364 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4. Trying to get from SCM.
recon_1  | 2023-04-04 07:56:29,416 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:55:59.480Z[UTC]] to Recon pipeline metadata.
dn3_1    | 2023-04-04 07:56:29,456 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1  | 2023-04-04 07:56:29,418 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:55:59.480Z[UTC]].
dn2_1    | 2023-04-04 07:56:59,466 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-04-04 07:54:20,987 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-04-04 07:55:59,282 [grpc-default-executor-2] INFO leader.FollowerInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E->e2957b23-687f-4626-af75-9b42f3a43b99: nextIndex: updateUnconditionally 53 -> 52
dn5_1    | 2023-04-04 07:55:59,287 [grpc-default-executor-2] INFO server.GrpcLogAppender: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E->c1077390-d65b-4523-9cd4-abe9e2c9eb94-AppendLogResponseHandler: follower responses appendEntries COMPLETED
recon_1  | 2023-04-04 07:56:29,419 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=f3dc0283-6172-4aac-8038-9f3ca91aaa35. Trying to get from SCM.
dn2_1    | 2023-04-04 07:56:59,467 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm1_1   | 2023-04-04 07:54:21,005 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=4, lastRpcResponseTime=15940) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:56:29,456 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-04-04 07:55:59,287 [grpc-default-executor-2] INFO leader.FollowerInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E->c1077390-d65b-4523-9cd4-abe9e2c9eb94: nextIndex: updateUnconditionally 53 -> 52
recon_1  | 2023-04-04 07:56:29,429 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: f3dc0283-6172-4aac-8038-9f3ca91aaa35, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:c1077390-d65b-4523-9cd4-abe9e2c9eb94, CreationTimestamp2023-04-04T07:55:59.519Z[UTC]] to Recon pipeline metadata.
dn2_1    | 2023-04-04 07:56:59,473 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-04-04 07:54:21,030 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=28, lastRpcResponseTime=15964) yet, just keep nextIndex unchanged and retry.
scm1_1   | 2023-04-04 07:54:22,265 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-04-04 07:55:59,287 [grpc-default-executor-2] INFO server.GrpcLogAppender: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E->c1077390-d65b-4523-9cd4-abe9e2c9eb94-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn5_1    | 2023-04-04 07:55:59,288 [grpc-default-executor-2] INFO leader.FollowerInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E->c1077390-d65b-4523-9cd4-abe9e2c9eb94: nextIndex: updateUnconditionally 52 -> 51
dn2_1    | 2023-04-04 07:56:59,505 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-04-04 07:54:22,294 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:22,295 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1294, lastRpcResponseTime=17229) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:56:29,456 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df
dn3_1    | 2023-04-04 07:56:29,456 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-04-04 07:55:59,294 [grpc-default-executor-4] INFO server.GrpcLogAppender: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E->e2957b23-687f-4626-af75-9b42f3a43b99-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm1_1   | 2023-04-04 07:54:22,303 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1301, lastRpcResponseTime=17237) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:56:29,456 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-04-04 07:56:29,456 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-04-04 07:55:59,294 [grpc-default-executor-4] INFO leader.FollowerInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E->e2957b23-687f-4626-af75-9b42f3a43b99: nextIndex: updateUnconditionally 52 -> 51
scm1_1   | 2023-04-04 07:54:22,316 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-04-04 07:56:29,456 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-04-04 07:56:29,456 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-04-04 07:55:59,296 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-EEB13EAC206E: Taking a snapshot at:(t:19, i:52) file /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/sm/snapshot.19_52
scm1_1   | 2023-04-04 07:54:22,325 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1, lastRpcResponseTime=17260) yet, just keep nextIndex unchanged and retry.
recon_1  | 2023-04-04 07:56:29,445 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f3dc0283-6172-4aac-8038-9f3ca91aaa35, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:c1077390-d65b-4523-9cd4-abe9e2c9eb94, CreationTimestamp2023-04-04T07:55:59.519Z[UTC]].
recon_1  | 2023-04-04 07:56:29,497 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=8037e277-427a-4034-9f8a-4e27919647df. Trying to get from SCM.
dn3_1    | 2023-04-04 07:56:29,456 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-04-04 07:55:59,296 [Command processor thread] INFO impl.StateMachineUpdater: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-StateMachineUpdater: set stopIndex = 52
scm1_1   | 2023-04-04 07:54:22,318 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-04-04 07:56:29,456 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-04-04 07:56:29,457 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1  | 2023-04-04 07:56:29,510 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 8037e277-427a-4034-9f8a-4e27919647df, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.502Z[UTC]] to Recon pipeline metadata.
scm1_1   | 2023-04-04 07:54:22,364 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=39, lastRpcResponseTime=17298) yet, just keep nextIndex unchanged and retry.
dn2_1    | 2023-04-04 07:56:59,507 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-04-04 07:55:59,306 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-EEB13EAC206E: Finished taking a snapshot at:(t:19, i:52) file:/data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e/sm/snapshot.19_52 took: 10 ms
dn3_1    | 2023-04-04 07:56:29,458 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
recon_1  | 2023-04-04 07:56:29,518 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8037e277-427a-4034-9f8a-4e27919647df, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.502Z[UTC]].
scm1_1   | 2023-04-04 07:54:23,578 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-04-04 07:56:59,507 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-04-04 07:56:29,458 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 2023-04-04 07:56:29,526 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8037e277-427a-4034-9f8a-4e27919647df reported by 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)
scm1_1   | 2023-04-04 07:54:23,579 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1, lastRpcResponseTime=18513) yet, just keep nextIndex unchanged and retry.
dn2_1    | 2023-04-04 07:56:59,507 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-04-04 07:56:29,988 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-c1077390-d65b-4523-9cd4-abe9e2c9eb94: Detected pause in JVM or host machine (eg GC): pause of approximately 230897712ns.
recon_1  | 2023-04-04 07:56:30,007 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8037e277-427a-4034-9f8a-4e27919647df reported by c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)
scm1_1   | 2023-04-04 07:54:23,578 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-04-04 07:56:59,509 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-04-04 07:55:59,307 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-StateMachineUpdater] INFO impl.StateMachineUpdater: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-StateMachineUpdater: Took a snapshot at index 52
dn3_1    | GC pool 'ParNew' had collection(s): count=1 time=528ms
scm1_1   | 2023-04-04 07:54:23,590 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=12, lastRpcResponseTime=18525) yet, just keep nextIndex unchanged and retry.
dn2_1    | 2023-04-04 07:56:59,515 [pool-26-thread-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF: start as a follower, conf=-1: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:55:59,307 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-StateMachineUpdater] INFO impl.StateMachineUpdater: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-StateMachineUpdater: snapshotIndex: updateIncreasingly 46 -> 52
dn5_1    | 2023-04-04 07:55:59,310 [Command processor thread] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: closes. applyIndex: 52
dn2_1    | 2023-04-04 07:56:59,517 [pool-26-thread-1] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-04-04 07:56:30,028 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-04-04 07:55:59,310 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn2_1    | 2023-04-04 07:56:59,517 [pool-26-thread-1] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-FollowerState
recon_1  | 2023-04-04 07:56:30,624 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8037e277-427a-4034-9f8a-4e27919647df reported by 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)
scm1_1   | 2023-04-04 07:54:24,837 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:24,838 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-04-04 07:56:59,532 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-588A1A0D07FF,id=e2957b23-687f-4626-af75-9b42f3a43b99
recon_1  | 2023-04-04 07:56:30,938 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3. Trying to get from SCM.
dn3_1    | 2023-04-04 07:56:30,052 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-04-04 07:56:30,054 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-04-04 07:55:59,312 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E-SegmentedRaftLogWorker close()
dn2_1    | 2023-04-04 07:56:59,534 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-04-04 07:56:59,534 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-04-04 07:56:59,534 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
recon_1  | 2023-04-04 07:56:30,941 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.531Z[UTC]] to Recon pipeline metadata.
scm1_1   | 2023-04-04 07:54:24,857 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=20, lastRpcResponseTime=19792) yet, just keep nextIndex unchanged and retry.
scm1_1   | 2023-04-04 07:54:24,863 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=25, lastRpcResponseTime=19797) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:56:30,043 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@741ac284] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0)], numOfContainers=2, numOfBlocks=3
dn2_1    | 2023-04-04 07:56:59,534 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1  | 2023-04-04 07:56:30,942 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.531Z[UTC]].
scm1_1   | 2023-04-04 07:54:26,102 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-04-04 07:55:59,331 [Command processor thread] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-EEB13EAC206E: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/a013e403-c1b0-4f4f-851c-eeb13eac206e
dn3_1    | 2023-04-04 07:56:30,059 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-04-04 07:56:59,540 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=a21f8cfe-4543-468b-bd60-588a1a0d07ff
recon_1  | 2023-04-04 07:56:30,942 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 reported by c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)
scm1_1   | 2023-04-04 07:54:26,110 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-04-04 07:55:59,333 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e command on datanode 2ebde02c-a404-41d0-92a4-7b6da490547a.
dn3_1    | 2023-04-04 07:56:30,061 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-04-04 07:56:30,065 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF: start as a follower, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-04-04 07:56:30,942 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8037e277-427a-4034-9f8a-4e27919647df reported by c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)
scm1_1   | 2023-04-04 07:54:26,144 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1307, lastRpcResponseTime=21079) yet, just keep nextIndex unchanged and retry.
dn5_1    | 2023-04-04 07:55:59,334 [Command processor thread] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: remove  FOLLOWER 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B:t15, leader=e2957b23-687f-4626-af75-9b42f3a43b99, voted=e2957b23-687f-4626-af75-9b42f3a43b99, raftlog=Memoized:2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLog:OPENED:c26, conf=21: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn2_1    | 2023-04-04 07:56:59,541 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=a21f8cfe-4543-468b-bd60-588a1a0d07ff.
recon_1  | 2023-04-04 07:56:31,461 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 reported by 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)
scm1_1   | 2023-04-04 07:54:26,158 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1321, lastRpcResponseTime=21093) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:56:30,066 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-04-04 07:55:59,334 [Command processor thread] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: shutdown
dn2_1    | 2023-04-04 07:56:59,543 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-04-04 07:56:31,462 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8037e277-427a-4034-9f8a-4e27919647df reported by 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)
scm1_1   | 2023-04-04 07:54:26,200 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-04-04 07:56:30,066 [pool-26-thread-1] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FollowerState
dn3_1    | 2023-04-04 07:56:30,069 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4E27919647DF,id=c1077390-d65b-4523-9cd4-abe9e2c9eb94
dn2_1    | 2023-04-04 07:56:59,543 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-04-04 07:56:31,583 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 reported by 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)
scm1_1   | 2023-04-04 07:54:26,206 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-04-04 07:55:59,334 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-0EBF5B06395B,id=2ebde02c-a404-41d0-92a4-7b6da490547a
scm1_1   | 2023-04-04 07:54:26,235 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1398, lastRpcResponseTime=21169) yet, just keep nextIndex unchanged and retry.
dn2_1    | 2023-04-04 07:57:04,621 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-FollowerState] INFO impl.FollowerState: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5103775807ns, electionTimeout:5078ms
scm1_1   | 2023-04-04 07:54:26,249 [grpc-default-executor-0] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=1412, lastRpcResponseTime=21184) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:56:30,069 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-04-04 07:55:59,334 [Command processor thread] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState
dn5_1    | 2023-04-04 07:55:59,334 [Command processor thread] INFO impl.StateMachineUpdater: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-StateMachineUpdater: set stopIndex = 26
dn2_1    | 2023-04-04 07:57:04,622 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-FollowerState] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: shutdown e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-FollowerState
dn2_1    | 2023-04-04 07:57:04,622 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-FollowerState] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-04-04 07:57:04,623 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-04-04 07:57:04,623 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-FollowerState] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4
dn2_1    | 2023-04-04 07:57:04,631 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:56:30,072 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-04-04 07:55:59,336 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState] INFO impl.FollowerState: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-FollowerState was interrupted
recon_1  | 2023-04-04 07:56:31,583 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8037e277-427a-4034-9f8a-4e27919647df reported by 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)
scm1_1   | 2023-04-04 07:54:26,298 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:26,300 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-04-04 07:54:26,302 [grpc-default-executor-4] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=36, lastRpcResponseTime=21236) yet, just keep nextIndex unchanged and retry.
dn2_1    | 2023-04-04 07:57:04,631 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4 PRE_VOTE round 0: result PASSED (term=0)
scm1_1   | 2023-04-04 07:54:26,308 [grpc-default-executor-3] WARN server.GrpcLogAppender: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64-GrpcLogAppender: Leader has not got in touch with Follower 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64(c-1,m0,n75, attendVote=true, lastRpcSendTime=43, lastRpcResponseTime=21242) yet, just keep nextIndex unchanged and retry.
dn3_1    | 2023-04-04 07:56:30,072 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-04-04 07:56:30,072 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-04-04 07:55:59,336 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-0EBF5B06395B: Taking a snapshot at:(t:15, i:26) file /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/sm/snapshot.15_26
scm1_1   | 2023-04-04 07:54:30,121 [grpc-default-executor-3] INFO leader.FollowerInfo: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64: nextIndex: updateUnconditionally 75 -> 73
scm1_1   | 2023-04-04 07:54:30,146 [grpc-default-executor-3] INFO leader.FollowerInfo: 861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7->e0c91ed2-48f2-4467-9ffd-e28e64336c64: nextIndex: updateUnconditionally 75 -> 73
dn2_1    | 2023-04-04 07:57:04,633 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:57:04,633 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO impl.LeaderElection: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4 ELECTION round 0: result PASSED (term=1)
recon_1  | 2023-04-04 07:56:34,614 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 reported by c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)
recon_1  | 2023-04-04 07:56:34,615 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8037e277-427a-4034-9f8a-4e27919647df reported by c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)
dn5_1    | 2023-04-04 07:55:59,338 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-0EBF5B06395B: Finished taking a snapshot at:(t:15, i:26) file:/data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b/sm/snapshot.15_26 took: 3 ms
dn2_1    | 2023-04-04 07:57:04,633 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: shutdown e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4
dn2_1    | 2023-04-04 07:57:04,633 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-04-04 07:57:04,633 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-588A1A0D07FF with new leaderId: e2957b23-687f-4626-af75-9b42f3a43b99
recon_1  | 2023-04-04 07:56:34,627 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 reported by 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)
dn2_1    | 2023-04-04 07:57:04,633 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF: change Leader from null to e2957b23-687f-4626-af75-9b42f3a43b99 at term 1 for becomeLeader, leader elected after 5214ms
dn2_1    | 2023-04-04 07:57:04,633 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-04-04 07:54:48,973 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm1_1   | 2023-04-04 07:54:57,355 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization started.
scm1_1   | 2023-04-04 07:54:57,462 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
scm1_1   | 2023-04-04 07:54:57,463 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
dn3_1    | 2023-04-04 07:56:30,076 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-04-04 07:57:04,634 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1  | 2023-04-04 07:56:34,628 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8037e277-427a-4034-9f8a-4e27919647df reported by 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)
scm1_1   | 2023-04-04 07:54:57,481 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: SCM Finalization has crossed checkpoint FINALIZATION_STARTED
dn5_1    | 2023-04-04 07:55:59,339 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-StateMachineUpdater] INFO impl.StateMachineUpdater: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-StateMachineUpdater: Took a snapshot at index 26
dn2_1    | 2023-04-04 07:57:04,634 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-04-04 07:56:30,084 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-04-04 07:56:30,101 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=8037e277-427a-4034-9f8a-4e27919647df
scm1_1   | 2023-04-04 07:54:57,566 [IPC Server handler 14 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 84e9f90e-0ab8-4731-82e1-fb0ad827e6e2, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:b7b7229a-e604-4787-8543-e6d8cdebe63b, CreationTimestamp2023-04-04T07:53:56.547874Z[UTC]] moved to CLOSED state
dn2_1    | 2023-04-04 07:57:04,634 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-04-04 07:56:30,920 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=8037e277-427a-4034-9f8a-4e27919647df.
dn3_1    | 2023-04-04 07:56:30,922 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94: new RaftServerImpl for group-1D7D5DE977D3:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm1_1   | 2023-04-04 07:54:57,629 [IPC Server handler 14 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1 closed for pipeline=PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e
dn5_1    | 2023-04-04 07:55:59,339 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-StateMachineUpdater] INFO impl.StateMachineUpdater: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-StateMachineUpdater: snapshotIndex: updateIncreasingly 20 -> 26
dn2_1    | 2023-04-04 07:57:04,635 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   | 2023-04-04 07:54:57,635 [IPC Server handler 14 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1001 closed for pipeline=PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e
dn5_1    | 2023-04-04 07:55:59,340 [Command processor thread] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: closes. applyIndex: 26
dn5_1    | 2023-04-04 07:55:59,341 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
recon_1  | 2023-04-04 07:56:35,947 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 reported by 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)
dn2_1    | 2023-04-04 07:57:04,635 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-04-04 07:57:04,635 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-04-04 07:54:57,635 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1, current state: CLOSING
dn2_1    | 2023-04-04 07:57:04,635 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1_1   | 2023-04-04 07:54:57,665 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1001, current state: CLOSING
recon_1  | 2023-04-04 07:56:35,947 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8037e277-427a-4034-9f8a-4e27919647df reported by 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)
recon_1  | 2023-04-04 07:56:35,950 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8037e277-427a-4034-9f8a-4e27919647df, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:56:20.502Z[UTC]] moved to OPEN state
recon_1  | 2023-04-04 07:56:36,704 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 reported by 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)
scm1_1   | 2023-04-04 07:54:57,670 [IPC Server handler 14 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a013e403-c1b0-4f4f-851c-eeb13eac206e, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:53:56.549531Z[UTC]] moved to CLOSED state
scm1_1   | 2023-04-04 07:54:57,680 [IPC Server handler 14 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d0f51999-1853-4262-b31a-8b1b4f9c62a2, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:c1077390-d65b-4523-9cd4-abe9e2c9eb94, CreationTimestamp2023-04-04T07:53:56.549746Z[UTC]] moved to CLOSED state
scm1_1   | 2023-04-04 07:54:57,702 [IPC Server handler 14 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 685cb9fd-d40d-4ecb-9ff2-6617081ae410, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:53:56.547713Z[UTC]] moved to CLOSED state
recon_1  | 2023-04-04 07:56:36,705 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:56:20.531Z[UTC]] moved to OPEN state
dn2_1    | 2023-04-04 07:57:04,636 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO impl.RoleInfo: e2957b23-687f-4626-af75-9b42f3a43b99: start e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderStateImpl
dn2_1    | 2023-04-04 07:57:04,636 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-04-04 07:57:04,640 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a21f8cfe-4543-468b-bd60-588a1a0d07ff/current/log_inprogress_0
dn3_1    | 2023-04-04 07:56:30,922 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-04-04 07:54:57,741 [IPC Server handler 14 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 0a9acdcd-53b0-4fd8-86de-f3d51d6ec210, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:e2957b23-687f-4626-af75-9b42f3a43b99, CreationTimestamp2023-04-04T07:53:56.518423Z[UTC]] moved to CLOSED state
recon_1  | 2023-04-04 07:56:51,062 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=2b2f258a-8c66-4c27-9e64-eba4a65dc133. Trying to get from SCM.
dn2_1    | 2023-04-04 07:57:04,669 [e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF-LeaderElection4] INFO server.RaftServer$Division: e2957b23-687f-4626-af75-9b42f3a43b99@group-588A1A0D07FF: set configuration 0: peers:[e2957b23-687f-4626-af75-9b42f3a43b99|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-04-04 07:57:17,189 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-04-04 07:56:30,922 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-04-04 07:56:30,923 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-04-04 07:55:59,341 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B-SegmentedRaftLogWorker close()
recon_1  | 2023-04-04 07:56:51,077 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 2b2f258a-8c66-4c27-9e64-eba4a65dc133, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:56:20.479Z[UTC]] to Recon pipeline metadata.
scm1_1   | 2023-04-04 07:54:57,774 [IPC Server handler 14 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #2 closed for pipeline=PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b
dn3_1    | 2023-04-04 07:56:30,923 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-04-04 07:57:32,410 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@4ef4f627] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(1),2(1)], numOfContainers=2, numOfBlocks=3
dn5_1    | 2023-04-04 07:55:59,345 [Command processor thread] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-0EBF5B06395B: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/3a70a706-d27b-47ff-9aed-0ebf5b06395b
dn5_1    | 2023-04-04 07:55:59,345 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b command on datanode 2ebde02c-a404-41d0-92a4-7b6da490547a.
dn3_1    | 2023-04-04 07:56:30,923 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-04-04 07:58:06,456 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@4ef4f627] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(2),2(2)], numOfContainers=2, numOfBlocks=3
dn5_1    | 2023-04-04 07:55:59,346 [Command processor thread] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: remove    LEADER 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6:t4, leader=2ebde02c-a404-41d0-92a4-7b6da490547a, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLog:OPENED:c6, conf=5: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn5_1    | 2023-04-04 07:55:59,346 [Command processor thread] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: shutdown
dn3_1    | 2023-04-04 07:56:30,923 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-04-04 07:58:06,456 [DeleteBlocksCommandHandlerThread-2] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1 <= 1
dn5_1    | 2023-04-04 07:55:59,346 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-65BC10982AC6,id=2ebde02c-a404-41d0-92a4-7b6da490547a
dn5_1    | 2023-04-04 07:55:59,346 [Command processor thread] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-LeaderStateImpl
dn3_1    | 2023-04-04 07:56:30,923 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3: ConfigurationManager, init=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-04-04 07:58:06,458 [DeleteBlocksCommandHandlerThread-3] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 2 <= 2
dn5_1    | 2023-04-04 07:55:59,346 [Command processor thread] INFO impl.PendingRequests: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-PendingRequests: sendNotLeaderResponses
dn5_1    | 2023-04-04 07:55:59,348 [Command processor thread] INFO impl.StateMachineUpdater: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-StateMachineUpdater: set stopIndex = 6
dn3_1    | 2023-04-04 07:56:30,923 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-04-04 07:58:17,191 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 6/4994 blocks from 2 candidate containers.
dn5_1    | 2023-04-04 07:55:59,348 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-65BC10982AC6: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6/sm/snapshot.4_6
recon_1  | 2023-04-04 07:56:51,077 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 2b2f258a-8c66-4c27-9e64-eba4a65dc133, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:56:20.479Z[UTC]].
dn3_1    | 2023-04-04 07:56:30,923 [Command processor thread] INFO server.RaftServer: c1077390-d65b-4523-9cd4-abe9e2c9eb94: addNew group-1D7D5DE977D3:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-1D7D5DE977D3:java.util.concurrent.CompletableFuture@7907a0d[Not completed]
dn2_1    | 2023-04-04 07:58:17,201 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/containerDir0/2/chunks/111677748019200003.block
dn5_1    | 2023-04-04 07:55:59,349 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-65BC10982AC6: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6/sm/snapshot.4_6 took: 2 ms
dn5_1    | 2023-04-04 07:55:59,350 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-StateMachineUpdater] INFO impl.StateMachineUpdater: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-StateMachineUpdater: Took a snapshot at index 6
dn3_1    | 2023-04-04 07:56:30,924 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-04-04 07:58:17,202 [BlockDeletingService#1] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/containerDir0/1/chunks/111677748019200001.block
recon_1  | 2023-04-04 07:56:51,655 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=6d92c706-b857-41ae-a811-0fceb345a748. Trying to get from SCM.
recon_1  | 2023-04-04 07:56:51,663 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 6d92c706-b857-41ae-a811-0fceb345a748, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:23.479Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-04-04 07:56:51,664 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6d92c706-b857-41ae-a811-0fceb345a748, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:23.479Z[UTC]].
recon_1  | 2023-04-04 07:56:51,665 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=6d92c706-b857-41ae-a811-0fceb345a748 reported by b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17)
scm1_1   | 2023-04-04 07:54:57,775 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2, current state: CLOSING
scm1_1   | 2023-04-04 07:54:57,819 [IPC Server handler 14 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1002 closed for pipeline=PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b
recon_1  | 2023-04-04 07:56:51,665 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6d92c706-b857-41ae-a811-0fceb345a748, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b7b7229a-e604-4787-8543-e6d8cdebe63b, CreationTimestamp2023-04-04T07:56:23.479Z[UTC]] moved to OPEN state
dn3_1    | 2023-04-04 07:56:30,924 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-04-04 07:56:30,925 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
recon_1  | 2023-04-04 07:56:59,458 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=a21f8cfe-4543-468b-bd60-588a1a0d07ff. Trying to get from SCM.
recon_1  | 2023-04-04 07:56:59,488 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: a21f8cfe-4543-468b-bd60-588a1a0d07ff, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:29.481Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-04-04 07:56:59,488 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a21f8cfe-4543-468b-bd60-588a1a0d07ff, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:29.481Z[UTC]].
dn5_1    | 2023-04-04 07:55:59,350 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-StateMachineUpdater] INFO impl.StateMachineUpdater: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn2_1    | 2023-04-04 07:58:17,207 [BlockDeletingService#1] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/containerDir0/1/chunks/111677748019200002.block
dn3_1    | 2023-04-04 07:56:30,925 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm1_1   | 2023-04-04 07:54:57,821 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1002, current state: CLOSING
recon_1  | 2023-04-04 07:56:59,490 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=a21f8cfe-4543-468b-bd60-588a1a0d07ff reported by e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18)
dn5_1    | 2023-04-04 07:55:59,350 [Command processor thread] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: closes. applyIndex: 6
dn3_1    | 2023-04-04 07:56:30,925 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-04-04 07:54:57,862 [IPC Server handler 14 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3a70a706-d27b-47ff-9aed-0ebf5b06395b, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:e2957b23-687f-4626-af75-9b42f3a43b99, CreationTimestamp2023-04-04T07:53:56.547504Z[UTC]] moved to CLOSED state
recon_1  | 2023-04-04 07:56:59,492 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a21f8cfe-4543-468b-bd60-588a1a0d07ff, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e2957b23-687f-4626-af75-9b42f3a43b99, CreationTimestamp2023-04-04T07:56:29.481Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:55:59,350 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn5_1    | 2023-04-04 07:55:59,351 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6-SegmentedRaftLogWorker close()
scm1_1   | 2023-04-04 07:54:57,889 [IPC Server handler 14 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 35a434de-4469-4302-9ebd-65bc10982ac6, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:53:56.545981Z[UTC]] moved to CLOSED state
recon_1  | 2023-04-04 07:56:59,846 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn5_1    | 2023-04-04 07:55:59,353 [Command processor thread] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-65BC10982AC6: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/35a434de-4469-4302-9ebd-65bc10982ac6
dn3_1    | 2023-04-04 07:56:30,926 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-04-04 07:54:57,889 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer:   Existing pipelines and containers will be closed during Upgrade.
dn3_1    | 2023-04-04 07:56:30,926 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-04-04 07:55:59,353 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=35a434de-4469-4302-9ebd-65bc10982ac6 command on datanode 2ebde02c-a404-41d0-92a4-7b6da490547a.
recon_1  | 2023-04-04 07:56:59,846 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
scm1_1   |   New pipelines creation will remain frozen until Upgrade is finalized.
dn3_1    | 2023-04-04 07:56:30,927 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-04-04 07:56:17,717 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 2023-04-04 07:56:59,847 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 155 
scm1_1   | 2023-04-04 07:54:57,902 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn3_1    | 2023-04-04 07:56:30,927 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-04-04 07:56:29,304 [Command processor thread] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: addNew group-5D3D419BA0D4:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] returns group-5D3D419BA0D4:java.util.concurrent.CompletableFuture@c4e724d[Not completed]
recon_1  | 2023-04-04 07:56:59,853 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 2, SequenceNumber diff: 3, SequenceNumber Lag from OM 0.
scm1_1   | 2023-04-04 07:54:57,907 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn3_1    | 2023-04-04 07:56:30,927 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-04-04 07:56:29,310 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a: new RaftServerImpl for group-5D3D419BA0D4:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
recon_1  | 2023-04-04 07:56:59,853 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 3 records
scm1_1   | 2023-04-04 07:54:57,927 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn3_1    | 2023-04-04 07:56:30,927 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 does not exist. Creating ...
dn5_1    | 2023-04-04 07:56:29,312 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
recon_1  | 2023-04-04 07:56:59,856 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-04-04 07:56:59,856 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
dn3_1    | 2023-04-04 07:56:30,929 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3/in_use.lock acquired by nodename 8@d6d8a15a26f6
dn5_1    | 2023-04-04 07:56:29,312 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 2023-04-04 07:57:00,138 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
scm1_1   | 2023-04-04 07:54:57,928 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn3_1    | 2023-04-04 07:56:30,931 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 has been successfully formatted.
dn5_1    | 2023-04-04 07:56:29,312 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1  | 2023-04-04 07:57:00,139 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
scm1_1   | 2023-04-04 07:54:57,929 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn3_1    | 2023-04-04 07:56:30,939 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-1D7D5DE977D3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-04-04 07:56:30,939 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | 2023-04-04 07:54:57,945 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
dn3_1    | 2023-04-04 07:56:30,939 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-04-04 07:56:29,312 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
recon_1  | 2023-04-04 07:57:00,139 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
scm1_1   | 2023-04-04 07:54:57,947 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY READONLY state.
dn3_1    | 2023-04-04 07:56:30,943 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-04-04 07:56:29,312 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-04-04 07:56:29,312 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1   | 2023-04-04 07:54:57,951 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=685cb9fd-d40d-4ecb-9ff2-6617081ae410 in state CLOSED which uses HEALTHY_READONLY datanode 14df8ad4-ce2f-4483-aada-29ab5cae3ffb. This will send close commands for its containers.
recon_1  | 2023-04-04 07:57:52,929 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2001 got from ha_dn3_1.ha_net.
dn3_1    | 2023-04-04 07:56:30,944 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-04-04 07:54:57,951 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
scm1_1   | 2023-04-04 07:54:57,951 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=84e9f90e-0ab8-4731-82e1-fb0ad827e6e2 in state CLOSED which uses HEALTHY_READONLY datanode b7b7229a-e604-4787-8543-e6d8cdebe63b. This will send close commands for its containers.
dn5_1    | 2023-04-04 07:56:29,313 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4: ConfigurationManager, init=-1: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-04-04 07:56:30,944 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1  | 2023-04-04 07:57:53,009 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #2001 to Recon.
scm1_1   | 2023-04-04 07:54:57,952 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
dn5_1    | 2023-04-04 07:56:29,313 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-04-04 07:56:30,945 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm1_1   | 2023-04-04 07:54:57,953 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e in state CLOSED which uses HEALTHY_READONLY datanode e2957b23-687f-4626-af75-9b42f3a43b99. This will send close commands for its containers.
scm1_1   | 2023-04-04 07:54:57,954 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=0a9acdcd-53b0-4fd8-86de-f3d51d6ec210 in state CLOSED which uses HEALTHY_READONLY datanode e2957b23-687f-4626-af75-9b42f3a43b99. This will send close commands for its containers.
recon_1  | 2023-04-04 07:58:00,156 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn5_1    | 2023-04-04 07:56:29,314 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-04-04 07:56:30,945 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1  | 2023-04-04 07:58:00,157 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
dn5_1    | 2023-04-04 07:56:29,315 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-04-04 07:54:57,954 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b in state CLOSED which uses HEALTHY_READONLY datanode e2957b23-687f-4626-af75-9b42f3a43b99. This will send close commands for its containers.
dn3_1    | 2023-04-04 07:56:30,945 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
recon_1  | 2023-04-04 07:58:00,157 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 158 
dn5_1    | 2023-04-04 07:56:29,315 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm1_1   | 2023-04-04 07:54:57,955 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY READONLY state.
dn3_1    | 2023-04-04 07:56:30,945 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3
recon_1  | 2023-04-04 07:58:00,176 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 6, SequenceNumber diff: 17, SequenceNumber Lag from OM 0.
dn5_1    | 2023-04-04 07:56:29,315 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm1_1   | 2023-04-04 07:54:57,958 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e in state CLOSED which uses HEALTHY_READONLY datanode 2ebde02c-a404-41d0-92a4-7b6da490547a. This will send close commands for its containers.
recon_1  | 2023-04-04 07:58:00,176 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 17 records
dn5_1    | 2023-04-04 07:56:29,316 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-04-04 07:54:57,958 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b in state CLOSED which uses HEALTHY_READONLY datanode 2ebde02c-a404-41d0-92a4-7b6da490547a. This will send close commands for its containers.
dn3_1    | 2023-04-04 07:56:30,946 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1  | 2023-04-04 07:58:00,196 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
dn5_1    | 2023-04-04 07:56:29,319 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-04-04 07:54:57,958 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=35a434de-4469-4302-9ebd-65bc10982ac6 in state CLOSED which uses HEALTHY_READONLY datanode 2ebde02c-a404-41d0-92a4-7b6da490547a. This will send close commands for its containers.
dn3_1    | 2023-04-04 07:56:30,946 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
recon_1  | 2023-04-04 07:58:00,197 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
dn5_1    | 2023-04-04 07:56:29,321 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1   | 2023-04-04 07:54:57,959 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
dn3_1    | 2023-04-04 07:56:30,946 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-04-04 07:56:29,321 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-04-04 07:54:57,960 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e in state CLOSED which uses HEALTHY_READONLY datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94. This will send close commands for its containers.
dn3_1    | 2023-04-04 07:56:30,946 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-04-04 07:56:30,946 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-04-04 07:56:29,321 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-04-04 07:54:57,963 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: SCM Finalization has crossed checkpoint MLV_EQUALS_SLV
dn3_1    | 2023-04-04 07:56:30,946 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1  | 2023-04-04 07:58:00,377 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
dn5_1    | 2023-04-04 07:56:29,321 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-04-04 07:56:30,946 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-04-04 07:54:57,964 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
recon_1  | 2023-04-04 07:58:00,394 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 1 OM DB update event(s).
dn3_1    | 2023-04-04 07:56:30,946 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-04-04 07:56:29,323 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4 does not exist. Creating ...
scm1_1   | 2023-04-04 07:54:57,971 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
recon_1  | 2023-04-04 07:58:00,481 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
dn3_1    | 2023-04-04 07:56:30,947 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-04-04 07:56:30,950 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-04-04 07:54:57,972 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d0f51999-1853-4262-b31a-8b1b4f9c62a2 in state CLOSED which uses HEALTHY_READONLY datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94. This will send close commands for its containers.
recon_1  | 2023-04-04 07:58:06,716 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2002 got from ha_dn4_1.ha_net.
scm1_1   | 2023-04-04 07:54:57,972 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b in state CLOSED which uses HEALTHY_READONLY datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94. This will send close commands for its containers.
dn5_1    | 2023-04-04 07:56:29,328 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4/in_use.lock acquired by nodename 7@7e8229df9409
dn3_1    | 2023-04-04 07:56:31,310 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-c1077390-d65b-4523-9cd4-abe9e2c9eb94: Detected pause in JVM or host machine (eg GC): pause of approximately 314296862ns.
recon_1  | 2023-04-04 07:58:06,731 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #2002 to Recon.
scm1_1   | 2023-04-04 07:55:02,964 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:29,334 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4 has been successfully formatted.
dn3_1    | GC pool 'ParNew' had collection(s): count=1 time=359ms
scm1_1   | 2023-04-04 07:55:07,964 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:29,337 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-5D3D419BA0D4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-04-04 07:56:31,336 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-04-04 07:56:31,336 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-04-04 07:56:31,337 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm1_1   | 2023-04-04 07:55:12,964 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:29,337 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-04-04 07:56:31,337 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-04-04 07:55:17,964 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:29,337 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-04-04 07:56:31,338 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-04-04 07:55:19,956 [IPC Server handler 39 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,337 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-04-04 07:56:31,346 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3: start as a follower, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:55:20,552 [IPC Server handler 25 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 2023-04-04 07:55:22,664 [IPC Server handler 32 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn3_1    | 2023-04-04 07:56:31,347 [pool-26-thread-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 2023-04-04 07:55:22,965 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:29,338 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-04-04 07:56:29,338 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-04-04 07:55:23,666 [IPC Server handler 32 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn3_1    | 2023-04-04 07:56:31,347 [pool-26-thread-1] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState
dn5_1    | 2023-04-04 07:56:29,340 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm1_1   | 2023-04-04 07:55:23,741 [IPC Server handler 28 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn3_1    | 2023-04-04 07:56:31,354 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1D7D5DE977D3,id=c1077390-d65b-4523-9cd4-abe9e2c9eb94
dn5_1    | 2023-04-04 07:56:29,342 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-04-04 07:56:31,355 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-04-04 07:55:23,770 [IPC Server handler 95 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,344 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-04-04 07:56:31,355 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-04-04 07:55:23,788 [IPC Server handler 98 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,344 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4
dn3_1    | 2023-04-04 07:56:31,355 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 2023-04-04 07:55:26,159 [IPC Server handler 70 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,344 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-04-04 07:56:31,355 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-04-04 07:55:26,497 [IPC Server handler 80 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,370 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-04-04 07:56:31,356 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-04-04 07:55:27,147 [IPC Server handler 70 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,371 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-04-04 07:56:31,356 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3
scm1_1   | 2023-04-04 07:55:27,225 [IPC Server handler 63 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn3_1    | 2023-04-04 07:56:31,374 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-04-04 07:55:27,252 [IPC Server handler 62 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 2023-04-04 07:55:27,417 [IPC Server handler 78 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn3_1    | 2023-04-04 07:56:31,750 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3.
dn5_1    | 2023-04-04 07:56:29,373 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm1_1   | 2023-04-04 07:55:27,417 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) reported CLOSED replica.
dn3_1    | 2023-04-04 07:56:34,603 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-FollowerState] INFO impl.FollowerState: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5204979015ns, electionTimeout:5154ms
dn5_1    | 2023-04-04 07:56:29,373 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-04-04 07:56:34,603 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-FollowerState] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-FollowerState
dn3_1    | 2023-04-04 07:56:34,603 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-FollowerState] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-04-04 07:56:29,373 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-04-04 07:55:27,520 [IPC Server handler 82 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn3_1    | 2023-04-04 07:56:34,603 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-04-04 07:56:34,603 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-FollowerState] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4
scm1_1   | 2023-04-04 07:55:27,637 [IPC Server handler 32 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,374 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-04-04 07:56:34,604 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:55:27,749 [IPC Server handler 28 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,374 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-04-04 07:56:34,604 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4 PRE_VOTE round 0: result PASSED (term=0)
scm1_1   | 2023-04-04 07:55:27,752 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) reported CLOSED replica.
dn5_1    | 2023-04-04 07:56:29,377 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm1_1   | 2023-04-04 07:55:27,770 [IPC Server handler 95 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn3_1    | 2023-04-04 07:56:34,606 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:55:27,834 [IPC Server handler 97 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,379 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-04-04 07:56:34,606 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4 ELECTION round 0: result PASSED (term=1)
scm1_1   | 2023-04-04 07:55:27,866 [IPC Server handler 83 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,396 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-04-04 07:56:34,606 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4
scm1_1   | 2023-04-04 07:55:27,907 [IPC Server handler 39 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,409 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-04-04 07:56:34,606 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1_1   | 2023-04-04 07:55:27,965 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:29,409 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-04-04 07:56:29,412 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-04-04 07:55:27,973 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
scm1_1   | 2023-04-04 07:55:28,127 [IPC Server handler 70 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,412 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-04-04 07:56:29,423 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4: start as a follower, conf=-1: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:55:28,128 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21) reported CLOSED replica.
dn3_1    | 2023-04-04 07:56:34,609 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9F3CA91AAA35 with new leaderId: c1077390-d65b-4523-9cd4-abe9e2c9eb94
dn5_1    | 2023-04-04 07:56:29,423 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-04-04 07:56:29,424 [pool-26-thread-1] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-FollowerState
scm1_1   | 2023-04-04 07:55:28,171 [IPC Server handler 63 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 2023-04-04 07:55:28,203 [IPC Server handler 62 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 2023-04-04 07:55:28,212 [IPC Server handler 69 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,425 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5D3D419BA0D4,id=2ebde02c-a404-41d0-92a4-7b6da490547a
dn5_1    | 2023-04-04 07:56:29,426 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-04-04 07:55:28,297 [IPC Server handler 71 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn3_1    | 2023-04-04 07:56:34,609 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35: change Leader from null to c1077390-d65b-4523-9cd4-abe9e2c9eb94 at term 1 for becomeLeader, leader elected after 5281ms
dn5_1    | 2023-04-04 07:56:29,426 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-04-04 07:56:29,426 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-04-04 07:56:34,609 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-04-04 07:56:34,609 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-04-04 07:56:29,426 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-04-04 07:56:29,436 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4
scm1_1   | 2023-04-04 07:55:28,299 [FixedThreadPoolWithAffinityExecutor-8-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19) reported CLOSED replica.
scm1_1   | 2023-04-04 07:55:28,332 [IPC Server handler 76 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,436 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4.
dn3_1    | 2023-04-04 07:56:34,609 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-04-04 07:56:34,610 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-04-04 07:56:29,437 [Command processor thread] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: addNew group-4E27919647DF:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns group-4E27919647DF:java.util.concurrent.CompletableFuture@69ca8cd2[Not completed]
dn5_1    | 2023-04-04 07:56:29,441 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a: new RaftServerImpl for group-4E27919647DF:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-04-04 07:56:29,445 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-04-04 07:56:34,610 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-04-04 07:56:34,610 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-04-04 07:56:29,445 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-04-04 07:56:29,446 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-04-04 07:56:34,610 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-04-04 07:55:28,360 [IPC Server handler 79 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,446 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-04-04 07:56:29,446 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-04-04 07:56:29,446 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-04-04 07:56:34,610 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1_1   | 2023-04-04 07:55:32,404 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for delTxnId, expected lastId is 0, actual lastId is 1000.
dn5_1    | 2023-04-04 07:56:29,446 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-04-04 07:56:29,446 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF: ConfigurationManager, init=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-04-04 07:56:29,449 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm1_1   | 2023-04-04 07:55:32,439 [IPC Server handler 6 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 1000 to 2000.
scm1_1   | 2023-04-04 07:55:32,966 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:29,450 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-04-04 07:56:29,450 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-04-04 07:56:29,450 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-04-04 07:56:29,450 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-04-04 07:56:34,610 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderStateImpl
dn3_1    | 2023-04-04 07:56:34,610 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-04-04 07:56:29,450 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-04-04 07:56:29,452 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-04-04 07:56:34,619 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/f3dc0283-6172-4aac-8038-9f3ca91aaa35/current/log_inprogress_0
dn3_1    | 2023-04-04 07:56:34,637 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35-LeaderElection4] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-9F3CA91AAA35: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:56:29,469 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-04-04 07:56:29,469 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-04-04 07:56:29,469 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-04-04 07:55:37,966 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-04-04 07:55:42,966 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:29,469 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-04-04 07:56:29,469 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 2023-04-04 07:55:47,967 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn3_1    | 2023-04-04 07:56:34,981 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF: receive requestVote(PRE_VOTE, 2ebde02c-a404-41d0-92a4-7b6da490547a, group-4E27919647DF, 0, (t:0, i:0))
dn5_1    | 2023-04-04 07:56:29,469 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df does not exist. Creating ...
dn5_1    | 2023-04-04 07:56:29,471 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df/in_use.lock acquired by nodename 7@7e8229df9409
dn3_1    | 2023-04-04 07:56:34,981 [grpc-default-executor-1] INFO impl.VoteContext: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FOLLOWER: accept PRE_VOTE from 2ebde02c-a404-41d0-92a4-7b6da490547a: our priority 0 <= candidate's priority 0
dn3_1    | 2023-04-04 07:56:34,981 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF replies to PRE_VOTE vote request: 2ebde02c-a404-41d0-92a4-7b6da490547a<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t0. Peer's state: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF:t0, leader=null, voted=, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:56:29,474 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df has been successfully formatted.
dn5_1    | 2023-04-04 07:56:29,476 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-4E27919647DF: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-04-04 07:56:29,476 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-04-04 07:56:29,476 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-04-04 07:55:49,950 [IPC Server handler 40 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 2023-04-04 07:55:50,532 [IPC Server handler 25 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
dn5_1    | 2023-04-04 07:56:29,476 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-04-04 07:56:29,476 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-04-04 07:56:35,254 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-04-04 07:56:35,254 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-04-04 07:56:29,476 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-04-04 07:56:29,477 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-04-04 07:56:35,858 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF: receive requestVote(PRE_VOTE, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb, group-4E27919647DF, 0, (t:0, i:0))
dn3_1    | 2023-04-04 07:56:35,858 [grpc-default-executor-1] INFO impl.VoteContext: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FOLLOWER: accept PRE_VOTE from 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: our priority 0 <= candidate's priority 1
dn5_1    | 2023-04-04 07:56:29,513 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-04-04 07:56:29,514 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-04-04 07:56:29,514 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df
scm1_1   | 2023-04-04 07:55:52,967 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-04-04 07:55:56,608 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=84e9f90e-0ab8-4731-82e1-fb0ad827e6e2 since it stays at CLOSED stage.
dn5_1    | 2023-04-04 07:56:29,514 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-04-04 07:56:29,516 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm1_1   | 2023-04-04 07:55:56,608 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=84e9f90e-0ab8-4731-82e1-fb0ad827e6e2 close command to datanode b7b7229a-e604-4787-8543-e6d8cdebe63b
scm1_1   | 2023-04-04 07:55:56,621 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 84e9f90e-0ab8-4731-82e1-fb0ad827e6e2, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:b7b7229a-e604-4787-8543-e6d8cdebe63b, CreationTimestamp2023-04-04T07:53:56.547874Z[UTC]] removed.
dn5_1    | 2023-04-04 07:56:29,516 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-04-04 07:56:29,516 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-04-04 07:56:35,858 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF replies to PRE_VOTE vote request: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t0. Peer's state: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF:t0, leader=null, voted=, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:56:35,887 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF: receive requestVote(ELECTION, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb, group-4E27919647DF, 1, (t:0, i:0))
dn5_1    | 2023-04-04 07:56:29,516 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-04-04 07:55:56,624 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e since it stays at CLOSED stage.
scm1_1   | 2023-04-04 07:55:56,625 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e close command to datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm1_1   | 2023-04-04 07:55:56,626 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e close command to datanode 2ebde02c-a404-41d0-92a4-7b6da490547a
dn5_1    | 2023-04-04 07:56:29,517 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-04-04 07:56:29,517 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-04-04 07:56:35,887 [grpc-default-executor-1] INFO impl.VoteContext: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FOLLOWER: accept ELECTION from 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: our priority 0 <= candidate's priority 1
dn3_1    | 2023-04-04 07:56:35,887 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn5_1    | 2023-04-04 07:56:29,517 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-04-04 07:56:29,517 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-04-04 07:56:35,888 [grpc-default-executor-1] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FollowerState
dn3_1    | 2023-04-04 07:56:35,888 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FollowerState] INFO impl.FollowerState: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FollowerState was interrupted
dn5_1    | 2023-04-04 07:56:29,519 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-04-04 07:56:29,675 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-04-04 07:56:29,675 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-04-04 07:55:56,628 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e close command to datanode e2957b23-687f-4626-af75-9b42f3a43b99
scm1_1   | 2023-04-04 07:55:56,638 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: a013e403-c1b0-4f4f-851c-eeb13eac206e, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:53:56.549531Z[UTC]] removed.
dn5_1    | 2023-04-04 07:56:29,675 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-04-04 07:56:35,888 [grpc-default-executor-1] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FollowerState
scm1_1   | 2023-04-04 07:55:56,640 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=d0f51999-1853-4262-b31a-8b1b4f9c62a2 since it stays at CLOSED stage.
scm1_1   | 2023-04-04 07:55:56,642 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=d0f51999-1853-4262-b31a-8b1b4f9c62a2 close command to datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm1_1   | 2023-04-04 07:55:56,652 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d0f51999-1853-4262-b31a-8b1b4f9c62a2, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:c1077390-d65b-4523-9cd4-abe9e2c9eb94, CreationTimestamp2023-04-04T07:53:56.549746Z[UTC]] removed.
scm1_1   | 2023-04-04 07:55:56,656 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=685cb9fd-d40d-4ecb-9ff2-6617081ae410 since it stays at CLOSED stage.
dn3_1    | 2023-04-04 07:56:35,890 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF replies to ELECTION vote request: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t1. Peer's state: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF:t1, leader=null, voted=14df8ad4-ce2f-4483-aada-29ab5cae3ffb, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:55:56,659 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=685cb9fd-d40d-4ecb-9ff2-6617081ae410 close command to datanode 14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn3_1    | 2023-04-04 07:56:35,891 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-04-04 07:56:29,676 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-04-04 07:55:56,666 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 685cb9fd-d40d-4ecb-9ff2-6617081ae410, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:53:56.547713Z[UTC]] removed.
dn3_1    | 2023-04-04 07:56:35,891 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-04-04 07:56:36,109 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4E27919647DF with new leaderId: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn3_1    | 2023-04-04 07:56:36,110 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF: change Leader from null to 14df8ad4-ce2f-4483-aada-29ab5cae3ffb at term 1 for appendEntries, leader elected after 6667ms
dn3_1    | 2023-04-04 07:56:36,110 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:56:36,110 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread1] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-04-04 07:56:36,112 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-4E27919647DF-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df/current/log_inprogress_0
dn5_1    | 2023-04-04 07:56:29,676 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-04-04 07:56:29,769 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF: start as a follower, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:55:56,668 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=0a9acdcd-53b0-4fd8-86de-f3d51d6ec210 since it stays at CLOSED stage.
dn5_1    | 2023-04-04 07:56:29,770 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-04-04 07:56:29,770 [pool-26-thread-1] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState
dn5_1    | 2023-04-04 07:56:29,770 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4E27919647DF,id=2ebde02c-a404-41d0-92a4-7b6da490547a
dn5_1    | 2023-04-04 07:56:29,770 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-04-04 07:55:56,668 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=0a9acdcd-53b0-4fd8-86de-f3d51d6ec210 close command to datanode e2957b23-687f-4626-af75-9b42f3a43b99
scm1_1   | 2023-04-04 07:55:56,677 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 0a9acdcd-53b0-4fd8-86de-f3d51d6ec210, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:e2957b23-687f-4626-af75-9b42f3a43b99, CreationTimestamp2023-04-04T07:53:56.518423Z[UTC]] removed.
dn3_1    | 2023-04-04 07:56:36,529 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState] INFO impl.FollowerState: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5181191385ns, electionTimeout:5154ms
dn3_1    | 2023-04-04 07:56:36,529 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState
dn5_1    | 2023-04-04 07:56:29,771 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-04-04 07:55:56,678 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b since it stays at CLOSED stage.
scm1_1   | 2023-04-04 07:55:56,678 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b close command to datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94
dn5_1    | 2023-04-04 07:56:29,771 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-04-04 07:56:29,771 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-04-04 07:56:29,771 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-04-04 07:56:36,529 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-04-04 07:56:36,530 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-04-04 07:56:29,804 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-04-04 07:55:56,678 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b close command to datanode 2ebde02c-a404-41d0-92a4-7b6da490547a
scm1_1   | 2023-04-04 07:55:56,678 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b close command to datanode e2957b23-687f-4626-af75-9b42f3a43b99
scm1_1   | 2023-04-04 07:55:56,690 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 3a70a706-d27b-47ff-9aed-0ebf5b06395b, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:e2957b23-687f-4626-af75-9b42f3a43b99, CreationTimestamp2023-04-04T07:53:56.547504Z[UTC]] removed.
scm1_1   | 2023-04-04 07:55:56,691 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=35a434de-4469-4302-9ebd-65bc10982ac6 since it stays at CLOSED stage.
scm1_1   | 2023-04-04 07:55:56,691 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=35a434de-4469-4302-9ebd-65bc10982ac6 close command to datanode 2ebde02c-a404-41d0-92a4-7b6da490547a
scm1_1   | 2023-04-04 07:55:56,701 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 35a434de-4469-4302-9ebd-65bc10982ac6, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:53:56.545981Z[UTC]] removed.
dn3_1    | 2023-04-04 07:56:36,531 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5
dn5_1    | 2023-04-04 07:56:29,804 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=8037e277-427a-4034-9f8a-4e27919647df
dn3_1    | 2023-04-04 07:56:36,532 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:56:36,534 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-04-04 07:56:36,534 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-04-04 07:56:36,534 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5-2] INFO server.GrpcServerProtocolClient: Build channel for 14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn3_1    | 2023-04-04 07:56:36,557 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
scm1_1   | 2023-04-04 07:55:57,967 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-04-04 07:55:57,973 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
dn5_1    | 2023-04-04 07:56:30,961 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@7fe07361] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0)], numOfContainers=2, numOfBlocks=3
dn5_1    | 2023-04-04 07:56:30,970 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=8037e277-427a-4034-9f8a-4e27919647df.
dn3_1    | 2023-04-04 07:56:36,557 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5] INFO impl.LeaderElection:   Response 0: c1077390-d65b-4523-9cd4-abe9e2c9eb94<-2ebde02c-a404-41d0-92a4-7b6da490547a#0:FAIL-t0
dn3_1    | 2023-04-04 07:56:36,557 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5] INFO impl.LeaderElection:   Response 1: c1077390-d65b-4523-9cd4-abe9e2c9eb94<-14df8ad4-ce2f-4483-aada-29ab5cae3ffb#0:OK-t0
dn3_1    | 2023-04-04 07:56:36,557 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5] INFO impl.LeaderElection: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5 PRE_VOTE round 0: result REJECTED
scm1_1   | 2023-04-04 07:55:58,322 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e is not found
scm1_1   | 2023-04-04 07:55:58,323 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=d0f51999-1853-4262-b31a-8b1b4f9c62a2 is not found
scm1_1   | 2023-04-04 07:55:58,323 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b is not found
scm1_1   | 2023-04-04 07:55:58,358 [IPC Server handler 79 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 6
scm1_1   | 2023-04-04 07:55:58,359 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=a013e403-c1b0-4f4f-851c-eeb13eac206e is not found
scm1_1   | 2023-04-04 07:55:58,359 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=0a9acdcd-53b0-4fd8-86de-f3d51d6ec210 is not found
scm1_1   | 2023-04-04 07:55:58,360 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=3a70a706-d27b-47ff-9aed-0ebf5b06395b is not found
scm1_1   | 2023-04-04 07:55:59,477 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY state.
dn5_1    | 2023-04-04 07:56:30,975 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a: new RaftServerImpl for group-1D7D5DE977D3:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-04-04 07:56:30,975 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-04-04 07:56:30,975 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-04-04 07:56:30,975 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-04-04 07:56:30,975 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-04-04 07:56:30,975 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-04-04 07:56:30,975 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-04-04 07:56:30,976 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3: ConfigurationManager, init=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-04-04 07:56:30,977 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-04-04 07:56:30,977 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-04-04 07:56:30,977 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-04-04 07:56:30,977 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-04-04 07:56:36,557 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
dn3_1    | 2023-04-04 07:56:36,557 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5
dn3_1    | 2023-04-04 07:56:36,557 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-LeaderElection5] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState
dn3_1    | 2023-04-04 07:56:36,559 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-04-04 07:56:36,559 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-04-04 07:56:36,661 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3: receive requestVote(PRE_VOTE, 2ebde02c-a404-41d0-92a4-7b6da490547a, group-1D7D5DE977D3, 0, (t:0, i:0))
dn3_1    | 2023-04-04 07:56:36,661 [grpc-default-executor-1] INFO impl.VoteContext: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FOLLOWER: accept PRE_VOTE from 2ebde02c-a404-41d0-92a4-7b6da490547a: our priority 0 <= candidate's priority 1
dn3_1    | 2023-04-04 07:56:36,662 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3 replies to PRE_VOTE vote request: 2ebde02c-a404-41d0-92a4-7b6da490547a<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t0. Peer's state: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3:t0, leader=null, voted=, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-04-04 07:56:36,674 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3: receive requestVote(ELECTION, 2ebde02c-a404-41d0-92a4-7b6da490547a, group-1D7D5DE977D3, 1, (t:0, i:0))
dn3_1    | 2023-04-04 07:56:36,675 [grpc-default-executor-1] INFO impl.VoteContext: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FOLLOWER: accept ELECTION from 2ebde02c-a404-41d0-92a4-7b6da490547a: our priority 0 <= candidate's priority 1
dn3_1    | 2023-04-04 07:56:36,675 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:2ebde02c-a404-41d0-92a4-7b6da490547a
scm1_1   | 2023-04-04 07:55:59,478 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-04-04 07:55:59,479 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY state.
scm1_1   | 2023-04-04 07:55:59,480 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-04-04 07:55:59,481 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4 to datanode:2ebde02c-a404-41d0-92a4-7b6da490547a
dn5_1    | 2023-04-04 07:56:30,977 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-04-04 07:56:30,977 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-04-04 07:56:30,978 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-04-04 07:56:36,675 [grpc-default-executor-1] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: shutdown c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState
dn3_1    | 2023-04-04 07:56:36,675 [grpc-default-executor-1] INFO impl.RoleInfo: c1077390-d65b-4523-9cd4-abe9e2c9eb94: start c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState
dn3_1    | 2023-04-04 07:56:36,675 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState] INFO impl.FollowerState: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState was interrupted
scm1_1   | 2023-04-04 07:55:59,514 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:55:59.480Z[UTC]].
scm1_1   | 2023-04-04 07:55:59,519 [RatisPipelineUtilsThread - 0] ERROR scm.SCMCommonPlacementPolicy: Unable to find enough nodes that meet the space requirement of 1073741824 bytes for metadata and 1073741824 bytes for data in healthy node set. Required 3. Found 2.
scm1_1   | 2023-04-04 07:55:59,519 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=f3dc0283-6172-4aac-8038-9f3ca91aaa35 to datanode:c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm1_1   | 2023-04-04 07:55:59,535 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f3dc0283-6172-4aac-8038-9f3ca91aaa35, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:55:59.519Z[UTC]].
scm1_1   | 2023-04-04 07:55:59,540 [RatisPipelineUtilsThread - 0] ERROR scm.SCMCommonPlacementPolicy: Unable to find enough nodes that meet the space requirement of 1073741824 bytes for metadata and 1073741824 bytes for data in healthy node set. Required 3. Found 2.
scm1_1   | 2023-04-04 07:56:00,483 [SCMBlockDeletingService#0] INFO block.SCMBlockDeletingService: Totally added 6 blocks to be deleted for 2 datanodes, task elapsed time: 2ms
scm1_1   | 2023-04-04 07:56:02,967 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:30,979 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-04-04 07:56:30,979 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-04-04 07:56:36,685 [grpc-default-executor-1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3 replies to ELECTION vote request: 2ebde02c-a404-41d0-92a4-7b6da490547a<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t1. Peer's state: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3:t1, leader=null, voted=2ebde02c-a404-41d0-92a4-7b6da490547a, raftlog=Memoized:c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:56:07,968 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:30,979 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-04-04 07:56:36,686 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-04-04 07:56:12,968 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:30,979 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-04-04 07:56:36,686 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-04-04 07:56:17,968 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:30,982 [Command processor thread] INFO server.RaftServer: 2ebde02c-a404-41d0-92a4-7b6da490547a: addNew group-1D7D5DE977D3:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns      null 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn3_1    | 2023-04-04 07:56:36,759 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1D7D5DE977D3 with new leaderId: 2ebde02c-a404-41d0-92a4-7b6da490547a
scm1_1   | 2023-04-04 07:56:19,945 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=685cb9fd-d40d-4ecb-9ff2-6617081ae410 is not found
dn5_1    | 2023-04-04 07:56:31,002 [pool-26-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 does not exist. Creating ...
dn3_1    | 2023-04-04 07:56:36,760 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread1] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3: change Leader from null to 2ebde02c-a404-41d0-92a4-7b6da490547a at term 1 for appendEntries, leader elected after 5834ms
scm1_1   | 2023-04-04 07:56:20,478 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY state.
dn5_1    | 2023-04-04 07:56:31,003 [pool-26-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3/in_use.lock acquired by nodename 7@7e8229df9409
dn3_1    | 2023-04-04 07:56:36,777 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread2] INFO server.RaftServer$Division: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:56:20,478 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn5_1    | 2023-04-04 07:56:31,004 [pool-26-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 has been successfully formatted.
dn3_1    | 2023-04-04 07:56:36,778 [c1077390-d65b-4523-9cd4-abe9e2c9eb94-server-thread2] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-SegmentedRaftLogWorker: Starting segment from index:0
scm1_1   | 2023-04-04 07:56:20,479 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=2b2f258a-8c66-4c27-9e64-eba4a65dc133 to datanode:14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn5_1    | 2023-04-04 07:56:31,004 [pool-26-thread-1] INFO ratis.ContainerStateMachine: group-1D7D5DE977D3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-04-04 07:56:36,779 [c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c1077390-d65b-4523-9cd4-abe9e2c9eb94@group-1D7D5DE977D3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3/current/log_inprogress_0
scm1_1   | 2023-04-04 07:56:20,490 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 2b2f258a-8c66-4c27-9e64-eba4a65dc133, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.479Z[UTC]].
dn5_1    | 2023-04-04 07:56:31,004 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-04-04 07:57:06,166 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@741ac284] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(1),2(1)], numOfContainers=2, numOfBlocks=3
scm1_1   | 2023-04-04 07:56:20,502 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8037e277-427a-4034-9f8a-4e27919647df to datanode:14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn5_1    | 2023-04-04 07:56:31,005 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-04-04 07:57:06,167 [DeleteBlocksCommandHandlerThread-2] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1 <= 1
scm1_1   | 2023-04-04 07:56:20,503 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8037e277-427a-4034-9f8a-4e27919647df to datanode:c1077390-d65b-4523-9cd4-abe9e2c9eb94
dn5_1    | 2023-04-04 07:56:31,005 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-04-04 07:57:06,170 [DeleteBlocksCommandHandlerThread-3] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 2 <= 2
scm1_1   | 2023-04-04 07:56:20,503 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8037e277-427a-4034-9f8a-4e27919647df to datanode:2ebde02c-a404-41d0-92a4-7b6da490547a
dn5_1    | 2023-04-04 07:56:31,005 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-04-04 07:57:17,348 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 6/4994 blocks from 2 candidate containers.
scm1_1   | 2023-04-04 07:56:20,529 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8037e277-427a-4034-9f8a-4e27919647df, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.502Z[UTC]].
dn5_1    | 2023-04-04 07:56:31,005 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-04-04 07:57:17,360 [BlockDeletingService#2] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/containerDir0/1/chunks/111677748019200001.block
scm1_1   | 2023-04-04 07:56:20,531 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 to datanode:2ebde02c-a404-41d0-92a4-7b6da490547a
scm1_1   | 2023-04-04 07:56:20,532 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 to datanode:14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn5_1    | 2023-04-04 07:56:31,005 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-04-04 07:56:31,006 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-04-04 07:57:17,360 [BlockDeletingService#2] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/containerDir0/1/chunks/111677748019200002.block
dn5_1    | 2023-04-04 07:56:31,006 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-04-04 07:56:31,006 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: new 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3
dn5_1    | 2023-04-04 07:56:31,006 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-04-04 07:56:31,006 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-04-04 07:56:31,006 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-04-04 07:56:31,006 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-04-04 07:57:17,361 [BlockDeletingService#1] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/containerDir0/2/chunks/111677748019200003.block
scm1_1   | 2023-04-04 07:56:20,532 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 to datanode:c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm1_1   | 2023-04-04 07:56:20,556 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:20.531Z[UTC]].
dn5_1    | 2023-04-04 07:56:31,006 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-04-04 07:56:31,006 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-04-04 07:58:17,353 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 3/4997 blocks from 2 candidate containers.
dn5_1    | 2023-04-04 07:56:31,006 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-04-04 07:56:31,007 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-04-04 07:58:17,354 [BlockDeletingService#4] INFO background.BlockDeletingService: No transaction found in container 1 with pending delete block count 2
dn3_1    | 2023-04-04 07:58:17,356 [BlockDeletingService#0] INFO background.BlockDeletingService: No transaction found in container 2 with pending delete block count 1
scm1_1   | 2023-04-04 07:56:20,567 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3 contains same datanodes as previous pipelines: PipelineID=8037e277-427a-4034-9f8a-4e27919647df nodeIds: 2ebde02c-a404-41d0-92a4-7b6da490547a, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb, c1077390-d65b-4523-9cd4-abe9e2c9eb94
scm1_1   | 2023-04-04 07:56:20,568 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1_1   | 2023-04-04 07:56:22,968 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-04-04 07:56:23,478 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY state.
scm1_1   | 2023-04-04 07:56:23,479 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn5_1    | 2023-04-04 07:56:31,007 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm1_1   | 2023-04-04 07:56:23,480 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6d92c706-b857-41ae-a811-0fceb345a748 to datanode:b7b7229a-e604-4787-8543-e6d8cdebe63b
dn5_1    | 2023-04-04 07:56:31,009 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-04-04 07:56:23,486 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6d92c706-b857-41ae-a811-0fceb345a748, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:23.479Z[UTC]].
dn5_1    | 2023-04-04 07:56:31,445 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-2ebde02c-a404-41d0-92a4-7b6da490547a: Detected pause in JVM or host machine (eg GC): pause of approximately 252480047ns.
scm1_1   | 2023-04-04 07:56:23,489 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 3.
dn5_1    | GC pool 'ParNew' had collection(s): count=1 time=426ms
scm1_1   | 2023-04-04 07:56:27,970 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:31,473 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-04-04 07:56:29,343 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:55:59.480Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:56:31,474 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-04-04 07:56:29,393 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f3dc0283-6172-4aac-8038-9f3ca91aaa35, Nodes: c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:c1077390-d65b-4523-9cd4-abe9e2c9eb94, CreationTimestamp2023-04-04T07:55:59.519Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:56:31,474 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm1_1   | 2023-04-04 07:56:29,480 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY state.
dn5_1    | 2023-04-04 07:56:31,474 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-04-04 07:56:29,480 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn5_1    | 2023-04-04 07:56:31,475 [pool-26-thread-1] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-04-04 07:56:29,481 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a21f8cfe-4543-468b-bd60-588a1a0d07ff to datanode:e2957b23-687f-4626-af75-9b42f3a43b99
dn5_1    | 2023-04-04 07:56:31,477 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3: start as a follower, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:56:29,493 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a21f8cfe-4543-468b-bd60-588a1a0d07ff, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-04-04T07:56:29.481Z[UTC]].
scm1_1   | 2023-04-04 07:56:29,503 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn5_1    | 2023-04-04 07:56:31,477 [pool-26-thread-1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 2023-04-04 07:56:32,970 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-04-04 07:56:31,477 [pool-26-thread-1] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-FollowerState
scm1_1   | 2023-04-04 07:56:35,948 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8037e277-427a-4034-9f8a-4e27919647df, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19)2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:56:20.502Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:56:31,486 [pool-26-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1D7D5DE977D3,id=2ebde02c-a404-41d0-92a4-7b6da490547a
scm1_1   | 2023-04-04 07:56:36,702 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3, Nodes: 2ebde02c-a404-41d0-92a4-7b6da490547a(ha_dn5_1.ha_net/10.9.0.21)14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20)c1077390-d65b-4523-9cd4-abe9e2c9eb94(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:2ebde02c-a404-41d0-92a4-7b6da490547a, CreationTimestamp2023-04-04T07:56:20.531Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:56:31,486 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-04-04 07:56:31,486 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-04-04 07:56:37,970 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Open pipeline found after SCM finalization
dn5_1    | 2023-04-04 07:56:31,486 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 2023-04-04 07:56:37,978 [IPC Server handler 14 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn5_1    | 2023-04-04 07:56:31,486 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-04-04 07:56:51,055 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 2b2f258a-8c66-4c27-9e64-eba4a65dc133, Nodes: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:14df8ad4-ce2f-4483-aada-29ab5cae3ffb, CreationTimestamp2023-04-04T07:56:20.479Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:56:31,487 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-04-04 07:56:51,679 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6d92c706-b857-41ae-a811-0fceb345a748, Nodes: b7b7229a-e604-4787-8543-e6d8cdebe63b(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b7b7229a-e604-4787-8543-e6d8cdebe63b, CreationTimestamp2023-04-04T07:56:23.479Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:56:31,490 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-04-04 07:56:59,466 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a21f8cfe-4543-468b-bd60-588a1a0d07ff, Nodes: e2957b23-687f-4626-af75-9b42f3a43b99(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e2957b23-687f-4626-af75-9b42f3a43b99, CreationTimestamp2023-04-04T07:56:29.481Z[UTC]] moved to OPEN state
dn5_1    | 2023-04-04 07:56:31,490 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3
scm1_1   | 2023-04-04 07:56:59,506 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm1_1   | 2023-04-04 07:57:00,482 [SCMBlockDeletingService#0] INFO block.SCMBlockDeletingService: Totally added 9 blocks to be deleted for 3 datanodes, task elapsed time: 1ms
dn5_1    | 2023-04-04 07:56:31,737 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3.
dn5_1    | 2023-04-04 07:56:34,620 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-FollowerState] INFO impl.FollowerState: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5195331444ns, electionTimeout:5162ms
dn5_1    | 2023-04-04 07:56:34,620 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-FollowerState] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-FollowerState
dn5_1    | 2023-04-04 07:56:34,620 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-FollowerState] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-04-04 07:56:34,621 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-04-04 07:56:34,621 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-FollowerState] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4
dn5_1    | 2023-04-04 07:56:34,622 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:57:29,509 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn5_1    | 2023-04-04 07:56:34,622 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4 PRE_VOTE round 0: result PASSED (term=0)
dn5_1    | 2023-04-04 07:56:34,623 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-04-04 07:57:50,377 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 2000.
dn5_1    | 2023-04-04 07:56:34,623 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4 ELECTION round 0: result PASSED (term=1)
scm1_1   | 2023-04-04 07:57:50,396 [IPC Server handler 6 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 2000 to 3000.
dn5_1    | 2023-04-04 07:56:34,623 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4
scm1_1   | 2023-04-04 07:57:50,423 [861060d2-f30f-4260-bac0-b4e7af8aba9d@group-0BB16BE3F1B7-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
dn5_1    | 2023-04-04 07:56:34,623 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1_1   | 2023-04-04 07:57:50,434 [IPC Server handler 6 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019203000 to 111677748019204000.
dn5_1    | 2023-04-04 07:56:34,623 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5D3D419BA0D4 with new leaderId: 2ebde02c-a404-41d0-92a4-7b6da490547a
dn5_1    | 2023-04-04 07:56:34,624 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4: change Leader from null to 2ebde02c-a404-41d0-92a4-7b6da490547a at term 1 for becomeLeader, leader elected after 5308ms
scm1_1   | 2023-04-04 07:57:59,511 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn5_1    | 2023-04-04 07:56:34,624 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-04-04 07:58:00,482 [SCMBlockDeletingService#0] INFO block.SCMBlockDeletingService: Totally added 3 blocks to be deleted for 1 datanodes, task elapsed time: 1ms
dn5_1    | 2023-04-04 07:56:34,624 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-04-04 07:56:34,624 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-04-04 07:56:34,626 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-04-04 07:56:34,626 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-04-04 07:56:34,626 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-04-04 07:56:34,627 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-04-04 07:56:34,628 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-04-04 07:56:34,629 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderStateImpl
dn5_1    | 2023-04-04 07:56:34,630 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-04-04 07:56:34,632 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/442e37ae-d6fa-4ab1-a55d-5d3d419ba0d4/current/log_inprogress_0
dn5_1    | 2023-04-04 07:56:34,634 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4-LeaderElection4] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-5D3D419BA0D4: set configuration 0: peers:[2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:56:34,971 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState] INFO impl.FollowerState: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5200917427ns, electionTimeout:5166ms
dn5_1    | 2023-04-04 07:56:34,971 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState
dn5_1    | 2023-04-04 07:56:34,971 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-04-04 07:56:34,971 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-04-04 07:56:34,972 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5
dn5_1    | 2023-04-04 07:56:34,977 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:56:34,979 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-04-04 07:56:34,984 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-04-04 07:56:34,982 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5-2] INFO server.GrpcServerProtocolClient: Build channel for 14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn5_1    | 2023-04-04 07:56:35,032 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn5_1    | 2023-04-04 07:56:35,032 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5] INFO impl.LeaderElection:   Response 0: 2ebde02c-a404-41d0-92a4-7b6da490547a<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t0
dn5_1    | 2023-04-04 07:56:35,036 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5] INFO impl.LeaderElection:   Response 1: 2ebde02c-a404-41d0-92a4-7b6da490547a<-14df8ad4-ce2f-4483-aada-29ab5cae3ffb#0:FAIL-t0
dn5_1    | 2023-04-04 07:56:35,037 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5 PRE_VOTE round 0: result REJECTED
dn5_1    | 2023-04-04 07:56:35,037 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
dn5_1    | 2023-04-04 07:56:35,038 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5
dn5_1    | 2023-04-04 07:56:35,038 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-LeaderElection5] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState
dn5_1    | 2023-04-04 07:56:35,041 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-04-04 07:56:35,041 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-04-04 07:56:35,853 [grpc-default-executor-4] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF: receive requestVote(PRE_VOTE, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb, group-4E27919647DF, 0, (t:0, i:0))
dn5_1    | 2023-04-04 07:56:35,853 [grpc-default-executor-4] INFO impl.VoteContext: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FOLLOWER: accept PRE_VOTE from 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: our priority 0 <= candidate's priority 1
dn5_1    | 2023-04-04 07:56:35,853 [grpc-default-executor-4] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF replies to PRE_VOTE vote request: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb<-2ebde02c-a404-41d0-92a4-7b6da490547a#0:OK-t0. Peer's state: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF:t0, leader=null, voted=, raftlog=Memoized:2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:56:35,981 [grpc-default-executor-4] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF: receive requestVote(ELECTION, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb, group-4E27919647DF, 1, (t:0, i:0))
dn5_1    | 2023-04-04 07:56:35,981 [grpc-default-executor-4] INFO impl.VoteContext: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FOLLOWER: accept ELECTION from 14df8ad4-ce2f-4483-aada-29ab5cae3ffb: our priority 0 <= candidate's priority 1
dn5_1    | 2023-04-04 07:56:35,981 [grpc-default-executor-4] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn5_1    | 2023-04-04 07:56:35,981 [grpc-default-executor-4] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState
dn5_1    | 2023-04-04 07:56:35,981 [grpc-default-executor-4] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState
dn5_1    | 2023-04-04 07:56:35,982 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState] INFO impl.FollowerState: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState was interrupted
dn5_1    | 2023-04-04 07:56:35,983 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-04-04 07:56:35,983 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-04-04 07:56:35,983 [grpc-default-executor-4] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF replies to ELECTION vote request: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb<-2ebde02c-a404-41d0-92a4-7b6da490547a#0:OK-t1. Peer's state: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF:t1, leader=null, voted=14df8ad4-ce2f-4483-aada-29ab5cae3ffb, raftlog=Memoized:2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:56:36,116 [2ebde02c-a404-41d0-92a4-7b6da490547a-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4E27919647DF with new leaderId: 14df8ad4-ce2f-4483-aada-29ab5cae3ffb
dn5_1    | 2023-04-04 07:56:36,116 [2ebde02c-a404-41d0-92a4-7b6da490547a-server-thread1] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF: change Leader from null to 14df8ad4-ce2f-4483-aada-29ab5cae3ffb at term 1 for appendEntries, leader elected after 6666ms
dn5_1    | 2023-04-04 07:56:36,139 [2ebde02c-a404-41d0-92a4-7b6da490547a-server-thread2] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:56:36,139 [2ebde02c-a404-41d0-92a4-7b6da490547a-server-thread2] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-04-04 07:56:36,142 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-4E27919647DF-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8037e277-427a-4034-9f8a-4e27919647df/current/log_inprogress_0
dn5_1    | 2023-04-04 07:56:36,541 [grpc-default-executor-4] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3: receive requestVote(PRE_VOTE, c1077390-d65b-4523-9cd4-abe9e2c9eb94, group-1D7D5DE977D3, 0, (t:0, i:0))
dn5_1    | 2023-04-04 07:56:36,541 [grpc-default-executor-4] INFO impl.VoteContext: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-FOLLOWER: reject PRE_VOTE from c1077390-d65b-4523-9cd4-abe9e2c9eb94: our priority 1 > candidate's priority 0
dn5_1    | 2023-04-04 07:56:36,541 [grpc-default-executor-4] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3 replies to PRE_VOTE vote request: c1077390-d65b-4523-9cd4-abe9e2c9eb94<-2ebde02c-a404-41d0-92a4-7b6da490547a#0:FAIL-t0. Peer's state: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3:t0, leader=null, voted=, raftlog=Memoized:2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:56:36,647 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-FollowerState] INFO impl.FollowerState: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5169796719ns, electionTimeout:5157ms
dn5_1    | 2023-04-04 07:56:36,647 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-FollowerState] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-FollowerState
dn5_1    | 2023-04-04 07:56:36,648 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-FollowerState] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-04-04 07:56:36,648 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-04-04 07:56:36,648 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-FollowerState] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6
dn5_1    | 2023-04-04 07:56:36,654 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:56:36,655 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-04-04 07:56:36,655 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-04-04 07:56:36,669 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-04-04 07:56:36,669 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO impl.LeaderElection:   Response 0: 2ebde02c-a404-41d0-92a4-7b6da490547a<-14df8ad4-ce2f-4483-aada-29ab5cae3ffb#0:OK-t0
dn5_1    | 2023-04-04 07:56:36,669 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6 PRE_VOTE round 0: result PASSED
dn5_1    | 2023-04-04 07:56:36,672 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6 ELECTION round 0: submit vote requests at term 1 for -1: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:56:36,676 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-04-04 07:56:36,676 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-04-04 07:56:36,693 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-04-04 07:56:36,695 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO impl.LeaderElection:   Response 0: 2ebde02c-a404-41d0-92a4-7b6da490547a<-c1077390-d65b-4523-9cd4-abe9e2c9eb94#0:OK-t1
dn5_1    | 2023-04-04 07:56:36,695 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO impl.LeaderElection: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6 ELECTION round 0: result PASSED
dn5_1    | 2023-04-04 07:56:36,696 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: shutdown 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6
dn5_1    | 2023-04-04 07:56:36,696 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn5_1    | 2023-04-04 07:56:36,696 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1D7D5DE977D3 with new leaderId: 2ebde02c-a404-41d0-92a4-7b6da490547a
dn5_1    | 2023-04-04 07:56:36,696 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3: change Leader from null to 2ebde02c-a404-41d0-92a4-7b6da490547a at term 1 for becomeLeader, leader elected after 5718ms
dn5_1    | 2023-04-04 07:56:36,697 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-04-04 07:56:36,700 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-04-04 07:56:36,700 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-04-04 07:56:36,714 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-04-04 07:56:36,717 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-04-04 07:56:36,723 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-04-04 07:56:36,723 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-04-04 07:56:36,724 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-04-04 07:56:36,725 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-04-04 07:56:36,725 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-04-04 07:56:36,725 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-04-04 07:56:36,726 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-04-04 07:56:36,726 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-04-04 07:56:36,726 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-04-04 07:56:36,727 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-04-04 07:56:36,727 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-04-04 07:56:36,729 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-04-04 07:56:36,729 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-04-04 07:56:36,729 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-04-04 07:56:36,729 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-04-04 07:56:36,730 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-04-04 07:56:36,730 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-04-04 07:56:36,730 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-04-04 07:56:36,730 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-04-04 07:56:36,730 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO impl.RoleInfo: 2ebde02c-a404-41d0-92a4-7b6da490547a: start 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderStateImpl
dn5_1    | 2023-04-04 07:56:36,731 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-04-04 07:56:36,733 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d6b6c5b9-ec1e-455b-92bc-1d7d5de977d3/current/log_inprogress_0
dn5_1    | 2023-04-04 07:56:36,746 [2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3-LeaderElection6] INFO server.RaftServer$Division: 2ebde02c-a404-41d0-92a4-7b6da490547a@group-1D7D5DE977D3: set configuration 0: peers:[c1077390-d65b-4523-9cd4-abe9e2c9eb94|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 2ebde02c-a404-41d0-92a4-7b6da490547a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 14df8ad4-ce2f-4483-aada-29ab5cae3ffb|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-04-04 07:57:09,474 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@7fe07361] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(1),2(1)], numOfContainers=2, numOfBlocks=3
dn5_1    | 2023-04-04 07:57:09,474 [DeleteBlocksCommandHandlerThread-3] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 2 <= 2
dn5_1    | 2023-04-04 07:57:09,482 [DeleteBlocksCommandHandlerThread-2] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1 <= 1
dn5_1    | 2023-04-04 07:57:17,719 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 6/4994 blocks from 2 candidate containers.
dn5_1    | 2023-04-04 07:57:17,725 [BlockDeletingService#2] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/containerDir0/1/chunks/111677748019200001.block
dn5_1    | 2023-04-04 07:57:17,725 [BlockDeletingService#2] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/containerDir0/1/chunks/111677748019200002.block
dn5_1    | 2023-04-04 07:57:17,726 [BlockDeletingService#1] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-dff08a8b-27a1-4745-956e-0bb16be3f1b7/current/containerDir0/2/chunks/111677748019200003.block
dn5_1    | 2023-04-04 07:57:53,131 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-2ebde02c-a404-41d0-92a4-7b6da490547a: Detected pause in JVM or host machine (eg GC): pause of approximately 133383747ns.
dn5_1    | GC pool 'ParNew' had collection(s): count=1 time=145ms
dn5_1    | 2023-04-04 07:58:17,730 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 3/4997 blocks from 2 candidate containers.
dn5_1    | 2023-04-04 07:58:17,731 [BlockDeletingService#4] INFO background.BlockDeletingService: No transaction found in container 1 with pending delete block count 2
dn5_1    | 2023-04-04 07:58:17,732 [BlockDeletingService#2] INFO background.BlockDeletingService: No transaction found in container 2 with pending delete block count 1
