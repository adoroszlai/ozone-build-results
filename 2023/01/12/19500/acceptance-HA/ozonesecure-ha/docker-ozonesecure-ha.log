Attaching to ozonesecure-ha_s3g_1, ozonesecure-ha_scm2.org_1, ozonesecure-ha_scm3.org_1, ozonesecure-ha_om1_1, ozonesecure-ha_om2_1, ozonesecure-ha_kms_1, ozonesecure-ha_kdc_1, ozonesecure-ha_datanode3_1, ozonesecure-ha_datanode2_1, ozonesecure-ha_om3_1, ozonesecure-ha_recon_1, ozonesecure-ha_scm1.org_1, ozonesecure-ha_datanode1_1
datanode1_1  | Sleeping for 5 seconds
datanode1_1  | Waiting for the service scm3.org:9894
datanode1_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode1_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode1_1  | 2023-01-12 05:43:23,896 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode1_1  | /************************************************************
datanode1_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode1_1  | STARTUP_MSG:   host = fb1a7b8e0c66/172.25.0.102
datanode1_1  | STARTUP_MSG:   args = []
datanode1_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode1_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode1_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
datanode1_1  | STARTUP_MSG:   java = 11.0.14.1
datanode1_1  | ************************************************************/
datanode1_1  | 2023-01-12 05:43:23,975 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode1_1  | 2023-01-12 05:43:24,696 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode1_1  | 2023-01-12 05:43:25,615 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode1_1  | 2023-01-12 05:43:27,430 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode1_1  | 2023-01-12 05:43:27,431 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode1_1  | 2023-01-12 05:43:28,649 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:fb1a7b8e0c66 ip:172.25.0.102
datanode1_1  | 2023-01-12 05:43:33,963 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode1_1  | 2023-01-12 05:43:35,582 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode1_1  | 2023-01-12 05:43:35,585 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode1_1  | 2023-01-12 05:43:39,207 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode1_1  | 2023-01-12 05:43:39,207 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode1_1  | 2023-01-12 05:43:39,224 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode1_1  | 2023-01-12 05:43:39,232 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode1_1  | 2023-01-12 05:43:49,659 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode1_1  | 2023-01-12 05:43:50,009 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.102,host:fb1a7b8e0c66
datanode1_1  | 2023-01-12 05:43:50,016 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode1_1  | 2023-01-12 05:43:50,062 [main] ERROR client.DNCertificateClient: Invalid domain fb1a7b8e0c66
datanode1_1  | 2023-01-12 05:43:50,064 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@fb1a7b8e0c66
datanode1_1  | 2023-01-12 05:43:57,577 [main] INFO client.DNCertificateClient: Loading certificate from location:/data/metadata/dn/certs.
datanode1_1  | 2023-01-12 05:43:57,667 [main] INFO client.DNCertificateClient: Added certificate [
datanode1_1  | [
datanode1_1  |   Version: V3
datanode1_1  |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
datanode1_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode1_1  | 
datanode1_1  |   Key:  Sun RSA public key, 2048 bits
datanode1_1  |   params: null
datanode1_1  |   modulus: 30346053453237189834112040673541629567010306125791898570104054454047501736657892214383462264526301500298692652758375836429638347531164751981653358525504527825570400558838334901393005984876414719430775458652128247547444727818535668600972192623637356865083888753952717740665794517921055984453520353917733198649461590018542814322448272297366500121604394586840649019781675207082962097691558583700862118486739043197335879323701508955628436573724419921778589743880043303622162563304571704603236749325057266808761842228199352214967718782980075765678400986380731445934917806096546808808883868771850900183098234200323729287601
datanode1_1  |   public exponent: 65537
datanode1_1  |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
datanode1_1  |                To: Sun Feb 20 00:00:00 UTC 2028]
datanode1_1  |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
datanode1_1  |   SerialNumber: [    01]
datanode1_1  | 
datanode1_1  | Certificate Extensions: 3
datanode1_1  | [1]: ObjectId: 2.5.29.19 Criticality=true
datanode1_1  | BasicConstraints:[
datanode1_1  |   CA:true
datanode1_1  |   PathLen:2147483647
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | [2]: ObjectId: 2.5.29.15 Criticality=true
datanode1_1  | KeyUsage [
datanode1_1  |   Key_CertSign
datanode1_1  |   Crl_Sign
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | [3]: ObjectId: 2.5.29.17 Criticality=false
datanode1_1  | SubjectAlternativeName [
datanode1_1  |   IPAddress: 172.25.0.116
datanode1_1  |   DNSName: scm1.org
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | ]
datanode1_1  |   Algorithm: [SHA256withRSA]
datanode1_1  |   Signature:
datanode1_1  | 0000: 59 E4 14 CA 42 0B 71 BE   61 3A FA 41 71 F2 48 C6  Y...B.q.a:.Aq.H.
datanode1_1  | 0010: 35 3A DF E8 FE E3 74 A0   7E 57 15 0F F2 39 95 34  5:....t..W...9.4
datanode1_1  | 0020: E6 A7 48 CE 06 EA B9 42   CC AD DC 81 09 F1 F2 3E  ..H....B.......>
datanode1_1  | 0030: 94 27 78 5F 6B 04 F9 F4   E3 A1 71 61 8F 5A EF 6D  .'x_k.....qa.Z.m
datanode1_1  | 0040: 72 EA 9B CB 23 B9 31 9F   D7 B3 7B 86 35 09 CF D1  r...#.1.....5...
datanode1_1  | 0050: D6 81 A8 2E 21 01 F7 A9   B4 78 00 21 D5 3C 4A 61  ....!....x.!.<Ja
datanode1_1  | 0060: 7F 91 56 B9 50 6C 2C 23   0F D3 49 6E 80 28 7A 03  ..V.Pl,#..In.(z.
datanode1_1  | 0070: 32 46 42 54 4F 22 06 7F   B1 7D 41 A0 09 27 CF 7A  2FBTO"....A..'.z
datanode1_1  | 0080: F0 82 C6 00 70 41 5D 8F   83 5F E6 EB 74 1F B7 A2  ....pA].._..t...
datanode1_1  | 0090: 85 64 3E AE D6 66 71 7C   BE 7F 92 D0 79 64 71 C0  .d>..fq.....ydq.
datanode1_1  | 00A0: 8F 76 8C F6 D1 A4 67 C4   F4 11 8D 75 FC DE EF EE  .v....g....u....
datanode1_1  | 00B0: D9 7F 75 F3 5F AC 76 DF   23 F9 19 99 25 F9 31 8E  ..u._.v.#...%.1.
datanode1_1  | 00C0: AA 69 DC 7B 7E 4E 62 10   80 B9 5E 00 F8 1D 16 71  .i...Nb...^....q
datanode1_1  | 00D0: 7F D4 C7 FD 20 63 0B 0A   7E 36 7D F4 4C 85 A7 61  .... c...6..L..a
datanode1_1  | 00E0: E7 00 61 A2 83 40 78 92   3E A1 9A E0 B8 E4 A1 A0  ..a..@x.>.......
datanode1_1  | 00F0: 45 B5 33 7C 03 24 98 E7   DE 69 74 45 16 2B 06 3F  E.3..$...itE.+.?
datanode1_1  | 
datanode1_1  | ] from file:/data/metadata/dn/certs/ROOTCA-1.crt.
datanode1_1  | 2023-01-12 05:43:57,710 [main] INFO client.DNCertificateClient: Added certificate [
datanode1_1  | [
datanode1_1  |   Version: V3
datanode1_1  |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=dn@fb1a7b8e0c66
datanode1_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode1_1  | 
datanode1_1  |   Key:  Sun RSA public key, 2048 bits
datanode1_1  |   params: null
datanode2_1  | Sleeping for 5 seconds
datanode2_1  | Waiting for the service scm3.org:9894
datanode2_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode2_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode2_1  | 2023-01-12 05:43:24,905 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode2_1  | /************************************************************
datanode2_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode2_1  | STARTUP_MSG:   host = e522f50beb6a/172.25.0.103
datanode2_1  | STARTUP_MSG:   args = []
datanode2_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode2_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode2_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
datanode2_1  | STARTUP_MSG:   java = 11.0.14.1
datanode2_1  | ************************************************************/
datanode2_1  | 2023-01-12 05:43:25,024 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode2_1  | 2023-01-12 05:43:25,647 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode2_1  | 2023-01-12 05:43:26,837 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode2_1  | 2023-01-12 05:43:28,415 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode2_1  | 2023-01-12 05:43:28,415 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode2_1  | 2023-01-12 05:43:29,550 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:e522f50beb6a ip:172.25.0.103
datanode2_1  | 2023-01-12 05:43:34,978 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode2_1  | 2023-01-12 05:43:36,422 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode2_1  | 2023-01-12 05:43:36,422 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode2_1  | 2023-01-12 05:43:39,708 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode2_1  | 2023-01-12 05:43:39,710 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode2_1  | 2023-01-12 05:43:39,724 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode2_1  | 2023-01-12 05:43:39,739 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode2_1  | 2023-01-12 05:43:49,884 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode2_1  | 2023-01-12 05:43:50,174 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.103,host:e522f50beb6a
datanode2_1  | 2023-01-12 05:43:50,185 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode2_1  | 2023-01-12 05:43:50,210 [main] ERROR client.DNCertificateClient: Invalid domain e522f50beb6a
datanode2_1  | 2023-01-12 05:43:50,224 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@e522f50beb6a
datanode2_1  | 2023-01-12 05:43:58,282 [main] INFO client.DNCertificateClient: Loading certificate from location:/data/metadata/dn/certs.
datanode2_1  | 2023-01-12 05:43:58,441 [main] INFO client.DNCertificateClient: Added certificate [
datanode2_1  | [
datanode2_1  |   Version: V3
datanode2_1  |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
datanode2_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode2_1  | 
datanode2_1  |   Key:  Sun RSA public key, 2048 bits
datanode2_1  |   params: null
datanode2_1  |   modulus: 30346053453237189834112040673541629567010306125791898570104054454047501736657892214383462264526301500298692652758375836429638347531164751981653358525504527825570400558838334901393005984876414719430775458652128247547444727818535668600972192623637356865083888753952717740665794517921055984453520353917733198649461590018542814322448272297366500121604394586840649019781675207082962097691558583700862118486739043197335879323701508955628436573724419921778589743880043303622162563304571704603236749325057266808761842228199352214967718782980075765678400986380731445934917806096546808808883868771850900183098234200323729287601
datanode2_1  |   public exponent: 65537
datanode2_1  |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
datanode2_1  |                To: Sun Feb 20 00:00:00 UTC 2028]
datanode2_1  |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
datanode2_1  |   SerialNumber: [    01]
datanode2_1  | 
datanode2_1  | Certificate Extensions: 3
datanode2_1  | [1]: ObjectId: 2.5.29.19 Criticality=true
datanode2_1  | BasicConstraints:[
datanode2_1  |   CA:true
datanode2_1  |   PathLen:2147483647
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | [2]: ObjectId: 2.5.29.15 Criticality=true
datanode2_1  | KeyUsage [
datanode2_1  |   Key_CertSign
datanode2_1  |   Crl_Sign
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | [3]: ObjectId: 2.5.29.17 Criticality=false
datanode2_1  | SubjectAlternativeName [
datanode2_1  |   IPAddress: 172.25.0.116
datanode2_1  |   DNSName: scm1.org
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | ]
datanode2_1  |   Algorithm: [SHA256withRSA]
datanode2_1  |   Signature:
datanode2_1  | 0000: 59 E4 14 CA 42 0B 71 BE   61 3A FA 41 71 F2 48 C6  Y...B.q.a:.Aq.H.
datanode2_1  | 0010: 35 3A DF E8 FE E3 74 A0   7E 57 15 0F F2 39 95 34  5:....t..W...9.4
datanode2_1  | 0020: E6 A7 48 CE 06 EA B9 42   CC AD DC 81 09 F1 F2 3E  ..H....B.......>
datanode2_1  | 0030: 94 27 78 5F 6B 04 F9 F4   E3 A1 71 61 8F 5A EF 6D  .'x_k.....qa.Z.m
datanode2_1  | 0040: 72 EA 9B CB 23 B9 31 9F   D7 B3 7B 86 35 09 CF D1  r...#.1.....5...
datanode2_1  | 0050: D6 81 A8 2E 21 01 F7 A9   B4 78 00 21 D5 3C 4A 61  ....!....x.!.<Ja
datanode2_1  | 0060: 7F 91 56 B9 50 6C 2C 23   0F D3 49 6E 80 28 7A 03  ..V.Pl,#..In.(z.
datanode2_1  | 0070: 32 46 42 54 4F 22 06 7F   B1 7D 41 A0 09 27 CF 7A  2FBTO"....A..'.z
datanode2_1  | 0080: F0 82 C6 00 70 41 5D 8F   83 5F E6 EB 74 1F B7 A2  ....pA].._..t...
datanode2_1  | 0090: 85 64 3E AE D6 66 71 7C   BE 7F 92 D0 79 64 71 C0  .d>..fq.....ydq.
datanode2_1  | 00A0: 8F 76 8C F6 D1 A4 67 C4   F4 11 8D 75 FC DE EF EE  .v....g....u....
datanode2_1  | 00B0: D9 7F 75 F3 5F AC 76 DF   23 F9 19 99 25 F9 31 8E  ..u._.v.#...%.1.
datanode2_1  | 00C0: AA 69 DC 7B 7E 4E 62 10   80 B9 5E 00 F8 1D 16 71  .i...Nb...^....q
datanode2_1  | 00D0: 7F D4 C7 FD 20 63 0B 0A   7E 36 7D F4 4C 85 A7 61  .... c...6..L..a
datanode2_1  | 00E0: E7 00 61 A2 83 40 78 92   3E A1 9A E0 B8 E4 A1 A0  ..a..@x.>.......
datanode2_1  | 00F0: 45 B5 33 7C 03 24 98 E7   DE 69 74 45 16 2B 06 3F  E.3..$...itE.+.?
datanode2_1  | 
datanode2_1  | ] from file:/data/metadata/dn/certs/ROOTCA-1.crt.
datanode2_1  | 2023-01-12 05:43:58,516 [main] INFO client.DNCertificateClient: Added certificate [
datanode2_1  | [
datanode2_1  |   Version: V3
datanode2_1  |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=dn@e522f50beb6a
datanode2_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode2_1  | 
datanode3_1  | Sleeping for 5 seconds
datanode3_1  | Waiting for the service scm3.org:9894
datanode3_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode3_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode3_1  | 2023-01-12 05:43:24,928 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode3_1  | /************************************************************
datanode3_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode3_1  | STARTUP_MSG:   host = 789a46e3b060/172.25.0.104
datanode3_1  | STARTUP_MSG:   args = []
datanode3_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode3_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode3_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
datanode3_1  | STARTUP_MSG:   java = 11.0.14.1
datanode3_1  | ************************************************************/
datanode3_1  | 2023-01-12 05:43:25,046 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode3_1  | 2023-01-12 05:43:25,794 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode3_1  | 2023-01-12 05:43:27,041 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode3_1  | 2023-01-12 05:43:28,840 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode3_1  | 2023-01-12 05:43:28,843 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode3_1  | 2023-01-12 05:43:30,251 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:789a46e3b060 ip:172.25.0.104
datanode3_1  | 2023-01-12 05:43:35,874 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode3_1  | 2023-01-12 05:43:37,187 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode3_1  | 2023-01-12 05:43:37,194 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode3_1  | 2023-01-12 05:43:40,257 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode3_1  | 2023-01-12 05:43:40,267 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode3_1  | 2023-01-12 05:43:40,271 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode3_1  | 2023-01-12 05:43:40,294 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode3_1  | 2023-01-12 05:43:49,596 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode3_1  | 2023-01-12 05:43:49,968 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.104,host:789a46e3b060
datanode3_1  | 2023-01-12 05:43:49,970 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode3_1  | 2023-01-12 05:43:50,002 [main] ERROR client.DNCertificateClient: Invalid domain 789a46e3b060
datanode3_1  | 2023-01-12 05:43:50,005 [main] INFO client.DNCertificateClient: Created csr for DN-> subject:dn@789a46e3b060
datanode3_1  | 2023-01-12 05:43:56,920 [main] INFO client.DNCertificateClient: Loading certificate from location:/data/metadata/dn/certs.
datanode3_1  | 2023-01-12 05:43:57,151 [main] INFO client.DNCertificateClient: Added certificate [
datanode3_1  | [
datanode3_1  |   Version: V3
datanode3_1  |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
datanode3_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode3_1  | 
datanode3_1  |   Key:  Sun RSA public key, 2048 bits
datanode3_1  |   params: null
datanode3_1  |   modulus: 30346053453237189834112040673541629567010306125791898570104054454047501736657892214383462264526301500298692652758375836429638347531164751981653358525504527825570400558838334901393005984876414719430775458652128247547444727818535668600972192623637356865083888753952717740665794517921055984453520353917733198649461590018542814322448272297366500121604394586840649019781675207082962097691558583700862118486739043197335879323701508955628436573724419921778589743880043303622162563304571704603236749325057266808761842228199352214967718782980075765678400986380731445934917806096546808808883868771850900183098234200323729287601
datanode3_1  |   public exponent: 65537
datanode3_1  |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
datanode3_1  |                To: Sun Feb 20 00:00:00 UTC 2028]
datanode3_1  |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
datanode3_1  |   SerialNumber: [    01]
datanode3_1  | 
datanode3_1  | Certificate Extensions: 3
datanode3_1  | [1]: ObjectId: 2.5.29.19 Criticality=true
datanode3_1  | BasicConstraints:[
datanode3_1  |   CA:true
datanode3_1  |   PathLen:2147483647
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | [2]: ObjectId: 2.5.29.15 Criticality=true
datanode3_1  | KeyUsage [
datanode3_1  |   Key_CertSign
datanode3_1  |   Crl_Sign
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | [3]: ObjectId: 2.5.29.17 Criticality=false
datanode3_1  | SubjectAlternativeName [
datanode3_1  |   IPAddress: 172.25.0.116
datanode3_1  |   DNSName: scm1.org
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | ]
datanode3_1  |   Algorithm: [SHA256withRSA]
datanode3_1  |   Signature:
datanode3_1  | 0000: 59 E4 14 CA 42 0B 71 BE   61 3A FA 41 71 F2 48 C6  Y...B.q.a:.Aq.H.
datanode3_1  | 0010: 35 3A DF E8 FE E3 74 A0   7E 57 15 0F F2 39 95 34  5:....t..W...9.4
datanode3_1  | 0020: E6 A7 48 CE 06 EA B9 42   CC AD DC 81 09 F1 F2 3E  ..H....B.......>
datanode3_1  | 0030: 94 27 78 5F 6B 04 F9 F4   E3 A1 71 61 8F 5A EF 6D  .'x_k.....qa.Z.m
datanode3_1  | 0040: 72 EA 9B CB 23 B9 31 9F   D7 B3 7B 86 35 09 CF D1  r...#.1.....5...
datanode3_1  | 0050: D6 81 A8 2E 21 01 F7 A9   B4 78 00 21 D5 3C 4A 61  ....!....x.!.<Ja
datanode3_1  | 0060: 7F 91 56 B9 50 6C 2C 23   0F D3 49 6E 80 28 7A 03  ..V.Pl,#..In.(z.
datanode1_1  |   modulus: 23606795785534949711499385186934038916293098040045092755606371305495771250193951433576886487345053020256311037161050386501364060510097808691699962227044890430503077722169441923100161726065841417518150604587458731157896656062294873357262278672135734502969067068962392994715158807705588975788966526798492203281980704377693412861269900206059432310738779447767300844513572886653823824634351489452030827159383004154639897918842578756791382644372985089452238874772273620393383345502701118852753223368876617842851611498889146985020323808240721946350054394988567399124997551901783132672881432868134080751963304883088870742299
datanode1_1  |   public exponent: 65537
datanode1_1  |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
datanode1_1  |                To: Fri Jan 12 00:00:00 UTC 2024]
datanode1_1  |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
datanode1_1  |   SerialNumber: [    01363b43 89be]
datanode1_1  | 
datanode1_1  | Certificate Extensions: 2
datanode1_1  | [1]: ObjectId: 2.5.29.15 Criticality=true
datanode1_1  | KeyUsage [
datanode1_1  |   DigitalSignature
datanode1_1  |   Key_Encipherment
datanode1_1  |   Data_Encipherment
datanode1_1  |   Key_Agreement
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | [2]: ObjectId: 2.5.29.17 Criticality=false
datanode1_1  | SubjectAlternativeName [
datanode1_1  |   IPAddress: 172.25.0.102
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | ]
datanode1_1  |   Algorithm: [SHA256withRSA]
datanode1_1  |   Signature:
datanode1_1  | 0000: 94 42 F8 E1 E7 F8 AA DD   91 1D 65 DD B0 F7 2D E6  .B........e...-.
datanode1_1  | 0010: EA 9F E6 6A 24 D7 41 A0   2E 76 60 CF 28 A7 B2 7D  ...j$.A..v`.(...
datanode1_1  | 0020: 57 BE BC 9E 78 A1 05 EC   4C CA EB 95 D8 E3 95 68  W...x...L......h
datanode1_1  | 0030: DB 28 02 F2 4A 22 07 BC   C2 2B 13 99 BA A0 A6 D0  .(..J"...+......
datanode1_1  | 0040: 29 AE 48 6D AF 57 90 28   BE 18 89 96 12 0E BE 91  ).Hm.W.(........
datanode1_1  | 0050: 1C 05 E3 BF 43 15 CF A9   16 46 06 C6 E6 D9 4C 36  ....C....F....L6
datanode1_1  | 0060: 0E 3F 43 DA 09 0B 22 F2   E0 12 C4 A2 1E 7F 08 B3  .?C...".........
datanode1_1  | 0070: 7F DE 61 6F 1A FE 9A 12   4D 55 8C 8A 8D DB 9A FD  ..ao....MU......
datanode1_1  | 0080: D0 4E 6F B1 6F EF AF 7E   60 4E 57 A9 E3 C2 1A 8E  .No.o...`NW.....
datanode1_1  | 0090: A7 AD AD 41 34 3B FD 14   8E 3C AC 2E 05 F0 25 64  ...A4;...<....%d
datanode1_1  | 00A0: 54 A4 FF 12 95 E7 64 2A   9D FB BF 12 A0 33 4B 15  T.....d*.....3K.
datanode1_1  | 00B0: 8A B9 6F F4 2E AF C8 9C   D3 37 7A C8 20 7E 6D C5  ..o......7z. .m.
datanode1_1  | 00C0: 9C 1C 7C 8E B1 CD 74 90   1D 3A CA B7 DB 74 3B 02  ......t..:...t;.
datanode1_1  | 00D0: F0 B1 21 73 5C 76 7E 1C   9E 5C 50 7E E7 C0 73 08  ..!s\v...\P...s.
datanode1_1  | 00E0: 6E BE 8D 5D 3B 0A C3 AD   FE 24 00 70 05 1D 8F FA  n..];....$.p....
datanode1_1  | 00F0: 3C 56 D3 A3 B3 DE E6 BA   72 92 DE AD F8 E8 83 E9  <V......r.......
datanode1_1  | 
datanode1_1  | ] from file:/data/metadata/dn/certs/1332434143678.crt.
datanode1_1  | 2023-01-12 05:43:57,740 [main] INFO client.DNCertificateClient: Added certificate [
datanode1_1  | [
datanode1_1  |   Version: V3
datanode1_1  |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
datanode1_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode1_1  | 
datanode1_1  |   Key:  Sun RSA public key, 2048 bits
datanode1_1  |   params: null
datanode1_1  |   modulus: 21838495709719174090944017025941967908224182768319498180393936128987176221476088419281179276073314404186963320459954621839794791804718496604717042740409331050670636901380744918424604519064284257795379580082372800428810093145199831557386718853872621914973921810155992135096086488011377748521113854732595726272396205530150917174127452444863380225901746740157534773765777653459292048522370651962381319687098813297977743208706305312169254062598162002740262036220188710005290347458814949877767522524881554322161686065229152643489368945279202993130114091164740627711162880068229981146789996366091625523376604972129960509039
datanode1_1  |   public exponent: 65537
datanode1_1  |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
datanode1_1  |                To: Sun Feb 20 00:00:00 UTC 2028]
datanode1_1  |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
datanode1_1  |   SerialNumber: [    011a69de ba83]
datanode1_1  | 
datanode1_1  | Certificate Extensions: 3
datanode1_1  | [1]: ObjectId: 2.5.29.19 Criticality=true
datanode1_1  | BasicConstraints:[
datanode1_1  |   CA:true
datanode1_1  |   PathLen:2147483647
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | [2]: ObjectId: 2.5.29.15 Criticality=true
datanode1_1  | KeyUsage [
datanode1_1  |   DigitalSignature
datanode1_1  |   Key_Encipherment
datanode1_1  |   Data_Encipherment
datanode1_1  |   Key_Agreement
datanode1_1  |   Key_CertSign
datanode1_1  |   Crl_Sign
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | [3]: ObjectId: 2.5.29.17 Criticality=false
datanode1_1  | SubjectAlternativeName [
datanode1_1  |   IPAddress: 172.25.0.116
datanode1_1  |   DNSName: scm1.org
datanode1_1  | ]
datanode1_1  | 
datanode1_1  | ]
datanode1_1  |   Algorithm: [SHA256withRSA]
datanode1_1  |   Signature:
datanode1_1  | 0000: AB 5F B6 3B B0 5C 88 AB   DD 25 28 3F F6 7C 1A 59  ._.;.\...%(?...Y
datanode1_1  | 0010: AC B5 48 EF C8 75 0D 95   65 13 6C C4 5D 9A 7D D4  ..H..u..e.l.]...
datanode1_1  | 0020: 93 AF C2 9C CA DD 42 6C   C6 2D B6 5D EE CC 9A F0  ......Bl.-.]....
datanode1_1  | 0030: 95 DD 7D 3C D8 E8 28 A1   B8 09 E1 BA E1 DD 81 7A  ...<..(........z
datanode1_1  | 0040: 66 DB F9 45 B9 70 8B 0D   3A 5C A7 86 6A 8C F7 A7  f..E.p..:\..j...
datanode1_1  | 0050: E8 12 BD F5 3C 4C 8A 79   F9 53 EB 48 F0 23 70 F9  ....<L.y.S.H.#p.
datanode1_1  | 0060: 0F CC 68 07 FB 60 A9 BB   E7 35 31 EE D3 42 6A 9A  ..h..`...51..Bj.
datanode1_1  | 0070: 57 01 2A 55 0E E3 37 79   D8 76 24 C2 48 70 6F EB  W.*U..7y.v$.Hpo.
datanode1_1  | 0080: 61 A9 C9 E5 DC 38 A2 BF   D8 E8 6F 36 A5 D1 AB 3A  a....8....o6...:
datanode1_1  | 0090: 96 38 C7 8C 74 F7 01 E9   73 1E 4E 1D F8 E9 FA A8  .8..t...s.N.....
datanode1_1  | 00A0: 61 56 DA 60 4C 30 BF 35   59 8D 3B 69 0B 81 8D A5  aV.`L0.5Y.;i....
datanode1_1  | 00B0: 6B 1F 89 42 C0 DF B3 8D   F3 50 25 03 65 F9 37 86  k..B.....P%.e.7.
datanode1_1  | 00C0: 03 54 EB EC 49 05 E6 8D   AD B3 7E 57 37 64 8C 60  .T..I......W7d.`
datanode1_1  | 00D0: E2 61 4E 15 16 4B FC C7   AA D8 F7 E0 CC 71 75 DA  .aN..K.......qu.
datanode1_1  | 00E0: 6F A8 66 F2 98 35 28 51   F9 FC 7B F3 90 FD 46 64  o.f..5(Q......Fd
datanode1_1  | 00F0: D3 1C BD 33 23 88 5C 9A   F7 A8 F9 0D C0 CD 9B 86  ...3#.\.........
datanode1_1  | 
datanode1_1  | ] from file:/data/metadata/dn/certs/CA-1212956981891.crt.
datanode1_1  | 2023-01-12 05:43:57,784 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor is started with first delay 29096162256 ms and interval 86400000 ms.
datanode1_1  | 2023-01-12 05:43:57,785 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode1_1  | 2023-01-12 05:43:57,944 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode1_1  | 2023-01-12 05:43:59,651 [main] INFO reflections.Reflections: Reflections took 1336 ms to scan 2 urls, producing 97 keys and 217 values 
datanode1_1  | 2023-01-12 05:44:00,439 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode1_1  | 2023-01-12 05:44:02,118 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode1_1  | 2023-01-12 05:44:02,257 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode1_1  | 2023-01-12 05:44:02,302 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode1_1  | 2023-01-12 05:44:02,323 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode1_1  | 2023-01-12 05:44:02,710 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode1_1  | 2023-01-12 05:44:02,860 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2023-01-12 05:44:02,867 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode3_1  | 0070: 32 46 42 54 4F 22 06 7F   B1 7D 41 A0 09 27 CF 7A  2FBTO"....A..'.z
datanode3_1  | 0080: F0 82 C6 00 70 41 5D 8F   83 5F E6 EB 74 1F B7 A2  ....pA].._..t...
datanode3_1  | 0090: 85 64 3E AE D6 66 71 7C   BE 7F 92 D0 79 64 71 C0  .d>..fq.....ydq.
datanode3_1  | 00A0: 8F 76 8C F6 D1 A4 67 C4   F4 11 8D 75 FC DE EF EE  .v....g....u....
datanode3_1  | 00B0: D9 7F 75 F3 5F AC 76 DF   23 F9 19 99 25 F9 31 8E  ..u._.v.#...%.1.
datanode3_1  | 00C0: AA 69 DC 7B 7E 4E 62 10   80 B9 5E 00 F8 1D 16 71  .i...Nb...^....q
datanode3_1  | 00D0: 7F D4 C7 FD 20 63 0B 0A   7E 36 7D F4 4C 85 A7 61  .... c...6..L..a
datanode3_1  | 00E0: E7 00 61 A2 83 40 78 92   3E A1 9A E0 B8 E4 A1 A0  ..a..@x.>.......
datanode3_1  | 00F0: 45 B5 33 7C 03 24 98 E7   DE 69 74 45 16 2B 06 3F  E.3..$...itE.+.?
datanode3_1  | 
datanode3_1  | ] from file:/data/metadata/dn/certs/ROOTCA-1.crt.
datanode3_1  | 2023-01-12 05:43:57,222 [main] INFO client.DNCertificateClient: Added certificate [
datanode3_1  | [
datanode3_1  |   Version: V3
datanode3_1  |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=dn@789a46e3b060
datanode3_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode3_1  | 
datanode3_1  |   Key:  Sun RSA public key, 2048 bits
datanode3_1  |   params: null
datanode2_1  |   Key:  Sun RSA public key, 2048 bits
datanode2_1  |   params: null
datanode2_1  |   modulus: 23201305681597008257441552980714446864227047622193085231828079614607088092741787181456003738161263402155442983571548132773255943152005050068966973663659767193656780011307984441674251457474152292873850199065824789279323813520238838082934786737234452100994736820579860864926562266794283721254545909260412793437455623301230440918896204694279299876075792209086392129156527147062244752843606839203817668612298959763888968107798161020878717210290703327709183328476642322572222088482637239233209240894122790182328496810144691187595194502015126954732647964187474869689407604092900991587802310612814152621806519964506554794939
datanode2_1  |   public exponent: 65537
datanode2_1  |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
datanode2_1  |                To: Fri Jan 12 00:00:00 UTC 2024]
datanode2_1  |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
datanode2_1  |   SerialNumber: [    013654d8 0d13]
datanode2_1  | 
datanode2_1  | Certificate Extensions: 2
datanode2_1  | [1]: ObjectId: 2.5.29.15 Criticality=true
datanode2_1  | KeyUsage [
datanode2_1  |   DigitalSignature
datanode3_1  |   modulus: 24152557481584161759665068641785552786850725431806030918792758899434827269650704903842021018056836331807501960302774748432071067407432330347790584887621042144646780123177244045017197961192172117371035632848220481452888209683844751828800264810713670264503230380447730680813328650925334460677228698752460697074251193329625569064797811548739627303578848621528166903071647145226876805793403500373487162808638890979931786886849554304665683788145426191240690919754522840102131528789252323866957634393691348689449877036444099446478065160805152730993965213577706967620824667443339606323237441312995819550709068163074277578069
datanode3_1  |   public exponent: 65537
datanode3_1  |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
datanode3_1  |                To: Fri Jan 12 00:00:00 UTC 2024]
datanode3_1  |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
datanode3_1  |   SerialNumber: [    013603c8 bb80]
datanode3_1  | 
datanode3_1  | Certificate Extensions: 2
datanode3_1  | [1]: ObjectId: 2.5.29.15 Criticality=true
datanode3_1  | KeyUsage [
datanode3_1  |   DigitalSignature
datanode3_1  |   Key_Encipherment
datanode3_1  |   Data_Encipherment
datanode3_1  |   Key_Agreement
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | [2]: ObjectId: 2.5.29.17 Criticality=false
datanode3_1  | SubjectAlternativeName [
datanode3_1  |   IPAddress: 172.25.0.104
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | ]
datanode3_1  |   Algorithm: [SHA256withRSA]
datanode3_1  |   Signature:
datanode3_1  | 0000: 6D 7C 1C 92 64 1C 7D 8F   AD 8E 79 E0 3E 0F E7 62  m...d.....y.>..b
datanode3_1  | 0010: A3 C8 1D 6C 0E 9C A0 C6   2D 94 DA D8 90 4F D4 3D  ...l....-....O.=
datanode3_1  | 0020: 74 EC 0F 4B 3B 78 F5 A7   95 5E 63 A2 27 C6 85 19  t..K;x...^c.'...
datanode3_1  | 0030: FE 56 56 80 2D C9 FB 24   34 E7 99 B1 E2 83 3F 6C  .VV.-..$4.....?l
datanode3_1  | 0040: 6C 41 EF 65 11 B7 A5 15   E4 D7 63 19 7F 2D BE 38  lA.e......c..-.8
datanode3_1  | 0050: 09 58 04 91 DF 61 B0 94   A4 EC 41 C0 E9 80 74 FB  .X...a....A...t.
datanode3_1  | 0060: 89 1D 4C FD FD 29 6F 5E   09 13 72 3C A4 4F 46 57  ..L..)o^..r<.OFW
datanode3_1  | 0070: 01 3F 7C E9 EF 2A 4A 65   A1 9D 6F 27 67 CF 70 7F  .?...*Je..o'g.p.
datanode3_1  | 0080: AC 20 41 2A 4E 76 95 7F   BF BD 11 DF BA FD B9 2E  . A*Nv..........
datanode3_1  | 0090: 4E 66 90 D3 2D B8 58 11   3B EF 75 CB E8 9A BB 1A  Nf..-.X.;.u.....
datanode3_1  | 00A0: BE 46 5F 15 A6 A0 B5 61   2E 62 1F 80 C8 A6 F0 0F  .F_....a.b......
datanode3_1  | 00B0: 79 EE C5 DC B0 69 DB 23   15 EA 08 F1 C9 F6 5A 22  y....i.#......Z"
datanode3_1  | 00C0: 1A CA 9C E0 96 7F BD 66   96 A2 30 75 28 23 DA C4  .......f..0u(#..
datanode3_1  | 00D0: 21 1B 17 36 1F 86 9A FA   2C 3B 48 9B 27 C5 15 6D  !..6....,;H.'..m
datanode3_1  | 00E0: CB 39 8B CE 41 85 91 F5   BD B9 7A 0C 1D F3 48 DA  .9..A.....z...H.
datanode3_1  | 00F0: 1A 01 FE 18 3E B2 21 78   CE 63 D2 0D 0E F1 C9 58  ....>.!x.c.....X
datanode3_1  | 
datanode3_1  | ] from file:/data/metadata/dn/certs/1331503348608.crt.
datanode3_1  | 2023-01-12 05:43:57,267 [main] INFO client.DNCertificateClient: Added certificate [
datanode3_1  | [
datanode3_1  |   Version: V3
datanode3_1  |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
datanode3_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode3_1  | 
datanode3_1  |   Key:  Sun RSA public key, 2048 bits
datanode3_1  |   params: null
datanode3_1  |   modulus: 21838495709719174090944017025941967908224182768319498180393936128987176221476088419281179276073314404186963320459954621839794791804718496604717042740409331050670636901380744918424604519064284257795379580082372800428810093145199831557386718853872621914973921810155992135096086488011377748521113854732595726272396205530150917174127452444863380225901746740157534773765777653459292048522370651962381319687098813297977743208706305312169254062598162002740262036220188710005290347458814949877767522524881554322161686065229152643489368945279202993130114091164740627711162880068229981146789996366091625523376604972129960509039
datanode3_1  |   public exponent: 65537
datanode3_1  |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
datanode3_1  |                To: Sun Feb 20 00:00:00 UTC 2028]
datanode3_1  |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
datanode3_1  |   SerialNumber: [    011a69de ba83]
datanode3_1  | 
datanode3_1  | Certificate Extensions: 3
datanode3_1  | [1]: ObjectId: 2.5.29.19 Criticality=true
datanode3_1  | BasicConstraints:[
datanode3_1  |   CA:true
datanode3_1  |   PathLen:2147483647
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | [2]: ObjectId: 2.5.29.15 Criticality=true
datanode2_1  |   Key_Encipherment
datanode2_1  |   Data_Encipherment
datanode2_1  |   Key_Agreement
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | [2]: ObjectId: 2.5.29.17 Criticality=false
datanode2_1  | SubjectAlternativeName [
datanode2_1  |   IPAddress: 172.25.0.103
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | ]
datanode2_1  |   Algorithm: [SHA256withRSA]
datanode2_1  |   Signature:
datanode2_1  | 0000: 04 48 92 71 14 97 7F 38   B8 24 DF F7 75 AE 5F 85  .H.q...8.$..u._.
datanode2_1  | 0010: 8E A5 9A 16 BB D9 37 92   E7 50 0B 20 2C 13 E9 2A  ......7..P. ,..*
datanode2_1  | 0020: A4 73 70 F6 8B BA F1 DC   4E 9C 69 A9 D5 06 84 76  .sp.....N.i....v
datanode2_1  | 0030: D6 67 B8 23 C5 36 C3 AD   3E 52 9D F7 04 3E 6E DE  .g.#.6..>R...>n.
datanode2_1  | 0040: 56 45 4E B1 26 77 C3 61   C5 D9 57 88 C5 1A D0 D3  VEN.&w.a..W.....
datanode2_1  | 0050: B6 2A 7F F5 17 16 04 9E   9F 3D C8 5D B1 EE 71 EE  .*.......=.]..q.
datanode2_1  | 0060: 18 99 6F 0C E7 81 C5 9C   63 CC 6C 69 B2 EE 00 F9  ..o.....c.li....
datanode2_1  | 0070: 62 29 31 A7 64 50 97 BE   76 99 36 F9 09 AE 6A 0F  b)1.dP..v.6...j.
datanode2_1  | 0080: 9C 93 96 4E 90 2B E3 6E   B1 83 63 99 20 C2 AB B5  ...N.+.n..c. ...
datanode2_1  | 0090: CD CF 2D 85 9E 10 66 D5   FD 86 47 CB 0C 8E 89 23  ..-...f...G....#
datanode2_1  | 00A0: 20 28 E2 81 B1 1F 39 82   7A FA B1 1E AE 50 35 9E   (....9.z....P5.
datanode2_1  | 00B0: 9F 22 C3 8C 87 AC EF 38   87 D4 EC 39 9B C9 27 D7  .".....8...9..'.
datanode2_1  | 00C0: 86 0A F7 0D E4 23 9E EA   B1 55 89 CC 35 52 1A C0  .....#...U..5R..
datanode2_1  | 00D0: E8 15 67 BE 42 BC F1 B8   37 B4 E0 D9 7E 1B 51 2D  ..g.B...7.....Q-
datanode2_1  | 00E0: 9F E8 62 36 B7 6E 27 30   B6 77 A9 14 5E 42 E2 00  ..b6.n'0.w..^B..
datanode2_1  | 00F0: 43 68 75 9B 1C 71 12 BD   D2 F4 DE 64 65 94 9E 30  Chu..q.....de..0
datanode2_1  | 
datanode2_1  | ] from file:/data/metadata/dn/certs/1332863307027.crt.
datanode2_1  | 2023-01-12 05:43:58,562 [main] INFO client.DNCertificateClient: Added certificate [
datanode2_1  | [
datanode2_1  |   Version: V3
datanode2_1  |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
datanode2_1  |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
datanode2_1  | 
datanode2_1  |   Key:  Sun RSA public key, 2048 bits
datanode2_1  |   params: null
datanode2_1  |   modulus: 21838495709719174090944017025941967908224182768319498180393936128987176221476088419281179276073314404186963320459954621839794791804718496604717042740409331050670636901380744918424604519064284257795379580082372800428810093145199831557386718853872621914973921810155992135096086488011377748521113854732595726272396205530150917174127452444863380225901746740157534773765777653459292048522370651962381319687098813297977743208706305312169254062598162002740262036220188710005290347458814949877767522524881554322161686065229152643489368945279202993130114091164740627711162880068229981146789996366091625523376604972129960509039
datanode2_1  |   public exponent: 65537
datanode2_1  |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
datanode2_1  |                To: Sun Feb 20 00:00:00 UTC 2028]
datanode2_1  |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
datanode2_1  |   SerialNumber: [    011a69de ba83]
datanode2_1  | 
datanode2_1  | Certificate Extensions: 3
datanode2_1  | [1]: ObjectId: 2.5.29.19 Criticality=true
datanode2_1  | BasicConstraints:[
datanode2_1  |   CA:true
datanode2_1  |   PathLen:2147483647
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | [2]: ObjectId: 2.5.29.15 Criticality=true
datanode2_1  | KeyUsage [
datanode2_1  |   DigitalSignature
datanode2_1  |   Key_Encipherment
datanode2_1  |   Data_Encipherment
datanode2_1  |   Key_Agreement
datanode2_1  |   Key_CertSign
datanode2_1  |   Crl_Sign
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | [3]: ObjectId: 2.5.29.17 Criticality=false
datanode2_1  | SubjectAlternativeName [
datanode2_1  |   IPAddress: 172.25.0.116
datanode2_1  |   DNSName: scm1.org
datanode2_1  | ]
datanode2_1  | 
datanode2_1  | ]
datanode2_1  |   Algorithm: [SHA256withRSA]
datanode2_1  |   Signature:
datanode2_1  | 0000: AB 5F B6 3B B0 5C 88 AB   DD 25 28 3F F6 7C 1A 59  ._.;.\...%(?...Y
datanode2_1  | 0010: AC B5 48 EF C8 75 0D 95   65 13 6C C4 5D 9A 7D D4  ..H..u..e.l.]...
datanode2_1  | 0020: 93 AF C2 9C CA DD 42 6C   C6 2D B6 5D EE CC 9A F0  ......Bl.-.]....
datanode2_1  | 0030: 95 DD 7D 3C D8 E8 28 A1   B8 09 E1 BA E1 DD 81 7A  ...<..(........z
datanode2_1  | 0040: 66 DB F9 45 B9 70 8B 0D   3A 5C A7 86 6A 8C F7 A7  f..E.p..:\..j...
datanode2_1  | 0050: E8 12 BD F5 3C 4C 8A 79   F9 53 EB 48 F0 23 70 F9  ....<L.y.S.H.#p.
datanode2_1  | 0060: 0F CC 68 07 FB 60 A9 BB   E7 35 31 EE D3 42 6A 9A  ..h..`...51..Bj.
datanode2_1  | 0070: 57 01 2A 55 0E E3 37 79   D8 76 24 C2 48 70 6F EB  W.*U..7y.v$.Hpo.
datanode2_1  | 0080: 61 A9 C9 E5 DC 38 A2 BF   D8 E8 6F 36 A5 D1 AB 3A  a....8....o6...:
datanode2_1  | 0090: 96 38 C7 8C 74 F7 01 E9   73 1E 4E 1D F8 E9 FA A8  .8..t...s.N.....
datanode2_1  | 00A0: 61 56 DA 60 4C 30 BF 35   59 8D 3B 69 0B 81 8D A5  aV.`L0.5Y.;i....
datanode2_1  | 00B0: 6B 1F 89 42 C0 DF B3 8D   F3 50 25 03 65 F9 37 86  k..B.....P%.e.7.
datanode2_1  | 00C0: 03 54 EB EC 49 05 E6 8D   AD B3 7E 57 37 64 8C 60  .T..I......W7d.`
datanode2_1  | 00D0: E2 61 4E 15 16 4B FC C7   AA D8 F7 E0 CC 71 75 DA  .aN..K.......qu.
datanode2_1  | 00E0: 6F A8 66 F2 98 35 28 51   F9 FC 7B F3 90 FD 46 64  o.f..5(Q......Fd
datanode2_1  | 00F0: D3 1C BD 33 23 88 5C 9A   F7 A8 F9 0D C0 CD 9B 86  ...3#.\.........
datanode2_1  | 
datanode2_1  | ] from file:/data/metadata/dn/certs/CA-1212956981891.crt.
datanode1_1  | 2023-01-12 05:44:02,874 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode1_1  | 2023-01-12 05:44:02,876 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode1_1  | 2023-01-12 05:44:02,877 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode1_1  | 2023-01-12 05:44:03,285 [Thread-8] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode1_1  | 2023-01-12 05:44:03,297 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode1_1  | 2023-01-12 05:44:10,356 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode1_1  | 2023-01-12 05:44:11,743 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode1_1  | 2023-01-12 05:44:11,934 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode1_1  | 2023-01-12 05:44:12,501 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2023-01-12 05:44:13,712 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode1_1  | 2023-01-12 05:44:14,823 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode1_1  | 2023-01-12 05:44:14,849 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode1_1  | 2023-01-12 05:44:14,858 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode1_1  | 2023-01-12 05:44:14,864 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode1_1  | 2023-01-12 05:44:14,868 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode1_1  | 2023-01-12 05:44:14,870 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode1_1  | 2023-01-12 05:44:14,880 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode1_1  | 2023-01-12 05:44:14,913 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-12 05:44:14,925 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode1_1  | 2023-01-12 05:44:14,928 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2023-01-12 05:44:15,132 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode1_1  | 2023-01-12 05:44:15,217 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode1_1  | 2023-01-12 05:44:15,221 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode1_1  | 2023-01-12 05:44:24,252 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode1_1  | 2023-01-12 05:44:24,428 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode1_1  | 2023-01-12 05:44:24,439 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode1_1  | 2023-01-12 05:44:24,448 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode1_1  | 2023-01-12 05:44:24,451 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode1_1  | 2023-01-12 05:44:24,488 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode1_1  | 2023-01-12 05:44:24,502 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode1_1  | 2023-01-12 05:44:24,570 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode1_1  | 2023-01-12 05:44:24,580 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode1_1  | 2023-01-12 05:44:24,974 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode1_1  | 2023-01-12 05:44:24,978 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode1_1  | 2023-01-12 05:44:25,520 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode1_1  | 2023-01-12 05:44:25,526 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode1_1  | 2023-01-12 05:44:25,533 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-01-12 05:44:25,537 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-01-12 05:44:25,620 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-01-12 05:44:25,728 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x32c993c6] REGISTERED
datanode1_1  | 2023-01-12 05:44:25,737 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x32c993c6] BIND: 0.0.0.0/0.0.0.0:0
datanode1_1  | 2023-01-12 05:44:25,789 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x32c993c6, L:/0.0.0.0:36079] ACTIVE
datanode1_1  | 2023-01-12 05:44:26,246 [main] INFO ssl.PemFileBasedKeyStoresFactory: SERVER KeyStore reloading at 60000 millis.
datanode1_1  | 2023-01-12 05:44:26,294 [main] INFO ssl.PemFileBasedKeyStoresFactory: SERVER TrustStore reloading at 60000 millis.
datanode1_1  | 2023-01-12 05:44:26,352 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode1_1  | 2023-01-12 05:44:27,674 [main] INFO token.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
datanode1_1  | 2023-01-12 05:44:27,738 [main] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
datanode1_1  | 2023-01-12 05:44:28,426 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode1_1  | 2023-01-12 05:44:28,428 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode1_1  | 2023-01-12 05:44:28,430 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode1_1  | 2023-01-12 05:44:28,716 [main] INFO util.log: Logging initialized @78532ms to org.eclipse.jetty.util.log.Slf4jLog
datanode1_1  | 2023-01-12 05:44:30,134 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode1_1  | 2023-01-12 05:44:30,234 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode1_1  | 2023-01-12 05:44:30,264 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode1_1  | 2023-01-12 05:44:30,264 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode1_1  | 2023-01-12 05:44:30,273 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode3_1  | KeyUsage [
datanode3_1  |   DigitalSignature
datanode3_1  |   Key_Encipherment
datanode3_1  |   Data_Encipherment
datanode3_1  |   Key_Agreement
datanode3_1  |   Key_CertSign
datanode3_1  |   Crl_Sign
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | [3]: ObjectId: 2.5.29.17 Criticality=false
datanode3_1  | SubjectAlternativeName [
datanode3_1  |   IPAddress: 172.25.0.116
datanode3_1  |   DNSName: scm1.org
datanode3_1  | ]
datanode3_1  | 
datanode3_1  | ]
datanode3_1  |   Algorithm: [SHA256withRSA]
datanode3_1  |   Signature:
datanode3_1  | 0000: AB 5F B6 3B B0 5C 88 AB   DD 25 28 3F F6 7C 1A 59  ._.;.\...%(?...Y
datanode3_1  | 0010: AC B5 48 EF C8 75 0D 95   65 13 6C C4 5D 9A 7D D4  ..H..u..e.l.]...
datanode3_1  | 0020: 93 AF C2 9C CA DD 42 6C   C6 2D B6 5D EE CC 9A F0  ......Bl.-.]....
datanode3_1  | 0030: 95 DD 7D 3C D8 E8 28 A1   B8 09 E1 BA E1 DD 81 7A  ...<..(........z
datanode3_1  | 0040: 66 DB F9 45 B9 70 8B 0D   3A 5C A7 86 6A 8C F7 A7  f..E.p..:\..j...
datanode3_1  | 0050: E8 12 BD F5 3C 4C 8A 79   F9 53 EB 48 F0 23 70 F9  ....<L.y.S.H.#p.
datanode3_1  | 0060: 0F CC 68 07 FB 60 A9 BB   E7 35 31 EE D3 42 6A 9A  ..h..`...51..Bj.
datanode3_1  | 0070: 57 01 2A 55 0E E3 37 79   D8 76 24 C2 48 70 6F EB  W.*U..7y.v$.Hpo.
datanode3_1  | 0080: 61 A9 C9 E5 DC 38 A2 BF   D8 E8 6F 36 A5 D1 AB 3A  a....8....o6...:
datanode3_1  | 0090: 96 38 C7 8C 74 F7 01 E9   73 1E 4E 1D F8 E9 FA A8  .8..t...s.N.....
datanode3_1  | 00A0: 61 56 DA 60 4C 30 BF 35   59 8D 3B 69 0B 81 8D A5  aV.`L0.5Y.;i....
datanode3_1  | 00B0: 6B 1F 89 42 C0 DF B3 8D   F3 50 25 03 65 F9 37 86  k..B.....P%.e.7.
datanode3_1  | 00C0: 03 54 EB EC 49 05 E6 8D   AD B3 7E 57 37 64 8C 60  .T..I......W7d.`
datanode3_1  | 00D0: E2 61 4E 15 16 4B FC C7   AA D8 F7 E0 CC 71 75 DA  .aN..K.......qu.
datanode3_1  | 00E0: 6F A8 66 F2 98 35 28 51   F9 FC 7B F3 90 FD 46 64  o.f..5(Q......Fd
datanode3_1  | 00F0: D3 1C BD 33 23 88 5C 9A   F7 A8 F9 0D C0 CD 9B 86  ...3#.\.........
datanode3_1  | 
datanode3_1  | ] from file:/data/metadata/dn/certs/CA-1212956981891.crt.
datanode3_1  | 2023-01-12 05:43:57,314 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor is started with first delay 29096162718 ms and interval 86400000 ms.
datanode3_1  | 2023-01-12 05:43:57,315 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode3_1  | 2023-01-12 05:43:57,534 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode3_1  | 2023-01-12 05:43:59,086 [main] INFO reflections.Reflections: Reflections took 1175 ms to scan 2 urls, producing 97 keys and 217 values 
datanode3_1  | 2023-01-12 05:43:59,662 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode3_1  | 2023-01-12 05:44:01,327 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode3_1  | 2023-01-12 05:44:01,553 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode3_1  | 2023-01-12 05:44:01,604 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode3_1  | 2023-01-12 05:44:01,617 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode3_1  | 2023-01-12 05:44:02,120 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode3_1  | 2023-01-12 05:44:02,332 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2023-01-12 05:44:02,362 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode3_1  | 2023-01-12 05:44:02,375 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode3_1  | 2023-01-12 05:44:02,376 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode3_1  | 2023-01-12 05:44:02,378 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode3_1  | 2023-01-12 05:44:02,772 [Thread-8] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode3_1  | 2023-01-12 05:44:02,787 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode3_1  | 2023-01-12 05:44:09,994 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode3_1  | 2023-01-12 05:44:11,217 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode3_1  | 2023-01-12 05:44:11,298 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode3_1  | 2023-01-12 05:44:11,698 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2023-01-12 05:44:12,782 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode3_1  | 2023-01-12 05:44:13,823 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode3_1  | 2023-01-12 05:44:13,845 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode3_1  | 2023-01-12 05:44:13,856 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode3_1  | 2023-01-12 05:44:13,856 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode3_1  | 2023-01-12 05:44:13,859 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode3_1  | 2023-01-12 05:44:13,867 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode3_1  | 2023-01-12 05:44:13,879 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode3_1  | 2023-01-12 05:44:13,886 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-12 05:44:13,895 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode3_1  | 2023-01-12 05:44:13,900 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-01-12 05:44:14,049 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode3_1  | 2023-01-12 05:44:14,105 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode3_1  | 2023-01-12 05:44:14,116 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode3_1  | 2023-01-12 05:44:22,710 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode3_1  | 2023-01-12 05:44:22,873 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode3_1  | 2023-01-12 05:44:22,891 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode3_1  | 2023-01-12 05:44:22,899 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode3_1  | 2023-01-12 05:44:22,901 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode3_1  | 2023-01-12 05:44:22,936 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode3_1  | 2023-01-12 05:44:22,955 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode3_1  | 2023-01-12 05:44:23,006 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode3_1  | 2023-01-12 05:44:23,021 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode3_1  | 2023-01-12 05:44:23,753 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode3_1  | 2023-01-12 05:44:23,762 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode3_1  | 2023-01-12 05:44:24,264 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode3_1  | 2023-01-12 05:44:24,277 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode3_1  | 2023-01-12 05:44:24,277 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-01-12 05:44:24,277 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-01-12 05:44:24,320 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-01-12 05:44:24,402 [75176c82-cf64-45b0-aad5-7780ce7245eb-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xf11669a1] REGISTERED
datanode3_1  | 2023-01-12 05:44:24,429 [75176c82-cf64-45b0-aad5-7780ce7245eb-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xf11669a1] BIND: 0.0.0.0/0.0.0.0:0
datanode3_1  | 2023-01-12 05:44:24,452 [75176c82-cf64-45b0-aad5-7780ce7245eb-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xf11669a1, L:/0.0.0.0:45895] ACTIVE
datanode3_1  | 2023-01-12 05:44:24,924 [main] INFO ssl.PemFileBasedKeyStoresFactory: SERVER KeyStore reloading at 60000 millis.
datanode3_1  | 2023-01-12 05:44:25,017 [main] INFO ssl.PemFileBasedKeyStoresFactory: SERVER TrustStore reloading at 60000 millis.
datanode3_1  | 2023-01-12 05:44:25,144 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode3_1  | 2023-01-12 05:44:26,532 [main] INFO token.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
datanode3_1  | 2023-01-12 05:44:26,561 [main] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
datanode3_1  | 2023-01-12 05:44:27,093 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode3_1  | 2023-01-12 05:44:27,094 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode3_1  | 2023-01-12 05:44:27,094 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode3_1  | 2023-01-12 05:44:27,427 [main] INFO util.log: Logging initialized @76530ms to org.eclipse.jetty.util.log.Slf4jLog
datanode3_1  | 2023-01-12 05:44:28,673 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode3_1  | 2023-01-12 05:44:28,731 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode3_1  | 2023-01-12 05:44:28,747 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode3_1  | 2023-01-12 05:44:28,747 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode3_1  | 2023-01-12 05:44:28,759 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode3_1  | 2023-01-12 05:44:28,782 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode3_1  | 2023-01-12 05:44:29,160 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode3_1  | 2023-01-12 05:44:29,178 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode1_1  | 2023-01-12 05:44:30,299 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode1_1  | 2023-01-12 05:44:30,743 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode1_1  | 2023-01-12 05:44:30,762 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode1_1  | 2023-01-12 05:44:31,144 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode1_1  | 2023-01-12 05:44:31,144 [main] INFO server.session: No SessionScavenger set, using defaults
datanode1_1  | 2023-01-12 05:44:31,169 [main] INFO server.session: node0 Scavenging every 660000ms
datanode1_1  | 2023-01-12 05:44:31,500 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2023-01-12 05:44:31,517 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2afd8972{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode1_1  | 2023-01-12 05:44:31,524 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6ccf06f1{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode1_1  | 2023-01-12 05:44:32,352 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2023-01-12 05:44:32,503 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@434c179e{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-13464406422036600197/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode1_1  | 2023-01-12 05:44:32,593 [main] INFO server.AbstractConnector: Started ServerConnector@9930ff6{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode1_1  | 2023-01-12 05:44:32,594 [main] INFO server.Server: Started @82410ms
datanode1_1  | 2023-01-12 05:44:32,648 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode1_1  | 2023-01-12 05:44:32,648 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode1_1  | 2023-01-12 05:44:32,656 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode1_1  | 2023-01-12 05:44:32,681 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode1_1  | 2023-01-12 05:44:32,995 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2a3a1462] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode1_1  | 2023-01-12 05:44:33,779 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode1_1  | 2023-01-12 05:44:33,844 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode1_1  | 2023-01-12 05:44:37,467 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-dd9a252d-f4dd-4162-963a-067d6dacb49f/DS-21e21131-aa1a-43a0-a6f9-ada747a56bff/container.db to cache
datanode1_1  | 2023-01-12 05:44:37,485 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-dd9a252d-f4dd-4162-963a-067d6dacb49f/DS-21e21131-aa1a-43a0-a6f9-ada747a56bff/container.db for volume DS-21e21131-aa1a-43a0-a6f9-ada747a56bff
datanode1_1  | 2023-01-12 05:44:37,527 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode1_1  | 2023-01-12 05:44:37,641 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode1_1  | 2023-01-12 05:44:38,013 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode1_1  | 2023-01-12 05:44:38,053 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
datanode1_1  | 2023-01-12 05:44:38,452 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.RaftServer: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: start RPC server
datanode1_1  | 2023-01-12 05:44:38,516 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: GrpcService started, listening on 9858
datanode1_1  | 2023-01-12 05:44:38,529 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: GrpcService started, listening on 9856
datanode1_1  | 2023-01-12 05:44:38,538 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: GrpcService started, listening on 9857
datanode1_1  | 2023-01-12 05:44:38,603 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3 is started using port 9858 for RATIS
datanode1_1  | 2023-01-12 05:44:38,603 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3 is started using port 9857 for RATIS_ADMIN
datanode1_1  | 2023-01-12 05:44:38,603 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3 is started using port 9856 for RATIS_SERVER
datanode1_1  | 2023-01-12 05:44:38,605 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: Started
datanode1_1  | 2023-01-12 05:44:38,713 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2023-01-12 05:44:38,714 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2023-01-12 05:45:12,686 [Command processor thread] INFO server.RaftServer: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: addNew group-A6BDFD03825D:[a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER] returns group-A6BDFD03825D:java.util.concurrent.CompletableFuture@11659bd1[Not completed]
datanode1_1  | 2023-01-12 05:45:13,108 [pool-24-thread-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: new RaftServerImpl for group-A6BDFD03825D:[a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-01-12 05:45:13,125 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-01-12 05:45:13,135 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-01-12 05:45:13,138 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-01-12 05:45:13,139 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-01-12 05:45:13,142 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-01-12 05:45:13,155 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-01-12 05:45:13,310 [pool-24-thread-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D: ConfigurationManager, init=-1: peers:[a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-01-12 05:45:13,321 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-01-12 05:45:13,416 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-01-12 05:45:13,422 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-01-12 05:45:13,660 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-01-12 05:45:13,697 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-01-12 05:45:13,709 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-01-12 05:45:14,312 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-01-12 05:45:14,321 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-01-12 05:45:14,331 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-01-12 05:45:14,337 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-01-12 05:45:14,349 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-01-12 05:45:14,355 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/86cb76d7-d1bd-4dd6-8116-a6bdfd03825d does not exist. Creating ...
datanode1_1  | 2023-01-12 05:45:14,419 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/86cb76d7-d1bd-4dd6-8116-a6bdfd03825d/in_use.lock acquired by nodename 7@fb1a7b8e0c66
datanode1_1  | 2023-01-12 05:45:14,476 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/86cb76d7-d1bd-4dd6-8116-a6bdfd03825d has been successfully formatted.
datanode1_1  | 2023-01-12 05:45:14,604 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-A6BDFD03825D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-01-12 05:45:14,626 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-01-12 05:45:14,873 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-01-12 05:45:14,874 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-12 05:45:14,885 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-01-12 05:45:14,888 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-01-12 05:45:14,944 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-01-12 05:45:15,014 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-01-12 05:45:15,018 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-01-12 05:45:15,104 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/86cb76d7-d1bd-4dd6-8116-a6bdfd03825d
datanode1_1  | 2023-01-12 05:45:15,109 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-01-12 05:45:15,114 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-01-12 05:45:15,117 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-01-12 05:45:15,128 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-01-12 05:45:15,136 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-01-12 05:45:15,145 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-01-12 05:45:15,147 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-01-12 05:45:15,149 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-01-12 05:45:15,257 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-01-12 05:45:15,275 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-12 05:45:15,481 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-01-12 05:45:15,493 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-01-12 05:45:15,512 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-01-12 05:45:15,576 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-01-12 05:45:15,577 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-01-12 05:45:15,605 [pool-24-thread-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D: start as a follower, conf=-1: peers:[a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-12 05:45:15,608 [pool-24-thread-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-01-12 05:45:15,615 [pool-24-thread-1] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: start a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-FollowerState
datanode1_1  | 2023-01-12 05:45:15,686 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A6BDFD03825D,id=a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
datanode1_1  | 2023-01-12 05:45:15,689 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-01-12 05:45:15,690 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-12 05:45:15,696 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-12 05:45:15,696 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-01-12 05:45:15,706 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-01-12 05:45:15,717 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
kms_1        | Sleeping for 5 seconds
kms_1        | WARNING: /opt/hadoop/temp does not exist. Creating.
datanode2_1  | 2023-01-12 05:43:58,618 [main] INFO client.DNCertificateClient: CertificateLifetimeMonitor is started with first delay 29096161423 ms and interval 86400000 ms.
datanode2_1  | 2023-01-12 05:43:58,618 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode2_1  | 2023-01-12 05:43:58,796 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode2_1  | 2023-01-12 05:44:00,468 [main] INFO reflections.Reflections: Reflections took 1330 ms to scan 2 urls, producing 97 keys and 217 values 
datanode2_1  | 2023-01-12 05:44:01,253 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode2_1  | 2023-01-12 05:44:03,086 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode2_1  | 2023-01-12 05:44:03,257 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode2_1  | 2023-01-12 05:44:03,291 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode2_1  | 2023-01-12 05:44:03,300 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode2_1  | 2023-01-12 05:44:03,582 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode2_1  | 2023-01-12 05:44:03,759 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2023-01-12 05:44:03,769 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode2_1  | 2023-01-12 05:44:03,770 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode2_1  | 2023-01-12 05:44:03,770 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode2_1  | 2023-01-12 05:44:03,770 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode2_1  | 2023-01-12 05:44:04,118 [Thread-8] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode2_1  | 2023-01-12 05:44:04,122 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode2_1  | 2023-01-12 05:44:11,203 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode2_1  | 2023-01-12 05:44:13,016 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
datanode2_1  | 2023-01-12 05:44:13,231 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig1-
datanode2_1  | 2023-01-12 05:44:13,783 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2023-01-12 05:44:14,524 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode2_1  | 2023-01-12 05:44:15,211 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode2_1  | 2023-01-12 05:44:15,221 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode2_1  | 2023-01-12 05:44:15,222 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode2_1  | 2023-01-12 05:44:15,223 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode2_1  | 2023-01-12 05:44:15,247 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode2_1  | 2023-01-12 05:44:15,248 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode2_1  | 2023-01-12 05:44:15,252 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode2_1  | 2023-01-12 05:44:15,256 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-12 05:44:15,259 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode2_1  | 2023-01-12 05:44:15,261 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2023-01-12 05:44:15,328 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode2_1  | 2023-01-12 05:44:15,352 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode2_1  | 2023-01-12 05:44:15,361 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode2_1  | 2023-01-12 05:44:24,767 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode2_1  | 2023-01-12 05:44:25,043 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode2_1  | 2023-01-12 05:44:25,050 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode2_1  | 2023-01-12 05:44:25,053 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode2_1  | 2023-01-12 05:44:25,078 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode2_1  | 2023-01-12 05:44:25,097 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode2_1  | 2023-01-12 05:44:25,104 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode2_1  | 2023-01-12 05:44:25,170 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode2_1  | 2023-01-12 05:44:25,179 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
datanode2_1  | 2023-01-12 05:44:25,546 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode2_1  | 2023-01-12 05:44:25,553 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode2_1  | 2023-01-12 05:44:26,045 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode2_1  | 2023-01-12 05:44:26,051 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode2_1  | 2023-01-12 05:44:26,105 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-01-12 05:44:26,105 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-01-12 05:44:26,180 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-01-12 05:44:26,243 [08ecee04-dee1-41b4-94f3-4660f61ac91b-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xdfc3d6cd] REGISTERED
datanode2_1  | 2023-01-12 05:44:26,254 [08ecee04-dee1-41b4-94f3-4660f61ac91b-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xdfc3d6cd] BIND: 0.0.0.0/0.0.0.0:0
datanode2_1  | 2023-01-12 05:44:26,323 [08ecee04-dee1-41b4-94f3-4660f61ac91b-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xdfc3d6cd, L:/0.0.0.0:40109] ACTIVE
datanode2_1  | 2023-01-12 05:44:26,592 [main] INFO ssl.PemFileBasedKeyStoresFactory: SERVER KeyStore reloading at 60000 millis.
datanode2_1  | 2023-01-12 05:44:26,638 [main] INFO ssl.PemFileBasedKeyStoresFactory: SERVER TrustStore reloading at 60000 millis.
datanode2_1  | 2023-01-12 05:44:26,715 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode2_1  | 2023-01-12 05:44:28,180 [main] INFO token.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
datanode2_1  | 2023-01-12 05:44:28,231 [main] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
datanode2_1  | 2023-01-12 05:44:29,038 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode2_1  | 2023-01-12 05:44:29,041 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode2_1  | 2023-01-12 05:44:29,044 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode2_1  | 2023-01-12 05:44:29,362 [main] INFO util.log: Logging initialized @78150ms to org.eclipse.jetty.util.log.Slf4jLog
datanode2_1  | 2023-01-12 05:44:30,023 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode2_1  | 2023-01-12 05:44:30,071 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode2_1  | 2023-01-12 05:44:30,081 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode2_1  | 2023-01-12 05:44:30,081 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode3_1  | 2023-01-12 05:44:29,577 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode3_1  | 2023-01-12 05:44:29,578 [main] INFO server.session: No SessionScavenger set, using defaults
datanode3_1  | 2023-01-12 05:44:29,603 [main] INFO server.session: node0 Scavenging every 600000ms
datanode3_1  | 2023-01-12 05:44:29,902 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-01-12 05:44:29,923 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6b71fded{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode3_1  | 2023-01-12 05:44:29,927 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2f5a23c1{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode3_1  | 2023-01-12 05:44:30,896 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2023-01-12 05:44:31,028 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@428169d{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-14325258927665261239/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode3_1  | 2023-01-12 05:44:31,116 [main] INFO server.AbstractConnector: Started ServerConnector@3bde85b0{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode3_1  | 2023-01-12 05:44:31,119 [main] INFO server.Server: Started @80219ms
datanode3_1  | 2023-01-12 05:44:31,147 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode3_1  | 2023-01-12 05:44:31,147 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode3_1  | 2023-01-12 05:44:31,159 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode3_1  | 2023-01-12 05:44:31,191 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode3_1  | 2023-01-12 05:44:31,432 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@32192dff] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode3_1  | 2023-01-12 05:44:32,251 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode3_1  | 2023-01-12 05:44:32,308 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode3_1  | 2023-01-12 05:44:37,827 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-dd9a252d-f4dd-4162-963a-067d6dacb49f/DS-d5d26543-2198-46f1-a824-81fad543161f/container.db to cache
datanode3_1  | 2023-01-12 05:44:37,830 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-dd9a252d-f4dd-4162-963a-067d6dacb49f/DS-d5d26543-2198-46f1-a824-81fad543161f/container.db for volume DS-d5d26543-2198-46f1-a824-81fad543161f
datanode3_1  | 2023-01-12 05:44:37,849 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode3_1  | 2023-01-12 05:44:37,905 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode3_1  | 2023-01-12 05:44:38,163 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode3_1  | 2023-01-12 05:44:38,177 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 75176c82-cf64-45b0-aad5-7780ce7245eb
datanode3_1  | 2023-01-12 05:44:38,389 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.RaftServer: 75176c82-cf64-45b0-aad5-7780ce7245eb: start RPC server
datanode3_1  | 2023-01-12 05:44:38,410 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 75176c82-cf64-45b0-aad5-7780ce7245eb: GrpcService started, listening on 9858
datanode3_1  | 2023-01-12 05:44:38,430 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 75176c82-cf64-45b0-aad5-7780ce7245eb: GrpcService started, listening on 9856
datanode3_1  | 2023-01-12 05:44:38,458 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 75176c82-cf64-45b0-aad5-7780ce7245eb: GrpcService started, listening on 9857
datanode3_1  | 2023-01-12 05:44:38,526 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 75176c82-cf64-45b0-aad5-7780ce7245eb is started using port 9858 for RATIS
datanode3_1  | 2023-01-12 05:44:38,526 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 75176c82-cf64-45b0-aad5-7780ce7245eb is started using port 9857 for RATIS_ADMIN
datanode3_1  | 2023-01-12 05:44:38,527 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 75176c82-cf64-45b0-aad5-7780ce7245eb is started using port 9856 for RATIS_SERVER
datanode3_1  | 2023-01-12 05:44:38,530 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-75176c82-cf64-45b0-aad5-7780ce7245eb: Started
datanode3_1  | 2023-01-12 05:44:38,714 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-01-12 05:44:38,714 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2023-01-12 05:44:38,751 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode3_1  | java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:309)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:509)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode3_1  | Caused by: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode3_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
kdc_1        | Jan 12 05:41:30 kdc krb5kdc[7](info): Loaded
kdc_1        | Jan 12 05:41:30 kdc krb5kdc[7](Error): preauth spake failed to initialize: No SPAKE preauth groups configured
kdc_1        | Jan 12 05:41:30 kdc krb5kdc[7](info): setting up network...
kdc_1        | Jan 12 05:41:30 kdc krb5kdc[7](info): setsockopt(8,IPV6_V6ONLY,1) worked
kdc_1        | Jan 12 05:41:30 kdc krb5kdc[7](info): setsockopt(10,IPV6_V6ONLY,1) worked
kdc_1        | Jan 12 05:41:30 kdc krb5kdc[7](info): set up 4 sockets
kdc_1        | Jan 12 05:41:30 kdc krb5kdc[7](info): commencing operation
kdc_1        | krb5kdc: starting...
kdc_1        | Jan 12 05:41:31 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1673502091, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:41:38 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1673502098, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:41:42 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.114: ISSUE: authtime 1673502102, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, s3g/s3g@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:41:44 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.115: ISSUE: authtime 1673502104, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:41:59 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1673502119, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:42:07 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.116: ISSUE: authtime 1673502127, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:42:12 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1673502119, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:42:14 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: ISSUE: authtime 1673502104, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:42:14 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1673502098, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:42:26 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1673502146, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:42:27 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1673502147, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:42:40 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1673502146, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:42:41 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1673502147, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:42:45 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1673502165, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:42:48 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1673502168, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:42:49 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.118: ISSUE: authtime 1673502168, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:42:59 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1673502165, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:43:00 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1673502180, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:43:03 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1673502183, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:43:13 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.118: ISSUE: authtime 1673502180, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:43:35 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.102: ISSUE: authtime 1673502215, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:43:36 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.103: ISSUE: authtime 1673502216, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:43:36 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.104: ISSUE: authtime 1673502216, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:43:40 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1673502183, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:43:40 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1673502220, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:43:41 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1673502221, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:43:43 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1673502223, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:43:45 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.111: ISSUE: authtime 1673502220, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:43:46 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.113: ISSUE: authtime 1673502221, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:43:48 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.112: ISSUE: authtime 1673502223, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:43:49 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1673502229, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:43:54 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.104: ISSUE: authtime 1673502216, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:43:54 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.102: ISSUE: authtime 1673502215, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:43:55 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.103: ISSUE: authtime 1673502216, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:44:33 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.104: ISSUE: authtime 1673502216, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jan 12 05:44:34 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.103: ISSUE: authtime 1673502216, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jan 12 05:44:35 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.102: ISSUE: authtime 1673502215, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jan 12 05:44:36 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1673502276, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:44:37 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1673502277, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:44:37 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1673502277, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
datanode2_1  | 2023-01-12 05:44:30,084 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode2_1  | 2023-01-12 05:44:30,098 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode2_1  | 2023-01-12 05:44:30,400 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode2_1  | 2023-01-12 05:44:30,416 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode2_1  | 2023-01-12 05:44:30,690 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode2_1  | 2023-01-12 05:44:30,690 [main] INFO server.session: No SessionScavenger set, using defaults
datanode2_1  | 2023-01-12 05:44:30,700 [main] INFO server.session: node0 Scavenging every 600000ms
datanode2_1  | 2023-01-12 05:44:30,923 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2023-01-12 05:44:30,944 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d0ca8a5{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode2_1  | 2023-01-12 05:44:30,947 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5c4e86e7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode2_1  | 2023-01-12 05:44:31,785 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2023-01-12 05:44:31,906 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1bd97254{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-4216882005829496752/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode2_1  | 2023-01-12 05:44:31,982 [main] INFO server.AbstractConnector: Started ServerConnector@4a609803{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode2_1  | 2023-01-12 05:44:31,984 [main] INFO server.Server: Started @80775ms
datanode2_1  | 2023-01-12 05:44:32,015 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode2_1  | 2023-01-12 05:44:32,016 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode2_1  | 2023-01-12 05:44:32,021 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode2_1  | 2023-01-12 05:44:32,040 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode2_1  | 2023-01-12 05:44:32,199 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6c6a7add] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode2_1  | 2023-01-12 05:44:32,840 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode2_1  | 2023-01-12 05:44:32,894 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode2_1  | 2023-01-12 05:44:37,562 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-dd9a252d-f4dd-4162-963a-067d6dacb49f/DS-e2cb44be-d623-4a88-8ff0-3d395d13f412/container.db to cache
datanode2_1  | 2023-01-12 05:44:37,562 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-dd9a252d-f4dd-4162-963a-067d6dacb49f/DS-e2cb44be-d623-4a88-8ff0-3d395d13f412/container.db for volume DS-e2cb44be-d623-4a88-8ff0-3d395d13f412
datanode2_1  | 2023-01-12 05:44:37,593 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode2_1  | 2023-01-12 05:44:37,596 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode2_1  | 2023-01-12 05:44:37,618 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode2_1  | 2023-01-12 05:44:37,621 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 08ecee04-dee1-41b4-94f3-4660f61ac91b
datanode2_1  | 2023-01-12 05:44:37,650 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.RaftServer: 08ecee04-dee1-41b4-94f3-4660f61ac91b: start RPC server
datanode2_1  | 2023-01-12 05:44:37,770 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: 08ecee04-dee1-41b4-94f3-4660f61ac91b: GrpcService started, listening on 9858
datanode2_1  | 2023-01-12 05:44:37,799 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: 08ecee04-dee1-41b4-94f3-4660f61ac91b: GrpcService started, listening on 9856
datanode2_1  | 2023-01-12 05:44:37,812 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: 08ecee04-dee1-41b4-94f3-4660f61ac91b: GrpcService started, listening on 9857
datanode2_1  | 2023-01-12 05:44:37,831 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 08ecee04-dee1-41b4-94f3-4660f61ac91b is started using port 9858 for RATIS
datanode2_1  | 2023-01-12 05:44:37,833 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 08ecee04-dee1-41b4-94f3-4660f61ac91b is started using port 9857 for RATIS_ADMIN
datanode2_1  | 2023-01-12 05:44:37,833 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 08ecee04-dee1-41b4-94f3-4660f61ac91b is started using port 9856 for RATIS_SERVER
datanode2_1  | 2023-01-12 05:44:37,834 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-08ecee04-dee1-41b4-94f3-4660f61ac91b: Started
datanode2_1  | 2023-01-12 05:44:37,920 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2023-01-12 05:44:37,921 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2023-01-12 05:45:11,642 [Command processor thread] INFO server.RaftServer: 08ecee04-dee1-41b4-94f3-4660f61ac91b: addNew group-ACB2C75A2877:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER] returns group-ACB2C75A2877:java.util.concurrent.CompletableFuture@5d32a1d2[Not completed]
datanode2_1  | 2023-01-12 05:45:11,907 [pool-24-thread-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b: new RaftServerImpl for group-ACB2C75A2877:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-01-12 05:45:11,918 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-01-12 05:45:11,919 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-01-12 05:45:11,920 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-01-12 05:45:11,921 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-01-12 05:45:11,922 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-01-12 05:45:15,838 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=86cb76d7-d1bd-4dd6-8116-a6bdfd03825d
datanode1_1  | 2023-01-12 05:45:15,852 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=86cb76d7-d1bd-4dd6-8116-a6bdfd03825d.
datanode1_1  | 2023-01-12 05:45:15,859 [Command processor thread] INFO server.RaftServer: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: addNew group-ACB2C75A2877:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER] returns group-ACB2C75A2877:java.util.concurrent.CompletableFuture@1747f32c[Not completed]
datanode1_1  | 2023-01-12 05:45:15,896 [pool-24-thread-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: new RaftServerImpl for group-ACB2C75A2877:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-01-12 05:45:15,898 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-01-12 05:45:15,898 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-01-12 05:45:15,899 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-01-12 05:45:15,907 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-01-12 05:45:15,912 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-01-12 05:45:15,912 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-01-12 05:45:15,913 [pool-24-thread-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877: ConfigurationManager, init=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-01-12 05:45:15,913 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-01-12 05:45:15,916 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-01-12 05:45:15,916 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2023-01-12 05:45:15,917 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-01-12 05:45:15,919 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-01-12 05:45:15,920 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-01-12 05:45:15,923 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-01-12 05:45:15,929 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-01-12 05:45:15,931 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-01-12 05:45:15,935 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-01-12 05:45:15,936 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-01-12 05:45:15,937 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877 does not exist. Creating ...
datanode1_1  | 2023-01-12 05:45:15,943 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877/in_use.lock acquired by nodename 7@fb1a7b8e0c66
datanode1_1  | 2023-01-12 05:45:15,950 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877 has been successfully formatted.
datanode1_1  | 2023-01-12 05:45:15,953 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-ACB2C75A2877: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-01-12 05:45:15,994 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-01-12 05:45:16,003 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-01-12 05:45:16,017 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-12 05:45:16,040 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-01-12 05:45:16,040 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-01-12 05:45:16,041 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-01-12 05:45:16,047 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-01-12 05:45:16,047 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-01-12 05:45:16,055 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877
datanode1_1  | 2023-01-12 05:45:16,060 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-01-12 05:45:16,065 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-01-12 05:45:16,065 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-01-12 05:45:16,066 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-01-12 05:45:16,066 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-01-12 05:45:16,069 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1        | Sleeping for 5 seconds
om1_1        | Waiting for the service scm3.org:9894
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2023-01-12 05:43:24,709 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = [--init]
om1_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
om1_1        | STARTUP_MSG:   java = 11.0.14.1
om1_1        | ************************************************************/
om1_1        | 2023-01-12 05:43:24,819 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2023-01-12 05:43:34,640 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1        | 2023-01-12 05:43:37,791 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om1_1        | 2023-01-12 05:43:38,789 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-01-12 05:43:38,795 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om1: om1
om1_1        | 2023-01-12 05:43:38,796 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om1: om1
om1_1        | 2023-01-12 05:43:41,596 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2023-01-12 05:43:41,598 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2023-01-12 05:43:41,785 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-01-12 05:43:43,059 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f;layoutVersion=3
om1_1        | 2023-01-12 05:43:46,783 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om1_1        | 2023-01-12 05:43:46,784 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om1_1        | 2023-01-12 05:43:52,700 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
datanode2_1  | 2023-01-12 05:45:11,923 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-01-12 05:45:11,981 [pool-24-thread-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877: ConfigurationManager, init=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-01-12 05:45:11,982 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-01-12 05:45:12,038 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-01-12 05:45:12,042 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-01-12 05:45:12,096 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-01-12 05:45:12,122 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-01-12 05:45:12,126 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-01-12 05:45:12,484 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-01-12 05:45:12,489 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-01-12 05:45:12,492 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-01-12 05:45:12,494 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-01-12 05:45:12,501 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-01-12 05:45:12,505 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877 does not exist. Creating ...
datanode2_1  | 2023-01-12 05:45:12,570 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877/in_use.lock acquired by nodename 7@e522f50beb6a
datanode2_1  | 2023-01-12 05:45:12,611 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877 has been successfully formatted.
datanode2_1  | 2023-01-12 05:45:12,692 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-ACB2C75A2877: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-01-12 05:45:12,711 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-01-12 05:45:12,877 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-01-12 05:45:12,880 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-12 05:45:12,894 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-01-12 05:45:12,910 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-01-12 05:45:12,975 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-01-12 05:45:13,085 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-01-12 05:45:13,094 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-01-12 05:45:13,220 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877
datanode2_1  | 2023-01-12 05:45:13,224 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-01-12 05:45:13,235 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-01-12 05:45:13,251 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-01-12 05:45:13,258 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-01-12 05:45:13,276 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-01-12 05:45:13,291 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-01-12 05:45:13,310 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-01-12 05:45:13,312 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-01-12 05:45:13,454 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-01-12 05:45:13,459 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-12 05:45:13,694 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-01-12 05:45:13,698 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-01-12 05:45:13,699 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-01-12 05:45:13,750 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-01-12 05:45:13,751 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-01-12 05:45:13,757 [pool-24-thread-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877: start as a follower, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-12 05:45:13,760 [pool-24-thread-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode3_1  | 	... 1 more
datanode3_1  | Caused by: java.util.ConcurrentModificationException
datanode3_1  | 	at java.base/java.util.ArrayList$Itr.checkForComodification(ArrayList.java:1043)
datanode3_1  | 	at java.base/java.util.ArrayList$Itr.next(ArrayList.java:997)
datanode3_1  | 	at org.apache.hadoop.hdds.protocol.DatanodeDetails.toProtoBuilder(DatanodeDetails.java:433)
datanode3_1  | 	at org.apache.hadoop.hdds.protocol.DatanodeDetails.toProto(DatanodeDetails.java:393)
datanode3_1  | 	at org.apache.hadoop.hdds.protocol.DatanodeDetails.getProtoBufMessage(DatanodeDetails.java:389)
datanode3_1  | 	at org.apache.hadoop.hdds.protocol.DatanodeDetails.getExtendedProtoBufMessage(DatanodeDetails.java:455)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.RegisterEndpointTask.call(RegisterEndpointTask.java:158)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.RegisterEndpointTask.call(RegisterEndpointTask.java:52)
datanode3_1  | 	... 4 more
datanode3_1  | 2023-01-12 05:45:12,978 [Command processor thread] INFO server.RaftServer: 75176c82-cf64-45b0-aad5-7780ce7245eb: addNew group-ACB2C75A2877:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER] returns group-ACB2C75A2877:java.util.concurrent.CompletableFuture@77ef128c[Not completed]
datanode3_1  | 2023-01-12 05:45:13,363 [pool-24-thread-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb: new RaftServerImpl for group-ACB2C75A2877:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-01-12 05:45:13,383 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-01-12 05:45:13,386 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-01-12 05:45:13,387 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-01-12 05:45:13,391 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-01-12 05:45:13,391 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-01-12 05:45:13,395 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-01-12 05:45:13,506 [pool-24-thread-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877: ConfigurationManager, init=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-01-12 05:45:13,521 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-01-12 05:45:13,637 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-01-12 05:45:13,646 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-01-12 05:45:13,773 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-01-12 05:45:13,832 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-01-12 05:45:13,837 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-01-12 05:45:14,439 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-01-12 05:45:14,451 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-01-12 05:45:14,451 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-01-12 05:45:14,464 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-01-12 05:45:14,474 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-01-12 05:45:14,474 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877 does not exist. Creating ...
datanode3_1  | 2023-01-12 05:45:14,537 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877/in_use.lock acquired by nodename 7@789a46e3b060
datanode3_1  | 2023-01-12 05:45:14,606 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877 has been successfully formatted.
datanode3_1  | 2023-01-12 05:45:14,692 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-ACB2C75A2877: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-01-12 05:45:14,698 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-01-12 05:45:14,884 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-01-12 05:45:14,899 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-12 05:45:14,920 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-01-12 05:45:14,958 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-01-12 05:45:15,082 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-01-12 05:45:15,171 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-01-12 05:45:15,183 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-01-12 05:45:15,254 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877
datanode3_1  | 2023-01-12 05:45:15,290 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
kdc_1        | Jan 12 05:44:42 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.111: ISSUE: authtime 1673502276, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:44:43 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.113: ISSUE: authtime 1673502277, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:44:43 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.112: ISSUE: authtime 1673502277, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:44:45 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1673502229, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 12 05:44:51 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1673502291, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 12 05:45:22 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: ISSUE: authtime 1673502104, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jan 12 05:45:28 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1673502291, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
om1_1        | 2023-01-12 05:43:52,795 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om1_1        | 2023-01-12 05:43:52,807 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om1_1        | 2023-01-12 05:43:52,809 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om1_1        | 2023-01-12 05:44:01,700 [main] INFO om.OzoneManager: Init response: GETCERT
om1_1        | 2023-01-12 05:44:02,156 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.111,host:om1
om1_1        | 2023-01-12 05:44:02,157 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om1_1        | 2023-01-12 05:44:02,171 [main] ERROR security.OMCertificateClient: Invalid domain om1
om1_1        | 2023-01-12 05:44:02,179 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om1_1        | 2023-01-12 05:44:02,195 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-01-12 05:44:02,195 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om1: om1
om1_1        | 2023-01-12 05:44:02,220 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om1: om1
om1_1        | 2023-01-12 05:44:02,241 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om1,ip:172.25.0.111,scmId:0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1,clusterId:CID-dd9a252d-f4dd-4162-963a-067d6dacb49f,subject:om1
om1_1        | 2023-01-12 05:44:05,334 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om1_1        | 2023-01-12 05:44:05,429 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1        | /************************************************************
om1_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om1/172.25.0.111
om1_1        | ************************************************************/
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2023-01-12 05:44:19,998 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = []
om1_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
om1_1        | STARTUP_MSG:   java = 11.0.14.1
om1_1        | ************************************************************/
om1_1        | 2023-01-12 05:44:20,121 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2023-01-12 05:44:28,689 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1        | 2023-01-12 05:44:32,224 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om1_1        | 2023-01-12 05:44:32,983 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2023-01-12 05:44:32,984 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om1: om1
om1_1        | 2023-01-12 05:44:32,991 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om1: om1
om1_1        | 2023-01-12 05:44:33,086 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-01-12 05:44:33,361 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om1_1        | 2023-01-12 05:44:35,270 [main] INFO reflections.Reflections: Reflections took 1618 ms to scan 1 urls, producing 115 keys and 335 values [using 2 cores]
om1_1        | 2023-01-12 05:44:36,862 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2023-01-12 05:44:36,863 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2023-01-12 05:44:36,887 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-01-12 05:44:40,688 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | 2023-01-12 05:44:41,421 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om1_1        | 2023-01-12 05:44:46,645 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2023-01-12 05:44:46,725 [main] INFO security.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om1_1        | 2023-01-12 05:44:47,769 [main] INFO security.OMCertificateClient: Added certificate [
om1_1        | [
om1_1        |   Version: V3
om1_1        |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
om1_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om1_1        | 
om1_1        |   Key:  Sun RSA public key, 2048 bits
om1_1        |   params: null
om1_1        |   modulus: 30346053453237189834112040673541629567010306125791898570104054454047501736657892214383462264526301500298692652758375836429638347531164751981653358525504527825570400558838334901393005984876414719430775458652128247547444727818535668600972192623637356865083888753952717740665794517921055984453520353917733198649461590018542814322448272297366500121604394586840649019781675207082962097691558583700862118486739043197335879323701508955628436573724419921778589743880043303622162563304571704603236749325057266808761842228199352214967718782980075765678400986380731445934917806096546808808883868771850900183098234200323729287601
om1_1        |   public exponent: 65537
om1_1        |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
om1_1        |                To: Sun Feb 20 00:00:00 UTC 2028]
om1_1        |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
om1_1        |   SerialNumber: [    01]
om1_1        | 
om1_1        | Certificate Extensions: 3
om1_1        | [1]: ObjectId: 2.5.29.19 Criticality=true
om1_1        | BasicConstraints:[
om1_1        |   CA:true
om1_1        |   PathLen:2147483647
om1_1        | ]
om1_1        | 
om1_1        | [2]: ObjectId: 2.5.29.15 Criticality=true
om1_1        | KeyUsage [
om1_1        |   Key_CertSign
datanode1_1  | 2023-01-12 05:45:16,073 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-01-12 05:45:16,074 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-01-12 05:45:16,087 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-01-12 05:45:16,093 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-12 05:45:18,613 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: Detected pause in JVM or host machine (eg GC): pause of approximately 2288464676ns.
datanode1_1  | GC pool 'ParNew' had collection(s): count=1 time=83ms
datanode1_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2430ms
datanode1_1  | 2023-01-12 05:45:18,686 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2a3a1462] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2414ms
datanode1_1  | GC pool 'ParNew' had collection(s): count=1 time=83ms
datanode1_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2430ms
datanode1_1  | 2023-01-12 05:45:18,745 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-01-12 05:45:18,745 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-01-12 05:45:18,745 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-01-12 05:45:18,754 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-01-12 05:45:18,754 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-01-12 05:45:18,787 [pool-24-thread-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877: start as a follower, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-12 05:45:18,793 [pool-24-thread-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-01-12 05:45:18,811 [pool-24-thread-1] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: start a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState
datanode1_1  | 2023-01-12 05:45:18,812 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-ACB2C75A2877,id=a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
datanode1_1  | 2023-01-12 05:45:18,812 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-01-12 05:45:18,812 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-01-12 05:45:18,812 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-01-12 05:45:18,813 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-01-12 05:45:18,818 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-12 05:45:18,850 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-12 05:45:18,856 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877
datanode1_1  | 2023-01-12 05:45:19,070 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-01-12 05:45:20,843 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-FollowerState] INFO impl.FollowerState: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5228294081ns, electionTimeout:5141ms
datanode1_1  | 2023-01-12 05:45:20,851 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-FollowerState] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: shutdown a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-FollowerState
datanode1_1  | 2023-01-12 05:45:20,854 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-FollowerState] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2023-01-12 05:45:20,888 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode1_1  | 2023-01-12 05:45:20,898 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-FollowerState] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: start a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1
datanode1_1  | 2023-01-12 05:45:20,970 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO impl.LeaderElection: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-12 05:45:20,997 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO impl.LeaderElection: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode1_1  | 2023-01-12 05:45:21,027 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: shutdown a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1
datanode1_1  | 2023-01-12 05:45:21,048 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode1_1  | 2023-01-12 05:45:21,050 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A6BDFD03825D with new leaderId: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
datanode1_1  | 2023-01-12 05:45:21,130 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D: change Leader from null to a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3 at term 1 for becomeLeader, leader elected after 7404ms
datanode1_1  | 2023-01-12 05:45:21,240 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode1_1  | 2023-01-12 05:45:21,297 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-01-12 05:45:21,306 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode1_1  | 2023-01-12 05:45:21,397 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode1_1  | 2023-01-12 05:45:21,398 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode1_1  | 2023-01-12 05:45:21,408 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode1_1  | 2023-01-12 05:45:21,550 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2023-01-12 05:45:21,587 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode1_1  | 2023-01-12 05:45:21,660 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: start a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderStateImpl
datanode1_1  | 2023-01-12 05:45:22,007 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2023-01-12 05:45:22,812 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-LeaderElection1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D: set configuration 0: peers:[a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-12 05:45:23,593 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-A6BDFD03825D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/86cb76d7-d1bd-4dd6-8116-a6bdfd03825d/current/log_inprogress_0
datanode1_1  | 2023-01-12 05:45:24,041 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState] INFO impl.FollowerState: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5229907425ns, electionTimeout:5190ms
datanode1_1  | 2023-01-12 05:45:24,044 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: shutdown a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState
datanode1_1  | 2023-01-12 05:45:24,044 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2023-01-12 05:45:24,044 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode1_1  | 2023-01-12 05:45:24,044 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: start a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2
datanode1_1  | 2023-01-12 05:45:24,093 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2] INFO impl.LeaderElection: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-12 05:45:24,995 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: Detected pause in JVM or host machine (eg GC): pause of approximately 842854682ns.
datanode1_1  | GC pool 'ParNew' had collection(s): count=1 time=884ms
datanode1_1  | 2023-01-12 05:45:25,206 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-12 05:45:25,242 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 75176c82-cf64-45b0-aad5-7780ce7245eb
datanode1_1  | 2023-01-12 05:45:25,263 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-12 05:45:25,305 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 08ecee04-dee1-41b4-94f3-4660f61ac91b
datanode1_1  | 2023-01-12 05:45:27,176 [grpc-default-executor-2] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877: receive requestVote(ELECTION, 75176c82-cf64-45b0-aad5-7780ce7245eb, group-ACB2C75A2877, 1, (t:0, i:0))
datanode1_1  | 2023-01-12 05:45:27,210 [grpc-default-executor-2] INFO impl.VoteContext: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-CANDIDATE: reject ELECTION from 75176c82-cf64-45b0-aad5-7780ce7245eb: already has voted for a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3 at current term 1
datanode1_1  | 2023-01-12 05:45:27,366 [grpc-default-executor-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877: receive requestVote(ELECTION, 08ecee04-dee1-41b4-94f3-4660f61ac91b, group-ACB2C75A2877, 1, (t:0, i:0))
datanode1_1  | 2023-01-12 05:45:27,376 [grpc-default-executor-2] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877 replies to ELECTION vote request: 75176c82-cf64-45b0-aad5-7780ce7245eb<-a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3#0:FAIL-t1. Peer's state: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877:t1, leader=null, voted=a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3, raftlog=Memoized:a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-12 05:45:27,429 [grpc-default-executor-1] INFO impl.VoteContext: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-CANDIDATE: reject ELECTION from 08ecee04-dee1-41b4-94f3-4660f61ac91b: already has voted for a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3 at current term 1
datanode2_1  | 2023-01-12 05:45:13,765 [pool-24-thread-1] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: start 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState
datanode2_1  | 2023-01-12 05:45:13,814 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-ACB2C75A2877,id=08ecee04-dee1-41b4-94f3-4660f61ac91b
datanode2_1  | 2023-01-12 05:45:13,834 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-12 05:45:13,847 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-12 05:45:13,851 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-01-12 05:45:13,855 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-01-12 05:45:13,861 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2023-01-12 05:45:13,875 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-01-12 05:45:14,038 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877
datanode2_1  | 2023-01-12 05:45:14,181 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-01-12 05:45:19,062 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO impl.FollowerState: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5300153392ns, electionTimeout:5197ms
datanode2_1  | 2023-01-12 05:45:19,063 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: shutdown 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState
datanode2_1  | 2023-01-12 05:45:19,064 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2023-01-12 05:45:19,092 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2023-01-12 05:45:19,092 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: start 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1
datanode2_1  | 2023-01-12 05:45:19,194 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO impl.LeaderElection: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-12 05:45:19,404 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-12 05:45:19,404 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-12 05:45:19,474 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
datanode2_1  | 2023-01-12 05:45:19,474 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 75176c82-cf64-45b0-aad5-7780ce7245eb
datanode2_1  | 2023-01-12 05:45:21,255 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-08ecee04-dee1-41b4-94f3-4660f61ac91b: Detected pause in JVM or host machine (eg GC): pause of approximately 121259696ns. No GCs detected.
datanode2_1  | 2023-01-12 05:45:24,459 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO impl.LeaderElection: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1: ELECTION TIMEOUT received 0 response(s) and 0 exception(s):
datanode2_1  | 2023-01-12 05:45:24,459 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO impl.LeaderElection: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1 ELECTION round 0: result TIMEOUT
datanode2_1  | 2023-01-12 05:45:24,462 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO impl.LeaderElection: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1 ELECTION round 1: submit vote requests at term 2 for -1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-12 05:45:24,463 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-12 05:45:24,463 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-12 05:45:24,777 [grpc-default-executor-0] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877: receive requestVote(ELECTION, 75176c82-cf64-45b0-aad5-7780ce7245eb, group-ACB2C75A2877, 1, (t:0, i:0))
datanode2_1  | 2023-01-12 05:45:24,847 [grpc-default-executor-0] INFO impl.VoteContext: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-CANDIDATE: reject ELECTION from 75176c82-cf64-45b0-aad5-7780ce7245eb: current term 2 > candidate's term 1
datanode2_1  | 2023-01-12 05:45:24,964 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO impl.LeaderElection: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode2_1  | 2023-01-12 05:45:25,002 [grpc-default-executor-0] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877 replies to ELECTION vote request: 75176c82-cf64-45b0-aad5-7780ce7245eb<-08ecee04-dee1-41b4-94f3-4660f61ac91b#0:FAIL-t2. Peer's state: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877:t2, leader=null, voted=08ecee04-dee1-41b4-94f3-4660f61ac91b, raftlog=Memoized:08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-12 05:45:25,002 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO impl.LeaderElection:   Response 0: 08ecee04-dee1-41b4-94f3-4660f61ac91b<-75176c82-cf64-45b0-aad5-7780ce7245eb#0:FAIL-t2
datanode2_1  | 2023-01-12 05:45:25,003 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO impl.LeaderElection: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1 ELECTION round 1: result REJECTED
datanode2_1  | 2023-01-12 05:45:25,005 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode2_1  | 2023-01-12 05:45:25,005 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: shutdown 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1
datanode2_1  | 2023-01-12 05:45:25,006 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection1] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: start 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState
datanode2_1  | 2023-01-12 05:45:25,071 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-12 05:45:25,075 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-12 05:45:25,586 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-01-12 05:45:28,385 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877.
datanode2_1  | 2023-01-12 05:45:28,388 [Command processor thread] INFO server.RaftServer: 08ecee04-dee1-41b4-94f3-4660f61ac91b: addNew group-52A62DFB0A44:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER] returns group-52A62DFB0A44:java.util.concurrent.CompletableFuture@4254b110[Not completed]
datanode2_1  | 2023-01-12 05:45:28,394 [pool-24-thread-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b: new RaftServerImpl for group-52A62DFB0A44:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-01-12 05:45:28,401 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-01-12 05:45:28,403 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-01-12 05:45:28,404 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-01-12 05:45:28,404 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-01-12 05:45:28,404 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-01-12 05:45:28,404 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-01-12 05:45:28,406 [pool-24-thread-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44: ConfigurationManager, init=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-01-12 05:45:28,407 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-01-12 05:45:28,408 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-01-12 05:45:28,408 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-01-12 05:45:28,408 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-01-12 05:45:28,409 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-01-12 05:45:28,410 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-01-12 05:45:28,451 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-01-12 05:45:28,455 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-01-12 05:45:28,455 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-01-12 05:45:28,457 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-01-12 05:45:28,457 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-01-12 05:45:28,465 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e22252e4-1bd7-4367-9a96-52a62dfb0a44 does not exist. Creating ...
datanode2_1  | 2023-01-12 05:45:28,480 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e22252e4-1bd7-4367-9a96-52a62dfb0a44/in_use.lock acquired by nodename 7@e522f50beb6a
datanode2_1  | 2023-01-12 05:45:28,515 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e22252e4-1bd7-4367-9a96-52a62dfb0a44 has been successfully formatted.
datanode2_1  | 2023-01-12 05:45:28,522 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-52A62DFB0A44: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-01-12 05:45:28,536 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-01-12 05:45:28,537 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-01-12 05:45:28,576 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | Sleeping for 5 seconds
om2_1        | Waiting for the service scm3.org:9894
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-01-12 05:43:26,902 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = [--init]
om2_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
om2_1        | STARTUP_MSG:   java = 11.0.14.1
om2_1        | ************************************************************/
om2_1        | 2023-01-12 05:43:27,025 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2023-01-12 05:43:36,899 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1        | 2023-01-12 05:43:40,226 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om2_1        | 2023-01-12 05:43:40,971 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-01-12 05:43:40,977 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om2: om2
om2_1        | 2023-01-12 05:43:40,978 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om2: om2
om2_1        | 2023-01-12 05:43:43,928 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2023-01-12 05:43:43,929 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2023-01-12 05:43:44,212 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-01-12 05:43:45,240 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f;layoutVersion=3
om2_1        | 2023-01-12 05:43:49,839 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om2_1        | 2023-01-12 05:43:49,840 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om2_1        | 2023-01-12 05:43:54,678 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2023-01-12 05:43:54,743 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om2_1        | 2023-01-12 05:43:54,751 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om2_1        | 2023-01-12 05:43:54,769 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om2_1        | 2023-01-12 05:43:59,204 [main] INFO om.OzoneManager: Init response: GETCERT
om2_1        | 2023-01-12 05:43:59,541 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.112,host:om2
om2_1        | 2023-01-12 05:43:59,542 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om2_1        | 2023-01-12 05:43:59,553 [main] ERROR security.OMCertificateClient: Invalid domain om2
om2_1        | 2023-01-12 05:43:59,555 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om2_1        | 2023-01-12 05:43:59,556 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-01-12 05:43:59,558 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om2: om2
om2_1        | 2023-01-12 05:43:59,559 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om2: om2
om2_1        | 2023-01-12 05:43:59,571 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om2,ip:172.25.0.112,scmId:0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1,clusterId:CID-dd9a252d-f4dd-4162-963a-067d6dacb49f,subject:om2
om2_1        | 2023-01-12 05:44:02,998 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om2_1        | 2023-01-12 05:44:03,131 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1        | /************************************************************
om2_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om2/172.25.0.112
om2_1        | ************************************************************/
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2023-01-12 05:44:18,470 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = []
om2_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
om2_1        | STARTUP_MSG:   java = 11.0.14.1
om2_1        | ************************************************************/
om2_1        | 2023-01-12 05:44:18,569 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2023-01-12 05:44:28,257 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1        | 2023-01-12 05:44:31,936 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om2_1        | 2023-01-12 05:44:32,841 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2023-01-12 05:44:32,847 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om2: om2
om2_1        | 2023-01-12 05:44:32,849 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om2: om2
om2_1        | 2023-01-12 05:44:33,029 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-01-12 05:44:33,520 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om2_1        | 2023-01-12 05:44:35,944 [main] INFO reflections.Reflections: Reflections took 1909 ms to scan 1 urls, producing 115 keys and 335 values [using 2 cores]
om2_1        | 2023-01-12 05:44:38,136 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2023-01-12 05:44:38,139 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2023-01-12 05:44:38,139 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-01-12 05:44:41,730 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | 2023-01-12 05:44:42,556 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om2_1        | 2023-01-12 05:44:47,715 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2023-01-12 05:44:47,791 [main] INFO security.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om2_1        | 2023-01-12 05:44:48,653 [main] INFO security.OMCertificateClient: Added certificate [
om2_1        | [
om2_1        |   Version: V3
om2_1        |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
om2_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om2_1        | 
om2_1        |   Key:  Sun RSA public key, 2048 bits
om2_1        |   params: null
om2_1        |   modulus: 30346053453237189834112040673541629567010306125791898570104054454047501736657892214383462264526301500298692652758375836429638347531164751981653358525504527825570400558838334901393005984876414719430775458652128247547444727818535668600972192623637356865083888753952717740665794517921055984453520353917733198649461590018542814322448272297366500121604394586840649019781675207082962097691558583700862118486739043197335879323701508955628436573724419921778589743880043303622162563304571704603236749325057266808761842228199352214967718782980075765678400986380731445934917806096546808808883868771850900183098234200323729287601
om2_1        |   public exponent: 65537
om2_1        |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
om2_1        |                To: Sun Feb 20 00:00:00 UTC 2028]
om2_1        |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
om2_1        |   SerialNumber: [    01]
om2_1        | 
om2_1        | Certificate Extensions: 3
om2_1        | [1]: ObjectId: 2.5.29.19 Criticality=true
om2_1        | BasicConstraints:[
om2_1        |   CA:true
om2_1        |   PathLen:2147483647
om2_1        | ]
om2_1        | 
om2_1        | [2]: ObjectId: 2.5.29.15 Criticality=true
om2_1        | KeyUsage [
om2_1        |   Key_CertSign
om2_1        |   Crl_Sign
om2_1        | ]
om2_1        | 
om2_1        | [3]: ObjectId: 2.5.29.17 Criticality=false
om2_1        | SubjectAlternativeName [
om2_1        |   IPAddress: 172.25.0.116
om2_1        |   DNSName: scm1.org
om2_1        | ]
om2_1        | 
om2_1        | ]
om2_1        |   Algorithm: [SHA256withRSA]
om2_1        |   Signature:
om2_1        | 0000: 59 E4 14 CA 42 0B 71 BE   61 3A FA 41 71 F2 48 C6  Y...B.q.a:.Aq.H.
om2_1        | 0010: 35 3A DF E8 FE E3 74 A0   7E 57 15 0F F2 39 95 34  5:....t..W...9.4
om1_1        |   Crl_Sign
om1_1        | ]
om1_1        | 
om1_1        | [3]: ObjectId: 2.5.29.17 Criticality=false
om1_1        | SubjectAlternativeName [
om1_1        |   IPAddress: 172.25.0.116
om1_1        |   DNSName: scm1.org
om1_1        | ]
om1_1        | 
om1_1        | ]
om1_1        |   Algorithm: [SHA256withRSA]
om1_1        |   Signature:
om1_1        | 0000: 59 E4 14 CA 42 0B 71 BE   61 3A FA 41 71 F2 48 C6  Y...B.q.a:.Aq.H.
om1_1        | 0010: 35 3A DF E8 FE E3 74 A0   7E 57 15 0F F2 39 95 34  5:....t..W...9.4
datanode1_1  | 2023-01-12 05:45:27,455 [grpc-default-executor-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877 replies to ELECTION vote request: 08ecee04-dee1-41b4-94f3-4660f61ac91b<-a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3#0:FAIL-t1. Peer's state: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877:t1, leader=null, voted=a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3, raftlog=Memoized:a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-12 05:45:27,491 [grpc-default-executor-3] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877: receive requestVote(ELECTION, 08ecee04-dee1-41b4-94f3-4660f61ac91b, group-ACB2C75A2877, 2, (t:0, i:0))
datanode1_1  | 2023-01-12 05:45:27,517 [grpc-default-executor-3] INFO impl.VoteContext: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-CANDIDATE: accept ELECTION from 08ecee04-dee1-41b4-94f3-4660f61ac91b: our priority 0 <= candidate's priority 0
datanode1_1  | 2023-01-12 05:45:27,532 [grpc-default-executor-3] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877: changes role from CANDIDATE to FOLLOWER at term 2 for candidate:08ecee04-dee1-41b4-94f3-4660f61ac91b
datanode1_1  | 2023-01-12 05:45:27,541 [grpc-default-executor-3] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: shutdown a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2
datanode1_1  | 2023-01-12 05:45:27,585 [grpc-default-executor-3] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: start a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState
datanode1_1  | 2023-01-12 05:45:27,605 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-12 05:45:27,645 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-12 05:45:27,609 [grpc-default-executor-3] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877 replies to ELECTION vote request: 08ecee04-dee1-41b4-94f3-4660f61ac91b<-a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3#0:OK-t2. Peer's state: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877:t2, leader=null, voted=08ecee04-dee1-41b4-94f3-4660f61ac91b, raftlog=Memoized:a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-12 05:45:28,267 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2] INFO impl.LeaderElection: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2: ELECTION DISCOVERED_A_NEW_TERM (term=2) received 1 response(s) and 0 exception(s):
datanode1_1  | 2023-01-12 05:45:28,267 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2] INFO impl.LeaderElection:   Response 0: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3<-75176c82-cf64-45b0-aad5-7780ce7245eb#0:FAIL-t2
datanode1_1  | 2023-01-12 05:45:28,270 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2] INFO impl.LeaderElection: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-LeaderElection2 ELECTION round 0: result DISCOVERED_A_NEW_TERM (term=2)
datanode1_1  | 2023-01-12 05:45:28,488 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-01-12 05:45:29,481 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877.
datanode1_1  | 2023-01-12 05:45:29,489 [pool-24-thread-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: new RaftServerImpl for group-52A62DFB0A44:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode1_1  | 2023-01-12 05:45:29,490 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2023-01-12 05:45:29,490 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2023-01-12 05:45:29,490 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2023-01-12 05:45:29,490 [Command processor thread] INFO server.RaftServer: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: addNew group-52A62DFB0A44:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER] returns group-52A62DFB0A44:java.util.concurrent.CompletableFuture@1a14f4c4[Not completed]
datanode1_1  | 2023-01-12 05:45:29,495 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2023-01-12 05:45:29,496 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2023-01-12 05:45:29,496 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2023-01-12 05:45:29,497 [pool-24-thread-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44: ConfigurationManager, init=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2023-01-12 05:45:29,497 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2023-01-12 05:45:29,498 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2023-01-12 05:45:29,498 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1        | 0020: E6 A7 48 CE 06 EA B9 42   CC AD DC 81 09 F1 F2 3E  ..H....B.......>
om1_1        | 0030: 94 27 78 5F 6B 04 F9 F4   E3 A1 71 61 8F 5A EF 6D  .'x_k.....qa.Z.m
om1_1        | 0040: 72 EA 9B CB 23 B9 31 9F   D7 B3 7B 86 35 09 CF D1  r...#.1.....5...
om1_1        | 0050: D6 81 A8 2E 21 01 F7 A9   B4 78 00 21 D5 3C 4A 61  ....!....x.!.<Ja
om1_1        | 0060: 7F 91 56 B9 50 6C 2C 23   0F D3 49 6E 80 28 7A 03  ..V.Pl,#..In.(z.
om1_1        | 0070: 32 46 42 54 4F 22 06 7F   B1 7D 41 A0 09 27 CF 7A  2FBTO"....A..'.z
om1_1        | 0080: F0 82 C6 00 70 41 5D 8F   83 5F E6 EB 74 1F B7 A2  ....pA].._..t...
om1_1        | 0090: 85 64 3E AE D6 66 71 7C   BE 7F 92 D0 79 64 71 C0  .d>..fq.....ydq.
om1_1        | 00A0: 8F 76 8C F6 D1 A4 67 C4   F4 11 8D 75 FC DE EF EE  .v....g....u....
om1_1        | 00B0: D9 7F 75 F3 5F AC 76 DF   23 F9 19 99 25 F9 31 8E  ..u._.v.#...%.1.
om1_1        | 00C0: AA 69 DC 7B 7E 4E 62 10   80 B9 5E 00 F8 1D 16 71  .i...Nb...^....q
om1_1        | 00D0: 7F D4 C7 FD 20 63 0B 0A   7E 36 7D F4 4C 85 A7 61  .... c...6..L..a
om1_1        | 00E0: E7 00 61 A2 83 40 78 92   3E A1 9A E0 B8 E4 A1 A0  ..a..@x.>.......
om1_1        | 00F0: 45 B5 33 7C 03 24 98 E7   DE 69 74 45 16 2B 06 3F  E.3..$...itE.+.?
om1_1        | 
om1_1        | ] from file:/data/metadata/om/certs/ROOTCA-1.crt.
om1_1        | 2023-01-12 05:44:47,799 [main] INFO security.OMCertificateClient: Added certificate [
om1_1        | [
om1_1        |   Version: V3
om1_1        |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=om1
om1_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om1_1        | 
om1_1        |   Key:  Sun RSA public key, 2048 bits
om1_1        |   params: null
om1_1        |   modulus: 26278158980348066850734500969878000884148538422401632816651215541349610785210065915423839154239665128088946893672336114984707867573522191870860427453975232097348811869738955182702427977557155947136786079491958912424665209287511347838045036752369163140838082529283003435659131861897440516310153588004328449553251943780157183666202124123713787588291797495221709553987141447888165755015866885173912018174320186302825049645681380566282898722904288242367055105785983827302869892450812365251888940140985503493876484293610377880610387325371113315413514956250516307083566089692098910484929472414306844993544227139618021377941
om1_1        |   public exponent: 65537
om1_1        |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
om1_1        |                To: Fri Jan 12 00:00:00 UTC 2024]
om1_1        |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
om1_1        |   SerialNumber: [    01383f45 06a5]
om1_1        | 
om1_1        | Certificate Extensions: 2
om1_1        | [1]: ObjectId: 2.5.29.15 Criticality=true
om1_1        | KeyUsage [
om1_1        |   DigitalSignature
om1_1        |   Key_Encipherment
om1_1        |   Data_Encipherment
om1_1        |   Key_Agreement
om1_1        | ]
om1_1        | 
om1_1        | [2]: ObjectId: 2.5.29.17 Criticality=false
om1_1        | SubjectAlternativeName [
om1_1        |   IPAddress: 172.25.0.111
om1_1        |   Other-Name: Unrecognized ObjectIdentifier: 2.16.840.1.113730.3.1.34
om1_1        | ]
om1_1        | 
om1_1        | ]
om1_1        |   Algorithm: [SHA256withRSA]
om1_1        |   Signature:
om1_1        | 0000: 53 0B 4C 04 26 03 CD F1   C6 28 49 7A 80 C1 70 6D  S.L.&....(Iz..pm
om1_1        | 0010: D1 F1 41 B7 A7 F3 18 34   38 F2 F8 1B 34 27 5C 97  ..A....48...4'\.
om1_1        | 0020: A8 24 C8 28 33 BF E9 46   DF 38 5E 9D 99 3B 2A B5  .$.(3..F.8^..;*.
om1_1        | 0030: 45 3A A1 5F 7C 22 FF 83   6B 6F 6B B1 80 B9 B1 AD  E:._."..kok.....
om1_1        | 0040: 5C C7 76 69 6B 87 EA 9E   D8 2B AE AC D4 67 FF 19  \.vik....+...g..
om1_1        | 0050: C2 B3 53 0A FB 5A AC 73   F4 A9 4E C1 80 11 10 5C  ..S..Z.s..N....\
om1_1        | 0060: 86 6E D3 E7 6B 20 58 5E   05 C1 0E 59 1B E9 69 B4  .n..k X^...Y..i.
om1_1        | 0070: 56 96 01 16 CC 04 B4 92   53 85 7B 6C D2 12 15 4E  V.......S..l...N
om1_1        | 0080: 8B 2B 88 05 F9 38 15 1B   42 D6 13 FA 3F BD 1F D7  .+...8..B...?...
om1_1        | 0090: D1 B3 31 2C B1 EE AC F5   82 1B 2E 7C 86 9F E8 7E  ..1,............
om1_1        | 00A0: 0E F3 80 1F 18 F3 8B 7C   6A 6A 5C 2B 20 C0 09 BF  ........jj\+ ...
om1_1        | 00B0: 22 66 22 E7 93 C4 8C 0C   AB 25 90 BA 92 0C 45 36  "f"......%....E6
om1_1        | 00C0: F0 0A BB 71 72 74 3C 79   92 62 11 7E E8 95 A5 2C  ...qrt<y.b.....,
om1_1        | 00D0: 49 92 11 99 2F FE D5 57   47 0A F9 70 2F 92 B3 B0  I.../..WG..p/...
om1_1        | 00E0: 75 3F C1 A1 2C 54 78 6B   CC 6E 94 56 7C 38 94 7B  u?..,Txk.n.V.8..
om1_1        | 00F0: BA 60 4A 27 AC 11 E2 7F   67 55 1A 4F 54 E0 31 55  .`J'....gU.OT.1U
om1_1        | 
om1_1        | ] from file:/data/metadata/om/certs/1341091284645.crt.
om1_1        | 2023-01-12 05:44:47,821 [main] INFO security.OMCertificateClient: Added certificate [
om1_1        | [
om1_1        |   Version: V3
om1_1        |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
om1_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om1_1        | 
om1_1        |   Key:  Sun RSA public key, 2048 bits
om1_1        |   params: null
om1_1        |   modulus: 21838495709719174090944017025941967908224182768319498180393936128987176221476088419281179276073314404186963320459954621839794791804718496604717042740409331050670636901380744918424604519064284257795379580082372800428810093145199831557386718853872621914973921810155992135096086488011377748521113854732595726272396205530150917174127452444863380225901746740157534773765777653459292048522370651962381319687098813297977743208706305312169254062598162002740262036220188710005290347458814949877767522524881554322161686065229152643489368945279202993130114091164740627711162880068229981146789996366091625523376604972129960509039
om1_1        |   public exponent: 65537
om1_1        |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
om1_1        |                To: Sun Feb 20 00:00:00 UTC 2028]
om1_1        |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
om1_1        |   SerialNumber: [    011a69de ba83]
om1_1        | 
om1_1        | Certificate Extensions: 3
om1_1        | [1]: ObjectId: 2.5.29.19 Criticality=true
om1_1        | BasicConstraints:[
om1_1        |   CA:true
om1_1        |   PathLen:2147483647
om1_1        | ]
om1_1        | 
om1_1        | [2]: ObjectId: 2.5.29.15 Criticality=true
om1_1        | KeyUsage [
om1_1        |   DigitalSignature
om1_1        |   Key_Encipherment
om1_1        |   Data_Encipherment
om1_1        |   Key_Agreement
om1_1        |   Key_CertSign
om1_1        |   Crl_Sign
om1_1        | ]
om1_1        | 
om1_1        | [3]: ObjectId: 2.5.29.17 Criticality=false
om1_1        | SubjectAlternativeName [
om1_1        |   IPAddress: 172.25.0.116
om1_1        |   DNSName: scm1.org
om1_1        | ]
om1_1        | 
om1_1        | ]
om1_1        |   Algorithm: [SHA256withRSA]
om1_1        |   Signature:
om1_1        | 0000: AB 5F B6 3B B0 5C 88 AB   DD 25 28 3F F6 7C 1A 59  ._.;.\...%(?...Y
om1_1        | 0010: AC B5 48 EF C8 75 0D 95   65 13 6C C4 5D 9A 7D D4  ..H..u..e.l.]...
om1_1        | 0020: 93 AF C2 9C CA DD 42 6C   C6 2D B6 5D EE CC 9A F0  ......Bl.-.]....
om1_1        | 0030: 95 DD 7D 3C D8 E8 28 A1   B8 09 E1 BA E1 DD 81 7A  ...<..(........z
om1_1        | 0040: 66 DB F9 45 B9 70 8B 0D   3A 5C A7 86 6A 8C F7 A7  f..E.p..:\..j...
om2_1        | 0020: E6 A7 48 CE 06 EA B9 42   CC AD DC 81 09 F1 F2 3E  ..H....B.......>
om2_1        | 0030: 94 27 78 5F 6B 04 F9 F4   E3 A1 71 61 8F 5A EF 6D  .'x_k.....qa.Z.m
om2_1        | 0040: 72 EA 9B CB 23 B9 31 9F   D7 B3 7B 86 35 09 CF D1  r...#.1.....5...
om2_1        | 0050: D6 81 A8 2E 21 01 F7 A9   B4 78 00 21 D5 3C 4A 61  ....!....x.!.<Ja
om2_1        | 0060: 7F 91 56 B9 50 6C 2C 23   0F D3 49 6E 80 28 7A 03  ..V.Pl,#..In.(z.
om2_1        | 0070: 32 46 42 54 4F 22 06 7F   B1 7D 41 A0 09 27 CF 7A  2FBTO"....A..'.z
om2_1        | 0080: F0 82 C6 00 70 41 5D 8F   83 5F E6 EB 74 1F B7 A2  ....pA].._..t...
om2_1        | 0090: 85 64 3E AE D6 66 71 7C   BE 7F 92 D0 79 64 71 C0  .d>..fq.....ydq.
om2_1        | 00A0: 8F 76 8C F6 D1 A4 67 C4   F4 11 8D 75 FC DE EF EE  .v....g....u....
om2_1        | 00B0: D9 7F 75 F3 5F AC 76 DF   23 F9 19 99 25 F9 31 8E  ..u._.v.#...%.1.
om2_1        | 00C0: AA 69 DC 7B 7E 4E 62 10   80 B9 5E 00 F8 1D 16 71  .i...Nb...^....q
om2_1        | 00D0: 7F D4 C7 FD 20 63 0B 0A   7E 36 7D F4 4C 85 A7 61  .... c...6..L..a
om2_1        | 00E0: E7 00 61 A2 83 40 78 92   3E A1 9A E0 B8 E4 A1 A0  ..a..@x.>.......
om2_1        | 00F0: 45 B5 33 7C 03 24 98 E7   DE 69 74 45 16 2B 06 3F  E.3..$...itE.+.?
om2_1        | 
om2_1        | ] from file:/data/metadata/om/certs/ROOTCA-1.crt.
om2_1        | 2023-01-12 05:44:48,696 [main] INFO security.OMCertificateClient: Added certificate [
om2_1        | [
om2_1        |   Version: V3
om2_1        |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=om2
om2_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om2_1        | 
om2_1        |   Key:  Sun RSA public key, 2048 bits
om2_1        |   params: null
om2_1        |   modulus: 23023575833274233376857308455981926596725922081678964176545085872027355755316162645744081480801251140654654550974353679835704391912487815352332635409843684896493868354288932655889715500149308660801848135279335202121251966118538352838732897951598246249877632998365972572359449994756991069156602224031899139803621693803784244249088457081985218426413305180082620621571443141618151470977590481349354889555341457575727833767042369090706551175264861645539207142545068389644874514899327091473149460663679581160432361517168965957036513531890317402244783568387228664569613584647021468377915576317849084662553067981574412780959
om2_1        |   public exponent: 65537
om2_1        |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
om2_1        |                To: Fri Jan 12 00:00:00 UTC 2024]
om2_1        |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
om2_1        |   SerialNumber: [    0137a7ee 1130]
om2_1        | 
om2_1        | Certificate Extensions: 2
om2_1        | [1]: ObjectId: 2.5.29.15 Criticality=true
om2_1        | KeyUsage [
om2_1        |   DigitalSignature
om2_1        |   Key_Encipherment
om2_1        |   Data_Encipherment
om2_1        |   Key_Agreement
om2_1        | ]
om2_1        | 
om2_1        | [2]: ObjectId: 2.5.29.17 Criticality=false
om2_1        | SubjectAlternativeName [
om2_1        |   IPAddress: 172.25.0.112
om2_1        |   Other-Name: Unrecognized ObjectIdentifier: 2.16.840.1.113730.3.1.34
om2_1        | ]
om2_1        | 
om2_1        | ]
om2_1        |   Algorithm: [SHA256withRSA]
om2_1        |   Signature:
om2_1        | 0000: 8A FB BC 9F 54 52 E1 98   FD 63 D6 76 B0 B5 B7 7A  ....TR...c.v...z
om2_1        | 0010: 31 65 42 CE F7 32 AF ED   07 CD 1B 5B 44 D0 2F 54  1eB..2.....[D./T
om2_1        | 0020: 8A B2 64 29 E0 34 BF 6E   B5 F4 16 F5 D3 08 EC 3F  ..d).4.n.......?
om2_1        | 0030: A7 14 CE 50 AB D7 1A 43   FE B1 E8 1C 59 C8 F2 C4  ...P...C....Y...
om2_1        | 0040: 1F C8 67 7A 31 3A 86 D8   71 A3 76 74 6E 31 6B 2E  ..gz1:..q.vtn1k.
om2_1        | 0050: 81 61 68 DA 44 3C BE 93   21 8E A7 99 A9 13 FF 37  .ah.D<..!......7
om2_1        | 0060: C1 FA 76 35 C5 9B D5 5C   21 1C E3 DE D3 18 9D 74  ..v5...\!......t
om2_1        | 0070: 1D A0 3C D0 37 BE AB EA   58 FF 2B 06 A9 19 AA 17  ..<.7...X.+.....
om2_1        | 0080: 71 E3 C8 D5 5A 00 10 66   BA DC 93 37 8A AE E4 E0  q...Z..f...7....
om2_1        | 0090: 70 41 00 9D 6C 03 45 71   AA 99 03 67 46 54 34 C5  pA..l.Eq...gFT4.
om2_1        | 00A0: F5 14 F1 4E 36 01 29 55   BC A5 C1 E1 A1 3C 03 DD  ...N6.)U.....<..
om2_1        | 00B0: 97 AB 0A 44 5D 73 A2 4A   11 2B 13 0A 89 27 93 7C  ...D]s.J.+...'..
om2_1        | 00C0: 51 61 8F 1F 32 68 39 A3   4A E3 F1 8A F4 0B 24 41  Qa..2h9.J.....$A
om2_1        | 00D0: 79 5E AA 07 D7 AA F7 EF   A6 76 25 07 BD 87 22 C4  y^.......v%...".
om2_1        | 00E0: E8 3F E6 BD F4 A0 47 54   22 B5 B9 30 EC 09 8C 0C  .?....GT"..0....
om2_1        | 00F0: ED D9 18 89 A8 A5 0A FB   C2 A9 E5 95 52 9C 37 E6  ............R.7.
om2_1        | 
om2_1        | ] from file:/data/metadata/om/certs/1338552226096.crt.
om2_1        | 2023-01-12 05:44:48,722 [main] INFO security.OMCertificateClient: Added certificate [
om2_1        | [
om2_1        |   Version: V3
om2_1        |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
om2_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om2_1        | 
om2_1        |   Key:  Sun RSA public key, 2048 bits
om2_1        |   params: null
om2_1        |   modulus: 21838495709719174090944017025941967908224182768319498180393936128987176221476088419281179276073314404186963320459954621839794791804718496604717042740409331050670636901380744918424604519064284257795379580082372800428810093145199831557386718853872621914973921810155992135096086488011377748521113854732595726272396205530150917174127452444863380225901746740157534773765777653459292048522370651962381319687098813297977743208706305312169254062598162002740262036220188710005290347458814949877767522524881554322161686065229152643489368945279202993130114091164740627711162880068229981146789996366091625523376604972129960509039
om2_1        |   public exponent: 65537
om2_1        |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
om2_1        |                To: Sun Feb 20 00:00:00 UTC 2028]
om2_1        |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
om2_1        |   SerialNumber: [    011a69de ba83]
om2_1        | 
om2_1        | Certificate Extensions: 3
om2_1        | [1]: ObjectId: 2.5.29.19 Criticality=true
om2_1        | BasicConstraints:[
om2_1        |   CA:true
om2_1        |   PathLen:2147483647
om2_1        | ]
om2_1        | 
om2_1        | [2]: ObjectId: 2.5.29.15 Criticality=true
om2_1        | KeyUsage [
om2_1        |   DigitalSignature
om2_1        |   Key_Encipherment
om2_1        |   Data_Encipherment
om2_1        |   Key_Agreement
om2_1        |   Key_CertSign
om2_1        |   Crl_Sign
om2_1        | ]
om2_1        | 
om2_1        | [3]: ObjectId: 2.5.29.17 Criticality=false
om2_1        | SubjectAlternativeName [
om2_1        |   IPAddress: 172.25.0.116
om2_1        |   DNSName: scm1.org
om2_1        | ]
om2_1        | 
om2_1        | ]
om2_1        |   Algorithm: [SHA256withRSA]
om2_1        |   Signature:
om2_1        | 0000: AB 5F B6 3B B0 5C 88 AB   DD 25 28 3F F6 7C 1A 59  ._.;.\...%(?...Y
om2_1        | 0010: AC B5 48 EF C8 75 0D 95   65 13 6C C4 5D 9A 7D D4  ..H..u..e.l.]...
om2_1        | 0020: 93 AF C2 9C CA DD 42 6C   C6 2D B6 5D EE CC 9A F0  ......Bl.-.]....
om2_1        | 0030: 95 DD 7D 3C D8 E8 28 A1   B8 09 E1 BA E1 DD 81 7A  ...<..(........z
om2_1        | 0040: 66 DB F9 45 B9 70 8B 0D   3A 5C A7 86 6A 8C F7 A7  f..E.p..:\..j...
om2_1        | 0050: E8 12 BD F5 3C 4C 8A 79   F9 53 EB 48 F0 23 70 F9  ....<L.y.S.H.#p.
om2_1        | 0060: 0F CC 68 07 FB 60 A9 BB   E7 35 31 EE D3 42 6A 9A  ..h..`...51..Bj.
om2_1        | 0070: 57 01 2A 55 0E E3 37 79   D8 76 24 C2 48 70 6F EB  W.*U..7y.v$.Hpo.
om2_1        | 0080: 61 A9 C9 E5 DC 38 A2 BF   D8 E8 6F 36 A5 D1 AB 3A  a....8....o6...:
om2_1        | 0090: 96 38 C7 8C 74 F7 01 E9   73 1E 4E 1D F8 E9 FA A8  .8..t...s.N.....
om2_1        | 00A0: 61 56 DA 60 4C 30 BF 35   59 8D 3B 69 0B 81 8D A5  aV.`L0.5Y.;i....
datanode3_1  | 2023-01-12 05:45:15,290 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-01-12 05:45:15,291 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-01-12 05:45:15,310 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-01-12 05:45:15,310 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-01-12 05:45:15,312 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-01-12 05:45:15,316 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-01-12 05:45:15,320 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-01-12 05:45:15,433 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-01-12 05:45:15,441 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-12 05:45:15,658 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-01-12 05:45:15,661 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-01-12 05:45:15,661 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-01-12 05:45:15,713 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-01-12 05:45:15,732 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-01-12 05:45:15,739 [pool-24-thread-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877: start as a follower, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-12 05:45:15,744 [pool-24-thread-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-01-12 05:45:15,749 [pool-24-thread-1] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: start 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState
datanode3_1  | 2023-01-12 05:45:15,783 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-ACB2C75A2877,id=75176c82-cf64-45b0-aad5-7780ce7245eb
datanode3_1  | 2023-01-12 05:45:15,784 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-12 05:45:15,790 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-12 05:45:15,798 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-01-12 05:45:15,799 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-01-12 05:45:15,806 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-01-12 05:45:15,811 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-01-12 05:45:15,921 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877
datanode3_1  | 2023-01-12 05:45:16,036 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-01-12 05:45:20,818 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO impl.FollowerState: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5068396893ns, electionTimeout:5013ms
datanode3_1  | 2023-01-12 05:45:20,822 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: shutdown 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState
datanode3_1  | 2023-01-12 05:45:20,828 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2023-01-12 05:45:20,894 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2023-01-12 05:45:20,896 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: start 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1
datanode3_1  | 2023-01-12 05:45:21,000 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1] INFO impl.LeaderElection: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-12 05:45:21,091 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-12 05:45:21,091 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-12 05:45:21,388 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 08ecee04-dee1-41b4-94f3-4660f61ac91b
datanode3_1  | 2023-01-12 05:45:21,388 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
datanode3_1  | 2023-01-12 05:45:24,109 [grpc-default-executor-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877: receive requestVote(ELECTION, 08ecee04-dee1-41b4-94f3-4660f61ac91b, group-ACB2C75A2877, 1, (t:0, i:0))
datanode3_1  | 2023-01-12 05:45:24,160 [grpc-default-executor-1] INFO impl.VoteContext: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-CANDIDATE: reject ELECTION from 08ecee04-dee1-41b4-94f3-4660f61ac91b: already has voted for 75176c82-cf64-45b0-aad5-7780ce7245eb at current term 1
datanode3_1  | 2023-01-12 05:45:24,387 [grpc-default-executor-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877 replies to ELECTION vote request: 08ecee04-dee1-41b4-94f3-4660f61ac91b<-75176c82-cf64-45b0-aad5-7780ce7245eb#0:FAIL-t1. Peer's state: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877:t1, leader=null, voted=75176c82-cf64-45b0-aad5-7780ce7245eb, raftlog=Memoized:75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-12 05:45:24,704 [grpc-default-executor-2] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877: receive requestVote(ELECTION, 08ecee04-dee1-41b4-94f3-4660f61ac91b, group-ACB2C75A2877, 2, (t:0, i:0))
datanode3_1  | 2023-01-12 05:45:24,705 [grpc-default-executor-2] INFO impl.VoteContext: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-CANDIDATE: reject ELECTION from 08ecee04-dee1-41b4-94f3-4660f61ac91b: our priority 1 > candidate's priority 0
datanode3_1  | 2023-01-12 05:45:24,709 [grpc-default-executor-2] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877: changes role from CANDIDATE to FOLLOWER at term 2 for candidate:08ecee04-dee1-41b4-94f3-4660f61ac91b
datanode3_1  | 2023-01-12 05:45:24,712 [grpc-default-executor-2] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: shutdown 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1
datanode3_1  | 2023-01-12 05:45:24,719 [grpc-default-executor-2] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: start 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState
datanode3_1  | 2023-01-12 05:45:24,730 [grpc-default-executor-2] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877 replies to ELECTION vote request: 08ecee04-dee1-41b4-94f3-4660f61ac91b<-75176c82-cf64-45b0-aad5-7780ce7245eb#0:FAIL-t2. Peer's state: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877:t2, leader=null, voted=null, raftlog=Memoized:75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-12 05:45:24,768 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-12 05:45:24,769 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-12 05:45:25,313 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1] INFO impl.LeaderElection: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1: ELECTION DISCOVERED_A_NEW_TERM (term=2) received 1 response(s) and 0 exception(s):
datanode3_1  | 2023-01-12 05:45:25,315 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1] INFO impl.LeaderElection:   Response 0: 75176c82-cf64-45b0-aad5-7780ce7245eb<-08ecee04-dee1-41b4-94f3-4660f61ac91b#0:FAIL-t2
datanode3_1  | 2023-01-12 05:45:25,315 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1] INFO impl.LeaderElection: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection1 ELECTION round 0: result DISCOVERED_A_NEW_TERM (term=2)
datanode3_1  | 2023-01-12 05:45:25,662 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-01-12 05:45:28,076 [grpc-default-executor-2] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877: receive requestVote(ELECTION, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3, group-ACB2C75A2877, 1, (t:0, i:0))
datanode3_1  | 2023-01-12 05:45:28,076 [grpc-default-executor-2] INFO impl.VoteContext: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FOLLOWER: reject ELECTION from a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: current term 2 > candidate's term 1
datanode3_1  | 2023-01-12 05:45:28,077 [grpc-default-executor-2] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877 replies to ELECTION vote request: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3<-75176c82-cf64-45b0-aad5-7780ce7245eb#0:FAIL-t2. Peer's state: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877:t2, leader=null, voted=null, raftlog=Memoized:75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-12 05:45:28,133 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877.
datanode3_1  | 2023-01-12 05:45:28,142 [pool-24-thread-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb: new RaftServerImpl for group-444D5AC91262:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-01-12 05:45:28,145 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-01-12 05:45:28,145 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-01-12 05:45:28,146 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-01-12 05:45:28,147 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-01-12 05:45:28,147 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-01-12 05:45:28,148 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-01-12 05:45:28,148 [pool-24-thread-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262: ConfigurationManager, init=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-01-12 05:45:28,578 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-01-12 05:45:28,579 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-01-12 05:45:28,607 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-01-12 05:45:28,624 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-01-12 05:45:28,655 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-01-12 05:45:28,673 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e22252e4-1bd7-4367-9a96-52a62dfb0a44
datanode2_1  | 2023-01-12 05:45:28,674 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-01-12 05:45:28,674 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-01-12 05:45:28,686 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-01-12 05:45:28,688 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-01-12 05:45:28,695 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-01-12 05:45:28,695 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-01-12 05:45:28,698 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-01-12 05:45:28,699 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-01-12 05:45:28,711 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-01-12 05:45:28,718 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-12 05:45:28,902 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-01-12 05:45:28,948 [grpc-default-executor-0] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877: receive requestVote(ELECTION, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3, group-ACB2C75A2877, 1, (t:0, i:0))
datanode2_1  | 2023-01-12 05:45:29,001 [grpc-default-executor-0] INFO impl.VoteContext: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FOLLOWER: reject ELECTION from a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: current term 2 > candidate's term 1
datanode2_1  | 2023-01-12 05:45:29,002 [grpc-default-executor-0] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877 replies to ELECTION vote request: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3<-08ecee04-dee1-41b4-94f3-4660f61ac91b#0:FAIL-t2. Peer's state: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877:t2, leader=null, voted=08ecee04-dee1-41b4-94f3-4660f61ac91b, raftlog=Memoized:08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-12 05:45:28,905 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-08ecee04-dee1-41b4-94f3-4660f61ac91b: Detected pause in JVM or host machine (eg GC): pause of approximately 130377096ns. No GCs detected.
datanode2_1  | 2023-01-12 05:45:28,965 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-01-12 05:45:29,060 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-01-12 05:45:29,061 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-01-12 05:45:29,061 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-01-12 05:45:29,071 [pool-24-thread-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44: start as a follower, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-12 05:45:29,071 [pool-24-thread-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2023-01-12 05:45:29,071 [pool-24-thread-1] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: start 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-FollowerState
datanode2_1  | 2023-01-12 05:45:29,116 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-52A62DFB0A44,id=08ecee04-dee1-41b4-94f3-4660f61ac91b
datanode2_1  | 2023-01-12 05:45:29,117 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-01-12 05:45:29,118 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-01-12 05:45:29,118 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2023-01-12 05:45:29,118 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-01-12 05:45:29,121 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-12 05:45:29,121 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44
datanode2_1  | 2023-01-12 05:45:29,127 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode2_1  | 2023-01-12 05:45:29,132 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-12 05:45:29,879 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-01-12 05:45:29,498 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2023-01-12 05:45:29,498 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2023-01-12 05:45:29,498 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2023-01-12 05:45:29,515 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2023-01-12 05:45:29,517 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode1_1  | 2023-01-12 05:45:29,517 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode1_1  | 2023-01-12 05:45:29,517 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode1_1  | 2023-01-12 05:45:29,518 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode1_1  | 2023-01-12 05:45:29,523 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e22252e4-1bd7-4367-9a96-52a62dfb0a44 does not exist. Creating ...
datanode1_1  | 2023-01-12 05:45:29,542 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e22252e4-1bd7-4367-9a96-52a62dfb0a44/in_use.lock acquired by nodename 7@fb1a7b8e0c66
datanode1_1  | 2023-01-12 05:45:29,581 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e22252e4-1bd7-4367-9a96-52a62dfb0a44 has been successfully formatted.
datanode1_1  | 2023-01-12 05:45:29,585 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-52A62DFB0A44: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2023-01-12 05:45:29,585 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2023-01-12 05:45:29,587 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2023-01-12 05:45:29,588 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-12 05:45:29,588 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode1_1  | 2023-01-12 05:45:29,588 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode1_1  | 2023-01-12 05:45:29,588 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-01-12 05:45:29,590 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2023-01-12 05:45:29,592 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2023-01-12 05:45:29,592 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e22252e4-1bd7-4367-9a96-52a62dfb0a44
datanode1_1  | 2023-01-12 05:45:29,594 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode1_1  | 2023-01-12 05:45:29,594 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2023-01-12 05:45:29,626 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2023-01-12 05:45:29,628 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2023-01-12 05:45:29,630 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2023-01-12 05:45:29,639 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2023-01-12 05:45:29,639 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2023-01-12 05:45:29,640 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2023-01-12 05:45:29,666 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2023-01-12 05:45:29,673 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2023-01-12 05:45:29,748 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode1_1  | 2023-01-12 05:45:29,750 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode1_1  | 2023-01-12 05:45:29,751 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2023-01-12 05:45:29,754 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-01-12 05:45:29,762 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2023-01-12 05:45:29,780 [pool-24-thread-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44: start as a follower, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-12 05:45:29,780 [pool-24-thread-1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2023-01-12 05:45:29,781 [pool-24-thread-1] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: start a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-FollowerState
datanode1_1  | 2023-01-12 05:45:29,795 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-52A62DFB0A44,id=a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
datanode1_1  | 2023-01-12 05:45:29,797 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2023-01-12 05:45:29,798 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2023-01-12 05:45:29,798 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2023-01-12 05:45:29,798 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2023-01-12 05:45:29,799 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-12 05:45:29,828 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44
datanode1_1  | 2023-01-12 05:45:29,834 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-01-12 05:45:28,149 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-01-12 05:45:28,148 [Command processor thread] INFO server.RaftServer: 75176c82-cf64-45b0-aad5-7780ce7245eb: addNew group-444D5AC91262:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER] returns group-444D5AC91262:java.util.concurrent.CompletableFuture@7310de96[Not completed]
datanode3_1  | 2023-01-12 05:45:28,150 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-01-12 05:45:28,151 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-01-12 05:45:28,153 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-01-12 05:45:28,154 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-01-12 05:45:28,155 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-01-12 05:45:28,158 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-01-12 05:45:28,160 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-01-12 05:45:28,161 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-01-12 05:45:28,161 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-01-12 05:45:28,163 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-01-12 05:45:28,163 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/48a05780-a7a4-4d83-bfcd-444d5ac91262 does not exist. Creating ...
datanode3_1  | 2023-01-12 05:45:28,170 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/48a05780-a7a4-4d83-bfcd-444d5ac91262/in_use.lock acquired by nodename 7@789a46e3b060
datanode3_1  | 2023-01-12 05:45:28,175 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/48a05780-a7a4-4d83-bfcd-444d5ac91262 has been successfully formatted.
datanode3_1  | 2023-01-12 05:45:28,218 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-444D5AC91262: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-01-12 05:45:28,228 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-01-12 05:45:28,229 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-01-12 05:45:28,229 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-12 05:45:28,229 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-01-12 05:45:28,229 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-01-12 05:45:28,230 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-01-12 05:45:28,318 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-01-12 05:45:28,331 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-01-12 05:45:28,333 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/48a05780-a7a4-4d83-bfcd-444d5ac91262
datanode3_1  | 2023-01-12 05:45:28,333 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-01-12 05:45:28,334 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-01-12 05:45:28,338 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-01-12 05:45:28,347 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-01-12 05:45:28,348 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-01-12 05:45:28,349 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-01-12 05:45:28,350 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-01-12 05:45:28,351 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-01-12 05:45:28,419 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-01-12 05:45:28,420 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-12 05:45:28,616 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-01-12 05:45:28,619 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-01-12 05:45:28,620 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-01-12 05:45:28,623 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-01-12 05:45:28,623 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-01-12 05:45:28,636 [pool-24-thread-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262: start as a follower, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-12 05:45:28,637 [pool-24-thread-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2023-01-12 05:45:28,638 [pool-24-thread-1] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: start 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-FollowerState
datanode3_1  | 2023-01-12 05:45:28,649 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-444D5AC91262,id=75176c82-cf64-45b0-aad5-7780ce7245eb
datanode3_1  | 2023-01-12 05:45:28,656 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-01-12 05:45:28,657 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-01-12 05:45:28,658 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om1_1        | 0050: E8 12 BD F5 3C 4C 8A 79   F9 53 EB 48 F0 23 70 F9  ....<L.y.S.H.#p.
om1_1        | 0060: 0F CC 68 07 FB 60 A9 BB   E7 35 31 EE D3 42 6A 9A  ..h..`...51..Bj.
om1_1        | 0070: 57 01 2A 55 0E E3 37 79   D8 76 24 C2 48 70 6F EB  W.*U..7y.v$.Hpo.
om1_1        | 0080: 61 A9 C9 E5 DC 38 A2 BF   D8 E8 6F 36 A5 D1 AB 3A  a....8....o6...:
om1_1        | 0090: 96 38 C7 8C 74 F7 01 E9   73 1E 4E 1D F8 E9 FA A8  .8..t...s.N.....
om1_1        | 00A0: 61 56 DA 60 4C 30 BF 35   59 8D 3B 69 0B 81 8D A5  aV.`L0.5Y.;i....
om1_1        | 00B0: 6B 1F 89 42 C0 DF B3 8D   F3 50 25 03 65 F9 37 86  k..B.....P%.e.7.
om1_1        | 00C0: 03 54 EB EC 49 05 E6 8D   AD B3 7E 57 37 64 8C 60  .T..I......W7d.`
om1_1        | 00D0: E2 61 4E 15 16 4B FC C7   AA D8 F7 E0 CC 71 75 DA  .aN..K.......qu.
om1_1        | 00E0: 6F A8 66 F2 98 35 28 51   F9 FC 7B F3 90 FD 46 64  o.f..5(Q......Fd
om1_1        | 00F0: D3 1C BD 33 23 88 5C 9A   F7 A8 F9 0D C0 CD 9B 86  ...3#.\.........
om1_1        | 
om1_1        | ] from file:/data/metadata/om/certs/CA-1212956981891.crt.
om1_1        | 2023-01-12 05:44:47,828 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor is started with first delay 29096112175 ms and interval 86400000 ms.
om1_1        | 2023-01-12 05:44:48,049 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2023-01-12 05:44:49,236 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om1_1        | 2023-01-12 05:44:49,241 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1        | 2023-01-12 05:44:50,428 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1        | 2023-01-12 05:44:50,562 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om1_1        | 2023-01-12 05:44:50,564 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om1_1        | 2023-01-12 05:44:52,054 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om1_1        | 2023-01-12 05:44:52,692 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2023-01-12 05:44:52,695 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1        | 2023-01-12 05:44:52,758 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om1_1        | 2023-01-12 05:44:53,479 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode2_1  | 2023-01-12 05:45:30,102 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO impl.FollowerState: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5096489358ns, electionTimeout:5026ms
datanode2_1  | 2023-01-12 05:45:30,117 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: shutdown 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState
datanode2_1  | 2023-01-12 05:45:30,117 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode2_1  | 2023-01-12 05:45:30,118 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2023-01-12 05:45:30,118 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: start 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection2
datanode2_1  | 2023-01-12 05:45:30,118 [grpc-default-executor-0] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877: receive requestVote(ELECTION, 75176c82-cf64-45b0-aad5-7780ce7245eb, group-ACB2C75A2877, 3, (t:0, i:0))
datanode2_1  | 2023-01-12 05:45:30,132 [grpc-default-executor-0] INFO impl.VoteContext: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-CANDIDATE: accept ELECTION from 75176c82-cf64-45b0-aad5-7780ce7245eb: our priority 0 <= candidate's priority 1
datanode2_1  | 2023-01-12 05:45:30,132 [grpc-default-executor-0] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877: changes role from CANDIDATE to FOLLOWER at term 3 for candidate:75176c82-cf64-45b0-aad5-7780ce7245eb
datanode2_1  | 2023-01-12 05:45:30,132 [grpc-default-executor-0] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: shutdown 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection2
datanode2_1  | 2023-01-12 05:45:30,133 [grpc-default-executor-0] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: start 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState
datanode2_1  | 2023-01-12 05:45:30,134 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection2] INFO impl.LeaderElection: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-LeaderElection2: skip running since this is already CLOSING
datanode2_1  | 2023-01-12 05:45:30,159 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-12 05:45:30,175 [grpc-default-executor-0] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877 replies to ELECTION vote request: 75176c82-cf64-45b0-aad5-7780ce7245eb<-08ecee04-dee1-41b4-94f3-4660f61ac91b#0:OK-t3. Peer's state: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877:t3, leader=null, voted=75176c82-cf64-45b0-aad5-7780ce7245eb, raftlog=Memoized:08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-12 05:45:30,201 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-12 05:45:30,706 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44.
datanode2_1  | 2023-01-12 05:45:30,710 [Command processor thread] INFO server.RaftServer: 08ecee04-dee1-41b4-94f3-4660f61ac91b: addNew group-60CEAE25F148:[08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER] returns group-60CEAE25F148:java.util.concurrent.CompletableFuture@5d9a898[Not completed]
datanode2_1  | 2023-01-12 05:45:31,034 [pool-24-thread-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b: new RaftServerImpl for group-60CEAE25F148:[08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode2_1  | 2023-01-12 05:45:31,044 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2023-01-12 05:45:31,044 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2023-01-12 05:45:31,044 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2023-01-12 05:45:31,045 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2023-01-12 05:45:31,045 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2023-01-12 05:45:31,046 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2023-01-12 05:45:31,051 [pool-24-thread-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-60CEAE25F148: ConfigurationManager, init=-1: peers:[08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2023-01-12 05:45:31,053 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2023-01-12 05:45:31,053 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2023-01-12 05:45:31,053 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2023-01-12 05:45:31,053 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2023-01-12 05:45:31,054 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2023-01-12 05:45:31,061 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2023-01-12 05:45:31,063 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2023-01-12 05:45:31,071 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode2_1  | 2023-01-12 05:45:31,072 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode2_1  | 2023-01-12 05:45:31,073 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode2_1  | 2023-01-12 05:45:31,074 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode2_1  | 2023-01-12 05:45:31,074 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/003546ba-ec7b-4d1e-b502-60ceae25f148 does not exist. Creating ...
om1_1        | 2023-01-12 05:44:53,528 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2023-01-12 05:44:53,743 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: id1 and peers: om1:9872, om3:9872, om2:9872
om1_1        | 2023-01-12 05:44:53,822 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om1_1        | 2023-01-12 05:44:54,995 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-01-12 05:44:55,018 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om1_1        | 2023-01-12 05:44:55,113 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1        | 2023-01-12 05:44:55,568 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1        | 2023-01-12 05:44:55,574 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1        | 2023-01-12 05:44:55,577 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1        | 2023-01-12 05:44:55,578 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1        | 2023-01-12 05:44:55,587 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1        | 2023-01-12 05:44:55,588 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1        | 2023-01-12 05:44:55,589 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1        | 2023-01-12 05:44:55,602 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-01-12 05:44:55,608 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1        | 2023-01-12 05:44:55,613 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2023-01-12 05:44:55,697 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1        | 2023-01-12 05:44:55,719 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1        | 2023-01-12 05:44:55,720 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1        | 2023-01-12 05:44:58,487 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
om1_1        | 2023-01-12 05:44:58,628 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
om1_1        | 2023-01-12 05:44:58,631 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
om1_1        | 2023-01-12 05:44:58,635 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
om1_1        | 2023-01-12 05:44:58,642 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
om1_1        | 2023-01-12 05:44:58,658 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
om1_1        | 2023-01-12 05:44:58,661 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
om1_1        | 2023-01-12 05:44:58,705 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
om1_1        | 2023-01-12 05:44:58,721 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
om1_1        | 2023-01-12 05:44:59,235 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
om1_1        | 2023-01-12 05:44:59,239 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
om1_1        | 2023-01-12 05:44:59,597 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1        | 2023-01-12 05:44:59,598 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1        | 2023-01-12 05:44:59,601 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2023-01-12 05:44:59,601 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2023-01-12 05:44:59,627 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2023-01-12 05:44:59,695 [om1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xba81943f] REGISTERED
om1_1        | 2023-01-12 05:44:59,709 [main] INFO server.RaftServer: om1: addNew group-562213E44849:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-562213E44849:java.util.concurrent.CompletableFuture@4a9cb9d8[Not completed]
om1_1        | 2023-01-12 05:44:59,722 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
datanode1_1  | 2023-01-12 05:45:29,849 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-12 05:45:30,066 [grpc-default-executor-3] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877: receive requestVote(ELECTION, 75176c82-cf64-45b0-aad5-7780ce7245eb, group-ACB2C75A2877, 3, (t:0, i:0))
datanode1_1  | 2023-01-12 05:45:30,066 [grpc-default-executor-3] INFO impl.VoteContext: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FOLLOWER: accept ELECTION from 75176c82-cf64-45b0-aad5-7780ce7245eb: our priority 0 <= candidate's priority 1
datanode1_1  | 2023-01-12 05:45:30,067 [grpc-default-executor-3] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:75176c82-cf64-45b0-aad5-7780ce7245eb
datanode1_1  | 2023-01-12 05:45:30,068 [grpc-default-executor-3] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: shutdown a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState
datanode1_1  | 2023-01-12 05:45:30,068 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState] INFO impl.FollowerState: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState was interrupted
datanode1_1  | 2023-01-12 05:45:30,069 [grpc-default-executor-3] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: start a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState
datanode1_1  | 2023-01-12 05:45:30,082 [grpc-default-executor-3] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877 replies to ELECTION vote request: 75176c82-cf64-45b0-aad5-7780ce7245eb<-a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3#0:OK-t3. Peer's state: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877:t3, leader=null, voted=75176c82-cf64-45b0-aad5-7780ce7245eb, raftlog=Memoized:a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-12 05:45:30,108 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-12 05:45:30,109 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode1_1  | 2023-01-12 05:45:30,425 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode1_1  | 2023-01-12 05:45:31,395 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44.
datanode1_1  | 2023-01-12 05:45:31,981 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-ACB2C75A2877 with new leaderId: 75176c82-cf64-45b0-aad5-7780ce7245eb
datanode1_1  | 2023-01-12 05:45:31,981 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3-server-thread1] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877: change Leader from null to 75176c82-cf64-45b0-aad5-7780ce7245eb at term 3 for appendEntries, leader elected after 16064ms
datanode1_1  | 2023-01-12 05:45:32,225 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: Detected pause in JVM or host machine (eg GC): pause of approximately 126946606ns. No GCs detected.
datanode1_1  | 2023-01-12 05:45:32,242 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3-server-thread3] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877: set configuration 0: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-12 05:45:32,248 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3-server-thread3] INFO segmented.SegmentedRaftLogWorker: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2023-01-12 05:45:32,252 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-ACB2C75A2877-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877/current/log_inprogress_0
datanode1_1  | 2023-01-12 05:45:34,241 [grpc-default-executor-0] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44: receive requestVote(ELECTION, 75176c82-cf64-45b0-aad5-7780ce7245eb, group-52A62DFB0A44, 1, (t:0, i:0))
datanode1_1  | 2023-01-12 05:45:34,242 [grpc-default-executor-0] INFO impl.VoteContext: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-FOLLOWER: accept ELECTION from 75176c82-cf64-45b0-aad5-7780ce7245eb: our priority 0 <= candidate's priority 0
datanode1_1  | 2023-01-12 05:45:34,242 [grpc-default-executor-0] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:75176c82-cf64-45b0-aad5-7780ce7245eb
datanode1_1  | 2023-01-12 05:45:34,242 [grpc-default-executor-0] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: shutdown a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-FollowerState
datanode1_1  | 2023-01-12 05:45:34,243 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-FollowerState] INFO impl.FollowerState: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-FollowerState was interrupted
datanode1_1  | 2023-01-12 05:45:34,257 [grpc-default-executor-0] INFO impl.RoleInfo: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3: start a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-FollowerState
datanode1_1  | 2023-01-12 05:45:34,262 [grpc-default-executor-0] INFO server.RaftServer$Division: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44 replies to ELECTION vote request: 75176c82-cf64-45b0-aad5-7780ce7245eb<-a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3#0:OK-t1. Peer's state: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44:t1, leader=null, voted=75176c82-cf64-45b0-aad5-7780ce7245eb, raftlog=Memoized:a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode1_1  | 2023-01-12 05:45:34,282 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode1_1  | 2023-01-12 05:45:34,289 [a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-12 05:45:31,092 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/003546ba-ec7b-4d1e-b502-60ceae25f148/in_use.lock acquired by nodename 7@e522f50beb6a
datanode2_1  | 2023-01-12 05:45:31,102 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/003546ba-ec7b-4d1e-b502-60ceae25f148 has been successfully formatted.
datanode2_1  | 2023-01-12 05:45:31,133 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-60CEAE25F148: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2023-01-12 05:45:31,143 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2023-01-12 05:45:31,157 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2023-01-12 05:45:31,158 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-12 05:45:31,158 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode2_1  | 2023-01-12 05:45:31,159 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode2_1  | 2023-01-12 05:45:31,182 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-01-12 05:45:31,184 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2023-01-12 05:45:31,186 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2023-01-12 05:45:31,186 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-60CEAE25F148-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/003546ba-ec7b-4d1e-b502-60ceae25f148
datanode2_1  | 2023-01-12 05:45:31,193 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode2_1  | 2023-01-12 05:45:31,195 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2023-01-12 05:45:31,195 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2023-01-12 05:45:31,205 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2023-01-12 05:45:31,206 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2023-01-12 05:45:31,206 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2023-01-12 05:45:31,217 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2023-01-12 05:45:31,217 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2023-01-12 05:45:31,219 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2023-01-12 05:45:31,227 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2023-01-12 05:45:31,409 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode2_1  | 2023-01-12 05:45:31,410 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode2_1  | 2023-01-12 05:45:31,411 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2023-01-12 05:45:31,413 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-60CEAE25F148-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-01-12 05:45:31,414 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-60CEAE25F148-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2023-01-12 05:45:31,421 [pool-24-thread-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-60CEAE25F148: start as a follower, conf=-1: peers:[08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-12 05:45:31,422 [pool-24-thread-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-60CEAE25F148: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2023-01-12 05:45:31,422 [pool-24-thread-1] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: start 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-60CEAE25F148-FollowerState
datanode2_1  | 2023-01-12 05:45:31,425 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-60CEAE25F148,id=08ecee04-dee1-41b4-94f3-4660f61ac91b
datanode2_1  | 2023-01-12 05:45:31,426 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2023-01-12 05:45:31,426 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2023-01-12 05:45:31,426 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2023-01-12 05:45:31,426 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2023-01-12 05:45:31,432 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-60CEAE25F148-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-12 05:45:31,433 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-60CEAE25F148-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-12 05:45:31,433 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=003546ba-ec7b-4d1e-b502-60ceae25f148
datanode2_1  | 2023-01-12 05:45:31,434 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=003546ba-ec7b-4d1e-b502-60ceae25f148.
datanode2_1  | 2023-01-12 05:45:32,078 [08ecee04-dee1-41b4-94f3-4660f61ac91b-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-ACB2C75A2877 with new leaderId: 75176c82-cf64-45b0-aad5-7780ce7245eb
datanode2_1  | 2023-01-12 05:45:32,088 [08ecee04-dee1-41b4-94f3-4660f61ac91b-server-thread1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877: change Leader from null to 75176c82-cf64-45b0-aad5-7780ce7245eb at term 3 for appendEntries, leader elected after 19985ms
datanode2_1  | 2023-01-12 05:45:32,149 [08ecee04-dee1-41b4-94f3-4660f61ac91b-server-thread2] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877: set configuration 0: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | Sleeping for 5 seconds
om3_1        | Waiting for the service scm3.org:9894
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2023-01-12 05:43:26,275 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = [--init]
om3_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
om3_1        | STARTUP_MSG:   java = 11.0.14.1
om3_1        | ************************************************************/
om3_1        | 2023-01-12 05:43:26,372 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2023-01-12 05:43:36,006 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1        | 2023-01-12 05:43:40,071 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om3_1        | 2023-01-12 05:43:40,792 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-01-12 05:43:40,795 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om3: om3
om3_1        | 2023-01-12 05:43:40,797 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om3: om3
om3_1        | 2023-01-12 05:43:42,115 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2023-01-12 05:43:42,116 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2023-01-12 05:43:42,272 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-01-12 05:43:43,418 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f;layoutVersion=3
om3_1        | 2023-01-12 05:43:47,050 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
om3_1        | 2023-01-12 05:43:47,051 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om3_1        | 2023-01-12 05:43:52,680 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2023-01-12 05:43:52,719 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om3_1        | 2023-01-12 05:43:52,733 [main] INFO security.OMCertificateClient: Certificate client init case: 0
om3_1        | 2023-01-12 05:43:52,744 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om3_1        | 2023-01-12 05:43:59,910 [main] INFO om.OzoneManager: Init response: GETCERT
om3_1        | 2023-01-12 05:44:00,468 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.113,host:om3
om3_1        | 2023-01-12 05:44:00,473 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om3_1        | 2023-01-12 05:44:00,521 [main] ERROR security.OMCertificateClient: Invalid domain om3
om3_1        | 2023-01-12 05:44:00,529 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om3_1        | 2023-01-12 05:44:00,543 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-01-12 05:44:00,552 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om3: om3
om3_1        | 2023-01-12 05:44:00,555 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om3: om3
om3_1        | 2023-01-12 05:44:00,581 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om3,ip:172.25.0.113,scmId:0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1,clusterId:CID-dd9a252d-f4dd-4162-963a-067d6dacb49f,subject:om3
om3_1        | 2023-01-12 05:44:04,451 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om3_1        | 2023-01-12 05:44:04,545 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1        | /************************************************************
om3_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om3/172.25.0.113
om3_1        | ************************************************************/
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2023-01-12 05:44:19,318 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
datanode3_1  | 2023-01-12 05:45:28,658 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-01-12 05:45:28,660 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-12 05:45:28,662 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-12 05:45:28,670 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=48a05780-a7a4-4d83-bfcd-444d5ac91262
datanode3_1  | 2023-01-12 05:45:28,670 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=48a05780-a7a4-4d83-bfcd-444d5ac91262.
datanode3_1  | 2023-01-12 05:45:28,672 [Command processor thread] INFO server.RaftServer: 75176c82-cf64-45b0-aad5-7780ce7245eb: addNew group-52A62DFB0A44:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER] returns group-52A62DFB0A44:java.util.concurrent.CompletableFuture@2b3b7627[Not completed]
datanode3_1  | 2023-01-12 05:45:28,696 [pool-24-thread-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb: new RaftServerImpl for group-52A62DFB0A44:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode3_1  | 2023-01-12 05:45:28,698 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2023-01-12 05:45:28,698 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2023-01-12 05:45:28,699 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2023-01-12 05:45:28,699 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2023-01-12 05:45:28,700 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2023-01-12 05:45:28,701 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2023-01-12 05:45:28,701 [pool-24-thread-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44: ConfigurationManager, init=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2023-01-12 05:45:28,702 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2023-01-12 05:45:28,702 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2023-01-12 05:45:28,703 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2023-01-12 05:45:28,711 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2023-01-12 05:45:28,715 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2023-01-12 05:45:28,716 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2023-01-12 05:45:28,720 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-01-12 05:45:28,720 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode3_1  | 2023-01-12 05:45:28,722 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode3_1  | 2023-01-12 05:45:28,723 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode3_1  | 2023-01-12 05:45:28,723 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode3_1  | 2023-01-12 05:45:28,724 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e22252e4-1bd7-4367-9a96-52a62dfb0a44 does not exist. Creating ...
datanode3_1  | 2023-01-12 05:45:28,730 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e22252e4-1bd7-4367-9a96-52a62dfb0a44/in_use.lock acquired by nodename 7@789a46e3b060
datanode3_1  | 2023-01-12 05:45:28,735 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e22252e4-1bd7-4367-9a96-52a62dfb0a44 has been successfully formatted.
datanode3_1  | 2023-01-12 05:45:28,740 [pool-24-thread-1] INFO ratis.ContainerStateMachine: group-52A62DFB0A44: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2023-01-12 05:45:28,740 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2023-01-12 05:45:28,741 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2023-01-12 05:45:28,741 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-12 05:45:28,742 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode3_1  | 2023-01-12 05:45:28,745 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode3_1  | 2023-01-12 05:45:28,746 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-01-12 05:45:28,751 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2023-01-12 05:45:28,751 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2023-01-12 05:45:28,751 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e22252e4-1bd7-4367-9a96-52a62dfb0a44
datanode3_1  | 2023-01-12 05:45:28,752 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode3_1  | 2023-01-12 05:45:28,752 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2023-01-12 05:45:28,752 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2023-01-12 05:45:28,752 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2023-01-12 05:45:28,753 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2023-01-12 05:45:28,754 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2023-01-12 05:45:28,754 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2023-01-12 05:45:28,756 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2023-01-12 05:45:28,806 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2023-01-12 05:45:28,812 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-12 05:45:28,965 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-01-12 05:45:28,968 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode3_1  | 2023-01-12 05:45:28,968 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2023-01-12 05:45:28,970 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-01-12 05:45:28,971 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2023-01-12 05:45:28,973 [pool-24-thread-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44: start as a follower, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-12 05:45:28,973 [pool-24-thread-1] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1        | 00B0: 6B 1F 89 42 C0 DF B3 8D   F3 50 25 03 65 F9 37 86  k..B.....P%.e.7.
om2_1        | 00C0: 03 54 EB EC 49 05 E6 8D   AD B3 7E 57 37 64 8C 60  .T..I......W7d.`
om2_1        | 00D0: E2 61 4E 15 16 4B FC C7   AA D8 F7 E0 CC 71 75 DA  .aN..K.......qu.
om2_1        | 00E0: 6F A8 66 F2 98 35 28 51   F9 FC 7B F3 90 FD 46 64  o.f..5(Q......Fd
om2_1        | 00F0: D3 1C BD 33 23 88 5C 9A   F7 A8 F9 0D C0 CD 9B 86  ...3#.\.........
om2_1        | 
om2_1        | ] from file:/data/metadata/om/certs/CA-1212956981891.crt.
om2_1        | 2023-01-12 05:44:48,741 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor is started with first delay 29096111266 ms and interval 86400000 ms.
om2_1        | 2023-01-12 05:44:48,951 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2023-01-12 05:44:49,770 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om2_1        | 2023-01-12 05:44:49,780 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om2_1        | 2023-01-12 05:44:51,802 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1        | 2023-01-12 05:44:51,891 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om2_1        | 2023-01-12 05:44:51,893 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om2_1        | 2023-01-12 05:44:52,346 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om2_1        | 2023-01-12 05:44:53,122 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2023-01-12 05:44:53,124 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1        | 2023-01-12 05:44:53,241 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om2_1        | 2023-01-12 05:44:54,235 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1        | 2023-01-12 05:44:54,336 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2023-01-12 05:44:54,673 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: id1 and peers: om2:9872, om1:9872, om3:9872
om2_1        | 2023-01-12 05:44:54,842 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om2_1        | 2023-01-12 05:44:56,162 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om2_1        | 2023-01-12 05:44:56,193 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om2_1        | 2023-01-12 05:44:56,288 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1        | 2023-01-12 05:44:56,559 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1        | 2023-01-12 05:44:56,567 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1        | 2023-01-12 05:44:56,570 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1        | 2023-01-12 05:44:56,573 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1        | 2023-01-12 05:44:56,575 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1        | 2023-01-12 05:44:56,576 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1        | 2023-01-12 05:44:56,580 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1        | 2023-01-12 05:44:56,585 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-01-12 05:44:56,589 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1        | 2023-01-12 05:44:56,592 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2023-01-12 05:44:56,647 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1        | 2023-01-12 05:44:56,668 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1        | 2023-01-12 05:44:56,669 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1        | 2023-01-12 05:44:59,456 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
om2_1        | 2023-01-12 05:44:59,585 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
om2_1        | 2023-01-12 05:44:59,588 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
om2_1        | 2023-01-12 05:44:59,594 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
om2_1        | 2023-01-12 05:44:59,598 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
om2_1        | 2023-01-12 05:44:59,605 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
om2_1        | 2023-01-12 05:44:59,607 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
om2_1        | 2023-01-12 05:44:59,626 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
om2_1        | 2023-01-12 05:44:59,661 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
om2_1        | 2023-01-12 05:45:00,059 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
om2_1        | 2023-01-12 05:45:00,060 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
om2_1        | 2023-01-12 05:45:00,315 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1        | 2023-01-12 05:45:00,321 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1        | 2023-01-12 05:45:00,321 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-01-12 05:45:00,321 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2023-01-12 05:45:00,359 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2023-01-12 05:45:00,398 [om2-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xd957d5a9] REGISTERED
om2_1        | 2023-01-12 05:45:00,413 [om2-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xd957d5a9] BIND: 0.0.0.0/0.0.0.0:0
om2_1        | 2023-01-12 05:45:00,422 [om2-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xd957d5a9, L:/0.0.0.0:36399] ACTIVE
om2_1        | 2023-01-12 05:45:00,427 [main] INFO server.RaftServer: om2: addNew group-562213E44849:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-562213E44849:java.util.concurrent.CompletableFuture@735d1db7[Not completed]
om2_1        | 2023-01-12 05:45:00,427 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
recon_1      | Sleeping for 5 seconds
recon_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1      | 2023-01-12 05:41:38,548 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1      | /************************************************************
recon_1      | STARTUP_MSG: Starting ReconServer
recon_1      | STARTUP_MSG:   host = recon/172.25.0.115
recon_1      | STARTUP_MSG:   args = []
recon_1      | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.23.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.3.23.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.23.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.23.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.23.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
recon_1      | STARTUP_MSG:   java = 11.0.14.1
recon_1      | ************************************************************/
recon_1      | 2023-01-12 05:41:38,595 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1      | 2023-01-12 05:41:41,171 [main] INFO reflections.Reflections: Reflections took 369 ms to scan 1 urls, producing 17 keys and 54 values 
recon_1      | 2023-01-12 05:41:43,993 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1      | 2023-01-12 05:41:44,134 [main] INFO recon.ReconServer: Ozone security is enabled. Attempting login for Recon service. Principal: recon/recon@EXAMPLE.COM, keytab: /etc/security/keytabs/recon.keytab
recon_1      | 2023-01-12 05:41:44,940 [main] INFO security.UserGroupInformation: Login successful for user recon/recon@EXAMPLE.COM using keytab file recon.keytab. Keytab auto renewal enabled : false
recon_1      | 2023-01-12 05:41:44,943 [main] INFO recon.ReconServer: Recon login successful.
recon_1      | 2023-01-12 05:41:44,995 [main] INFO recon.ReconServer: ReconStorageConfig initialized.Initializing certificate.
recon_1      | 2023-01-12 05:41:44,995 [main] INFO recon.ReconServer: Initializing secure Recon.
recon_1      | 2023-01-12 05:41:47,142 [main] ERROR client.ReconCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
recon_1      | 2023-01-12 05:41:47,152 [main] INFO client.ReconCertificateClient: Certificate client init case: 0
recon_1      | 2023-01-12 05:41:47,153 [main] INFO client.ReconCertificateClient: Creating keypair for client as keypair and certificate not found.
recon_1      | 2023-01-12 05:41:48,953 [main] INFO recon.ReconServer: Init response: GETCERT
recon_1      | 2023-01-12 05:41:48,954 [main] INFO client.ReconCertificateClient: Creating CSR for Recon.
recon_1      | 2023-01-12 05:41:48,981 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.115,host:recon
recon_1      | 2023-01-12 05:41:48,981 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = []
om3_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
om3_1        | STARTUP_MSG:   java = 11.0.14.1
om3_1        | ************************************************************/
om3_1        | 2023-01-12 05:44:19,415 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2023-01-12 05:44:28,108 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1        | 2023-01-12 05:44:32,346 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om3_1        | 2023-01-12 05:44:33,015 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2023-01-12 05:44:33,016 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om3: om3
om3_1        | 2023-01-12 05:44:33,019 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om3: om3
om3_1        | 2023-01-12 05:44:33,131 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-01-12 05:44:33,409 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om3_1        | 2023-01-12 05:44:35,678 [main] INFO reflections.Reflections: Reflections took 1759 ms to scan 1 urls, producing 115 keys and 335 values [using 2 cores]
om3_1        | 2023-01-12 05:44:37,566 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2023-01-12 05:44:37,567 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2023-01-12 05:44:37,569 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-01-12 05:44:41,815 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | 2023-01-12 05:44:42,392 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
om3_1        | 2023-01-12 05:44:47,043 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2023-01-12 05:44:47,084 [main] INFO security.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om3_1        | 2023-01-12 05:44:47,894 [main] INFO security.OMCertificateClient: Added certificate [
om3_1        | [
om3_1        |   Version: V3
om3_1        |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
om3_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om3_1        | 
om3_1        |   Key:  Sun RSA public key, 2048 bits
om3_1        |   params: null
om3_1        |   modulus: 30346053453237189834112040673541629567010306125791898570104054454047501736657892214383462264526301500298692652758375836429638347531164751981653358525504527825570400558838334901393005984876414719430775458652128247547444727818535668600972192623637356865083888753952717740665794517921055984453520353917733198649461590018542814322448272297366500121604394586840649019781675207082962097691558583700862118486739043197335879323701508955628436573724419921778589743880043303622162563304571704603236749325057266808761842228199352214967718782980075765678400986380731445934917806096546808808883868771850900183098234200323729287601
om3_1        |   public exponent: 65537
om3_1        |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
om3_1        |                To: Sun Feb 20 00:00:00 UTC 2028]
om3_1        |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
om3_1        |   SerialNumber: [    01]
om3_1        | 
om3_1        | Certificate Extensions: 3
om3_1        | [1]: ObjectId: 2.5.29.19 Criticality=true
om3_1        | BasicConstraints:[
om3_1        |   CA:true
om3_1        |   PathLen:2147483647
om3_1        | ]
om3_1        | 
om3_1        | [2]: ObjectId: 2.5.29.15 Criticality=true
om3_1        | KeyUsage [
om3_1        |   Key_CertSign
om3_1        |   Crl_Sign
om3_1        | ]
om3_1        | 
om3_1        | [3]: ObjectId: 2.5.29.17 Criticality=false
om3_1        | SubjectAlternativeName [
om3_1        |   IPAddress: 172.25.0.116
om3_1        |   DNSName: scm1.org
om3_1        | ]
om3_1        | 
om3_1        | ]
om3_1        |   Algorithm: [SHA256withRSA]
om3_1        |   Signature:
om3_1        | 0000: 59 E4 14 CA 42 0B 71 BE   61 3A FA 41 71 F2 48 C6  Y...B.q.a:.Aq.H.
om3_1        | 0010: 35 3A DF E8 FE E3 74 A0   7E 57 15 0F F2 39 95 34  5:....t..W...9.4
om3_1        | 0020: E6 A7 48 CE 06 EA B9 42   CC AD DC 81 09 F1 F2 3E  ..H....B.......>
om3_1        | 0030: 94 27 78 5F 6B 04 F9 F4   E3 A1 71 61 8F 5A EF 6D  .'x_k.....qa.Z.m
datanode2_1  | 2023-01-12 05:45:32,235 [08ecee04-dee1-41b4-94f3-4660f61ac91b-server-thread2] INFO segmented.SegmentedRaftLogWorker: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-SegmentedRaftLogWorker: Starting segment from index:0
datanode2_1  | 2023-01-12 05:45:32,771 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-ACB2C75A2877-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877/current/log_inprogress_0
datanode2_1  | 2023-01-12 05:45:34,238 [grpc-default-executor-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44: receive requestVote(ELECTION, 75176c82-cf64-45b0-aad5-7780ce7245eb, group-52A62DFB0A44, 1, (t:0, i:0))
datanode2_1  | 2023-01-12 05:45:34,241 [grpc-default-executor-1] INFO impl.VoteContext: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-FOLLOWER: reject ELECTION from 75176c82-cf64-45b0-aad5-7780ce7245eb: our priority 1 > candidate's priority 0
datanode2_1  | 2023-01-12 05:45:34,242 [grpc-default-executor-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:75176c82-cf64-45b0-aad5-7780ce7245eb
datanode2_1  | 2023-01-12 05:45:34,243 [grpc-default-executor-1] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: shutdown 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-FollowerState
datanode2_1  | 2023-01-12 05:45:34,243 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-FollowerState] INFO impl.FollowerState: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-FollowerState was interrupted
datanode2_1  | 2023-01-12 05:45:34,249 [grpc-default-executor-1] INFO impl.RoleInfo: 08ecee04-dee1-41b4-94f3-4660f61ac91b: start 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-FollowerState
datanode2_1  | 2023-01-12 05:45:34,253 [grpc-default-executor-1] INFO server.RaftServer$Division: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44 replies to ELECTION vote request: 75176c82-cf64-45b0-aad5-7780ce7245eb<-08ecee04-dee1-41b4-94f3-4660f61ac91b#0:FAIL-t1. Peer's state: 08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44:t1, leader=null, voted=null, raftlog=Memoized:08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode2_1  | 2023-01-12 05:45:34,300 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode2_1  | 2023-01-12 05:45:34,301 [08ecee04-dee1-41b4-94f3-4660f61ac91b@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode2_1  | 2023-01-12 05:45:35,650 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-08ecee04-dee1-41b4-94f3-4660f61ac91b: Detected pause in JVM or host machine (eg GC): pause of approximately 113318485ns. No GCs detected.
om2_1        | 2023-01-12 05:45:00,553 [pool-28-thread-1] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-562213E44849:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1        | 2023-01-12 05:45:00,572 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1        | 2023-01-12 05:45:00,590 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1        | 2023-01-12 05:45:00,590 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1        | 2023-01-12 05:45:00,590 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2023-01-12 05:45:00,591 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2023-01-12 05:45:00,591 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1        | 2023-01-12 05:45:00,617 [main] INFO om.OzoneManager: Creating RPC Server
om2_1        | 2023-01-12 05:45:00,685 [pool-28-thread-1] INFO server.RaftServer$Division: om2@group-562213E44849: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1        | 2023-01-12 05:45:00,685 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2023-01-12 05:45:00,759 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1        | 2023-01-12 05:45:00,811 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1        | 2023-01-12 05:45:00,925 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1        | 2023-01-12 05:45:00,955 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1        | 2023-01-12 05:45:00,956 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1        | 2023-01-12 05:45:01,438 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1        | 2023-01-12 05:45:01,439 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1        | 2023-01-12 05:45:01,439 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1        | 2023-01-12 05:45:01,452 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1        | 2023-01-12 05:45:01,452 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1        | 2023-01-12 05:45:03,774 [main] INFO reflections.Reflections: Reflections took 2568 ms to scan 8 urls, producing 23 keys and 545 values [using 2 cores]
om2_1        | 2023-01-12 05:45:05,394 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1        | 2023-01-12 05:45:05,458 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1        | 2023-01-12 05:45:10,899 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1        | 2023-01-12 05:45:10,984 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1        | 2023-01-12 05:45:10,984 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1        | 2023-01-12 05:45:11,389 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/172.25.0.112:9862
om2_1        | 2023-01-12 05:45:11,392 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1        | 2023-01-12 05:45:11,422 [om2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om2_1        | 2023-01-12 05:45:11,456 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 7@om2
om2_1        | 2023-01-12 05:45:11,504 [om2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om2_1        | 2023-01-12 05:45:11,524 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1        | 2023-01-12 05:45:11,623 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1        | 2023-01-12 05:45:11,623 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-01-12 05:45:11,637 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1        | 2023-01-12 05:45:11,642 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1        | 2023-01-12 05:45:11,646 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2023-01-12 05:45:11,731 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1        | 2023-01-12 05:45:11,740 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1        | 2023-01-12 05:45:11,812 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-562213E44849-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om2_1        | 2023-01-12 05:45:11,818 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1        | 2023-01-12 05:45:11,826 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1        | 2023-01-12 05:45:11,836 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2023-01-12 05:45:11,840 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1        | 2023-01-12 05:45:11,844 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1        | 2023-01-12 05:45:11,858 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1        | 2023-01-12 05:45:11,859 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1        | 2023-01-12 05:45:11,867 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1        | 2023-01-12 05:45:11,962 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1        | 2023-01-12 05:45:11,965 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2023-01-12 05:45:12,091 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode3_1  | 2023-01-12 05:45:28,990 [pool-24-thread-1] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: start 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState
datanode3_1  | 2023-01-12 05:45:29,093 [pool-24-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-52A62DFB0A44,id=75176c82-cf64-45b0-aad5-7780ce7245eb
datanode3_1  | 2023-01-12 05:45:29,099 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2023-01-12 05:45:29,099 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2023-01-12 05:45:29,099 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2023-01-12 05:45:29,099 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2023-01-12 05:45:29,105 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-12 05:45:29,125 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-12 05:45:29,124 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44
datanode3_1  | 2023-01-12 05:45:29,135 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-01-12 05:45:29,742 [Command processor thread] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig2-
datanode3_1  | 2023-01-12 05:45:29,963 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO impl.FollowerState: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5243075979ns, electionTimeout:5193ms
datanode3_1  | 2023-01-12 05:45:29,971 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: shutdown 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState
datanode3_1  | 2023-01-12 05:45:29,971 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode3_1  | 2023-01-12 05:45:29,972 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2023-01-12 05:45:29,973 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-FollowerState] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: start 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2
datanode3_1  | 2023-01-12 05:45:30,014 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO impl.LeaderElection: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2 ELECTION round 0: submit vote requests at term 3 for -1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-12 05:45:30,029 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-12 05:45:30,029 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-12 05:45:30,102 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO impl.LeaderElection: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode3_1  | 2023-01-12 05:45:30,109 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO impl.LeaderElection:   Response 0: 75176c82-cf64-45b0-aad5-7780ce7245eb<-a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3#0:OK-t3
datanode3_1  | 2023-01-12 05:45:30,114 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO impl.LeaderElection: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2 ELECTION round 0: result PASSED
datanode3_1  | 2023-01-12 05:45:30,119 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: shutdown 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2
datanode3_1  | 2023-01-12 05:45:30,120 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
datanode3_1  | 2023-01-12 05:45:30,120 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-ACB2C75A2877 with new leaderId: 75176c82-cf64-45b0-aad5-7780ce7245eb
datanode3_1  | 2023-01-12 05:45:30,164 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877: change Leader from null to 75176c82-cf64-45b0-aad5-7780ce7245eb at term 3 for becomeLeader, leader elected after 16347ms
datanode3_1  | 2023-01-12 05:45:30,443 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode3_1  | 2023-01-12 05:45:30,584 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-01-12 05:45:30,598 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode3_1  | 2023-01-12 05:45:30,665 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode3_1  | 2023-01-12 05:45:30,668 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode3_1  | 2023-01-12 05:45:30,670 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode3_1  | 2023-01-12 05:45:30,723 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44.
datanode3_1  | 2023-01-12 05:45:30,766 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-01-12 05:45:30,823 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode3_1  | 2023-01-12 05:45:30,934 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode3_1  | 2023-01-12 05:45:30,934 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2023-01-12 05:45:30,935 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode3_1  | 2023-01-12 05:45:30,949 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode3_1  | 2023-01-12 05:45:30,966 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-01-12 05:45:30,976 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-01-12 05:45:30,978 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode3_1  | 2023-01-12 05:45:30,985 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode3_1  | 2023-01-12 05:45:30,992 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode3_1  | 2023-01-12 05:45:31,001 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-01-12 05:44:59,730 [om1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xba81943f] BIND: 0.0.0.0/0.0.0.0:0
om1_1        | 2023-01-12 05:44:59,748 [om1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xba81943f, L:/0.0.0.0:34655] ACTIVE
om1_1        | 2023-01-12 05:44:59,836 [pool-28-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-562213E44849:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1        | 2023-01-12 05:44:59,846 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1        | 2023-01-12 05:44:59,877 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1        | 2023-01-12 05:44:59,878 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1        | 2023-01-12 05:44:59,884 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2023-01-12 05:44:59,885 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2023-01-12 05:44:59,888 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1        | 2023-01-12 05:44:59,915 [main] INFO om.OzoneManager: Creating RPC Server
om1_1        | 2023-01-12 05:45:00,017 [pool-28-thread-1] INFO server.RaftServer$Division: om1@group-562213E44849: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1        | 2023-01-12 05:45:00,021 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2023-01-12 05:45:00,084 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1        | 2023-01-12 05:45:00,085 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1        | 2023-01-12 05:45:00,207 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1        | 2023-01-12 05:45:00,220 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1        | 2023-01-12 05:45:00,224 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1        | 2023-01-12 05:45:00,678 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1        | 2023-01-12 05:45:00,699 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1        | 2023-01-12 05:45:00,708 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1        | 2023-01-12 05:45:00,709 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1        | 2023-01-12 05:45:00,709 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1        | 2023-01-12 05:45:03,596 [main] INFO reflections.Reflections: Reflections took 3162 ms to scan 8 urls, producing 23 keys and 545 values [using 2 cores]
om1_1        | 2023-01-12 05:45:04,869 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1        | 2023-01-12 05:45:04,903 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1        | 2023-01-12 05:45:10,891 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1        | 2023-01-12 05:45:10,958 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1        | 2023-01-12 05:45:10,959 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1        | 2023-01-12 05:45:11,361 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/172.25.0.111:9862
om1_1        | 2023-01-12 05:45:11,362 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1        | 2023-01-12 05:45:11,391 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om1_1        | 2023-01-12 05:45:11,424 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 7@om1
om1_1        | 2023-01-12 05:45:11,496 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om1_1        | 2023-01-12 05:45:11,552 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1        | 2023-01-12 05:45:11,683 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1        | 2023-01-12 05:45:11,696 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-01-12 05:45:11,711 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1        | 2023-01-12 05:45:11,723 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1        | 2023-01-12 05:45:11,762 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2023-01-12 05:45:11,872 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1        | 2023-01-12 05:45:11,877 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1        | 2023-01-12 05:45:11,952 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-562213E44849-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om1_1        | 2023-01-12 05:45:11,959 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1        | 2023-01-12 05:45:11,963 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1        | 2023-01-12 05:45:11,979 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2023-01-12 05:45:11,981 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1        | 2023-01-12 05:45:11,991 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1        | 2023-01-12 05:45:12,004 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1        | 2023-01-12 05:45:12,023 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1        | 2023-01-12 05:45:12,028 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1        | 2023-01-12 05:45:12,175 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1        | 2023-01-12 05:45:12,177 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2023-01-12 05:45:12,327 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1        | 2023-01-12 05:45:12,330 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1        | 2023-01-12 05:45:12,331 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
recon_1      | 2023-01-12 05:41:48,991 [main] ERROR client.ReconCertificateClient: Invalid domain recon
recon_1      | 2023-01-12 05:41:52,156 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:41:54,159 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:41:56,160 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:41:58,162 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:42:00,164 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:42:02,166 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:42:04,168 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:42:06,171 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:42:08,173 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:42:10,175 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:42:12,178 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:42:14,520 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1      | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1      | 	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:93)
recon_1      | 	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:16080)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy39.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9961 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:42:16,522 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9961 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1        | 0040: 72 EA 9B CB 23 B9 31 9F   D7 B3 7B 86 35 09 CF D1  r...#.1.....5...
om3_1        | 0050: D6 81 A8 2E 21 01 F7 A9   B4 78 00 21 D5 3C 4A 61  ....!....x.!.<Ja
om3_1        | 0060: 7F 91 56 B9 50 6C 2C 23   0F D3 49 6E 80 28 7A 03  ..V.Pl,#..In.(z.
om3_1        | 0070: 32 46 42 54 4F 22 06 7F   B1 7D 41 A0 09 27 CF 7A  2FBTO"....A..'.z
om3_1        | 0080: F0 82 C6 00 70 41 5D 8F   83 5F E6 EB 74 1F B7 A2  ....pA].._..t...
om3_1        | 0090: 85 64 3E AE D6 66 71 7C   BE 7F 92 D0 79 64 71 C0  .d>..fq.....ydq.
om3_1        | 00A0: 8F 76 8C F6 D1 A4 67 C4   F4 11 8D 75 FC DE EF EE  .v....g....u....
om3_1        | 00B0: D9 7F 75 F3 5F AC 76 DF   23 F9 19 99 25 F9 31 8E  ..u._.v.#...%.1.
om3_1        | 00C0: AA 69 DC 7B 7E 4E 62 10   80 B9 5E 00 F8 1D 16 71  .i...Nb...^....q
om3_1        | 00D0: 7F D4 C7 FD 20 63 0B 0A   7E 36 7D F4 4C 85 A7 61  .... c...6..L..a
om3_1        | 00E0: E7 00 61 A2 83 40 78 92   3E A1 9A E0 B8 E4 A1 A0  ..a..@x.>.......
om3_1        | 00F0: 45 B5 33 7C 03 24 98 E7   DE 69 74 45 16 2B 06 3F  E.3..$...itE.+.?
om3_1        | 
om3_1        | ] from file:/data/metadata/om/certs/ROOTCA-1.crt.
om3_1        | 2023-01-12 05:44:47,917 [main] INFO security.OMCertificateClient: Added certificate [
om3_1        | [
om3_1        |   Version: V3
om3_1        |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
om3_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om3_1        | 
om3_1        |   Key:  Sun RSA public key, 2048 bits
om3_1        |   params: null
om3_1        |   modulus: 21838495709719174090944017025941967908224182768319498180393936128987176221476088419281179276073314404186963320459954621839794791804718496604717042740409331050670636901380744918424604519064284257795379580082372800428810093145199831557386718853872621914973921810155992135096086488011377748521113854732595726272396205530150917174127452444863380225901746740157534773765777653459292048522370651962381319687098813297977743208706305312169254062598162002740262036220188710005290347458814949877767522524881554322161686065229152643489368945279202993130114091164740627711162880068229981146789996366091625523376604972129960509039
om3_1        |   public exponent: 65537
om3_1        |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
om3_1        |                To: Sun Feb 20 00:00:00 UTC 2028]
om3_1        |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
om3_1        |   SerialNumber: [    011a69de ba83]
om3_1        | 
om3_1        | Certificate Extensions: 3
om3_1        | [1]: ObjectId: 2.5.29.19 Criticality=true
om3_1        | BasicConstraints:[
om3_1        |   CA:true
om3_1        |   PathLen:2147483647
om3_1        | ]
om3_1        | 
om3_1        | [2]: ObjectId: 2.5.29.15 Criticality=true
om3_1        | KeyUsage [
om3_1        |   DigitalSignature
om3_1        |   Key_Encipherment
om3_1        |   Data_Encipherment
om3_1        |   Key_Agreement
om3_1        |   Key_CertSign
om3_1        |   Crl_Sign
om3_1        | ]
om3_1        | 
om3_1        | [3]: ObjectId: 2.5.29.17 Criticality=false
om3_1        | SubjectAlternativeName [
om3_1        |   IPAddress: 172.25.0.116
om3_1        |   DNSName: scm1.org
om3_1        | ]
om3_1        | 
om3_1        | ]
om3_1        |   Algorithm: [SHA256withRSA]
om3_1        |   Signature:
om3_1        | 0000: AB 5F B6 3B B0 5C 88 AB   DD 25 28 3F F6 7C 1A 59  ._.;.\...%(?...Y
om3_1        | 0010: AC B5 48 EF C8 75 0D 95   65 13 6C C4 5D 9A 7D D4  ..H..u..e.l.]...
om3_1        | 0020: 93 AF C2 9C CA DD 42 6C   C6 2D B6 5D EE CC 9A F0  ......Bl.-.]....
om3_1        | 0030: 95 DD 7D 3C D8 E8 28 A1   B8 09 E1 BA E1 DD 81 7A  ...<..(........z
om3_1        | 0040: 66 DB F9 45 B9 70 8B 0D   3A 5C A7 86 6A 8C F7 A7  f..E.p..:\..j...
om3_1        | 0050: E8 12 BD F5 3C 4C 8A 79   F9 53 EB 48 F0 23 70 F9  ....<L.y.S.H.#p.
om3_1        | 0060: 0F CC 68 07 FB 60 A9 BB   E7 35 31 EE D3 42 6A 9A  ..h..`...51..Bj.
om3_1        | 0070: 57 01 2A 55 0E E3 37 79   D8 76 24 C2 48 70 6F EB  W.*U..7y.v$.Hpo.
om3_1        | 0080: 61 A9 C9 E5 DC 38 A2 BF   D8 E8 6F 36 A5 D1 AB 3A  a....8....o6...:
om3_1        | 0090: 96 38 C7 8C 74 F7 01 E9   73 1E 4E 1D F8 E9 FA A8  .8..t...s.N.....
om3_1        | 00A0: 61 56 DA 60 4C 30 BF 35   59 8D 3B 69 0B 81 8D A5  aV.`L0.5Y.;i....
s3g_1        | Sleeping for 5 seconds
s3g_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1        | 2023-01-12 05:41:42,776 [main] INFO security.UserGroupInformation: Login successful for user s3g/s3g@EXAMPLE.COM using keytab file s3g.keytab. Keytab auto renewal enabled : false
s3g_1        | 2023-01-12 05:41:42,795 [main] INFO s3.Gateway: S3Gateway login successful.
s3g_1        | 2023-01-12 05:41:43,180 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1        | 2023-01-12 05:41:43,181 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
s3g_1        | 2023-01-12 05:41:43,181 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.s3g.http.auth.type = kerberos
s3g_1        | 2023-01-12 05:41:43,325 [main] INFO util.log: Logging initialized @7464ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1        | 2023-01-12 05:41:44,108 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1        | 2023-01-12 05:41:44,179 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1        | 2023-01-12 05:41:44,189 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context s3gateway
s3g_1        | 2023-01-12 05:41:44,195 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
s3g_1        | 2023-01-12 05:41:44,196 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
s3g_1        | 2023-01-12 05:41:44,217 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.s3g.http.auth.kerberos.principal keytabKey: ozone.s3g.http.auth.kerberos.keytab
s3g_1        | 2023-01-12 05:41:44,667 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1        | /************************************************************
s3g_1        | STARTUP_MSG: Starting Gateway
s3g_1        | STARTUP_MSG:   host = s3g/172.25.0.114
s3g_1        | STARTUP_MSG:   args = []
s3g_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
s3g_1        | STARTUP_MSG:   java = 11.0.14.1
s3g_1        | ************************************************************/
s3g_1        | 2023-01-12 05:41:44,718 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1        | 2023-01-12 05:41:44,783 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1        | 2023-01-12 05:41:45,101 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1        | 2023-01-12 05:41:45,594 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1        | 2023-01-12 05:41:45,594 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1        | 2023-01-12 05:41:45,686 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1        | 2023-01-12 05:41:45,696 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1        | 2023-01-12 05:41:45,875 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1        | 2023-01-12 05:41:45,883 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1        | 2023-01-12 05:41:45,885 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1        | 2023-01-12 05:41:45,982 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | 2023-01-12 05:41:46,006 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2fab4aff{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1        | 2023-01-12 05:41:46,007 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@55f3c410{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1        | WARNING: An illegal reflective access operation has occurred
s3g_1        | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1        | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1        | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1        | WARNING: All illegal access operations will be denied in a future release
s3g_1        | 2023-01-12 05:41:52,527 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | Jan 12, 2023 5:41:54 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1        | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1        | 
s3g_1        | 2023-01-12 05:41:54,681 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4d065e1a{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-3895835097002068868/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1        | 2023-01-12 05:41:54,693 [main] INFO server.AbstractConnector: Started ServerConnector@b5cc23a{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1        | 2023-01-12 05:41:54,694 [main] INFO server.Server: Started @18834ms
s3g_1        | 2023-01-12 05:41:54,707 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1        | 2023-01-12 05:41:54,708 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1        | 2023-01-12 05:41:54,714 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
recon_1      | 2023-01-12 05:42:18,524 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy39.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9961 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:42:20,737 [main] INFO client.ReconCertificateClient: Loading certificate from location:/data/metadata/recon/certs.
recon_1      | 2023-01-12 05:42:20,777 [main] INFO client.ReconCertificateClient: Added certificate [
recon_1      | [
recon_1      |   Version: V3
recon_1      |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
recon_1      |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
recon_1      | 
recon_1      |   Key:  Sun RSA public key, 2048 bits
recon_1      |   params: null
recon_1      |   modulus: 30346053453237189834112040673541629567010306125791898570104054454047501736657892214383462264526301500298692652758375836429638347531164751981653358525504527825570400558838334901393005984876414719430775458652128247547444727818535668600972192623637356865083888753952717740665794517921055984453520353917733198649461590018542814322448272297366500121604394586840649019781675207082962097691558583700862118486739043197335879323701508955628436573724419921778589743880043303622162563304571704603236749325057266808761842228199352214967718782980075765678400986380731445934917806096546808808883868771850900183098234200323729287601
recon_1      |   public exponent: 65537
recon_1      |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
recon_1      |                To: Sun Feb 20 00:00:00 UTC 2028]
recon_1      |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
recon_1      |   SerialNumber: [    01]
recon_1      | 
recon_1      | Certificate Extensions: 3
recon_1      | [1]: ObjectId: 2.5.29.19 Criticality=true
recon_1      | BasicConstraints:[
recon_1      |   CA:true
recon_1      |   PathLen:2147483647
recon_1      | ]
recon_1      | 
recon_1      | [2]: ObjectId: 2.5.29.15 Criticality=true
recon_1      | KeyUsage [
recon_1      |   Key_CertSign
recon_1      |   Crl_Sign
recon_1      | ]
recon_1      | 
recon_1      | [3]: ObjectId: 2.5.29.17 Criticality=false
recon_1      | SubjectAlternativeName [
recon_1      |   IPAddress: 172.25.0.116
recon_1      |   DNSName: scm1.org
recon_1      | ]
recon_1      | 
recon_1      | ]
recon_1      |   Algorithm: [SHA256withRSA]
recon_1      |   Signature:
recon_1      | 0000: 59 E4 14 CA 42 0B 71 BE   61 3A FA 41 71 F2 48 C6  Y...B.q.a:.Aq.H.
recon_1      | 0010: 35 3A DF E8 FE E3 74 A0   7E 57 15 0F F2 39 95 34  5:....t..W...9.4
recon_1      | 0020: E6 A7 48 CE 06 EA B9 42   CC AD DC 81 09 F1 F2 3E  ..H....B.......>
recon_1      | 0030: 94 27 78 5F 6B 04 F9 F4   E3 A1 71 61 8F 5A EF 6D  .'x_k.....qa.Z.m
recon_1      | 0040: 72 EA 9B CB 23 B9 31 9F   D7 B3 7B 86 35 09 CF D1  r...#.1.....5...
recon_1      | 0050: D6 81 A8 2E 21 01 F7 A9   B4 78 00 21 D5 3C 4A 61  ....!....x.!.<Ja
recon_1      | 0060: 7F 91 56 B9 50 6C 2C 23   0F D3 49 6E 80 28 7A 03  ..V.Pl,#..In.(z.
recon_1      | 0070: 32 46 42 54 4F 22 06 7F   B1 7D 41 A0 09 27 CF 7A  2FBTO"....A..'.z
recon_1      | 0080: F0 82 C6 00 70 41 5D 8F   83 5F E6 EB 74 1F B7 A2  ....pA].._..t...
recon_1      | 0090: 85 64 3E AE D6 66 71 7C   BE 7F 92 D0 79 64 71 C0  .d>..fq.....ydq.
recon_1      | 00A0: 8F 76 8C F6 D1 A4 67 C4   F4 11 8D 75 FC DE EF EE  .v....g....u....
recon_1      | 00B0: D9 7F 75 F3 5F AC 76 DF   23 F9 19 99 25 F9 31 8E  ..u._.v.#...%.1.
recon_1      | 00C0: AA 69 DC 7B 7E 4E 62 10   80 B9 5E 00 F8 1D 16 71  .i...Nb...^....q
recon_1      | 00D0: 7F D4 C7 FD 20 63 0B 0A   7E 36 7D F4 4C 85 A7 61  .... c...6..L..a
recon_1      | 00E0: E7 00 61 A2 83 40 78 92   3E A1 9A E0 B8 E4 A1 A0  ..a..@x.>.......
recon_1      | 00F0: 45 B5 33 7C 03 24 98 E7   DE 69 74 45 16 2B 06 3F  E.3..$...itE.+.?
recon_1      | 
recon_1      | ] from file:/data/metadata/recon/certs/ROOTCA-1.crt.
recon_1      | 2023-01-12 05:42:20,798 [main] INFO client.ReconCertificateClient: Added certificate [
recon_1      | [
recon_1      |   Version: V3
recon_1      |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=recon@recon
recon_1      |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
recon_1      | 
recon_1      |   Key:  Sun RSA public key, 2048 bits
recon_1      |   params: null
recon_1      |   modulus: 20564924658946829684960849027156036010832613811913605726207531009438977023189942808768499980621987263681594972179748983577089821794074609049399973056495777492136374728492522750317553202978622004656225788265467176326104496996235401972160712148692216648553303547478722870111969890070600907111015289182274558393641479982503473834034454404863318545727823734444626146406572241323755799729999350276352192757256831261002714930564827020837456664571488086805762620425474912989123110234187349878943580827650703705121116121750290691098558956131399507613996299307520860802956404826331589155002524624508443832382615744248981007547
recon_1      |   public exponent: 65537
recon_1      |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
recon_1      |                To: Fri Jan 12 00:00:00 UTC 2024]
recon_1      |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
recon_1      |   SerialNumber: [    01200d9d 0fa4]
recon_1      | 
recon_1      | Certificate Extensions: 2
recon_1      | [1]: ObjectId: 2.5.29.15 Criticality=true
recon_1      | KeyUsage [
recon_1      |   DigitalSignature
recon_1      |   Key_Encipherment
recon_1      |   Data_Encipherment
recon_1      |   Key_Agreement
recon_1      | ]
recon_1      | 
recon_1      | [2]: ObjectId: 2.5.29.17 Criticality=false
recon_1      | SubjectAlternativeName [
recon_1      |   IPAddress: 172.25.0.115
recon_1      | ]
recon_1      | 
recon_1      | ]
recon_1      |   Algorithm: [SHA256withRSA]
recon_1      |   Signature:
recon_1      | 0000: 25 C0 33 97 C2 3B 3C AD   C4 43 41 DA 2D 76 CE 7A  %.3..;<..CA.-v.z
recon_1      | 0010: 81 B9 E2 5D FF 5B BE E3   2A 7A F7 E8 10 D9 A7 2B  ...].[..*z.....+
recon_1      | 0020: 5A 92 18 85 3C A9 C2 29   C9 75 89 C0 40 FF E3 67  Z...<..).u..@..g
recon_1      | 0030: 7F 11 6D 3E 22 31 DF 0F   07 E5 24 B5 1F 29 B2 3B  ..m>"1....$..).;
recon_1      | 0040: 82 8F 10 80 FE 52 66 1B   B0 A9 31 48 83 FA 34 34  .....Rf...1H..44
recon_1      | 0050: 82 92 5C 4A 6C 03 66 90   F1 15 01 C3 6D 16 AB 4E  ..\Jl.f.....m..N
recon_1      | 0060: 76 4B 8E DE 83 21 6F 77   7E F2 2C 04 B2 39 30 71  vK...!ow..,..90q
recon_1      | 0070: C2 53 62 FC 93 B9 37 EF   FD 6A F5 59 C4 CE 75 AB  .Sb...7..j.Y..u.
recon_1      | 0080: 89 5C EE 6B 78 9E F3 66   FD 8C 9C D0 BC F5 C7 B7  .\.kx..f........
recon_1      | 0090: 75 7A 5A 6B B7 00 D9 C7   E9 4C 22 DA EA 7A 79 1E  uzZk.....L"..zy.
recon_1      | 00A0: 3D 55 02 EB B6 FC C0 D6   0D 2A 9F 5A F3 60 B4 36  =U.......*.Z.`.6
recon_1      | 00B0: DA 04 CC 87 C2 A0 C2 68   7E B4 DF D9 C3 4F A5 FF  .......h.....O..
recon_1      | 00C0: CF 68 9C 3B 4E FB 62 8C   E5 0F EC A8 F9 99 45 5C  .h.;N.b.......E\
recon_1      | 00D0: 30 81 34 8B 26 3B F3 54   66 8B 69 70 C9 CC 97 D2  0.4.&;.Tf.ip....
recon_1      | 00E0: 6A C3 3B 3C 1B A0 99 FF   8A 23 3C B3 78 8B F4 B9  j.;<.....#<.x...
recon_1      | 00F0: 5E 44 30 39 EE 95 17 B0   16 D9 4D 3D F0 69 9C 93  ^D09......M=.i..
recon_1      | 
recon_1      | ] from file:/data/metadata/recon/certs/1237178978212.crt.
recon_1      | 2023-01-12 05:42:20,812 [main] INFO client.ReconCertificateClient: Added certificate [
recon_1      | [
recon_1      |   Version: V3
recon_1      |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
recon_1      |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
recon_1      | 
recon_1      |   Key:  Sun RSA public key, 2048 bits
recon_1      |   params: null
om1_1        | 2023-01-12 05:45:12,425 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1        | 2023-01-12 05:45:12,427 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1        | 2023-01-12 05:45:12,467 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-562213E44849: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-12 05:45:12,498 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om1_1        | 2023-01-12 05:45:12,516 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-562213E44849-FollowerState
om1_1        | 2023-01-12 05:45:12,641 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-01-12 05:45:12,642 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1        | 2023-01-12 05:45:12,719 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om1
om1_1        | 2023-01-12 05:45:12,768 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1        | 2023-01-12 05:45:12,777 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1        | 2023-01-12 05:45:12,780 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1        | 2023-01-12 05:45:12,803 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1        | 2023-01-12 05:45:12,943 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
om1_1        | 2023-01-12 05:45:13,143 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1        | 2023-01-12 05:45:13,246 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1        | 2023-01-12 05:45:13,250 [Listener at om1/9862] INFO om.OzoneManager: Starting OM block token secret manager
om1_1        | 2023-01-12 05:45:13,260 [Listener at om1/9862] INFO token.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om1_1        | 2023-01-12 05:45:13,280 [Listener at om1/9862] INFO om.OzoneManager: Starting OM delegation token secret manager
om1_1        | 2023-01-12 05:45:13,280 [Listener at om1/9862] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om1_1        | 2023-01-12 05:45:13,356 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1        | 2023-01-12 05:45:13,376 [Thread[Thread-19,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om1_1        | 2023-01-12 05:45:14,232 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1        | 2023-01-12 05:45:14,250 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om1_1        | 2023-01-12 05:45:14,250 [Listener at om1/9862] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om1_1        | 2023-01-12 05:45:14,729 [Listener at om1/9862] INFO util.log: Logging initialized @68063ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1        | 2023-01-12 05:45:16,457 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1        | 2023-01-12 05:45:16,594 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1        | 2023-01-12 05:45:16,613 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om1_1        | 2023-01-12 05:45:16,618 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om1_1        | 2023-01-12 05:45:16,622 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om1_1        | 2023-01-12 05:45:16,657 [Listener at om1/9862] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om1_1        | 2023-01-12 05:45:17,346 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1        | 2023-01-12 05:45:17,407 [Listener at om1/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om1_1        | 2023-01-12 05:45:17,747 [om1@group-562213E44849-FollowerState] INFO impl.FollowerState: om1@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5232357994ns, electionTimeout:5076ms
om1_1        | 2023-01-12 05:45:17,750 [om1@group-562213E44849-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-FollowerState
om1_1        | 2023-01-12 05:45:17,837 [om1@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1        | 2023-01-12 05:45:17,844 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1        | 2023-01-12 05:45:17,845 [om1@group-562213E44849-FollowerState] INFO impl.RoleInfo: om1: start om1@group-562213E44849-LeaderElection1
om1_1        | 2023-01-12 05:45:18,019 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-12 05:45:18,092 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1        | 2023-01-12 05:45:18,092 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1        | 2023-01-12 05:45:18,128 [Listener at om1/9862] INFO server.session: node0 Scavenging every 660000ms
om1_1        | 2023-01-12 05:45:18,565 [Listener at om1/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om1_1        | 2023-01-12 05:45:18,606 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@724acf69{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1        | 2023-01-12 05:45:18,624 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@514332a5{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om1_1        | 2023-01-12 05:45:18,713 [om1@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-01-12 05:45:18,713 [om1@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 00B0: 6B 1F 89 42 C0 DF B3 8D   F3 50 25 03 65 F9 37 86  k..B.....P%.e.7.
om3_1        | 00C0: 03 54 EB EC 49 05 E6 8D   AD B3 7E 57 37 64 8C 60  .T..I......W7d.`
om3_1        | 00D0: E2 61 4E 15 16 4B FC C7   AA D8 F7 E0 CC 71 75 DA  .aN..K.......qu.
om3_1        | 00E0: 6F A8 66 F2 98 35 28 51   F9 FC 7B F3 90 FD 46 64  o.f..5(Q......Fd
om3_1        | 00F0: D3 1C BD 33 23 88 5C 9A   F7 A8 F9 0D C0 CD 9B 86  ...3#.\.........
om3_1        | 
om3_1        | ] from file:/data/metadata/om/certs/CA-1212956981891.crt.
om3_1        | 2023-01-12 05:44:47,941 [main] INFO security.OMCertificateClient: Added certificate [
om3_1        | [
om3_1        |   Version: V3
om3_1        |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=om3
om3_1        |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
om3_1        | 
om3_1        |   Key:  Sun RSA public key, 2048 bits
om3_1        |   params: null
om3_1        |   modulus: 22156718986588824801764083996203176183920879955642879147508113306710263823070296446319638816492074041480695848205648174400176612792400500281321071327470337881848682514459111882593420277865614587918952089607950664367399548867507424594885116929665719046805834186862153947408142302974133346793314105091392582987296874142500854971192890008437568892626411904390826304270507831553921618355477665189169284596993388124553978930860184573466170966899151236276670381091608866585240217677092346487390558712284232815607494794907115537015361697368119360606065097094162836440007424530369639770427736694923167587442386592048652473993
om3_1        |   public exponent: 65537
om3_1        |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
om3_1        |                To: Fri Jan 12 00:00:00 UTC 2024]
om3_1        |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
om3_1        |   SerialNumber: [    0138055d 60b3]
om3_1        | 
om3_1        | Certificate Extensions: 2
om3_1        | [1]: ObjectId: 2.5.29.15 Criticality=true
om3_1        | KeyUsage [
om3_1        |   DigitalSignature
om3_1        |   Key_Encipherment
om3_1        |   Data_Encipherment
om3_1        |   Key_Agreement
om3_1        | ]
om3_1        | 
om3_1        | [2]: ObjectId: 2.5.29.17 Criticality=false
om3_1        | SubjectAlternativeName [
om3_1        |   IPAddress: 172.25.0.113
om3_1        |   Other-Name: Unrecognized ObjectIdentifier: 2.16.840.1.113730.3.1.34
om3_1        | ]
om3_1        | 
om3_1        | ]
om3_1        |   Algorithm: [SHA256withRSA]
om3_1        |   Signature:
om3_1        | 0000: 59 5B AC 04 31 55 D7 86   CB BA C7 9E 2C C7 6A B4  Y[..1U......,.j.
om2_1        | 2023-01-12 05:45:12,095 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1        | 2023-01-12 05:45:12,097 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1        | 2023-01-12 05:45:12,155 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1        | 2023-01-12 05:45:12,155 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1        | 2023-01-12 05:45:12,172 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-562213E44849: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-01-12 05:45:12,186 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1        | 2023-01-12 05:45:12,195 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-562213E44849-FollowerState
om2_1        | 2023-01-12 05:45:12,211 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om2
om2_1        | 2023-01-12 05:45:12,217 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-01-12 05:45:12,217 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-01-12 05:45:12,234 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1        | 2023-01-12 05:45:12,236 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1        | 2023-01-12 05:45:12,242 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1        | 2023-01-12 05:45:12,250 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1        | 2023-01-12 05:45:12,284 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
om2_1        | 2023-01-12 05:45:12,347 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1        | 2023-01-12 05:45:12,373 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1        | 2023-01-12 05:45:12,377 [Listener at om2/9862] INFO om.OzoneManager: Starting OM block token secret manager
om2_1        | 2023-01-12 05:45:12,379 [Listener at om2/9862] INFO token.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om2_1        | 2023-01-12 05:45:12,384 [Listener at om2/9862] INFO om.OzoneManager: Starting OM delegation token secret manager
om2_1        | 2023-01-12 05:45:12,384 [Listener at om2/9862] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om2_1        | 2023-01-12 05:45:12,397 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1        | 2023-01-12 05:45:12,429 [Thread[Thread-19,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om2_1        | 2023-01-12 05:45:13,032 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1        | 2023-01-12 05:45:13,041 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om2_1        | 2023-01-12 05:45:13,045 [Listener at om2/9862] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om2_1        | 2023-01-12 05:45:13,896 [Listener at om2/9862] INFO util.log: Logging initialized @69271ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1        | 2023-01-12 05:45:15,836 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1        | 2023-01-12 05:45:15,926 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1        | 2023-01-12 05:45:15,938 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om2_1        | 2023-01-12 05:45:15,944 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om2_1        | 2023-01-12 05:45:15,951 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om2_1        | 2023-01-12 05:45:15,980 [Listener at om2/9862] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om2_1        | 2023-01-12 05:45:16,751 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
om2_1        | 2023-01-12 05:45:16,766 [Listener at om2/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om2_1        | 2023-01-12 05:45:17,300 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1        | 2023-01-12 05:45:17,306 [om2@group-562213E44849-FollowerState] INFO impl.FollowerState: om2@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5112144246ns, electionTimeout:5083ms
om2_1        | 2023-01-12 05:45:17,313 [om2@group-562213E44849-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-FollowerState
om2_1        | 2023-01-12 05:45:17,318 [om2@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om2_1        | 2023-01-12 05:45:17,340 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om2_1        | 2023-01-12 05:45:17,341 [om2@group-562213E44849-FollowerState] INFO impl.RoleInfo: om2: start om2@group-562213E44849-LeaderElection1
om2_1        | 2023-01-12 05:45:17,341 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
om2_1        | 2023-01-12 05:45:17,378 [Listener at om2/9862] INFO server.session: node0 Scavenging every 660000ms
om2_1        | 2023-01-12 05:45:17,458 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-01-12 05:45:17,935 [Listener at om2/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om2_1        | 2023-01-12 05:45:17,988 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5461daa5{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1        | 2023-01-12 05:45:18,002 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@72f1a1a7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1        | 2023-01-12 05:45:18,192 [om2@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-01-12 05:45:18,192 [om2@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-12 05:45:31,003 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode3_1  | 2023-01-12 05:45:31,005 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode3_1  | 2023-01-12 05:45:31,006 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2023-01-12 05:45:31,006 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2023-01-12 05:45:31,006 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode3_1  | 2023-01-12 05:45:31,008 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode3_1  | 2023-01-12 05:45:31,022 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: start 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderStateImpl
datanode3_1  | 2023-01-12 05:45:31,253 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2023-01-12 05:45:31,565 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-LeaderElection2] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877: set configuration 0: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:0|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-12 05:45:32,292 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-ACB2C75A2877-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9debe22a-e51c-422a-aaf2-acb2c75a2877/current/log_inprogress_0
datanode3_1  | 2023-01-12 05:45:33,827 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-FollowerState] INFO impl.FollowerState: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5189215334ns, electionTimeout:5165ms
datanode3_1  | 2023-01-12 05:45:33,828 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-FollowerState] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: shutdown 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-FollowerState
datanode3_1  | 2023-01-12 05:45:33,828 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-FollowerState] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2023-01-12 05:45:33,829 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2023-01-12 05:45:33,829 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-FollowerState] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: start 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3
datanode3_1  | 2023-01-12 05:45:33,843 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO impl.LeaderElection: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-12 05:45:33,843 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO impl.LeaderElection: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3 ELECTION round 0: result PASSED (term=1)
datanode3_1  | 2023-01-12 05:45:33,844 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: shutdown 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3
datanode3_1  | 2023-01-12 05:45:33,844 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode3_1  | 2023-01-12 05:45:33,844 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-444D5AC91262 with new leaderId: 75176c82-cf64-45b0-aad5-7780ce7245eb
datanode3_1  | 2023-01-12 05:45:33,850 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262: change Leader from null to 75176c82-cf64-45b0-aad5-7780ce7245eb at term 1 for becomeLeader, leader elected after 5691ms
datanode3_1  | 2023-01-12 05:45:33,852 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode3_1  | 2023-01-12 05:45:33,859 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-01-12 05:45:33,861 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode3_1  | 2023-01-12 05:45:33,861 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode3_1  | 2023-01-12 05:45:33,864 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode3_1  | 2023-01-12 05:45:33,864 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode3_1  | 2023-01-12 05:45:33,865 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2023-01-12 05:45:33,867 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode3_1  | 2023-01-12 05:45:33,868 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: start 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderStateImpl
datanode3_1  | 2023-01-12 05:45:33,870 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2023-01-12 05:45:33,896 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/48a05780-a7a4-4d83-bfcd-444d5ac91262/current/log_inprogress_0
datanode3_1  | 2023-01-12 05:45:33,929 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262-LeaderElection3] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-444D5AC91262: set configuration 0: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-12 05:45:34,220 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState] INFO impl.FollowerState: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5241894399ns, electionTimeout:5095ms
datanode3_1  | 2023-01-12 05:45:34,222 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: shutdown 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState
datanode3_1  | 2023-01-12 05:45:34,222 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2023-01-12 05:45:34,222 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2023-01-12 05:45:34,222 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: start 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4
datanode3_1  | 2023-01-12 05:45:34,232 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4] INFO impl.LeaderElection: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[75176c82-cf64-45b0-aad5-7780ce7245eb|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:172.25.0.104:9858|priority:0|startupRole:FOLLOWER, 08ecee04-dee1-41b4-94f3-4660f61ac91b|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:172.25.0.103:9858|priority:1|startupRole:FOLLOWER, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:172.25.0.102:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode3_1  | 2023-01-12 05:45:34,233 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-12 05:45:34,233 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode3_1  | 2023-01-12 05:45:34,263 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4] INFO impl.LeaderElection: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode3_1  | 2023-01-12 05:45:34,264 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4] INFO impl.LeaderElection:   Response 0: 75176c82-cf64-45b0-aad5-7780ce7245eb<-08ecee04-dee1-41b4-94f3-4660f61ac91b#0:FAIL-t1
datanode3_1  | 2023-01-12 05:45:34,264 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4] INFO impl.LeaderElection: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4 ELECTION round 0: result REJECTED
datanode3_1  | 2023-01-12 05:45:34,264 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4] INFO server.RaftServer$Division: 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode3_1  | 2023-01-12 05:45:34,265 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: shutdown 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4
datanode3_1  | 2023-01-12 05:45:34,266 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-LeaderElection4] INFO impl.RoleInfo: 75176c82-cf64-45b0-aad5-7780ce7245eb: start 75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState
datanode3_1  | 2023-01-12 05:45:34,285 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode3_1  | 2023-01-12 05:45:34,295 [75176c82-cf64-45b0-aad5-7780ce7245eb@group-52A62DFB0A44-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | Sleeping for 5 seconds
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2023-01-12 05:41:42,125 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = [--init]
scm1.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.14.1
scm1.org_1   | ************************************************************/
scm1.org_1   | 2023-01-12 05:41:42,261 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2023-01-12 05:41:42,902 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-01-12 05:41:43,386 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2023-01-12 05:41:43,529 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2023-01-12 05:41:43,987 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2023-01-12 05:41:43,999 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2023-01-12 05:41:44,059 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm1.org_1   | 2023-01-12 05:41:47,767 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
scm1.org_1   | 2023-01-12 05:41:47,795 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm1.org_1   | 2023-01-12 05:41:47,796 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm1.org_1   | 2023-01-12 05:41:52,304 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm1.org_1   | 2023-01-12 05:41:55,786 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2023-01-12 05:41:55,814 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2023-01-12 05:41:56,116 [main] INFO utils.SelfSignedCertificate: Certificate 1 is issued by CN=scm@scm1.org,OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1,O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f to CN=scm@scm1.org,OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1,O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, valid from Thu Jan 12 00:00:00 UTC 2023 to Sun Feb 20 00:00:00 UTC 2028
scm1.org_1   | 2023-01-12 05:41:56,198 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2023-01-12 05:41:56,198 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2023-01-12 05:41:56,211 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm1.org,scmId:0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1,clusterId:CID-dd9a252d-f4dd-4162-963a-067d6dacb49f,subject:scm-sub@scm1.org
scm1.org_1   | 2023-01-12 05:41:56,385 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm1.org_1   | 2023-01-12 05:41:56,616 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2023-01-12 05:41:56,697 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-01-12 05:41:56,699 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-01-12 05:41:56,704 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-01-12 05:41:56,705 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-01-12 05:41:56,705 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1.org_1   | 2023-01-12 05:41:56,707 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2023-01-12 05:41:56,708 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2023-01-12 05:41:56,711 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-12 05:41:56,712 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2023-01-12 05:41:56,712 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-01-12 05:41:56,723 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-01-12 05:41:56,736 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1.org_1   | 2023-01-12 05:41:56,740 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-01-12 05:41:57,042 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
scm1.org_1   | 2023-01-12 05:41:57,056 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2023-01-12 05:41:57,188 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
scm1.org_1   | 2023-01-12 05:41:57,189 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-01-12 05:41:57,189 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm1.org_1   | 2023-01-12 05:41:57,190 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
scm1.org_1   | 2023-01-12 05:41:57,195 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm1.org_1   | 2023-01-12 05:41:57,196 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
scm1.org_1   | 2023-01-12 05:41:57,201 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
scm1.org_1   | 2023-01-12 05:41:57,202 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
scm1.org_1   | 2023-01-12 05:41:57,203 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
scm1.org_1   | 2023-01-12 05:41:57,204 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm1.org_1   | 2023-01-12 05:41:57,245 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1.org_1   | 2023-01-12 05:41:57,246 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1.org_1   | 2023-01-12 05:41:57,246 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-01-12 05:41:57,246 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-01-12 05:41:57,249 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-01-12 05:41:57,273 [main] INFO server.RaftServer: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: addNew group-067D6DACB49F:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER] returns group-067D6DACB49F:java.util.concurrent.CompletableFuture@3f96f020[Not completed]
scm1.org_1   | 2023-01-12 05:41:57,272 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xb90ae680] REGISTERED
scm1.org_1   | 2023-01-12 05:41:57,280 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xb90ae680] BIND: 0.0.0.0/0.0.0.0:0
scm1.org_1   | 2023-01-12 05:41:57,282 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xb90ae680, L:/0.0.0.0:34353] ACTIVE
scm1.org_1   | 2023-01-12 05:41:57,316 [pool-2-thread-1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: new RaftServerImpl for group-067D6DACB49F:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm1.org_1   | 2023-01-12 05:41:57,318 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2023-01-12 05:41:57,319 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2023-01-12 05:41:57,319 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2023-01-12 05:41:57,319 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-01-12 05:41:57,319 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-01-12 05:41:57,319 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2.org_1   | Sleeping for 5 seconds
scm2.org_1   | Waiting for the service scm1.org:9894
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2023-01-12 05:41:59,528 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm2.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.14.1
scm2.org_1   | ************************************************************/
scm2.org_1   | 2023-01-12 05:41:59,536 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2023-01-12 05:41:59,602 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-01-12 05:41:59,655 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2023-01-12 05:41:59,656 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2023-01-12 05:41:59,708 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2023-01-12 05:41:59,709 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2023-01-12 05:41:59,874 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2023-01-12 05:41:59,874 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2023-01-12 05:41:59,962 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
scm2.org_1   | 2023-01-12 05:42:02,258 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-12 05:42:04,260 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-12 05:42:06,263 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-12 05:42:08,264 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-12 05:42:10,274 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-12 05:42:13,345 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1 is not the leader. Could not determine the leader node.
scm2.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
scm2.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2.org_1   | , while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-12 05:42:15,351 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2023-01-12 05:42:17,475 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
om3_1        | 0010: 89 5B 35 3C 23 A1 29 C5   63 5E 46 00 6E 60 3B 4C  .[5<#.).c^F.n`;L
om3_1        | 0020: 2A 26 0D 38 ED 8B 8A FF   48 0C 7A 28 E3 26 A5 87  *&.8....H.z(.&..
om3_1        | 0030: 8A 7D CC 80 DD E9 0F 49   1A F3 A7 3F 07 7C D5 5C  .......I...?...\
om3_1        | 0040: 1B 9E 5C 90 53 45 01 96   89 72 5A 16 B9 1A 29 56  ..\.SE...rZ...)V
om3_1        | 0050: 74 35 E2 B0 43 3A E5 E8   4A 32 5B 40 31 8F ED FB  t5..C:..J2[@1...
om3_1        | 0060: 6F 18 BD 9C AF 89 7E 88   10 5F F4 BA 76 4E E4 B5  o........_..vN..
om3_1        | 0070: 9B A9 D0 67 83 42 3D EB   B3 6E 64 E3 DF 3B 0A 7B  ...g.B=..nd..;..
om3_1        | 0080: 15 FB DF 19 95 29 33 3B   B0 FB 40 ED 69 41 C7 D6  .....)3;..@.iA..
om3_1        | 0090: B8 3C B3 50 34 81 C8 7D   39 29 E5 CC 1A 9B F4 FE  .<.P4...9)......
om3_1        | 00A0: C8 93 27 A0 94 01 6D 3B   3B 08 2A 4C B0 E2 46 B7  ..'...m;;.*L..F.
om3_1        | 00B0: EF E8 30 9A DE AB 65 7C   D4 38 7B 1B AB CC CD 45  ..0...e..8.....E
om3_1        | 00C0: 5C 4A 6F 05 F8 91 74 ED   C8 A1 E7 A3 7A 88 20 B4  \Jo...t.....z. .
om3_1        | 00D0: 72 20 85 63 42 9E 93 E4   C2 C6 1E A9 5F B3 B7 A4  r .cB......._...
om3_1        | 00E0: 49 BB E2 4B D9 D3 4E F3   3B 2B BE 60 67 BF 2E DC  I..K..N.;+.`g...
om3_1        | 00F0: 60 AB C5 BD FE 05 2B 61   AB AD FD 64 0D E2 3C E2  `.....+a...d..<.
om3_1        | 
om3_1        | ] from file:/data/metadata/om/certs/1340119802035.crt.
om3_1        | 2023-01-12 05:44:47,956 [main] INFO security.OMCertificateClient: CertificateLifetimeMonitor is started with first delay 29096112053 ms and interval 86400000 ms.
om3_1        | 2023-01-12 05:44:48,203 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2023-01-12 05:44:49,115 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om3_1        | 2023-01-12 05:44:49,129 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om3_1        | 2023-01-12 05:44:51,004 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1        | 2023-01-12 05:44:51,206 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om3_1        | 2023-01-12 05:44:51,206 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om3_1        | 2023-01-12 05:44:52,324 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
om3_1        | 2023-01-12 05:44:52,852 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2023-01-12 05:44:52,855 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1        | 2023-01-12 05:44:52,911 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om3_1        | 2023-01-12 05:44:53,635 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1        | 2023-01-12 05:44:53,733 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2023-01-12 05:44:53,946 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: id1 and peers: om3:9872, om1:9872, om2:9872
om3_1        | 2023-01-12 05:44:54,015 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om3_1        | 2023-01-12 05:44:55,359 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om3_1        | 2023-01-12 05:44:55,389 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
om3_1        | 2023-01-12 05:44:55,480 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1        | 2023-01-12 05:44:55,899 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1        | 2023-01-12 05:44:55,913 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1        | 2023-01-12 05:44:55,915 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1        | 2023-01-12 05:44:55,918 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1        | 2023-01-12 05:44:55,921 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1        | 2023-01-12 05:44:55,921 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1        | 2023-01-12 05:44:55,924 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1        | 2023-01-12 05:44:55,932 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-01-12 05:44:55,934 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1        | 2023-01-12 05:44:55,937 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2023-01-12 05:44:56,001 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1        | 2023-01-12 05:44:56,014 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1        | 2023-01-12 05:44:56,016 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1        | 2023-01-12 05:44:58,772 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
om3_1        | 2023-01-12 05:44:58,964 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
om3_1        | 2023-01-12 05:44:58,967 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
om3_1        | 2023-01-12 05:44:58,972 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
om3_1        | 2023-01-12 05:44:58,985 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
om3_1        | 2023-01-12 05:44:59,001 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
om3_1        | 2023-01-12 05:44:59,006 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
om1_1        | 2023-01-12 05:45:18,758 [om1@group-562213E44849-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om1_1        | 2023-01-12 05:45:18,760 [om1@group-562213E44849-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om1_1        | 2023-01-12 05:45:19,145 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-om1: Detected pause in JVM or host machine (eg GC): pause of approximately 206324618ns. No GCs detected.
om1_1        | 2023-01-12 05:45:21,275 [Listener at om1/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om1_1        | 2023-01-12 05:45:21,601 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7a55004d{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-3582466834365995283/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om1_1        | 2023-01-12 05:45:21,842 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@2d2c3ab2{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1        | 2023-01-12 05:45:21,849 [Listener at om1/9862] INFO server.Server: Started @75179ms
om1_1        | 2023-01-12 05:45:22,083 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1        | 2023-01-12 05:45:22,083 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1        | 2023-01-12 05:45:22,086 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1        | 2023-01-12 05:45:22,086 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1        | 2023-01-12 05:45:22,117 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1        | 2023-01-12 05:45:23,149 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om1_1        | 2023-01-12 05:45:23,513 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:35837
om1_1        | 2023-01-12 05:45:23,659 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2023-01-12 05:45:23,934 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1: ELECTION TIMEOUT received 0 response(s) and 0 exception(s):
om1_1        | 2023-01-12 05:45:23,935 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 ELECTION round 0: result TIMEOUT
om1_1        | 2023-01-12 05:45:23,944 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 ELECTION round 1: submit vote requests at term 2 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-12 05:45:23,998 [om1@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-01-12 05:45:23,998 [om1@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1        | 2023-01-12 05:45:24,884 [Listener at om1/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om1_1        | 2023-01-12 05:45:25,086 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@53851bb9] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1        | 2023-01-12 05:45:27,023 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.999795901s. [buffered_nanos=3045696002, waiting_for_connection]
om1_1        | 2023-01-12 05:45:27,289 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.999713301s. [buffered_nanos=3101754707, remote_addr=om2/172.25.0.112:9872]
om1_1        | 2023-01-12 05:45:27,289 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1: ELECTION REJECTED received 0 response(s) and 2 exception(s):
om1_1        | 2023-01-12 05:45:27,292 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.999795901s. [buffered_nanos=3045696002, waiting_for_connection]
om1_1        | 2023-01-12 05:45:27,292 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.999713301s. [buffered_nanos=3101754707, remote_addr=om2/172.25.0.112:9872]
om1_1        | 2023-01-12 05:45:27,312 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 ELECTION round 1: result REJECTED
om1_1        | 2023-01-12 05:45:27,314 [om1@group-562213E44849-LeaderElection1] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
om1_1        | 2023-01-12 05:45:27,314 [om1@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-LeaderElection1
om1_1        | 2023-01-12 05:45:27,316 [om1@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-562213E44849-FollowerState
om1_1        | 2023-01-12 05:45:27,358 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-01-12 05:45:27,358 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1        | 2023-01-12 05:45:31,727 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849: receive requestVote(ELECTION, om3, group-562213E44849, 3, (t:0, i:~))
om1_1        | 2023-01-12 05:45:31,742 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-562213E44849-FOLLOWER: accept ELECTION from om3: our priority 0 <= candidate's priority 0
om1_1        | 2023-01-12 05:45:31,742 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:om3
om1_1        | 2023-01-12 05:45:31,742 [grpc-default-executor-0] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-FollowerState
om1_1        | 2023-01-12 05:45:31,742 [om1@group-562213E44849-FollowerState] INFO impl.FollowerState: om1@group-562213E44849-FollowerState was interrupted
om1_1        | 2023-01-12 05:45:31,751 [grpc-default-executor-0] INFO impl.RoleInfo: om1: start om1@group-562213E44849-FollowerState
om1_1        | 2023-01-12 05:45:31,774 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1        | 2023-01-12 05:45:31,774 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-01-12 05:41:57,326 [pool-2-thread-1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: ConfigurationManager, init=-1: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2023-01-12 05:41:57,326 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-01-12 05:41:57,331 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2023-01-12 05:41:57,331 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2023-01-12 05:41:57,339 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2023-01-12 05:41:57,342 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2023-01-12 05:41:57,342 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2023-01-12 05:41:57,394 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-01-12 05:41:57,394 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1.org_1   | 2023-01-12 05:41:57,395 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1.org_1   | 2023-01-12 05:41:57,395 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-01-12 05:41:57,396 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1.org_1   | 2023-01-12 05:41:57,396 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f does not exist. Creating ...
scm1.org_1   | 2023-01-12 05:41:57,402 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/in_use.lock acquired by nodename 90@scm1.org
scm1.org_1   | 2023-01-12 05:41:57,408 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f has been successfully formatted.
scm1.org_1   | 2023-01-12 05:41:57,413 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2023-01-12 05:41:57,434 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2023-01-12 05:41:57,435 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-12 05:41:57,439 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1.org_1   | 2023-01-12 05:41:57,443 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1.org_1   | 2023-01-12 05:41:57,446 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-01-12 05:41:57,452 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2023-01-12 05:41:57,452 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2023-01-12 05:41:57,462 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f
scm1.org_1   | 2023-01-12 05:41:57,462 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-01-12 05:41:57,462 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2023-01-12 05:41:57,463 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-01-12 05:41:57,464 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2023-01-12 05:41:57,464 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2023-01-12 05:41:57,465 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2023-01-12 05:41:57,465 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-01-12 05:41:57,466 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2023-01-12 05:41:57,476 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2023-01-12 05:41:57,477 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-12 05:41:57,488 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1.org_1   | 2023-01-12 05:41:57,488 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1.org_1   | 2023-01-12 05:41:57,489 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2023-01-12 05:41:57,495 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-01-12 05:41:57,495 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-01-12 05:41:57,497 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: start as a follower, conf=-1: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-12 05:41:57,497 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1.org_1   | 2023-01-12 05:41:57,498 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO impl.RoleInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: start 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState
scm1.org_1   | 2023-01-12 05:41:57,500 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-01-12 05:41:57,500 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-01-12 05:41:57,501 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-067D6DACB49F,id=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1
scm1.org_1   | 2023-01-12 05:41:57,503 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2023-01-12 05:41:57,503 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2023-01-12 05:41:57,504 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2023-01-12 05:41:57,505 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2023-01-12 05:41:57,511 [main] INFO server.RaftServer: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: start RPC server
scm1.org_1   | 2023-01-12 05:41:57,523 [main] INFO server.GrpcService: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: GrpcService started, listening on 9894
scm1.org_1   | 2023-01-12 05:41:57,525 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: Started
scm1.org_1   | 2023-01-12 05:42:02,678 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO impl.FollowerState: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5179580735ns, electionTimeout:5176ms
scm1.org_1   | 2023-01-12 05:42:02,679 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO impl.RoleInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: shutdown 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState
scm1.org_1   | 2023-01-12 05:42:02,679 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1.org_1   | 2023-01-12 05:42:02,682 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1.org_1   | 2023-01-12 05:42:02,682 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO impl.RoleInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: start 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1
scm1.org_1   | 2023-01-12 05:42:02,688 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO impl.LeaderElection: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-12 05:42:02,689 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO impl.LeaderElection: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm1.org_1   | 2023-01-12 05:42:02,689 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO impl.RoleInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: shutdown 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1
scm1.org_1   | 2023-01-12 05:42:02,690 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1.org_1   | 2023-01-12 05:42:02,690 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: change Leader from null to 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1 at term 1 for becomeLeader, leader elected after 5351ms
scm1.org_1   | 2023-01-12 05:42:02,695 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2023-01-12 05:42:02,700 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-01-12 05:42:02,701 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-01-12 05:42:02,706 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2023-01-12 05:42:02,706 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2023-01-12 05:42:02,707 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2023-01-12 05:42:02,712 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-01-12 05:42:02,713 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2023-01-12 05:42:02,715 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO impl.RoleInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: start 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderStateImpl
scm1.org_1   | 2023-01-12 05:42:02,734 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker: Starting segment from index:0
scm1.org_1   | 2023-01-12 05:42:02,771 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: set configuration 0: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-12 05:42:02,809 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_inprogress_0
scm1.org_1   | 2023-01-12 05:42:03,527 [main] INFO server.RaftServer: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: close
scm1.org_1   | 2023-01-12 05:42:03,528 [main] INFO server.GrpcService: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: shutdown server GrpcServerProtocolService now
scm1.org_1   | 2023-01-12 05:42:03,528 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: shutdown
scm1.org_1   | 2023-01-12 05:42:03,529 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-067D6DACB49F,id=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1
scm1.org_1   | 2023-01-12 05:42:03,529 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO impl.RoleInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: shutdown 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderStateImpl
scm1.org_1   | 2023-01-12 05:42:03,536 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO impl.PendingRequests: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-PendingRequests: sendNotLeaderResponses
scm1.org_1   | 2023-01-12 05:42:03,543 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO impl.StateMachineUpdater: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater: Took a snapshot at index 0
scm1.org_1   | 2023-01-12 05:42:03,543 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO impl.StateMachineUpdater: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm1.org_1   | 2023-01-12 05:42:03,550 [main] INFO server.GrpcService: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: shutdown server GrpcServerProtocolService successfully
scm1.org_1   | 2023-01-12 05:42:03,557 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO impl.StateMachineUpdater: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater: set stopIndex = 0
scm1.org_1   | 2023-01-12 05:42:03,558 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xb90ae680, L:/0.0.0.0:34353] CLOSE
scm1.org_1   | 2023-01-12 05:42:03,559 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xb90ae680, L:/0.0.0.0:34353] INACTIVE
scm1.org_1   | 2023-01-12 05:42:03,559 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xb90ae680, L:/0.0.0.0:34353] UNREGISTERED
scm1.org_1   | 2023-01-12 05:42:03,560 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: closes. applyIndex: 0
scm1.org_1   | 2023-01-12 05:42:03,571 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm1.org_1   | 2023-01-12 05:42:03,590 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker close()
scm1.org_1   | 2023-01-12 05:42:03,593 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: Stopped
scm1.org_1   | 2023-01-12 05:42:03,593 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-01-12 05:42:03,598 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f; layoutVersion=4; scmId=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1
scm1.org_1   | 2023-01-12 05:42:04,579 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm1.org/172.25.0.116
scm1.org_1   | ************************************************************/
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2023-01-12 05:42:06,120 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
om2_1        | 2023-01-12 05:45:18,240 [om2@group-562213E44849-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
om2_1        | 2023-01-12 05:45:18,241 [om2@group-562213E44849-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om2_1        | 2023-01-12 05:45:21,077 [Listener at om2/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om2_1        | 2023-01-12 05:45:21,326 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2de29667{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-13351690178097106423/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om2_1        | 2023-01-12 05:45:21,519 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@48b3a9ed{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1        | 2023-01-12 05:45:21,526 [Listener at om2/9862] INFO server.Server: Started @76895ms
om2_1        | 2023-01-12 05:45:21,617 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1        | 2023-01-12 05:45:21,617 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1        | 2023-01-12 05:45:21,631 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1        | 2023-01-12 05:45:21,634 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om2_1        | 2023-01-12 05:45:21,699 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om2_1        | 2023-01-12 05:45:22,791 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om2_1        | 2023-01-12 05:45:23,314 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1: ELECTION TIMEOUT received 0 response(s) and 0 exception(s):
om2_1        | 2023-01-12 05:45:23,318 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 ELECTION round 0: result TIMEOUT
om2_1        | 2023-01-12 05:45:23,342 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 ELECTION round 1: submit vote requests at term 2 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-01-12 05:45:23,376 [om2@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-01-12 05:45:23,381 [om2@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-01-12 05:45:24,025 [Listener at om2/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om2_1        | 2023-01-12 05:45:24,115 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1f520187] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1        | 2023-01-12 05:45:26,595 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.986210329s. [buffered_nanos=2921966610, remote_addr=om3/172.25.0.113:9872]
om2_1        | 2023-01-12 05:45:26,617 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.999801100s. [buffered_nanos=3009263571, remote_addr=om1/172.25.0.111:9872]
om2_1        | 2023-01-12 05:45:26,618 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1: ELECTION REJECTED received 0 response(s) and 2 exception(s):
om2_1        | 2023-01-12 05:45:26,620 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.986210329s. [buffered_nanos=2921966610, remote_addr=om3/172.25.0.113:9872]
om2_1        | 2023-01-12 05:45:26,621 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.999801100s. [buffered_nanos=3009263571, remote_addr=om1/172.25.0.111:9872]
om2_1        | 2023-01-12 05:45:26,622 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 ELECTION round 1: result REJECTED
om2_1        | 2023-01-12 05:45:26,627 [om2@group-562213E44849-LeaderElection1] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
om2_1        | 2023-01-12 05:45:26,627 [om2@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-LeaderElection1
om2_1        | 2023-01-12 05:45:26,667 [om2@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-562213E44849-FollowerState
om2_1        | 2023-01-12 05:45:26,726 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-01-12 05:45:26,728 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1        | 2023-01-12 05:45:27,172 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:46211
om2_1        | 2023-01-12 05:45:27,196 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om2_1        | 2023-01-12 05:45:31,771 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-562213E44849: receive requestVote(ELECTION, om3, group-562213E44849, 3, (t:0, i:~))
om2_1        | 2023-01-12 05:45:31,791 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-562213E44849-FOLLOWER: accept ELECTION from om3: our priority 0 <= candidate's priority 0
om2_1        | 2023-01-12 05:45:31,794 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:om3
om2_1        | 2023-01-12 05:45:31,794 [grpc-default-executor-0] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-FollowerState
om2_1        | 2023-01-12 05:45:31,795 [om2@group-562213E44849-FollowerState] INFO impl.FollowerState: om2@group-562213E44849-FollowerState was interrupted
om2_1        | 2023-01-12 05:45:31,798 [grpc-default-executor-0] INFO impl.RoleInfo: om2: start om2@group-562213E44849-FollowerState
om2_1        | 2023-01-12 05:45:31,820 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1        | 2023-01-12 05:45:31,821 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1      |   modulus: 21838495709719174090944017025941967908224182768319498180393936128987176221476088419281179276073314404186963320459954621839794791804718496604717042740409331050670636901380744918424604519064284257795379580082372800428810093145199831557386718853872621914973921810155992135096086488011377748521113854732595726272396205530150917174127452444863380225901746740157534773765777653459292048522370651962381319687098813297977743208706305312169254062598162002740262036220188710005290347458814949877767522524881554322161686065229152643489368945279202993130114091164740627711162880068229981146789996366091625523376604972129960509039
recon_1      |   public exponent: 65537
recon_1      |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
recon_1      |                To: Sun Feb 20 00:00:00 UTC 2028]
recon_1      |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
recon_1      |   SerialNumber: [    011a69de ba83]
recon_1      | 
recon_1      | Certificate Extensions: 3
recon_1      | [1]: ObjectId: 2.5.29.19 Criticality=true
recon_1      | BasicConstraints:[
recon_1      |   CA:true
recon_1      |   PathLen:2147483647
recon_1      | ]
recon_1      | 
recon_1      | [2]: ObjectId: 2.5.29.15 Criticality=true
recon_1      | KeyUsage [
recon_1      |   DigitalSignature
recon_1      |   Key_Encipherment
recon_1      |   Data_Encipherment
recon_1      |   Key_Agreement
recon_1      |   Key_CertSign
recon_1      |   Crl_Sign
recon_1      | ]
recon_1      | 
recon_1      | [3]: ObjectId: 2.5.29.17 Criticality=false
recon_1      | SubjectAlternativeName [
recon_1      |   IPAddress: 172.25.0.116
recon_1      |   DNSName: scm1.org
recon_1      | ]
recon_1      | 
recon_1      | ]
recon_1      |   Algorithm: [SHA256withRSA]
recon_1      |   Signature:
recon_1      | 0000: AB 5F B6 3B B0 5C 88 AB   DD 25 28 3F F6 7C 1A 59  ._.;.\...%(?...Y
recon_1      | 0010: AC B5 48 EF C8 75 0D 95   65 13 6C C4 5D 9A 7D D4  ..H..u..e.l.]...
recon_1      | 0020: 93 AF C2 9C CA DD 42 6C   C6 2D B6 5D EE CC 9A F0  ......Bl.-.]....
recon_1      | 0030: 95 DD 7D 3C D8 E8 28 A1   B8 09 E1 BA E1 DD 81 7A  ...<..(........z
recon_1      | 0040: 66 DB F9 45 B9 70 8B 0D   3A 5C A7 86 6A 8C F7 A7  f..E.p..:\..j...
recon_1      | 0050: E8 12 BD F5 3C 4C 8A 79   F9 53 EB 48 F0 23 70 F9  ....<L.y.S.H.#p.
recon_1      | 0060: 0F CC 68 07 FB 60 A9 BB   E7 35 31 EE D3 42 6A 9A  ..h..`...51..Bj.
recon_1      | 0070: 57 01 2A 55 0E E3 37 79   D8 76 24 C2 48 70 6F EB  W.*U..7y.v$.Hpo.
recon_1      | 0080: 61 A9 C9 E5 DC 38 A2 BF   D8 E8 6F 36 A5 D1 AB 3A  a....8....o6...:
recon_1      | 0090: 96 38 C7 8C 74 F7 01 E9   73 1E 4E 1D F8 E9 FA A8  .8..t...s.N.....
recon_1      | 00A0: 61 56 DA 60 4C 30 BF 35   59 8D 3B 69 0B 81 8D A5  aV.`L0.5Y.;i....
recon_1      | 00B0: 6B 1F 89 42 C0 DF B3 8D   F3 50 25 03 65 F9 37 86  k..B.....P%.e.7.
recon_1      | 00C0: 03 54 EB EC 49 05 E6 8D   AD B3 7E 57 37 64 8C 60  .T..I......W7d.`
recon_1      | 00D0: E2 61 4E 15 16 4B FC C7   AA D8 F7 E0 CC 71 75 DA  .aN..K.......qu.
recon_1      | 00E0: 6F A8 66 F2 98 35 28 51   F9 FC 7B F3 90 FD 46 64  o.f..5(Q......Fd
recon_1      | 00F0: D3 1C BD 33 23 88 5C 9A   F7 A8 F9 0D C0 CD 9B 86  ...3#.\.........
recon_1      | 
recon_1      | ] from file:/data/metadata/recon/certs/CA-1212956981891.crt.
recon_1      | 2023-01-12 05:42:20,825 [main] INFO client.ReconCertificateClient: CertificateLifetimeMonitor is started with first delay 29096259185 ms and interval 86400000 ms.
recon_1      | 2023-01-12 05:42:20,825 [main] INFO recon.ReconServer: Successfully stored SCM signed certificate, case:GETCERT.
recon_1      | 2023-01-12 05:42:21,566 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2023-01-12 05:42:24,307 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | WARNING: An illegal reflective access operation has occurred
recon_1      | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1      | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1      | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1      | WARNING: All illegal access operations will be denied in a future release
recon_1      | 2023-01-12 05:42:25,044 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1      | 2023-01-12 05:42:25,047 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.002 seconds to initialized 0 records to KEY_CONTAINER table
recon_1      | 2023-01-12 05:42:25,062 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2023-01-12 05:42:25,099 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | 2023-01-12 05:42:25,102 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1      | 2023-01-12 05:42:28,381 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1      | 2023-01-12 05:42:28,382 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
recon_1      | 2023-01-12 05:42:28,383 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.recon.http.auth.type = kerberos
recon_1      | 2023-01-12 05:42:28,408 [main] INFO util.log: Logging initialized @54142ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1      | 2023-01-12 05:42:28,729 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1      | 2023-01-12 05:42:28,742 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1        | 2023-01-12 05:45:31,915 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-562213E44849 replies to ELECTION vote request: om3<-om2#0:OK-t3. Peer's state: om2@group-562213E44849:t3, leader=null, voted=om3, raftlog=Memoized:om2@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-01-12 05:45:32,945 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-562213E44849: change Leader from null to om3 at term 3 for appendEntries, leader elected after 32020ms
om2_1        | 2023-01-12 05:45:32,971 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-562213E44849: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1        | 2023-01-12 05:45:32,984 [om2-server-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om2_1        | 2023-01-12 05:45:33,549 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = []
scm1.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.14.1
scm1.org_1   | ************************************************************/
scm1.org_1   | 2023-01-12 05:42:06,128 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2023-01-12 05:42:06,203 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-01-12 05:42:06,238 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2023-01-12 05:42:06,250 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2023-01-12 05:42:06,304 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2023-01-12 05:42:06,304 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2023-01-12 05:42:06,869 [main] INFO client.SCMCertificateClient: Loading certificate from location:/data/metadata/scm/sub-ca/certs.
scm1.org_1   | 2023-01-12 05:42:06,995 [main] INFO client.SCMCertificateClient: Added certificate [
scm1.org_1   | [
scm1.org_1   |   Version: V3
scm1.org_1   |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
scm1.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm1.org_1   | 
scm1.org_1   |   Key:  Sun RSA public key, 2048 bits
scm1.org_1   |   params: null
scm1.org_1   |   modulus: 21838495709719174090944017025941967908224182768319498180393936128987176221476088419281179276073314404186963320459954621839794791804718496604717042740409331050670636901380744918424604519064284257795379580082372800428810093145199831557386718853872621914973921810155992135096086488011377748521113854732595726272396205530150917174127452444863380225901746740157534773765777653459292048522370651962381319687098813297977743208706305312169254062598162002740262036220188710005290347458814949877767522524881554322161686065229152643489368945279202993130114091164740627711162880068229981146789996366091625523376604972129960509039
scm1.org_1   |   public exponent: 65537
scm1.org_1   |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
scm1.org_1   |                To: Sun Feb 20 00:00:00 UTC 2028]
scm1.org_1   |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
scm1.org_1   |   SerialNumber: [    011a69de ba83]
scm1.org_1   | 
scm1.org_1   | Certificate Extensions: 3
scm1.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm1.org_1   | BasicConstraints:[
scm1.org_1   |   CA:true
scm1.org_1   |   PathLen:2147483647
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm1.org_1   | KeyUsage [
scm1.org_1   |   DigitalSignature
scm1.org_1   |   Key_Encipherment
scm1.org_1   |   Data_Encipherment
scm1.org_1   |   Key_Agreement
scm1.org_1   |   Key_CertSign
scm1.org_1   |   Crl_Sign
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm1.org_1   | SubjectAlternativeName [
scm1.org_1   |   IPAddress: 172.25.0.116
scm1.org_1   |   DNSName: scm1.org
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | ]
scm1.org_1   |   Algorithm: [SHA256withRSA]
scm1.org_1   |   Signature:
scm1.org_1   | 0000: AB 5F B6 3B B0 5C 88 AB   DD 25 28 3F F6 7C 1A 59  ._.;.\...%(?...Y
scm1.org_1   | 0010: AC B5 48 EF C8 75 0D 95   65 13 6C C4 5D 9A 7D D4  ..H..u..e.l.]...
scm1.org_1   | 0020: 93 AF C2 9C CA DD 42 6C   C6 2D B6 5D EE CC 9A F0  ......Bl.-.]....
scm1.org_1   | 0030: 95 DD 7D 3C D8 E8 28 A1   B8 09 E1 BA E1 DD 81 7A  ...<..(........z
scm1.org_1   | 0040: 66 DB F9 45 B9 70 8B 0D   3A 5C A7 86 6A 8C F7 A7  f..E.p..:\..j...
scm1.org_1   | 0050: E8 12 BD F5 3C 4C 8A 79   F9 53 EB 48 F0 23 70 F9  ....<L.y.S.H.#p.
scm1.org_1   | 0060: 0F CC 68 07 FB 60 A9 BB   E7 35 31 EE D3 42 6A 9A  ..h..`...51..Bj.
scm1.org_1   | 0070: 57 01 2A 55 0E E3 37 79   D8 76 24 C2 48 70 6F EB  W.*U..7y.v$.Hpo.
scm1.org_1   | 0080: 61 A9 C9 E5 DC 38 A2 BF   D8 E8 6F 36 A5 D1 AB 3A  a....8....o6...:
scm1.org_1   | 0090: 96 38 C7 8C 74 F7 01 E9   73 1E 4E 1D F8 E9 FA A8  .8..t...s.N.....
scm1.org_1   | 00A0: 61 56 DA 60 4C 30 BF 35   59 8D 3B 69 0B 81 8D A5  aV.`L0.5Y.;i....
scm1.org_1   | 00B0: 6B 1F 89 42 C0 DF B3 8D   F3 50 25 03 65 F9 37 86  k..B.....P%.e.7.
scm1.org_1   | 00C0: 03 54 EB EC 49 05 E6 8D   AD B3 7E 57 37 64 8C 60  .T..I......W7d.`
om1_1        | 2023-01-12 05:45:31,850 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849 replies to ELECTION vote request: om3<-om1#0:OK-t3. Peer's state: om1@group-562213E44849:t3, leader=null, voted=om3, raftlog=Memoized:om1@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-12 05:45:33,009 [om1-server-thread1] INFO server.RaftServer$Division: om1@group-562213E44849: change Leader from null to om3 at term 3 for appendEntries, leader elected after 32802ms
om1_1        | 2023-01-12 05:45:33,142 [om1-server-thread2] INFO server.RaftServer$Division: om1@group-562213E44849: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1        | 2023-01-12 05:45:33,253 [om1-server-thread2] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om1_1        | 2023-01-12 05:45:33,653 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om3_1        | 2023-01-12 05:44:59,039 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
om3_1        | 2023-01-12 05:44:59,065 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
om3_1        | 2023-01-12 05:44:59,480 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
om3_1        | 2023-01-12 05:44:59,490 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
om3_1        | 2023-01-12 05:44:59,755 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1        | 2023-01-12 05:44:59,756 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1        | 2023-01-12 05:44:59,756 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2023-01-12 05:44:59,757 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-01-12 05:44:59,763 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2023-01-12 05:44:59,825 [main] INFO server.RaftServer: om3: addNew group-562213E44849:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-562213E44849:java.util.concurrent.CompletableFuture@253c82ad[Not completed]
om3_1        | 2023-01-12 05:44:59,825 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1        | 2023-01-12 05:44:59,820 [om3-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x7af6bb09] REGISTERED
om3_1        | 2023-01-12 05:44:59,856 [om3-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x7af6bb09] BIND: 0.0.0.0/0.0.0.0:0
om3_1        | 2023-01-12 05:44:59,862 [om3-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x7af6bb09, L:/0.0.0.0:33311] ACTIVE
om3_1        | 2023-01-12 05:45:00,040 [pool-28-thread-1] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-562213E44849:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1        | 2023-01-12 05:45:00,055 [main] INFO om.OzoneManager: Creating RPC Server
scm3.org_1   | Sleeping for 5 seconds
scm3.org_1   | Waiting for the service scm2.org:9894
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2023-01-12 05:42:47,164 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm3.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.14.1
scm3.org_1   | ************************************************************/
scm3.org_1   | 2023-01-12 05:42:47,225 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2023-01-12 05:42:47,537 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-01-12 05:42:47,632 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2023-01-12 05:42:47,639 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2023-01-12 05:42:47,862 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2023-01-12 05:42:47,862 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2023-01-12 05:42:48,512 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2023-01-12 05:42:48,512 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2023-01-12 05:42:48,686 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863]
scm3.org_1   | 2023-01-12 05:42:49,763 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm3.org_1   | 2023-01-12 05:42:51,410 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
scm3.org_1   | 2023-01-12 05:42:51,412 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm3.org_1   | 2023-01-12 05:42:51,415 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm3.org_1   | 2023-01-12 05:42:52,506 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm3.org_1   | 2023-01-12 05:42:52,689 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.118,host:scm3.org
scm3.org_1   | 2023-01-12 05:42:52,689 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm3.org_1   | 2023-01-12 05:42:52,699 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm3.org,scmId:df8756e7-ed3f-4e48-9521-edf82d63369c,clusterId:CID-dd9a252d-f4dd-4162-963a-067d6dacb49f,subject:scm-sub@scm3.org
scm3.org_1   | 2023-01-12 05:42:53,683 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm3.org_1   | 2023-01-12 05:42:53,698 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, SCMID df8756e7-ed3f-4e48-9521-edf82d63369c
scm3.org_1   | 2023-01-12 05:42:53,699 [main] INFO server.StorageContainerManager: Primary SCM Node ID 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1
scm3.org_1   | 2023-01-12 05:42:53,739 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm3.org/172.25.0.118
scm3.org_1   | ************************************************************/
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2023-01-12 05:42:57,262 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = []
scm3.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.14.1
scm3.org_1   | ************************************************************/
scm3.org_1   | 2023-01-12 05:42:57,312 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2023-01-12 05:42:57,637 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-01-12 05:42:57,826 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2023-01-12 05:42:57,881 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2023-01-12 05:42:58,082 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2023-01-12 05:42:58,082 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2023-01-12 05:42:59,406 [main] INFO client.SCMCertificateClient: Loading certificate from location:/data/metadata/scm/sub-ca/certs.
scm3.org_1   | 2023-01-12 05:42:59,811 [main] INFO client.SCMCertificateClient: Added certificate [
scm3.org_1   | [
scm3.org_1   |   Version: V3
scm3.org_1   |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=df8756e7-ed3f-4e48-9521-edf82d63369c, CN=scm-sub@scm3.org
scm3.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm3.org_1   | 
scm3.org_1   |   Key:  Sun RSA public key, 2048 bits
scm3.org_1   |   params: null
scm3.org_1   |   modulus: 31439838987359940847672787864083963360378260624695091519160734305192389485610772394399521681807429918073003682462830890666006767244776862222005719299506767339955138910025203202650074436980165069018188657063155996713340539661168314946288719493941356116799565404403658010245409754700058559369063901650876279138413954390667775201414734211767168371301571596950578634601064004664598423126746279716323151412041726656483642838355435658159036661643135288084986763336786919110622329674606866537834199093954734678690170122635096059874113311301609708637901036394833866877808065012903544980013131446382281017465395770048300694101
scm3.org_1   |   public exponent: 65537
scm3.org_1   |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
scm3.org_1   |                To: Sun Feb 20 00:00:00 UTC 2028]
scm1.org_1   | 00D0: E2 61 4E 15 16 4B FC C7   AA D8 F7 E0 CC 71 75 DA  .aN..K.......qu.
scm1.org_1   | 00E0: 6F A8 66 F2 98 35 28 51   F9 FC 7B F3 90 FD 46 64  o.f..5(Q......Fd
scm1.org_1   | 00F0: D3 1C BD 33 23 88 5C 9A   F7 A8 F9 0D C0 CD 9B 86  ...3#.\.........
scm1.org_1   | 
scm1.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/certificate.crt.
scm1.org_1   | 2023-01-12 05:42:07,007 [main] INFO client.SCMCertificateClient: Added certificate [
scm1.org_1   | [
scm1.org_1   |   Version: V3
scm1.org_1   |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
scm1.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm1.org_1   | 
scm1.org_1   |   Key:  Sun RSA public key, 2048 bits
scm1.org_1   |   params: null
scm1.org_1   |   modulus: 30346053453237189834112040673541629567010306125791898570104054454047501736657892214383462264526301500298692652758375836429638347531164751981653358525504527825570400558838334901393005984876414719430775458652128247547444727818535668600972192623637356865083888753952717740665794517921055984453520353917733198649461590018542814322448272297366500121604394586840649019781675207082962097691558583700862118486739043197335879323701508955628436573724419921778589743880043303622162563304571704603236749325057266808761842228199352214967718782980075765678400986380731445934917806096546808808883868771850900183098234200323729287601
scm1.org_1   |   public exponent: 65537
scm1.org_1   |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
scm1.org_1   |                To: Sun Feb 20 00:00:00 UTC 2028]
scm1.org_1   |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
scm1.org_1   |   SerialNumber: [    01]
scm1.org_1   | 
scm1.org_1   | Certificate Extensions: 3
scm1.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm1.org_1   | BasicConstraints:[
scm1.org_1   |   CA:true
scm1.org_1   |   PathLen:2147483647
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm1.org_1   | KeyUsage [
scm1.org_1   |   Key_CertSign
scm1.org_1   |   Crl_Sign
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm1.org_1   | SubjectAlternativeName [
scm1.org_1   |   IPAddress: 172.25.0.116
scm1.org_1   |   DNSName: scm1.org
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | ]
scm1.org_1   |   Algorithm: [SHA256withRSA]
scm1.org_1   |   Signature:
scm1.org_1   | 0000: 59 E4 14 CA 42 0B 71 BE   61 3A FA 41 71 F2 48 C6  Y...B.q.a:.Aq.H.
scm1.org_1   | 0010: 35 3A DF E8 FE E3 74 A0   7E 57 15 0F F2 39 95 34  5:....t..W...9.4
scm1.org_1   | 0020: E6 A7 48 CE 06 EA B9 42   CC AD DC 81 09 F1 F2 3E  ..H....B.......>
scm1.org_1   | 0030: 94 27 78 5F 6B 04 F9 F4   E3 A1 71 61 8F 5A EF 6D  .'x_k.....qa.Z.m
scm1.org_1   | 0040: 72 EA 9B CB 23 B9 31 9F   D7 B3 7B 86 35 09 CF D1  r...#.1.....5...
scm1.org_1   | 0050: D6 81 A8 2E 21 01 F7 A9   B4 78 00 21 D5 3C 4A 61  ....!....x.!.<Ja
scm1.org_1   | 0060: 7F 91 56 B9 50 6C 2C 23   0F D3 49 6E 80 28 7A 03  ..V.Pl,#..In.(z.
scm1.org_1   | 0070: 32 46 42 54 4F 22 06 7F   B1 7D 41 A0 09 27 CF 7A  2FBTO"....A..'.z
scm1.org_1   | 0080: F0 82 C6 00 70 41 5D 8F   83 5F E6 EB 74 1F B7 A2  ....pA].._..t...
scm1.org_1   | 0090: 85 64 3E AE D6 66 71 7C   BE 7F 92 D0 79 64 71 C0  .d>..fq.....ydq.
scm1.org_1   | 00A0: 8F 76 8C F6 D1 A4 67 C4   F4 11 8D 75 FC DE EF EE  .v....g....u....
scm1.org_1   | 00B0: D9 7F 75 F3 5F AC 76 DF   23 F9 19 99 25 F9 31 8E  ..u._.v.#...%.1.
scm1.org_1   | 00C0: AA 69 DC 7B 7E 4E 62 10   80 B9 5E 00 F8 1D 16 71  .i...Nb...^....q
scm1.org_1   | 00D0: 7F D4 C7 FD 20 63 0B 0A   7E 36 7D F4 4C 85 A7 61  .... c...6..L..a
scm1.org_1   | 00E0: E7 00 61 A2 83 40 78 92   3E A1 9A E0 B8 E4 A1 A0  ..a..@x.>.......
scm1.org_1   | 00F0: 45 B5 33 7C 03 24 98 E7   DE 69 74 45 16 2B 06 3F  E.3..$...itE.+.?
scm1.org_1   | 
scm1.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/CA-1.crt.
scm1.org_1   | 2023-01-12 05:42:07,013 [main] INFO client.SCMCertificateClient: Added certificate [
scm1.org_1   | [
scm1.org_1   |   Version: V3
scm1.org_1   |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm-sub@scm1.org
scm1.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm1.org_1   | 
scm1.org_1   |   Key:  Sun RSA public key, 2048 bits
scm1.org_1   |   params: null
scm1.org_1   |   modulus: 21838495709719174090944017025941967908224182768319498180393936128987176221476088419281179276073314404186963320459954621839794791804718496604717042740409331050670636901380744918424604519064284257795379580082372800428810093145199831557386718853872621914973921810155992135096086488011377748521113854732595726272396205530150917174127452444863380225901746740157534773765777653459292048522370651962381319687098813297977743208706305312169254062598162002740262036220188710005290347458814949877767522524881554322161686065229152643489368945279202993130114091164740627711162880068229981146789996366091625523376604972129960509039
scm1.org_1   |   public exponent: 65537
scm1.org_1   |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
scm1.org_1   |                To: Sun Feb 20 00:00:00 UTC 2028]
scm1.org_1   |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
om3_1        | 2023-01-12 05:45:00,057 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1        | 2023-01-12 05:45:00,071 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1        | 2023-01-12 05:45:00,071 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1        | 2023-01-12 05:45:00,071 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2023-01-12 05:45:00,071 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2023-01-12 05:45:00,073 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1        | 2023-01-12 05:45:00,166 [pool-28-thread-1] INFO server.RaftServer$Division: om3@group-562213E44849: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1        | 2023-01-12 05:45:00,172 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2023-01-12 05:45:00,251 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1        | 2023-01-12 05:45:00,257 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1        | 2023-01-12 05:45:00,343 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1        | 2023-01-12 05:45:00,387 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1        | 2023-01-12 05:45:00,389 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1        | 2023-01-12 05:45:00,982 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1        | 2023-01-12 05:45:00,983 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1        | 2023-01-12 05:45:00,984 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1        | 2023-01-12 05:45:00,989 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1        | 2023-01-12 05:45:00,990 [pool-28-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1        | 2023-01-12 05:45:03,083 [main] INFO reflections.Reflections: Reflections took 2535 ms to scan 8 urls, producing 23 keys and 545 values [using 2 cores]
om3_1        | 2023-01-12 05:45:04,426 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1        | 2023-01-12 05:45:04,473 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1        | 2023-01-12 05:45:10,358 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1        | 2023-01-12 05:45:10,500 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1        | 2023-01-12 05:45:10,500 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1        | 2023-01-12 05:45:10,937 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/172.25.0.113:9862
om3_1        | 2023-01-12 05:45:10,937 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1        | 2023-01-12 05:45:10,948 [om3-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om3_1        | 2023-01-12 05:45:10,964 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 7@om3
om3_1        | 2023-01-12 05:45:10,996 [om3-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om3_1        | 2023-01-12 05:45:11,015 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1        | 2023-01-12 05:45:11,067 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1        | 2023-01-12 05:45:11,067 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-01-12 05:45:11,076 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1        | 2023-01-12 05:45:11,081 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1        | 2023-01-12 05:45:11,096 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2023-01-12 05:45:11,127 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1        | 2023-01-12 05:45:11,130 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1        | 2023-01-12 05:45:11,162 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-562213E44849-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om3_1        | 2023-01-12 05:45:11,165 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1        | 2023-01-12 05:45:11,165 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1        | 2023-01-12 05:45:11,175 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   |   SerialNumber: [    011a69de ba83]
scm1.org_1   | 
scm1.org_1   | Certificate Extensions: 3
scm1.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm1.org_1   | BasicConstraints:[
scm1.org_1   |   CA:true
scm1.org_1   |   PathLen:2147483647
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm1.org_1   | KeyUsage [
scm1.org_1   |   DigitalSignature
scm1.org_1   |   Key_Encipherment
scm1.org_1   |   Data_Encipherment
scm1.org_1   |   Key_Agreement
scm1.org_1   |   Key_CertSign
scm1.org_1   |   Crl_Sign
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm1.org_1   | SubjectAlternativeName [
scm1.org_1   |   IPAddress: 172.25.0.116
scm1.org_1   |   DNSName: scm1.org
scm1.org_1   | ]
scm1.org_1   | 
scm1.org_1   | ]
scm1.org_1   |   Algorithm: [SHA256withRSA]
scm1.org_1   |   Signature:
scm1.org_1   | 0000: AB 5F B6 3B B0 5C 88 AB   DD 25 28 3F F6 7C 1A 59  ._.;.\...%(?...Y
scm1.org_1   | 0010: AC B5 48 EF C8 75 0D 95   65 13 6C C4 5D 9A 7D D4  ..H..u..e.l.]...
scm1.org_1   | 0020: 93 AF C2 9C CA DD 42 6C   C6 2D B6 5D EE CC 9A F0  ......Bl.-.]....
scm1.org_1   | 0030: 95 DD 7D 3C D8 E8 28 A1   B8 09 E1 BA E1 DD 81 7A  ...<..(........z
scm1.org_1   | 0040: 66 DB F9 45 B9 70 8B 0D   3A 5C A7 86 6A 8C F7 A7  f..E.p..:\..j...
scm1.org_1   | 0050: E8 12 BD F5 3C 4C 8A 79   F9 53 EB 48 F0 23 70 F9  ....<L.y.S.H.#p.
scm1.org_1   | 0060: 0F CC 68 07 FB 60 A9 BB   E7 35 31 EE D3 42 6A 9A  ..h..`...51..Bj.
scm1.org_1   | 0070: 57 01 2A 55 0E E3 37 79   D8 76 24 C2 48 70 6F EB  W.*U..7y.v$.Hpo.
scm1.org_1   | 0080: 61 A9 C9 E5 DC 38 A2 BF   D8 E8 6F 36 A5 D1 AB 3A  a....8....o6...:
scm1.org_1   | 0090: 96 38 C7 8C 74 F7 01 E9   73 1E 4E 1D F8 E9 FA A8  .8..t...s.N.....
scm1.org_1   | 00A0: 61 56 DA 60 4C 30 BF 35   59 8D 3B 69 0B 81 8D A5  aV.`L0.5Y.;i....
scm1.org_1   | 00B0: 6B 1F 89 42 C0 DF B3 8D   F3 50 25 03 65 F9 37 86  k..B.....P%.e.7.
scm1.org_1   | 00C0: 03 54 EB EC 49 05 E6 8D   AD B3 7E 57 37 64 8C 60  .T..I......W7d.`
scm1.org_1   | 00D0: E2 61 4E 15 16 4B FC C7   AA D8 F7 E0 CC 71 75 DA  .aN..K.......qu.
scm1.org_1   | 00E0: 6F A8 66 F2 98 35 28 51   F9 FC 7B F3 90 FD 46 64  o.f..5(Q......Fd
scm1.org_1   | 00F0: D3 1C BD 33 23 88 5C 9A   F7 A8 F9 0D C0 CD 9B 86  ...3#.\.........
scm1.org_1   | 
scm1.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/1212956981891.crt.
scm1.org_1   | 2023-01-12 05:42:07,023 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor is started with first delay 158696272983 ms and interval 86400000 ms.
scm1.org_1   | 2023-01-12 05:42:07,194 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm1.org_1   | 2023-01-12 05:42:07,198 [main] INFO server.StorageContainerManager: SCM login successful.
scm1.org_1   | 2023-01-12 05:42:07,276 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-01-12 05:42:07,450 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2023-01-12 05:42:07,761 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm1.org_1   | 2023-01-12 05:42:07,762 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1.org_1   | 2023-01-12 05:42:07,821 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2023-01-12 05:42:07,841 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1
scm1.org_1   | 2023-01-12 05:42:07,870 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-01-12 05:42:07,888 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-01-12 05:42:07,935 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2023-01-12 05:42:07,999 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-01-12 05:42:08,001 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-01-12 05:42:08,002 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1.org_1   | 2023-01-12 05:42:08,003 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1.org_1   | 2023-01-12 05:42:08,003 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1.org_1   | 2023-01-12 05:42:08,003 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2023-01-12 05:42:08,004 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2023-01-12 05:42:08,006 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-12 05:42:08,007 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2023-01-12 05:42:08,007 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-01-12 05:42:08,020 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-01-12 05:42:08,023 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1.org_1   | 2023-01-12 05:42:08,024 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-01-12 05:42:08,620 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
scm1.org_1   | 2023-01-12 05:42:08,751 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
scm1.org_1   | 2023-01-12 05:42:08,756 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
scm1.org_1   | 2023-01-12 05:42:08,756 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm1.org_1   | 2023-01-12 05:42:08,757 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
scm1.org_1   | 2023-01-12 05:42:08,760 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm1.org_1   | 2023-01-12 05:42:08,760 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
scm1.org_1   | 2023-01-12 05:42:08,766 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
scm1.org_1   | 2023-01-12 05:42:08,767 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
scm1.org_1   | 2023-01-12 05:42:08,801 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
scm1.org_1   | 2023-01-12 05:42:08,802 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm1.org_1   | 2023-01-12 05:42:08,849 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1.org_1   | 2023-01-12 05:42:08,850 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1.org_1   | 2023-01-12 05:42:08,850 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-01-12 05:42:08,850 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-01-12 05:42:08,855 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-01-12 05:42:18,085 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
scm2.org_1   | 2023-01-12 05:42:18,086 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm2.org_1   | 2023-01-12 05:42:18,087 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm2.org_1   | 2023-01-12 05:42:19,084 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm2.org_1   | 2023-01-12 05:42:19,156 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.117,host:scm2.org
scm2.org_1   | 2023-01-12 05:42:19,156 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm2.org_1   | 2023-01-12 05:42:19,160 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm2.org,scmId:52d7416a-527f-45fb-b83c-77d6ce7a6ad0,clusterId:CID-dd9a252d-f4dd-4162-963a-067d6dacb49f,subject:scm-sub@scm2.org
scm2.org_1   | 2023-01-12 05:42:20,341 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm2.org_1   | 2023-01-12 05:42:20,351 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, SCMID 52d7416a-527f-45fb-b83c-77d6ce7a6ad0
scm2.org_1   | 2023-01-12 05:42:20,351 [main] INFO server.StorageContainerManager: Primary SCM Node ID 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1
scm2.org_1   | 2023-01-12 05:42:20,361 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm2.org/172.25.0.117
scm2.org_1   | ************************************************************/
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2023-01-12 05:42:24,253 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = []
scm2.org_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.14.1
scm2.org_1   | ************************************************************/
scm2.org_1   | 2023-01-12 05:42:24,288 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2023-01-12 05:42:24,468 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-01-12 05:42:24,565 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2023-01-12 05:42:24,611 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2023-01-12 05:42:24,724 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2023-01-12 05:42:24,726 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2023-01-12 05:42:26,605 [main] INFO client.SCMCertificateClient: Loading certificate from location:/data/metadata/scm/sub-ca/certs.
scm2.org_1   | 2023-01-12 05:42:26,968 [main] INFO client.SCMCertificateClient: Added certificate [
scm2.org_1   | [
scm2.org_1   |   Version: V3
scm2.org_1   |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=52d7416a-527f-45fb-b83c-77d6ce7a6ad0, CN=scm-sub@scm2.org
scm2.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm2.org_1   | 
scm2.org_1   |   Key:  Sun RSA public key, 2048 bits
scm2.org_1   |   params: null
scm2.org_1   |   modulus: 21296067913955723361589627170314898002064641208623135524154294810131409877074508273082723718740147680966807311144252411367888376024996605152521958352406230391942492361201976100556293486851482946819261404201951249034837906375767864559215717665329223917742988572781734777135183776207026905309794420045141329320776269490042924812268866788557106068955556478992808330843770389983215232226165222849053432386265314464149078186792184829087939822863552303870605499037979304036928075309857134617034149797703121510372802427126836714186382585822920887501656803481248653324411932672950162045545903101492333324034336995087910441773
scm2.org_1   |   public exponent: 65537
scm2.org_1   |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
scm2.org_1   |                To: Sun Feb 20 00:00:00 UTC 2028]
scm2.org_1   |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
scm2.org_1   |   SerialNumber: [    011fd096 7347]
scm2.org_1   | 
scm2.org_1   | Certificate Extensions: 3
scm2.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm2.org_1   | BasicConstraints:[
scm2.org_1   |   CA:true
scm2.org_1   |   PathLen:2147483647
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm2.org_1   | KeyUsage [
scm2.org_1   |   DigitalSignature
scm2.org_1   |   Key_Encipherment
scm2.org_1   |   Data_Encipherment
scm2.org_1   |   Key_Agreement
scm2.org_1   |   Key_CertSign
scm2.org_1   |   Crl_Sign
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm2.org_1   | SubjectAlternativeName [
scm2.org_1   |   IPAddress: 172.25.0.117
scm2.org_1   |   DNSName: scm2.org
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | ]
scm2.org_1   |   Algorithm: [SHA256withRSA]
scm2.org_1   |   Signature:
scm2.org_1   | 0000: DD DA B9 E1 12 DC B9 19   AB 43 6C 4A F6 18 61 AF  .........ClJ..a.
scm2.org_1   | 0010: 63 58 4C 9F 55 7F E1 D3   97 AA B7 7F AD 28 7B BB  cXL.U........(..
scm2.org_1   | 0020: E5 21 4D A8 C7 21 5B 47   75 3B 80 C5 AE BD 29 29  .!M..![Gu;....))
scm2.org_1   | 0030: 6A CF 1F 7F 52 05 65 22   33 92 76 50 63 7A 98 D0  j...R.e"3.vPcz..
scm2.org_1   | 0040: F2 C2 8C 67 61 9B 69 DF   FE 65 1F 63 B7 C9 90 AD  ...ga.i..e.c....
scm2.org_1   | 0050: 48 64 69 41 62 B0 4D C9   A4 69 BB 4E F2 0F 37 82  HdiAb.M..i.N..7.
scm2.org_1   | 0060: D5 7C 07 84 B0 1A 7E AE   D5 CF 7B 97 E4 78 EB 80  .............x..
scm2.org_1   | 0070: 70 50 FC BB BE B2 16 05   36 A2 CA 9A 95 B2 A3 E9  pP......6.......
scm2.org_1   | 0080: 37 4E 5C 28 7C A8 C6 3E   D6 E4 C2 81 6A F1 17 E4  7N\(...>....j...
scm2.org_1   | 0090: 9D 69 39 A1 4C DC 8E F3   D9 9E 28 55 5B F6 D9 27  .i9.L.....(U[..'
scm2.org_1   | 00A0: 17 CC D4 B1 95 F6 65 E6   AC 48 2E 1A 07 6F 26 1F  ......e..H...o&.
scm2.org_1   | 00B0: A2 F7 86 90 5F 1C 62 A6   B8 79 9F C4 0F C9 85 70  ...._.b..y.....p
scm2.org_1   | 00C0: 13 4A E8 5D 47 9E F2 E8   C9 FD 9E 50 B5 4B 4B 29  .J.]G......P.KK)
scm2.org_1   | 00D0: 40 FB 79 CE E3 AC D3 7A   4F 76 1E 14 70 D9 D8 8F  @.y....zOv..p...
scm2.org_1   | 00E0: 77 4D 15 9B 15 68 06 B0   52 07 C0 B4 2C A2 D2 25  wM...h..R...,..%
scm2.org_1   | 00F0: 64 D5 A3 C3 E6 8A 1F 3C   1F 0B F9 6D 23 E1 B7 DB  d......<...m#...
scm2.org_1   | 
scm2.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/certificate.crt.
scm2.org_1   | 2023-01-12 05:42:26,998 [main] INFO client.SCMCertificateClient: Added certificate [
scm2.org_1   | [
scm2.org_1   |   Version: V3
scm2.org_1   |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
scm2.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm2.org_1   | 
scm2.org_1   |   Key:  Sun RSA public key, 2048 bits
scm2.org_1   |   params: null
scm3.org_1   |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
scm3.org_1   |   SerialNumber: [    0127aaca d16f]
scm3.org_1   | 
scm3.org_1   | Certificate Extensions: 3
scm3.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm3.org_1   | BasicConstraints:[
scm3.org_1   |   CA:true
scm3.org_1   |   PathLen:2147483647
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm3.org_1   | KeyUsage [
scm3.org_1   |   DigitalSignature
scm3.org_1   |   Key_Encipherment
scm3.org_1   |   Data_Encipherment
scm3.org_1   |   Key_Agreement
scm3.org_1   |   Key_CertSign
scm3.org_1   |   Crl_Sign
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm3.org_1   | SubjectAlternativeName [
scm3.org_1   |   IPAddress: 172.25.0.118
scm3.org_1   |   DNSName: scm3.org
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | ]
scm3.org_1   |   Algorithm: [SHA256withRSA]
scm3.org_1   |   Signature:
scm3.org_1   | 0000: BA D3 50 1A 0D EB 1A F5   78 59 92 B5 68 AA 13 3C  ..P.....xY..h..<
scm3.org_1   | 0010: 44 16 EF A7 A2 00 A3 47   EA 25 8D 9F 6B 10 1C 7E  D......G.%..k...
scm3.org_1   | 0020: A2 B5 91 A9 24 93 26 85   06 0B 2E EB 30 5F 97 66  ....$.&.....0_.f
scm3.org_1   | 0030: 1A 51 D2 04 DD 1A EA 5A   17 9A 54 5F 6B 33 BF 44  .Q.....Z..T_k3.D
scm3.org_1   | 0040: DB F9 F1 54 4F D3 A5 B8   FB 16 51 C2 45 D2 70 C4  ...TO.....Q.E.p.
scm3.org_1   | 0050: 19 C2 9F C3 F9 41 2A B5   17 01 69 86 83 FE C3 DF  .....A*...i.....
scm3.org_1   | 0060: 16 A1 36 F7 53 DB 88 AC   2C 3E 14 6F 12 95 86 2E  ..6.S...,>.o....
scm3.org_1   | 0070: DF CD 91 2E 6F 15 8F BE   B9 04 B5 73 50 B7 0C B4  ....o......sP...
scm3.org_1   | 0080: 27 B4 3C 54 9B 37 DD 8E   6F D8 D5 E3 2C B8 D4 1E  '.<T.7..o...,...
scm3.org_1   | 0090: 6D CF A4 E9 DD 76 25 20   46 CA 1F 1D 2C 30 13 6D  m....v% F...,0.m
scm3.org_1   | 00A0: 0B 7C 30 59 2B 0B 60 16   84 1B 15 C5 CC 2C B3 F9  ..0Y+.`......,..
scm3.org_1   | 00B0: 81 FC 85 98 9F 45 73 34   EB 6C 00 89 2E 7D 15 F6  .....Es4.l......
scm3.org_1   | 00C0: 5D AA 73 64 30 DD BA B7   65 4D C1 CD 36 8A FE 3B  ].sd0...eM..6..;
scm3.org_1   | 00D0: A7 E0 FF E0 50 E3 F1 A2   7F E1 40 E4 30 F9 E5 74  ....P.....@.0..t
scm3.org_1   | 00E0: 02 EA B8 DD A8 AA BC 60   C2 CC 17 34 26 9C E5 92  .......`...4&...
scm3.org_1   | 00F0: B5 3D 1C 3C E9 6B 97 65   A3 7D 2C 52 40 7A 1E 89  .=.<.k.e..,R@z..
scm3.org_1   | 
scm3.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/certificate.crt.
scm3.org_1   | 2023-01-12 05:42:59,827 [main] INFO client.SCMCertificateClient: Added certificate [
scm3.org_1   | [
scm3.org_1   |   Version: V3
scm3.org_1   |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=df8756e7-ed3f-4e48-9521-edf82d63369c, CN=scm-sub@scm3.org
scm3.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm3.org_1   | 
scm3.org_1   |   Key:  Sun RSA public key, 2048 bits
scm3.org_1   |   params: null
scm3.org_1   |   modulus: 31439838987359940847672787864083963360378260624695091519160734305192389485610772394399521681807429918073003682462830890666006767244776862222005719299506767339955138910025203202650074436980165069018188657063155996713340539661168314946288719493941356116799565404403658010245409754700058559369063901650876279138413954390667775201414734211767168371301571596950578634601064004664598423126746279716323151412041726656483642838355435658159036661643135288084986763336786919110622329674606866537834199093954734678690170122635096059874113311301609708637901036394833866877808065012903544980013131446382281017465395770048300694101
scm3.org_1   |   public exponent: 65537
scm3.org_1   |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
scm3.org_1   |                To: Sun Feb 20 00:00:00 UTC 2028]
scm3.org_1   |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
scm3.org_1   |   SerialNumber: [    0127aaca d16f]
scm3.org_1   | 
scm3.org_1   | Certificate Extensions: 3
scm3.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm3.org_1   | BasicConstraints:[
scm3.org_1   |   CA:true
scm3.org_1   |   PathLen:2147483647
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm3.org_1   | KeyUsage [
scm3.org_1   |   DigitalSignature
scm3.org_1   |   Key_Encipherment
scm3.org_1   |   Data_Encipherment
scm3.org_1   |   Key_Agreement
scm3.org_1   |   Key_CertSign
scm3.org_1   |   Crl_Sign
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm3.org_1   | SubjectAlternativeName [
scm3.org_1   |   IPAddress: 172.25.0.118
scm3.org_1   |   DNSName: scm3.org
scm3.org_1   | ]
om3_1        | 2023-01-12 05:45:11,177 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1        | 2023-01-12 05:45:11,178 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1        | 2023-01-12 05:45:11,190 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1        | 2023-01-12 05:45:11,203 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1        | 2023-01-12 05:45:11,214 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1        | 2023-01-12 05:45:11,296 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1        | 2023-01-12 05:45:11,311 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-01-12 05:45:11,456 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1        | 2023-01-12 05:45:11,461 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1        | 2023-01-12 05:45:11,463 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1        | 2023-01-12 05:45:11,505 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1        | 2023-01-12 05:45:11,507 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1        | 2023-01-12 05:45:11,513 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-562213E44849: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-01-12 05:45:11,522 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1        | 2023-01-12 05:45:11,538 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-562213E44849-FollowerState
om3_1        | 2023-01-12 05:45:11,550 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-01-12 05:45:11,551 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-01-12 05:45:11,686 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om3
om3_1        | 2023-01-12 05:45:11,760 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1        | 2023-01-12 05:45:11,778 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1        | 2023-01-12 05:45:11,842 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1        | 2023-01-12 05:45:11,876 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1        | 2023-01-12 05:45:12,009 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
om3_1        | 2023-01-12 05:45:12,273 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1        | 2023-01-12 05:45:12,354 [Listener at om3/9862] INFO om.OzoneManager: Starting OM block token secret manager
om3_1        | 2023-01-12 05:45:12,354 [Listener at om3/9862] INFO token.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om3_1        | 2023-01-12 05:45:12,384 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1        | 2023-01-12 05:45:12,389 [Listener at om3/9862] INFO om.OzoneManager: Starting OM delegation token secret manager
om3_1        | 2023-01-12 05:45:12,389 [Listener at om3/9862] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om3_1        | 2023-01-12 05:45:12,458 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1        | 2023-01-12 05:45:12,494 [Thread[Thread-19,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om3_1        | 2023-01-12 05:45:13,284 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1        | 2023-01-12 05:45:13,293 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om3_1        | 2023-01-12 05:45:13,293 [Listener at om3/9862] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om3_1        | 2023-01-12 05:45:14,127 [Listener at om3/9862] INFO util.log: Logging initialized @67709ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1        | 2023-01-12 05:45:16,127 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1        | 2023-01-12 05:45:16,287 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1        | 2023-01-12 05:45:16,309 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om3_1        | 2023-01-12 05:45:16,321 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om3_1        | 2023-01-12 05:45:16,324 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om3_1        | 2023-01-12 05:45:16,409 [Listener at om3/9862] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om3_1        | 2023-01-12 05:45:16,744 [om3@group-562213E44849-FollowerState] INFO impl.FollowerState: om3@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5206158825ns, electionTimeout:5183ms
om3_1        | 2023-01-12 05:45:16,748 [om3@group-562213E44849-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-FollowerState
om3_1        | 2023-01-12 05:45:16,771 [om3@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om3_1        | 2023-01-12 05:45:16,815 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om3_1        | 2023-01-12 05:45:16,817 [om3@group-562213E44849-FollowerState] INFO impl.RoleInfo: om3: start om3@group-562213E44849-LeaderElection1
om3_1        | 2023-01-12 05:45:16,911 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-01-12 05:45:17,062 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
om3_1        | 2023-01-12 05:45:17,096 [Listener at om3/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om3_1        | 2023-01-12 05:45:18,023 [om3@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-01-12 05:45:18,033 [om3@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-01-12 05:45:18,155 [om3@group-562213E44849-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om3_1        | 2023-01-12 05:45:18,156 [om3@group-562213E44849-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om3_1        | 2023-01-12 05:45:18,182 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1        | 2023-01-12 05:45:18,183 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
om3_1        | 2023-01-12 05:45:18,225 [Listener at om3/9862] INFO server.session: node0 Scavenging every 660000ms
om3_1        | 2023-01-12 05:45:18,610 [Listener at om3/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om3_1        | 2023-01-12 05:45:18,726 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@89cef1a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1.org_1   | 2023-01-12 05:42:08,870 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xfdf104c3] REGISTERED
scm1.org_1   | 2023-01-12 05:42:08,871 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xfdf104c3] BIND: 0.0.0.0/0.0.0.0:0
scm1.org_1   | 2023-01-12 05:42:08,865 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServer: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: found a subdirectory /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f
scm1.org_1   | 2023-01-12 05:42:08,874 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xfdf104c3, L:/0.0.0.0:37107] ACTIVE
scm1.org_1   | 2023-01-12 05:42:08,876 [main] INFO server.RaftServer: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: addNew group-067D6DACB49F:[] returns group-067D6DACB49F:java.util.concurrent.CompletableFuture@59b492ec[Not completed]
scm1.org_1   | 2023-01-12 05:42:08,909 [pool-17-thread-1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: new RaftServerImpl for group-067D6DACB49F:[] with SCMStateMachine:uninitialized
scm1.org_1   | 2023-01-12 05:42:08,910 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2023-01-12 05:42:08,911 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2023-01-12 05:42:08,911 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2023-01-12 05:42:08,911 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2023-01-12 05:42:08,911 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2023-01-12 05:42:08,912 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2023-01-12 05:42:08,919 [pool-17-thread-1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2023-01-12 05:42:08,919 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2023-01-12 05:42:08,926 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2023-01-12 05:42:08,926 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2023-01-12 05:42:08,934 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2023-01-12 05:42:08,937 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2023-01-12 05:42:08,937 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2023-01-12 05:42:08,979 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-01-12 05:42:08,980 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1.org_1   | 2023-01-12 05:42:08,981 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1.org_1   | 2023-01-12 05:42:08,981 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1.org_1   | 2023-01-12 05:42:08,982 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1.org_1   | 2023-01-12 05:42:08,984 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm1.org_1   | 2023-01-12 05:42:08,984 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm1.org_1   | 2023-01-12 05:42:08,985 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm1.org_1   | 2023-01-12 05:42:09,116 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm1.org_1   | 2023-01-12 05:42:09,316 [main] INFO reflections.Reflections: Reflections took 169 ms to scan 3 urls, producing 121 keys and 272 values 
scm1.org_1   | 2023-01-12 05:42:09,412 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm1.org_1   | 2023-01-12 05:42:09,412 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm1.org_1   | 2023-01-12 05:42:09,416 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm1.org_1   | 2023-01-12 05:42:09,418 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm1.org_1   | 2023-01-12 05:42:09,545 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm1.org_1   | 2023-01-12 05:42:09,561 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm1.org_1   | 2023-01-12 05:42:09,563 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1.org_1   | 2023-01-12 05:42:09,574 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm1.org_1   | 2023-01-12 05:42:09,636 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1.org_1   | 2023-01-12 05:42:09,637 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1.org_1   | 2023-01-12 05:42:09,643 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1.org_1   | 2023-01-12 05:42:09,644 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1.org_1   | 2023-01-12 05:42:09,647 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm1.org_1   | 2023-01-12 05:42:09,651 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1.org_1   | 2023-01-12 05:42:09,659 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1.org_1   | 2023-01-12 05:42:09,659 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1.org_1   | 2023-01-12 05:42:09,709 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1.org_1   | 2023-01-12 05:42:09,732 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1.org_1   | 2023-01-12 05:42:09,813 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm1.org_1   | 2023-01-12 05:42:09,835 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1.org_1   | 2023-01-12 05:42:09,836 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm1.org_1   | 2023-01-12 05:42:09,865 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm1.org_1   | 2023-01-12 05:42:09,870 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:42:09,872 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-01-12 05:42:09,920 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
recon_1      | 2023-01-12 05:42:28,749 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context recon
recon_1      | 2023-01-12 05:42:28,749 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
recon_1      | 2023-01-12 05:42:28,749 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
recon_1      | 2023-01-12 05:42:28,752 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.recon.http.auth.kerberos.principal keytabKey: ozone.recon.http.auth.kerberos.keytab
recon_1      | 2023-01-12 05:42:28,972 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1      | 2023-01-12 05:42:29,643 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1      | 2023-01-12 05:42:29,660 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1      | 2023-01-12 05:42:29,676 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1      | 2023-01-12 05:42:29,752 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'id1'.
recon_1      | 2023-01-12 05:42:31,631 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-01-12 05:42:32,341 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-01-12 05:42:32,596 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1      | 2023-01-12 05:42:32,596 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1      | 2023-01-12 05:42:32,794 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-01-12 05:42:33,276 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1      | 2023-01-12 05:42:33,525 [main] INFO reflections.Reflections: Reflections took 228 ms to scan 3 urls, producing 121 keys and 272 values 
recon_1      | 2023-01-12 05:42:33,767 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1      | 2023-01-12 05:42:33,963 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1      | 2023-01-12 05:42:33,991 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1      | 2023-01-12 05:42:34,001 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1      | 2023-01-12 05:42:34,138 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1      | 2023-01-12 05:42:34,255 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1      | 2023-01-12 05:42:34,295 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1      | 2023-01-12 05:42:34,499 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1      | 2023-01-12 05:42:34,893 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1      | 2023-01-12 05:42:34,893 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1      | 2023-01-12 05:42:35,121 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1      | 2023-01-12 05:42:35,227 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1      | 2023-01-12 05:42:35,227 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1      | 2023-01-12 05:42:36,100 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1      | 2023-01-12 05:42:36,104 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1      | 2023-01-12 05:42:36,249 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1      | 2023-01-12 05:42:36,252 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1      | 2023-01-12 05:42:36,255 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 660000ms
recon_1      | 2023-01-12 05:42:36,317 [Listener at 0.0.0.0/9891] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-01-12 05:42:36,325 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@13f358f8{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1      | 2023-01-12 05:42:36,326 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@33ef393a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1      | 2023-01-12 05:42:37,921 [Listener at 0.0.0.0/9891] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-01-12 05:42:37,937 [Listener at 0.0.0.0/9891] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2023-01-12 05:42:45,954 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2aeaa332{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-2862105624199049433/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1      | 2023-01-12 05:42:46,006 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@b1d7b09{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1      | 2023-01-12 05:42:46,007 [Listener at 0.0.0.0/9891] INFO server.Server: Started @71740ms
recon_1      | 2023-01-12 05:42:46,018 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1      | 2023-01-12 05:42:46,019 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1      | 2023-01-12 05:42:46,026 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1      | 2023-01-12 05:42:46,030 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1      | 2023-01-12 05:42:46,089 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1      | 2023-01-12 05:42:46,105 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1      | 2023-01-12 05:42:46,105 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1      | 2023-01-12 05:42:46,106 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2023-01-12 05:42:46,106 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
om3_1        | 2023-01-12 05:45:18,749 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4327b21f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om3_1        | 2023-01-12 05:45:21,428 [Listener at om3/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om3_1        | 2023-01-12 05:45:21,739 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3695deb0{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-14525283589117998434/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om3_1        | 2023-01-12 05:45:21,946 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@4962f58e{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om3_1        | 2023-01-12 05:45:21,948 [Listener at om3/9862] INFO server.Server: Started @75530ms
om3_1        | 2023-01-12 05:45:21,987 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1        | 2023-01-12 05:45:21,987 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1        | 2023-01-12 05:45:21,998 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1        | 2023-01-12 05:45:22,000 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1        | 2023-01-12 05:45:22,027 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1        | 2023-01-12 05:45:22,953 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-om3: Detected pause in JVM or host machine (eg GC): pause of approximately 363378488ns. No GCs detected.
om3_1        | 2023-01-12 05:45:23,163 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1: ELECTION TIMEOUT received 0 response(s) and 0 exception(s):
om3_1        | 2023-01-12 05:45:23,171 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 ELECTION round 0: result TIMEOUT
om3_1        | 2023-01-12 05:45:23,209 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 ELECTION round 1: submit vote requests at term 2 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-01-12 05:45:23,337 [om3@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-01-12 05:45:23,337 [om3@group-562213E44849-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-01-12 05:45:23,415 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om3_1        | 2023-01-12 05:45:24,512 [Listener at om3/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om3_1        | 2023-01-12 05:45:24,639 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1f520187] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1        | 2023-01-12 05:45:26,522 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.998148204s. [buffered_nanos=2639863693, remote_addr=om2/172.25.0.112:9872]
om3_1        | 2023-01-12 05:45:26,582 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.997703305s. [buffered_nanos=3091637446, remote_addr=om1/172.25.0.111:9872]
om3_1        | 2023-01-12 05:45:26,584 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1: ELECTION REJECTED received 0 response(s) and 2 exception(s):
om3_1        | 2023-01-12 05:45:26,587 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.998148204s. [buffered_nanos=2639863693, remote_addr=om2/172.25.0.112:9872]
om3_1        | 2023-01-12 05:45:26,589 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.997703305s. [buffered_nanos=3091637446, remote_addr=om1/172.25.0.111:9872]
om3_1        | 2023-01-12 05:45:26,590 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 ELECTION round 1: result REJECTED
om3_1        | 2023-01-12 05:45:26,604 [om3@group-562213E44849-LeaderElection1] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
om3_1        | 2023-01-12 05:45:26,607 [om3@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-LeaderElection1
om3_1        | 2023-01-12 05:45:26,609 [om3@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om3: start om3@group-562213E44849-FollowerState
om3_1        | 2023-01-12 05:45:26,639 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-01-12 05:45:26,639 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1        | 2023-01-12 05:45:27,990 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:41307
om3_1        | 2023-01-12 05:45:28,025 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om3_1        | 2023-01-12 05:45:31,707 [om3@group-562213E44849-FollowerState] INFO impl.FollowerState: om3@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5098301303ns, electionTimeout:5066ms
om3_1        | 2023-01-12 05:45:31,708 [om3@group-562213E44849-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-FollowerState
om3_1        | 2023-01-12 05:45:31,708 [om3@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
om3_1        | 2023-01-12 05:45:31,708 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om3_1        | 2023-01-12 05:45:31,708 [om3@group-562213E44849-FollowerState] INFO impl.RoleInfo: om3: start om3@group-562213E44849-LeaderElection2
om3_1        | 2023-01-12 05:45:31,712 [om3@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection2 ELECTION round 0: submit vote requests at term 3 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-01-12 05:45:31,716 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1        | 2023-01-12 05:45:31,716 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2.org_1   |   modulus: 30346053453237189834112040673541629567010306125791898570104054454047501736657892214383462264526301500298692652758375836429638347531164751981653358525504527825570400558838334901393005984876414719430775458652128247547444727818535668600972192623637356865083888753952717740665794517921055984453520353917733198649461590018542814322448272297366500121604394586840649019781675207082962097691558583700862118486739043197335879323701508955628436573724419921778589743880043303622162563304571704603236749325057266808761842228199352214967718782980075765678400986380731445934917806096546808808883868771850900183098234200323729287601
scm2.org_1   |   public exponent: 65537
scm2.org_1   |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
scm2.org_1   |                To: Sun Feb 20 00:00:00 UTC 2028]
scm2.org_1   |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
scm2.org_1   |   SerialNumber: [    01]
scm2.org_1   | 
scm2.org_1   | Certificate Extensions: 3
scm2.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm2.org_1   | BasicConstraints:[
scm2.org_1   |   CA:true
scm2.org_1   |   PathLen:2147483647
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm2.org_1   | KeyUsage [
scm2.org_1   |   Key_CertSign
scm2.org_1   |   Crl_Sign
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm2.org_1   | SubjectAlternativeName [
scm2.org_1   |   IPAddress: 172.25.0.116
scm2.org_1   |   DNSName: scm1.org
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | ]
scm2.org_1   |   Algorithm: [SHA256withRSA]
scm2.org_1   |   Signature:
scm2.org_1   | 0000: 59 E4 14 CA 42 0B 71 BE   61 3A FA 41 71 F2 48 C6  Y...B.q.a:.Aq.H.
scm2.org_1   | 0010: 35 3A DF E8 FE E3 74 A0   7E 57 15 0F F2 39 95 34  5:....t..W...9.4
scm2.org_1   | 0020: E6 A7 48 CE 06 EA B9 42   CC AD DC 81 09 F1 F2 3E  ..H....B.......>
scm2.org_1   | 0030: 94 27 78 5F 6B 04 F9 F4   E3 A1 71 61 8F 5A EF 6D  .'x_k.....qa.Z.m
scm2.org_1   | 0040: 72 EA 9B CB 23 B9 31 9F   D7 B3 7B 86 35 09 CF D1  r...#.1.....5...
scm2.org_1   | 0050: D6 81 A8 2E 21 01 F7 A9   B4 78 00 21 D5 3C 4A 61  ....!....x.!.<Ja
scm2.org_1   | 0060: 7F 91 56 B9 50 6C 2C 23   0F D3 49 6E 80 28 7A 03  ..V.Pl,#..In.(z.
scm2.org_1   | 0070: 32 46 42 54 4F 22 06 7F   B1 7D 41 A0 09 27 CF 7A  2FBTO"....A..'.z
scm2.org_1   | 0080: F0 82 C6 00 70 41 5D 8F   83 5F E6 EB 74 1F B7 A2  ....pA].._..t...
scm2.org_1   | 0090: 85 64 3E AE D6 66 71 7C   BE 7F 92 D0 79 64 71 C0  .d>..fq.....ydq.
scm2.org_1   | 00A0: 8F 76 8C F6 D1 A4 67 C4   F4 11 8D 75 FC DE EF EE  .v....g....u....
scm2.org_1   | 00B0: D9 7F 75 F3 5F AC 76 DF   23 F9 19 99 25 F9 31 8E  ..u._.v.#...%.1.
scm2.org_1   | 00C0: AA 69 DC 7B 7E 4E 62 10   80 B9 5E 00 F8 1D 16 71  .i...Nb...^....q
scm2.org_1   | 00D0: 7F D4 C7 FD 20 63 0B 0A   7E 36 7D F4 4C 85 A7 61  .... c...6..L..a
scm2.org_1   | 00E0: E7 00 61 A2 83 40 78 92   3E A1 9A E0 B8 E4 A1 A0  ..a..@x.>.......
scm2.org_1   | 00F0: 45 B5 33 7C 03 24 98 E7   DE 69 74 45 16 2B 06 3F  E.3..$...itE.+.?
scm2.org_1   | 
scm2.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/CA-1.crt.
scm2.org_1   | 2023-01-12 05:42:27,014 [main] INFO client.SCMCertificateClient: Added certificate [
scm2.org_1   | [
scm2.org_1   |   Version: V3
scm2.org_1   |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=52d7416a-527f-45fb-b83c-77d6ce7a6ad0, CN=scm-sub@scm2.org
scm2.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm2.org_1   | 
scm2.org_1   |   Key:  Sun RSA public key, 2048 bits
scm2.org_1   |   params: null
scm2.org_1   |   modulus: 21296067913955723361589627170314898002064641208623135524154294810131409877074508273082723718740147680966807311144252411367888376024996605152521958352406230391942492361201976100556293486851482946819261404201951249034837906375767864559215717665329223917742988572781734777135183776207026905309794420045141329320776269490042924812268866788557106068955556478992808330843770389983215232226165222849053432386265314464149078186792184829087939822863552303870605499037979304036928075309857134617034149797703121510372802427126836714186382585822920887501656803481248653324411932672950162045545903101492333324034336995087910441773
scm2.org_1   |   public exponent: 65537
scm2.org_1   |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
scm2.org_1   |                To: Sun Feb 20 00:00:00 UTC 2028]
om3_1        | 2023-01-12 05:45:32,001 [om3@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
om3_1        | 2023-01-12 05:45:32,003 [om3@group-562213E44849-LeaderElection2] INFO impl.LeaderElection:   Response 0: om3<-om2#0:OK-t3
om3_1        | 2023-01-12 05:45:32,005 [om3@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection2 ELECTION round 0: result PASSED
om3_1        | 2023-01-12 05:45:32,005 [om3@group-562213E44849-LeaderElection2] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-LeaderElection2
om3_1        | 2023-01-12 05:45:32,006 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
om3_1        | 2023-01-12 05:45:32,006 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServer$Division: om3@group-562213E44849: change Leader from null to om3 at term 3 for becomeLeader, leader elected after 31665ms
om3_1        | 2023-01-12 05:45:32,080 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om3_1        | 2023-01-12 05:45:32,145 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om3_1        | 2023-01-12 05:45:32,147 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om3_1        | 2023-01-12 05:45:32,189 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om3_1        | 2023-01-12 05:45:32,196 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om3_1        | 2023-01-12 05:45:32,198 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om3_1        | 2023-01-12 05:45:32,251 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om3_1        | 2023-01-12 05:45:32,277 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om3_1        | 2023-01-12 05:45:32,363 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om3_1        | 2023-01-12 05:45:32,363 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-01-12 05:45:32,364 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om3_1        | 2023-01-12 05:45:32,369 [om3@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om3_1        | 2023-01-12 05:45:32,373 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2023-01-12 05:45:32,373 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1        | 2023-01-12 05:45:32,373 [om3@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1        | 2023-01-12 05:45:32,373 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om3_1        | 2023-01-12 05:45:32,377 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om3_1        | 2023-01-12 05:45:32,379 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2023-01-12 05:45:32,379 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om3_1        | 2023-01-12 05:45:32,379 [om3@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om3_1        | 2023-01-12 05:45:32,380 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2023-01-12 05:45:32,380 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1        | 2023-01-12 05:45:32,381 [om3@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1        | 2023-01-12 05:45:32,381 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om3_1        | 2023-01-12 05:45:32,388 [om3@group-562213E44849-LeaderElection2] INFO impl.RoleInfo: om3: start om3@group-562213E44849-LeaderStateImpl
om3_1        | 2023-01-12 05:45:32,465 [om3@group-562213E44849-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om3_1        | 2023-01-12 05:45:32,582 [om3@group-562213E44849-LeaderElection2] INFO server.RaftServer$Division: om3@group-562213E44849: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1        | 2023-01-12 05:45:33,288 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om3_1        | 2023-01-12 05:45:34,058 [om3@group-562213E44849-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om3_1        | [id: "om1"
om3_1        | address: "om1:9872"
om3_1        | startupRole: FOLLOWER
om3_1        | , id: "om3"
om3_1        | address: "om3:9872"
om3_1        | startupRole: FOLLOWER
om3_1        | , id: "om2"
om3_1        | address: "om2:9872"
om3_1        | startupRole: FOLLOWER
om3_1        | ]
scm3.org_1   | 
scm3.org_1   | ]
scm3.org_1   |   Algorithm: [SHA256withRSA]
scm3.org_1   |   Signature:
scm3.org_1   | 0000: BA D3 50 1A 0D EB 1A F5   78 59 92 B5 68 AA 13 3C  ..P.....xY..h..<
scm3.org_1   | 0010: 44 16 EF A7 A2 00 A3 47   EA 25 8D 9F 6B 10 1C 7E  D......G.%..k...
scm3.org_1   | 0020: A2 B5 91 A9 24 93 26 85   06 0B 2E EB 30 5F 97 66  ....$.&.....0_.f
scm3.org_1   | 0030: 1A 51 D2 04 DD 1A EA 5A   17 9A 54 5F 6B 33 BF 44  .Q.....Z..T_k3.D
scm3.org_1   | 0040: DB F9 F1 54 4F D3 A5 B8   FB 16 51 C2 45 D2 70 C4  ...TO.....Q.E.p.
scm3.org_1   | 0050: 19 C2 9F C3 F9 41 2A B5   17 01 69 86 83 FE C3 DF  .....A*...i.....
scm3.org_1   | 0060: 16 A1 36 F7 53 DB 88 AC   2C 3E 14 6F 12 95 86 2E  ..6.S...,>.o....
scm3.org_1   | 0070: DF CD 91 2E 6F 15 8F BE   B9 04 B5 73 50 B7 0C B4  ....o......sP...
scm3.org_1   | 0080: 27 B4 3C 54 9B 37 DD 8E   6F D8 D5 E3 2C B8 D4 1E  '.<T.7..o...,...
scm3.org_1   | 0090: 6D CF A4 E9 DD 76 25 20   46 CA 1F 1D 2C 30 13 6D  m....v% F...,0.m
scm3.org_1   | 00A0: 0B 7C 30 59 2B 0B 60 16   84 1B 15 C5 CC 2C B3 F9  ..0Y+.`......,..
scm3.org_1   | 00B0: 81 FC 85 98 9F 45 73 34   EB 6C 00 89 2E 7D 15 F6  .....Es4.l......
scm3.org_1   | 00C0: 5D AA 73 64 30 DD BA B7   65 4D C1 CD 36 8A FE 3B  ].sd0...eM..6..;
scm3.org_1   | 00D0: A7 E0 FF E0 50 E3 F1 A2   7F E1 40 E4 30 F9 E5 74  ....P.....@.0..t
scm3.org_1   | 00E0: 02 EA B8 DD A8 AA BC 60   C2 CC 17 34 26 9C E5 92  .......`...4&...
scm3.org_1   | 00F0: B5 3D 1C 3C E9 6B 97 65   A3 7D 2C 52 40 7A 1E 89  .=.<.k.e..,R@z..
scm3.org_1   | 
scm3.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/1269880770927.crt.
scm3.org_1   | 2023-01-12 05:42:59,843 [main] INFO client.SCMCertificateClient: Added certificate [
scm3.org_1   | [
scm3.org_1   |   Version: V3
scm3.org_1   |   Subject: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
scm3.org_1   |   Signature Algorithm: SHA256withRSA, OID = 1.2.840.113549.1.1.11
scm3.org_1   | 
scm3.org_1   |   Key:  Sun RSA public key, 2048 bits
scm3.org_1   |   params: null
scm3.org_1   |   modulus: 30346053453237189834112040673541629567010306125791898570104054454047501736657892214383462264526301500298692652758375836429638347531164751981653358525504527825570400558838334901393005984876414719430775458652128247547444727818535668600972192623637356865083888753952717740665794517921055984453520353917733198649461590018542814322448272297366500121604394586840649019781675207082962097691558583700862118486739043197335879323701508955628436573724419921778589743880043303622162563304571704603236749325057266808761842228199352214967718782980075765678400986380731445934917806096546808808883868771850900183098234200323729287601
scm3.org_1   |   public exponent: 65537
scm3.org_1   |   Validity: [From: Thu Jan 12 00:00:00 UTC 2023,
scm3.org_1   |                To: Sun Feb 20 00:00:00 UTC 2028]
scm3.org_1   |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
scm3.org_1   |   SerialNumber: [    01]
scm3.org_1   | 
scm3.org_1   | Certificate Extensions: 3
scm3.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm3.org_1   | BasicConstraints:[
scm3.org_1   |   CA:true
scm3.org_1   |   PathLen:2147483647
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm3.org_1   | KeyUsage [
scm3.org_1   |   Key_CertSign
scm3.org_1   |   Crl_Sign
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm3.org_1   | SubjectAlternativeName [
scm3.org_1   |   IPAddress: 172.25.0.116
scm3.org_1   |   DNSName: scm1.org
scm3.org_1   | ]
scm3.org_1   | 
scm3.org_1   | ]
recon_1      | 2023-01-12 05:42:46,114 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1      | 2023-01-12 05:42:47,484 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1      | 2023-01-12 05:42:47,487 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1      | 2023-01-12 05:42:47,487 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1      | 2023-01-12 05:42:47,488 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1      | 2023-01-12 05:42:47,491 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1      | 2023-01-12 05:42:47,535 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1      | 2023-01-12 05:43:06,108 [pool-30-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1      | 2023-01-12 05:43:06,109 [pool-30-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1      | 2023-01-12 05:43:06,410 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 1 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:06,419 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:08,423 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 3 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:08,425 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 4 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:08,426 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:10,428 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 6 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:10,429 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 7 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:10,430 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:12,432 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 9 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:12,437 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 10 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:12,456 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:14,461 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 12 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:14,462 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 13 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:14,464 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:16,466 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 15 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:16,468 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 16 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:16,469 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 17 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:18,471 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 18 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:18,473 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 19 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:18,476 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 20 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:20,484 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 21 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:20,486 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 22 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:20,512 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 23 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:22,514 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 24 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:22,526 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 25 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:22,546 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 26 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:24,558 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 27 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:24,559 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 28 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:24,561 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 29 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:26,563 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 30 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:26,565 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 31 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:26,568 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 32 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:28,572 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 33 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:28,573 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 34 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:28,580 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 35 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:30,582 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 36 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:30,586 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 37 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:30,588 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 38 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:32,599 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 39 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:32,601 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 40 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:32,602 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 41 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:34,604 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 42 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:34,605 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 43 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:34,606 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 44 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:36,608 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 45 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:36,609 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 46 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:36,611 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 47 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:38,613 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 48 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:38,615 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 49 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:38,616 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 50 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:40,617 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 51 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:40,619 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 52 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:40,620 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 53 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:42,621 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 54 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:42,625 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 55 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:42,627 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 56 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:44,629 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 57 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:44,631 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 58 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:44,633 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 59 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:46,634 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 60 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:46,636 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 61 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:46,639 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 62 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:48,644 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 63 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:48,645 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 64 failover attempts. Trying to failover immediately.
scm1.org_1   | 2023-01-12 05:42:09,934 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2023-01-12 05:42:09,935 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 1212956981891 on primary SCM
scm1.org_1   | 2023-01-12 05:42:09,944 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
scm1.org_1   | 2023-01-12 05:42:09,982 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-01-12 05:42:10,029 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm1.org_1   | 2023-01-12 05:42:10,889 [Listener at 0.0.0.0/9961] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-01-12 05:42:10,898 [Listener at 0.0.0.0/9961] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-01-12 05:42:10,899 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1.org_1   | 2023-01-12 05:42:11,006 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-01-12 05:42:11,013 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-01-12 05:42:11,014 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm1.org_1   | 2023-01-12 05:42:11,048 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1.org_1   | 2023-01-12 05:42:11,063 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2023-01-12 05:42:11,064 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1.org_1   | 2023-01-12 05:42:11,245 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm1.org_1   | 2023-01-12 05:42:11,247 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm1.org_1   | Container Balancer status:
scm1.org_1   | Key                            Value
scm1.org_1   | Running                        true
scm1.org_1   | Container Balancer Configuration values:
scm1.org_1   | Key                                                Value
scm1.org_1   | Threshold                                          10
scm1.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1.org_1   | Max Size to Move per Iteration                     500GB
scm1.org_1   | Max Size Entering Target per Iteration             26GB
scm1.org_1   | Max Size Leaving Source per Iteration              26GB
scm1.org_1   | 
scm1.org_1   | 2023-01-12 05:42:11,248 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1.org_1   | 2023-01-12 05:42:11,252 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1.org_1   | 2023-01-12 05:42:11,258 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1.org_1   | 2023-01-12 05:42:11,264 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/in_use.lock acquired by nodename 7@scm1.org
scm1.org_1   | 2023-01-12 05:42:11,276 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1} from /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/raft-meta
scm1.org_1   | 2023-01-12 05:42:11,328 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: set configuration 0: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-12 05:42:11,334 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2023-01-12 05:42:11,349 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2023-01-12 05:42:11,350 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-12 05:42:11,354 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1.org_1   | 2023-01-12 05:42:11,355 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1.org_1   | 2023-01-12 05:42:11,360 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-01-12 05:42:11,371 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2023-01-12 05:42:11,373 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2023-01-12 05:42:11,382 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f
scm1.org_1   | 2023-01-12 05:42:11,382 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-01-12 05:42:11,383 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2023-01-12 05:42:11,384 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2023-01-12 05:42:11,385 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2023-01-12 05:42:11,385 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2023-01-12 05:42:11,386 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2023-01-12 05:42:11,386 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2023-01-12 05:42:11,387 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2023-01-12 05:42:11,403 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2023-01-12 05:42:11,403 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   |   Issuer: O=CID-dd9a252d-f4dd-4162-963a-067d6dacb49f, OU=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1, CN=scm@scm1.org
scm2.org_1   |   SerialNumber: [    011fd096 7347]
scm2.org_1   | 
scm2.org_1   | Certificate Extensions: 3
scm2.org_1   | [1]: ObjectId: 2.5.29.19 Criticality=true
scm2.org_1   | BasicConstraints:[
scm2.org_1   |   CA:true
scm2.org_1   |   PathLen:2147483647
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | [2]: ObjectId: 2.5.29.15 Criticality=true
scm2.org_1   | KeyUsage [
scm2.org_1   |   DigitalSignature
scm2.org_1   |   Key_Encipherment
scm2.org_1   |   Data_Encipherment
scm2.org_1   |   Key_Agreement
scm2.org_1   |   Key_CertSign
scm2.org_1   |   Crl_Sign
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | [3]: ObjectId: 2.5.29.17 Criticality=false
scm2.org_1   | SubjectAlternativeName [
scm2.org_1   |   IPAddress: 172.25.0.117
scm2.org_1   |   DNSName: scm2.org
scm2.org_1   | ]
scm2.org_1   | 
scm2.org_1   | ]
scm2.org_1   |   Algorithm: [SHA256withRSA]
scm2.org_1   |   Signature:
scm2.org_1   | 0000: DD DA B9 E1 12 DC B9 19   AB 43 6C 4A F6 18 61 AF  .........ClJ..a.
scm2.org_1   | 0010: 63 58 4C 9F 55 7F E1 D3   97 AA B7 7F AD 28 7B BB  cXL.U........(..
scm2.org_1   | 0020: E5 21 4D A8 C7 21 5B 47   75 3B 80 C5 AE BD 29 29  .!M..![Gu;....))
scm2.org_1   | 0030: 6A CF 1F 7F 52 05 65 22   33 92 76 50 63 7A 98 D0  j...R.e"3.vPcz..
scm2.org_1   | 0040: F2 C2 8C 67 61 9B 69 DF   FE 65 1F 63 B7 C9 90 AD  ...ga.i..e.c....
scm2.org_1   | 0050: 48 64 69 41 62 B0 4D C9   A4 69 BB 4E F2 0F 37 82  HdiAb.M..i.N..7.
scm2.org_1   | 0060: D5 7C 07 84 B0 1A 7E AE   D5 CF 7B 97 E4 78 EB 80  .............x..
scm2.org_1   | 0070: 70 50 FC BB BE B2 16 05   36 A2 CA 9A 95 B2 A3 E9  pP......6.......
scm2.org_1   | 0080: 37 4E 5C 28 7C A8 C6 3E   D6 E4 C2 81 6A F1 17 E4  7N\(...>....j...
scm2.org_1   | 0090: 9D 69 39 A1 4C DC 8E F3   D9 9E 28 55 5B F6 D9 27  .i9.L.....(U[..'
scm2.org_1   | 00A0: 17 CC D4 B1 95 F6 65 E6   AC 48 2E 1A 07 6F 26 1F  ......e..H...o&.
scm2.org_1   | 00B0: A2 F7 86 90 5F 1C 62 A6   B8 79 9F C4 0F C9 85 70  ...._.b..y.....p
scm2.org_1   | 00C0: 13 4A E8 5D 47 9E F2 E8   C9 FD 9E 50 B5 4B 4B 29  .J.]G......P.KK)
scm2.org_1   | 00D0: 40 FB 79 CE E3 AC D3 7A   4F 76 1E 14 70 D9 D8 8F  @.y....zOv..p...
scm2.org_1   | 00E0: 77 4D 15 9B 15 68 06 B0   52 07 C0 B4 2C A2 D2 25  wM...h..R...,..%
scm2.org_1   | 00F0: 64 D5 A3 C3 E6 8A 1F 3C   1F 0B F9 6D 23 E1 B7 DB  d......<...m#...
scm2.org_1   | 
scm2.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/1236155134791.crt.
scm2.org_1   | 2023-01-12 05:42:27,044 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor is started with first delay 158696252981 ms and interval 86400000 ms.
scm2.org_1   | 2023-01-12 05:42:27,455 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2023-01-12 05:42:27,460 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2023-01-12 05:42:27,624 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-01-12 05:42:28,156 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2023-01-12 05:42:28,997 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm2.org_1   | 2023-01-12 05:42:29,000 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2.org_1   | 2023-01-12 05:42:29,327 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2.org_1   | 2023-01-12 05:42:29,429 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:52d7416a-527f-45fb-b83c-77d6ce7a6ad0
scm2.org_1   | 2023-01-12 05:42:29,536 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm2.org_1   | 2023-01-12 05:42:29,565 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm2.org_1   | 2023-01-12 05:42:29,744 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2.org_1   | 2023-01-12 05:42:29,937 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2.org_1   | 2023-01-12 05:42:29,940 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2.org_1   | 2023-01-12 05:42:29,940 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2.org_1   | 2023-01-12 05:42:29,948 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2.org_1   | 2023-01-12 05:42:29,948 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2.org_1   | 2023-01-12 05:42:29,948 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2.org_1   | 2023-01-12 05:42:29,952 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2.org_1   | 2023-01-12 05:42:29,959 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-01-12 05:42:29,959 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2.org_1   | 2023-01-12 05:42:29,963 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2.org_1   | 2023-01-12 05:42:29,995 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2.org_1   | 2023-01-12 05:42:29,998 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm2.org_1   | 2023-01-12 05:42:29,999 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm2.org_1   | 2023-01-12 05:42:31,482 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
scm2.org_1   | 2023-01-12 05:42:31,768 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
scm2.org_1   | 2023-01-12 05:42:31,780 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
scm2.org_1   | 2023-01-12 05:42:31,780 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm2.org_1   | 2023-01-12 05:42:31,789 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
scm2.org_1   | 2023-01-12 05:42:31,800 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm2.org_1   | 2023-01-12 05:42:31,800 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
scm2.org_1   | 2023-01-12 05:42:31,835 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
scm2.org_1   | 2023-01-12 05:42:31,855 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
scm2.org_1   | 2023-01-12 05:42:31,958 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
scm2.org_1   | 2023-01-12 05:42:31,960 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm2.org_1   | 2023-01-12 05:42:32,121 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm2.org_1   | 2023-01-12 05:42:32,126 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2.org_1   | 2023-01-12 05:42:32,133 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2023-01-12 05:42:32,133 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2023-01-12 05:42:32,146 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-01-12 05:42:32,178 [main] INFO server.RaftServer: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0: addNew group-067D6DACB49F:[] returns group-067D6DACB49F:java.util.concurrent.CompletableFuture@59b492ec[Not completed]
scm2.org_1   | 2023-01-12 05:42:32,244 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x57362787] REGISTERED
scm2.org_1   | 2023-01-12 05:42:32,246 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x57362787] BIND: 0.0.0.0/0.0.0.0:0
scm2.org_1   | 2023-01-12 05:42:32,256 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x57362787, L:/0.0.0.0:33733] ACTIVE
scm2.org_1   | 2023-01-12 05:42:32,268 [pool-17-thread-1] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0: new RaftServerImpl for group-067D6DACB49F:[] with SCMStateMachine:uninitialized
scm2.org_1   | 2023-01-12 05:42:32,286 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm2.org_1   | 2023-01-12 05:42:32,291 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1      | 2023-01-12 05:43:48,646 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 65 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:50,647 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 66 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:50,650 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 67 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:50,651 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 68 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:52,652 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 69 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:52,653 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 70 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:52,654 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 71 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:54,656 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 72 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:54,657 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 73 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:54,659 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 74 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:56,660 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 75 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:56,661 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 76 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:56,662 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 77 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:43:58,663 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 78 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:58,665 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 79 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:43:58,666 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 80 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:00,668 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 81 failover attempts. Trying to failover immediately.
scm3.org_1   |   Algorithm: [SHA256withRSA]
scm3.org_1   |   Signature:
scm3.org_1   | 0000: 59 E4 14 CA 42 0B 71 BE   61 3A FA 41 71 F2 48 C6  Y...B.q.a:.Aq.H.
scm3.org_1   | 0010: 35 3A DF E8 FE E3 74 A0   7E 57 15 0F F2 39 95 34  5:....t..W...9.4
scm3.org_1   | 0020: E6 A7 48 CE 06 EA B9 42   CC AD DC 81 09 F1 F2 3E  ..H....B.......>
scm3.org_1   | 0030: 94 27 78 5F 6B 04 F9 F4   E3 A1 71 61 8F 5A EF 6D  .'x_k.....qa.Z.m
scm3.org_1   | 0040: 72 EA 9B CB 23 B9 31 9F   D7 B3 7B 86 35 09 CF D1  r...#.1.....5...
scm3.org_1   | 0050: D6 81 A8 2E 21 01 F7 A9   B4 78 00 21 D5 3C 4A 61  ....!....x.!.<Ja
scm3.org_1   | 0060: 7F 91 56 B9 50 6C 2C 23   0F D3 49 6E 80 28 7A 03  ..V.Pl,#..In.(z.
scm3.org_1   | 0070: 32 46 42 54 4F 22 06 7F   B1 7D 41 A0 09 27 CF 7A  2FBTO"....A..'.z
scm3.org_1   | 0080: F0 82 C6 00 70 41 5D 8F   83 5F E6 EB 74 1F B7 A2  ....pA].._..t...
scm3.org_1   | 0090: 85 64 3E AE D6 66 71 7C   BE 7F 92 D0 79 64 71 C0  .d>..fq.....ydq.
scm3.org_1   | 00A0: 8F 76 8C F6 D1 A4 67 C4   F4 11 8D 75 FC DE EF EE  .v....g....u....
scm3.org_1   | 00B0: D9 7F 75 F3 5F AC 76 DF   23 F9 19 99 25 F9 31 8E  ..u._.v.#...%.1.
scm3.org_1   | 00C0: AA 69 DC 7B 7E 4E 62 10   80 B9 5E 00 F8 1D 16 71  .i...Nb...^....q
scm3.org_1   | 00D0: 7F D4 C7 FD 20 63 0B 0A   7E 36 7D F4 4C 85 A7 61  .... c...6..L..a
scm3.org_1   | 00E0: E7 00 61 A2 83 40 78 92   3E A1 9A E0 B8 E4 A1 A0  ..a..@x.>.......
scm3.org_1   | 00F0: 45 B5 33 7C 03 24 98 E7   DE 69 74 45 16 2B 06 3F  E.3..$...itE.+.?
scm3.org_1   | 
scm3.org_1   | ] from file:/data/metadata/scm/sub-ca/certs/CA-1.crt.
scm3.org_1   | 2023-01-12 05:42:59,864 [main] INFO client.SCMCertificateClient: CertificateLifetimeMonitor is started with first delay 158696220153 ms and interval 86400000 ms.
scm3.org_1   | 2023-01-12 05:43:00,211 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2023-01-12 05:43:00,211 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2023-01-12 05:43:00,251 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-01-12 05:43:00,503 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2023-01-12 05:43:00,964 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm3.org_1   | 2023-01-12 05:43:00,964 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm3.org_1   | 2023-01-12 05:43:01,076 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3.org_1   | 2023-01-12 05:43:01,115 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:df8756e7-ed3f-4e48-9521-edf82d63369c
scm3.org_1   | 2023-01-12 05:43:01,170 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm3.org_1   | 2023-01-12 05:43:01,187 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm3.org_1   | 2023-01-12 05:43:01,266 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm3.org_1   | 2023-01-12 05:43:01,388 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm3.org_1   | 2023-01-12 05:43:01,391 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm3.org_1   | 2023-01-12 05:43:01,391 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm3.org_1   | 2023-01-12 05:43:01,392 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm3.org_1   | 2023-01-12 05:43:01,392 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm3.org_1   | 2023-01-12 05:43:01,392 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3.org_1   | 2023-01-12 05:43:01,393 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3.org_1   | 2023-01-12 05:43:01,394 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-01-12 05:43:01,395 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3.org_1   | 2023-01-12 05:43:01,396 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3.org_1   | 2023-01-12 05:43:01,420 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3.org_1   | 2023-01-12 05:43:01,429 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm3.org_1   | 2023-01-12 05:43:01,429 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm3.org_1   | 2023-01-12 05:43:02,239 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
scm3.org_1   | 2023-01-12 05:43:02,588 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
scm3.org_1   | 2023-01-12 05:43:02,605 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
scm3.org_1   | 2023-01-12 05:43:02,610 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm3.org_1   | 2023-01-12 05:43:02,613 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
scm3.org_1   | 2023-01-12 05:43:02,622 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm3.org_1   | 2023-01-12 05:43:02,624 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
scm3.org_1   | 2023-01-12 05:43:02,681 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
scm3.org_1   | 2023-01-12 05:43:02,691 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = GrpcTlsConfig0- (custom)
scm3.org_1   | 2023-01-12 05:43:02,788 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
scm3.org_1   | 2023-01-12 05:43:02,790 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm3.org_1   | 2023-01-12 05:43:03,026 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm3.org_1   | 2023-01-12 05:43:03,035 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3.org_1   | 2023-01-12 05:43:03,036 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-01-12 05:43:03,036 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2023-01-12 05:43:03,071 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2023-01-12 05:43:03,099 [df8756e7-ed3f-4e48-9521-edf82d63369c-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xe61ee118] REGISTERED
scm3.org_1   | 2023-01-12 05:43:03,108 [df8756e7-ed3f-4e48-9521-edf82d63369c-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xe61ee118] BIND: 0.0.0.0/0.0.0.0:0
scm3.org_1   | 2023-01-12 05:43:03,125 [main] INFO server.RaftServer: df8756e7-ed3f-4e48-9521-edf82d63369c: addNew group-067D6DACB49F:[] returns group-067D6DACB49F:java.util.concurrent.CompletableFuture@10a907ec[Not completed]
scm3.org_1   | 2023-01-12 05:43:03,128 [df8756e7-ed3f-4e48-9521-edf82d63369c-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xe61ee118, L:/0.0.0.0:37207] ACTIVE
scm3.org_1   | 2023-01-12 05:43:03,202 [pool-17-thread-1] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c: new RaftServerImpl for group-067D6DACB49F:[] with SCMStateMachine:uninitialized
scm3.org_1   | 2023-01-12 05:43:03,207 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3.org_1   | 2023-01-12 05:43:03,210 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm3.org_1   | 2023-01-12 05:43:03,211 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3.org_1   | 2023-01-12 05:43:03,211 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2023-01-12 05:43:03,214 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2023-01-12 05:43:03,214 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3.org_1   | 2023-01-12 05:43:03,241 [pool-17-thread-1] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3.org_1   | 2023-01-12 05:43:03,242 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2023-01-12 05:43:03,271 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3.org_1   | 2023-01-12 05:43:03,274 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3.org_1   | 2023-01-12 05:43:03,300 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm3.org_1   | 2023-01-12 05:43:03,316 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3.org_1   | 2023-01-12 05:43:03,318 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3.org_1   | 2023-01-12 05:43:03,446 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3.org_1   | 2023-01-12 05:43:03,447 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3.org_1   | 2023-01-12 05:43:03,450 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3.org_1   | 2023-01-12 05:43:03,451 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3.org_1   | 2023-01-12 05:43:03,453 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3.org_1   | 2023-01-12 05:43:03,460 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3.org_1   | 2023-01-12 05:43:03,460 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3.org_1   | 2023-01-12 05:43:03,461 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3.org_1   | 2023-01-12 05:43:03,960 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm3.org_1   | 2023-01-12 05:43:04,338 [main] INFO reflections.Reflections: Reflections took 324 ms to scan 3 urls, producing 121 keys and 272 values 
scm3.org_1   | 2023-01-12 05:43:04,689 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm3.org_1   | 2023-01-12 05:43:04,690 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm3.org_1   | 2023-01-12 05:43:04,706 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm3.org_1   | 2023-01-12 05:43:04,716 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3.org_1   | 2023-01-12 05:43:04,861 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3.org_1   | 2023-01-12 05:43:04,968 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm3.org_1   | 2023-01-12 05:43:04,975 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3.org_1   | 2023-01-12 05:43:05,017 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm3.org_1   | 2023-01-12 05:43:05,127 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm3.org_1   | 2023-01-12 05:43:05,128 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3.org_1   | 2023-01-12 05:43:05,144 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm3.org_1   | 2023-01-12 05:43:05,146 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3.org_1   | 2023-01-12 05:43:05,157 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3.org_1   | 2023-01-12 05:43:05,167 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
recon_1      | 2023-01-12 05:44:00,669 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 82 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:00,671 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 83 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:02,673 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 84 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:02,692 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 85 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:02,700 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 86 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:04,704 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 87 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:04,706 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 88 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:04,707 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 89 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:06,709 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 90 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:06,712 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 91 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:06,713 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 92 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:08,714 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 93 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:08,717 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 94 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:08,717 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 95 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:10,719 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 96 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:10,720 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 97 failover attempts. Trying to failover immediately.
scm1.org_1   | 2023-01-12 05:42:11,595 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1.org_1   | 2023-01-12 05:42:11,597 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1.org_1   | 2023-01-12 05:42:11,597 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2023-01-12 05:42:11,623 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: set configuration 0: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-12 05:42:11,624 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_inprogress_0
scm1.org_1   | 2023-01-12 05:42:11,640 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm1.org_1   | 2023-01-12 05:42:11,641 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2023-01-12 05:42:11,733 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: start as a follower, conf=0: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-12 05:42:11,734 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm1.org_1   | 2023-01-12 05:42:11,735 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO impl.RoleInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: start 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState
scm1.org_1   | 2023-01-12 05:42:11,737 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-067D6DACB49F,id=0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1
scm1.org_1   | 2023-01-12 05:42:11,740 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2023-01-12 05:42:11,740 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2023-01-12 05:42:11,741 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2023-01-12 05:42:11,744 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2023-01-12 05:42:11,748 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1.org_1   | 2023-01-12 05:42:11,751 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1.org_1   | 2023-01-12 05:42:11,752 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: start RPC server
scm1.org_1   | 2023-01-12 05:42:11,766 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: GrpcService started, listening on 9894
scm1.org_1   | 2023-01-12 05:42:11,771 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: Started
scm1.org_1   | 2023-01-12 05:42:11,777 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-01-12 05:42:11,777 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1.org_1   | 2023-01-12 05:42:11,792 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Starting token manager
scm1.org_1   | 2023-01-12 05:42:11,793 [Listener at 0.0.0.0/9860] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
scm1.org_1   | 2023-01-12 05:42:11,888 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1.org_1   | 2023-01-12 05:42:11,970 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1.org_1   | 2023-01-12 05:42:11,970 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1.org_1   | 2023-01-12 05:42:12,337 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1.org_1   | 2023-01-12 05:42:12,339 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-01-12 05:42:12,421 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1.org_1   | 2023-01-12 05:42:12,456 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1.org_1   | 2023-01-12 05:42:12,472 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1.org_1   | 2023-01-12 05:42:12,473 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-01-12 05:42:12,473 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1.org_1   | 2023-01-12 05:42:12,512 [Listener at 0.0.0.0/9860] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm1.org_1   | 2023-01-12 05:42:12,513 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-01-12 05:42:12,514 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm1.org_1   | 2023-01-12 05:42:12,514 [Listener at 0.0.0.0/9860] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm1.org_1   | 2023-01-12 05:42:12,671 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3d1b6816] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm1.org_1   | 2023-01-12 05:42:12,764 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1.org_1   | 2023-01-12 05:42:12,764 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm1.org_1   | 2023-01-12 05:42:12,768 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm1.org_1   | 2023-01-12 05:42:12,855 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @8059ms to org.eclipse.jetty.util.log.Slf4jLog
scm1.org_1   | 2023-01-12 05:42:12,991 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:37686
scm1.org_1   | 2023-01-12 05:42:13,025 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
recon_1      | 2023-01-12 05:44:10,721 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 98 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:12,723 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 99 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:12,728 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 100 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:12,750 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 101 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:14,752 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 102 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:14,755 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 103 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:14,757 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 104 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:16,759 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 105 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:16,763 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 106 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:16,764 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 107 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:18,769 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 108 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:18,773 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 109 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:18,778 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 110 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:20,789 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 111 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:20,790 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 112 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:20,793 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 113 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:22,795 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 114 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:22,796 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 115 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:22,797 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 116 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:24,798 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 117 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:24,801 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 118 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:24,803 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 119 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:26,807 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 120 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:26,808 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 121 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:26,809 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 122 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:28,813 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 123 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:28,814 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 124 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:28,815 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 125 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:30,817 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 126 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:30,818 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 127 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:30,820 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 128 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:32,822 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 129 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:32,823 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 130 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:32,824 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 131 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:33,988 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40792
recon_1      | 2023-01-12 05:44:34,047 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-12 05:44:34,834 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 132 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:34,845 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 133 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:34,855 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 134 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:34,939 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:44410
recon_1      | 2023-01-12 05:44:35,136 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-12 05:44:35,760 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:37974
recon_1      | 2023-01-12 05:44:35,797 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-12 05:44:36,934 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 135 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:36,991 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 136 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:37,048 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 137 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:38,806 [IPC Server handler 29 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
recon_1      | 2023-01-12 05:44:38,851 [IPC Server handler 29 on default port 9891] INFO node.SCMNodeManager: Registered Data node : a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886], networkLocation: /default-rack, certSerialId: 1332434143678, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-12 05:44:38,964 [IPC Server handler 32 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/08ecee04-dee1-41b4-94f3-4660f61ac91b
recon_1      | 2023-01-12 05:44:38,986 [IPC Server handler 32 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 08ecee04-dee1-41b4-94f3-4660f61ac91b{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1332863307027, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-12 05:44:39,058 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 138 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:39,060 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 139 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:39,075 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 140 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:39,152 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3 to Node DB.
recon_1      | 2023-01-12 05:44:39,154 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 08ecee04-dee1-41b4-94f3-4660f61ac91b to Node DB.
recon_1      | 2023-01-12 05:44:39,347 [IPC Server handler 2 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net
recon_1      | 2023-01-12 05:44:39,766 [IPC Server handler 78 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/75176c82-cf64-45b0-aad5-7780ce7245eb
recon_1      | 2023-01-12 05:44:39,771 [IPC Server handler 78 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 75176c82-cf64-45b0-aad5-7780ce7245eb{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1331503348608, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2023-01-12 05:44:39,772 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 75176c82-cf64-45b0-aad5-7780ce7245eb to Node DB.
scm1.org_1   | 2023-01-12 05:42:13,193 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1.org_1   | 2023-01-12 05:42:13,203 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1.org_1   | 2023-01-12 05:42:13,206 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm1.org_1   | 2023-01-12 05:42:13,207 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm1.org_1   | 2023-01-12 05:42:13,207 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm1.org_1   | 2023-01-12 05:42:13,210 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm1.org_1   | 2023-01-12 05:42:13,287 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm1.org_1   | 2023-01-12 05:42:13,291 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm1.org_1   | 2023-01-12 05:42:13,347 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm1.org_1   | 2023-01-12 05:42:13,348 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm1.org_1   | 2023-01-12 05:42:13,349 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm1.org_1   | 2023-01-12 05:42:13,365 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2023-01-12 05:42:13,375 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@509d0d21{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1.org_1   | 2023-01-12 05:42:13,376 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@27f7db5a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1.org_1   | 2023-01-12 05:42:13,488 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2023-01-12 05:42:13,502 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@591b9836{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-18218441796877048246/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1.org_1   | 2023-01-12 05:42:13,511 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@2efda33d{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1.org_1   | 2023-01-12 05:42:13,511 [Listener at 0.0.0.0/9860] INFO server.Server: Started @8714ms
scm1.org_1   | 2023-01-12 05:42:13,513 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1.org_1   | 2023-01-12 05:42:13,513 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1.org_1   | 2023-01-12 05:42:13,514 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1.org_1   | 2023-01-12 05:42:14,292 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:44205
scm1.org_1   | 2023-01-12 05:42:14,314 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:42:14,500 [IPC Server handler 0 on default port 9961] INFO ipc.Server: IPC Server handler 0 on default port 9961, call Call#0 Retry#12 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from 172.25.0.115:44205
scm1.org_1   | org.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1 is not the leader. Could not determine the leader node.
scm1.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:93)
scm1.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:16080)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
scm1.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1.org_1   | 2023-01-12 05:42:14,836 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:45374
scm1.org_1   | 2023-01-12 05:42:14,861 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-12 05:42:16,822 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO impl.FollowerState: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5086637269ns, electionTimeout:5069ms
scm1.org_1   | 2023-01-12 05:42:16,823 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO impl.RoleInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: shutdown 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState
scm1.org_1   | 2023-01-12 05:42:16,824 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm1.org_1   | 2023-01-12 05:42:16,828 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1.org_1   | 2023-01-12 05:42:16,828 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-FollowerState] INFO impl.RoleInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: start 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1
scm1.org_1   | 2023-01-12 05:42:16,839 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO impl.LeaderElection: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-12 05:42:16,840 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO impl.LeaderElection: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm1.org_1   | 2023-01-12 05:42:16,841 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO impl.RoleInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: shutdown 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1
scm1.org_1   | 2023-01-12 05:42:16,842 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm1.org_1   | 2023-01-12 05:42:16,842 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm1.org_1   | 2023-01-12 05:42:16,842 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm1.org_1   | 2023-01-12 05:42:16,844 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: change Leader from null to 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1 at term 2 for becomeLeader, leader elected after 7909ms
scm1.org_1   | 2023-01-12 05:42:16,851 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2023-01-12 05:42:16,856 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-01-12 05:42:16,856 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2023-01-12 05:42:16,862 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2023-01-12 05:42:16,862 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2023-01-12 05:42:16,863 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2023-01-12 05:42:16,868 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2023-01-12 05:42:16,869 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2023-01-12 05:42:16,872 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO impl.RoleInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: start 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderStateImpl
scm1.org_1   | 2023-01-12 05:42:16,879 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm1.org_1   | 2023-01-12 05:42:16,884 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_inprogress_0 to /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_0-0
scm1.org_1   | 2023-01-12 05:42:16,887 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderElection1] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: set configuration 1: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-12 05:42:16,908 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_inprogress_1
scm1.org_1   | 2023-01-12 05:42:16,917 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm1.org_1   | 2023-01-12 05:42:16,919 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm1.org_1   | 2023-01-12 05:42:16,924 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:42:16,924 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1.org_1   | 2023-01-12 05:42:16,925 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2023-01-12 05:42:16,925 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm1.org_1   | 2023-01-12 05:42:16,928 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2023-01-12 05:42:16,934 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1.org_1   | 2023-01-12 05:42:19,372 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:47520
scm1.org_1   | 2023-01-12 05:42:19,380 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:42:19,456 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm2.org, nodeId: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0
scm1.org_1   | 2023-01-12 05:42:19,600 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-01-12 05:42:20,268 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:42:20,268 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1.org_1   | 2023-01-12 05:42:20,268 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1.org_1   | 2023-01-12 05:42:20,527 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for RECON recon, UUID: 6d8ff86d-7dd2-497f-8575-428924ca8fc0
scm1.org_1   | 2023-01-12 05:42:20,649 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:42:40,404 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:60314
scm2.org_1   | 2023-01-12 05:42:32,291 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2.org_1   | 2023-01-12 05:42:32,291 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2023-01-12 05:42:32,292 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2023-01-12 05:42:32,294 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2.org_1   | 2023-01-12 05:42:32,315 [pool-17-thread-1] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2.org_1   | 2023-01-12 05:42:32,320 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2023-01-12 05:42:32,332 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2.org_1   | 2023-01-12 05:42:32,332 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2.org_1   | 2023-01-12 05:42:32,349 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2.org_1   | 2023-01-12 05:42:32,364 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2.org_1   | 2023-01-12 05:42:32,384 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2.org_1   | 2023-01-12 05:42:32,551 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2.org_1   | 2023-01-12 05:42:32,562 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2.org_1   | 2023-01-12 05:42:32,566 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2.org_1   | 2023-01-12 05:42:32,567 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2.org_1   | 2023-01-12 05:42:32,567 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2.org_1   | 2023-01-12 05:42:32,570 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm2.org_1   | 2023-01-12 05:42:32,574 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2.org_1   | 2023-01-12 05:42:32,574 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2.org_1   | 2023-01-12 05:42:33,014 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm2.org_1   | 2023-01-12 05:42:33,532 [main] INFO reflections.Reflections: Reflections took 385 ms to scan 3 urls, producing 121 keys and 272 values 
scm2.org_1   | 2023-01-12 05:42:33,823 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm2.org_1   | 2023-01-12 05:42:33,825 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm2.org_1   | 2023-01-12 05:42:33,836 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm2.org_1   | 2023-01-12 05:42:33,844 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm2.org_1   | 2023-01-12 05:42:34,159 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2.org_1   | 2023-01-12 05:42:34,221 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm2.org_1   | 2023-01-12 05:42:34,232 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2.org_1   | 2023-01-12 05:42:34,297 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm2.org_1   | 2023-01-12 05:42:34,495 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2.org_1   | 2023-01-12 05:42:34,499 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2.org_1   | 2023-01-12 05:42:34,521 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2.org_1   | 2023-01-12 05:42:34,524 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2.org_1   | 2023-01-12 05:42:34,552 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm2.org_1   | 2023-01-12 05:42:34,592 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm2.org_1   | 2023-01-12 05:42:34,631 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm2.org_1   | 2023-01-12 05:42:34,662 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm2.org_1   | 2023-01-12 05:42:34,862 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2.org_1   | 2023-01-12 05:42:34,988 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2.org_1   | 2023-01-12 05:42:35,279 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm2.org_1   | 2023-01-12 05:42:35,331 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2.org_1   | 2023-01-12 05:42:35,338 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm2.org_1   | 2023-01-12 05:42:35,384 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2.org_1   | 2023-01-12 05:42:35,399 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:42:35,414 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2023-01-12 05:42:35,552 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm2.org_1   | 2023-01-12 05:42:35,666 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-01-12 05:42:35,856 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm2.org_1   | 2023-01-12 05:42:38,420 [Listener at 0.0.0.0/9961] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-01-12 05:42:38,469 [Listener at 0.0.0.0/9961] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-01-12 05:42:38,470 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2.org_1   | 2023-01-12 05:42:38,620 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-01-12 05:42:38,656 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-01-12 05:42:38,659 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2.org_1   | 2023-01-12 05:42:38,815 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2.org_1   | 2023-01-12 05:42:38,864 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2023-01-12 05:42:38,866 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2.org_1   | 2023-01-12 05:42:39,260 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2.org_1   | 2023-01-12 05:42:39,321 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm2.org_1   | Container Balancer status:
scm2.org_1   | Key                            Value
scm2.org_1   | Running                        true
scm2.org_1   | Container Balancer Configuration values:
scm2.org_1   | Key                                                Value
scm2.org_1   | Threshold                                          10
scm2.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2.org_1   | Max Size to Move per Iteration                     500GB
scm2.org_1   | Max Size Entering Target per Iteration             26GB
scm2.org_1   | Max Size Leaving Source per Iteration              26GB
scm2.org_1   | 
scm2.org_1   | 2023-01-12 05:42:39,330 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2.org_1   | 2023-01-12 05:42:39,607 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2.org_1   | 2023-01-12 05:42:39,633 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2.org_1   | 2023-01-12 05:42:39,641 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f does not exist. Creating ...
scm2.org_1   | 2023-01-12 05:42:39,674 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/in_use.lock acquired by nodename 7@scm2.org
scm2.org_1   | 2023-01-12 05:42:39,691 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f has been successfully formatted.
scm2.org_1   | 2023-01-12 05:42:39,699 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2.org_1   | 2023-01-12 05:42:39,719 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2.org_1   | 2023-01-12 05:42:39,719 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-01-12 05:42:39,724 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2.org_1   | 2023-01-12 05:42:39,731 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2.org_1   | 2023-01-12 05:42:39,747 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2023-01-12 05:42:39,759 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2.org_1   | 2023-01-12 05:42:39,783 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2.org_1   | 2023-01-12 05:42:39,811 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f
scm2.org_1   | 2023-01-12 05:42:39,811 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2.org_1   | 2023-01-12 05:42:39,818 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2.org_1   | 2023-01-12 05:42:39,820 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2023-01-12 05:42:39,820 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2.org_1   | 2023-01-12 05:42:39,821 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2.org_1   | 2023-01-12 05:42:39,827 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2.org_1   | 2023-01-12 05:42:39,828 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2.org_1   | 2023-01-12 05:42:39,828 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2.org_1   | 2023-01-12 05:42:39,875 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2.org_1   | 2023-01-12 05:42:39,878 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2023-01-12 05:42:39,918 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2.org_1   | 2023-01-12 05:42:39,918 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2.org_1   | 2023-01-12 05:42:39,920 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2.org_1   | 2023-01-12 05:42:39,944 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2023-01-12 05:42:39,944 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2023-01-12 05:42:39,956 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm2.org_1   | 2023-01-12 05:42:39,961 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: changes role from      null to FOLLOWER at term 0 for startInitializing
scm2.org_1   | 2023-01-12 05:42:39,970 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-067D6DACB49F,id=52d7416a-527f-45fb-b83c-77d6ce7a6ad0
scm2.org_1   | 2023-01-12 05:42:39,975 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3.org_1   | 2023-01-12 05:43:05,183 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm3.org_1   | 2023-01-12 05:43:05,187 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm3.org_1   | 2023-01-12 05:43:05,341 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3.org_1   | 2023-01-12 05:43:05,472 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3.org_1   | 2023-01-12 05:43:05,626 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm3.org_1   | 2023-01-12 05:43:05,675 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3.org_1   | 2023-01-12 05:43:05,677 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm3.org_1   | 2023-01-12 05:43:05,707 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3.org_1   | 2023-01-12 05:43:05,741 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:43:05,750 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2023-01-12 05:43:05,884 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm3.org_1   | 2023-01-12 05:43:06,001 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-01-12 05:43:06,176 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm3.org_1   | 2023-01-12 05:43:08,381 [Listener at 0.0.0.0/9961] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-01-12 05:43:08,409 [Listener at 0.0.0.0/9961] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-01-12 05:43:08,415 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3.org_1   | 2023-01-12 05:43:08,521 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-01-12 05:43:08,545 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-01-12 05:43:08,556 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3.org_1   | 2023-01-12 05:43:08,679 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3.org_1   | 2023-01-12 05:43:08,711 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2023-01-12 05:43:08,714 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3.org_1   | 2023-01-12 05:43:08,959 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm3.org_1   | 2023-01-12 05:43:08,964 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm3.org_1   | Container Balancer status:
scm3.org_1   | Key                            Value
scm3.org_1   | Running                        true
scm3.org_1   | Container Balancer Configuration values:
scm3.org_1   | Key                                                Value
scm3.org_1   | Threshold                                          10
scm3.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm3.org_1   | Max Size to Move per Iteration                     500GB
scm3.org_1   | Max Size Entering Target per Iteration             26GB
scm3.org_1   | Max Size Leaving Source per Iteration              26GB
scm3.org_1   | 
scm3.org_1   | 2023-01-12 05:43:08,964 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3.org_1   | 2023-01-12 05:43:09,005 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3.org_1   | 2023-01-12 05:43:09,032 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3.org_1   | 2023-01-12 05:43:09,039 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f does not exist. Creating ...
scm3.org_1   | 2023-01-12 05:43:09,047 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/in_use.lock acquired by nodename 7@scm3.org
scm3.org_1   | 2023-01-12 05:43:09,080 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f has been successfully formatted.
scm3.org_1   | 2023-01-12 05:43:09,103 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3.org_1   | 2023-01-12 05:43:09,163 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3.org_1   | 2023-01-12 05:43:09,163 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-01-12 05:43:09,169 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3.org_1   | 2023-01-12 05:43:09,176 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3.org_1   | 2023-01-12 05:43:09,186 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2023-01-12 05:43:09,209 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3.org_1   | 2023-01-12 05:43:09,210 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3.org_1   | 2023-01-12 05:43:09,235 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f
scm3.org_1   | 2023-01-12 05:43:09,241 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3.org_1   | 2023-01-12 05:43:09,244 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3.org_1   | 2023-01-12 05:43:09,251 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2023-01-12 05:43:09,253 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3.org_1   | 2023-01-12 05:43:09,255 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3.org_1   | 2023-01-12 05:43:09,258 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3.org_1   | 2023-01-12 05:43:09,258 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3.org_1   | 2023-01-12 05:43:09,262 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3.org_1   | 2023-01-12 05:43:09,339 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3.org_1   | 2023-01-12 05:43:09,340 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2023-01-12 05:43:09,382 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3.org_1   | 2023-01-12 05:43:09,382 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3.org_1   | 2023-01-12 05:43:09,385 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3.org_1   | 2023-01-12 05:43:09,412 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2023-01-12 05:43:09,412 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2023-01-12 05:43:09,422 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm3.org_1   | 2023-01-12 05:43:09,423 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: changes role from      null to FOLLOWER at term 0 for startInitializing
recon_1      | 2023-01-12 05:44:40,399 [IPC Server handler 3 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net
recon_1      | 2023-01-12 05:44:41,097 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 141 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:41,098 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 142 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:41,099 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 143 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:41,212 [IPC Server handler 3 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net
recon_1      | 2023-01-12 05:44:41,580 [IPC Server handler 4 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net
recon_1      | 2023-01-12 05:44:43,101 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 144 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:43,106 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 145 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:43,107 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 146 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:45,109 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 147 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:45,110 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 148 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:45,114 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 149 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:47,115 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 150 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:47,117 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 151 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:47,118 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 152 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:49,120 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 153 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:49,121 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 154 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:49,123 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 155 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:51,131 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 156 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:51,131 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 157 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:51,133 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 158 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:53,135 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 159 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:53,139 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 160 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:53,140 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 161 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:55,143 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 162 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:55,144 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 163 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:55,153 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 164 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:57,165 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 165 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:57,166 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 166 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:57,168 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 167 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:44:59,170 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 168 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:59,171 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 169 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:44:59,172 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 170 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:45:01,173 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 171 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:45:01,176 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 172 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:45:01,179 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 173 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:45:03,181 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 174 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:45:03,181 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 175 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:45:03,185 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 176 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:45:10,390 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:39420
recon_1      | 2023-01-12 05:45:10,523 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-12 05:45:10,524 [IPC Server handler 4 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net
recon_1      | 2023-01-12 05:45:11,289 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:33476
recon_1      | 2023-01-12 05:45:11,404 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-12 05:45:11,635 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:32978
recon_1      | 2023-01-12 05:45:11,800 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-12 05:45:11,801 [IPC Server handler 95 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net
recon_1      | 2023-01-12 05:45:12,860 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877. Trying to get from SCM.
recon_1      | 2023-01-12 05:45:13,899 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 9debe22a-e51c-422a-aaf2-acb2c75a2877, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:40.913Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-01-12 05:45:14,161 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9debe22a-e51c-422a-aaf2-acb2c75a2877, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:40.913Z[UTC]].
recon_1      | 2023-01-12 05:45:14,208 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 reported by 08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-01-12 05:45:14,770 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=86cb76d7-d1bd-4dd6-8116-a6bdfd03825d. Trying to get from SCM.
recon_1      | 2023-01-12 05:45:14,803 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 86cb76d7-d1bd-4dd6-8116-a6bdfd03825d, Nodes: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:39.825Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-01-12 05:45:14,804 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 86cb76d7-d1bd-4dd6-8116-a6bdfd03825d, Nodes: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:39.825Z[UTC]].
recon_1      | 2023-01-12 05:45:14,804 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=86cb76d7-d1bd-4dd6-8116-a6bdfd03825d reported by a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-01-12 05:45:14,805 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 86cb76d7-d1bd-4dd6-8116-a6bdfd03825d, Nodes: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3, CreationTimestamp2023-01-12T05:44:39.825Z[UTC]] moved to OPEN state
recon_1      | 2023-01-12 05:45:14,978 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 reported by 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-01-12 05:45:15,986 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 reported by a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-01-12 05:45:21,077 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 reported by a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
scm1.org_1   | 2023-01-12 05:42:40,532 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-12 05:42:41,704 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:56508
scm1.org_1   | 2023-01-12 05:42:41,948 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-12 05:42:41,957 [IPC Server handler 0 on default port 9863] INFO ha.SCMRatisServerImpl: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: Submitting SetConfiguration request to Ratis server with new SCM peers list: [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-01-12 05:42:42,003 [IPC Server handler 0 on default port 9863] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: receive setConfiguration SetConfigurationRequest:client-9226EA6D9F22->0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F, cid=1, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-01-12 05:42:42,016 [IPC Server handler 0 on default port 9863] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-9226EA6D9F22->0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F, cid=1, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-01-12 05:42:42,146 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2023-01-12 05:42:42,146 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-12 05:42:42,147 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2023-01-12 05:42:42,163 [IPC Server handler 0 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2023-01-12 05:42:42,163 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-01-12 05:42:42,164 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-01-12 05:42:42,164 [IPC Server handler 0 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-01-12 05:42:42,164 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1.org_1   | 2023-01-12 05:42:42,184 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1.org_1   | 2023-01-12 05:42:42,217 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0-GrpcLogAppender: send 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1->52d7416a-527f-45fb-b83c-77d6ce7a6ad0#0-t2,notify:(t:1, i:0)
scm1.org_1   | 2023-01-12 05:42:42,233 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 52d7416a-527f-45fb-b83c-77d6ce7a6ad0
scm1.org_1   | 2023-01-12 05:42:44,093 [grpc-default-executor-0] INFO server.GrpcLogAppender: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0-InstallSnapshotResponseHandler: received the first reply 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1<-52d7416a-527f-45fb-b83c-77d6ce7a6ad0#0:OK-t0,ALREADY_INSTALLED
scm1.org_1   | 2023-01-12 05:42:44,118 [grpc-default-executor-0] INFO server.GrpcLogAppender: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1.org_1   | 2023-01-12 05:42:44,127 [grpc-default-executor-0] INFO leader.FollowerInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0: snapshotIndex: setUnconditionally 0 -> 0
scm1.org_1   | 2023-01-12 05:42:44,132 [grpc-default-executor-0] INFO leader.FollowerInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0: matchIndex: setUnconditionally 0 -> 0
scm1.org_1   | 2023-01-12 05:42:44,132 [grpc-default-executor-0] INFO leader.FollowerInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0: nextIndex: setUnconditionally 0 -> 1
scm1.org_1   | 2023-01-12 05:42:44,132 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0 acknowledged installing snapshot
scm1.org_1   | 2023-01-12 05:42:44,133 [grpc-default-executor-0] INFO leader.FollowerInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0: nextIndex: updateToMax old=1, new=1, updated? false
scm1.org_1   | 2023-01-12 05:42:44,662 [grpc-default-executor-0] INFO leader.FollowerInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0: nextIndex: updateUnconditionally 7 -> 0
scm1.org_1   | 2023-01-12 05:42:44,667 [grpc-default-executor-2] INFO leader.FollowerInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->52d7416a-527f-45fb-b83c-77d6ce7a6ad0: nextIndex: updateUnconditionally 7 -> 0
scm1.org_1   | 2023-01-12 05:42:45,815 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderStateImpl] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: set configuration 7: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-01-12 05:42:45,889 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderStateImpl] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: set configuration 9: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-12 05:42:45,971 [IPC Server handler 0 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0.
scm1.org_1   | 2023-01-12 05:42:46,395 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:39779
scm1.org_1   | 2023-01-12 05:42:46,460 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-12 05:42:49,586 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:56506
scm1.org_1   | 2023-01-12 05:42:49,639 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-12 05:42:49,682 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:58804
scm1.org_1   | 2023-01-12 05:42:49,695 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:42:53,210 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:57612
scm1.org_1   | 2023-01-12 05:42:53,233 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:42:53,240 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm3.org, nodeId: df8756e7-ed3f-4e48-9521-edf82d63369c
scm1.org_1   | 2023-01-12 05:42:53,301 [IPC Server handler 1 on default port 9961] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
scm1.org_1   | 2023-01-12 05:42:53,451 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:42:59,279 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:59728
scm1.org_1   | 2023-01-12 05:42:59,317 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-12 05:43:13,845 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:34894
scm1.org_1   | 2023-01-12 05:43:14,098 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-12 05:43:14,118 [IPC Server handler 27 on default port 9863] INFO ha.SCMRatisServerImpl: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: Submitting SetConfiguration request to Ratis server with new SCM peers list: [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, df8756e7-ed3f-4e48-9521-edf82d63369c|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]
scm1.org_1   | 2023-01-12 05:43:14,130 [IPC Server handler 27 on default port 9863] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: receive setConfiguration SetConfigurationRequest:client-9226EA6D9F22->0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F, cid=2, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, df8756e7-ed3f-4e48-9521-edf82d63369c|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-01-12 05:43:14,131 [IPC Server handler 27 on default port 9863] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-9226EA6D9F22->0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F, cid=2, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, df8756e7-ed3f-4e48-9521-edf82d63369c|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1.org_1   | 2023-01-12 05:43:14,133 [IPC Server handler 27 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2023-01-12 05:43:14,133 [IPC Server handler 27 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2023-01-12 05:43:14,135 [IPC Server handler 27 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2023-01-12 05:43:14,139 [IPC Server handler 27 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2023-01-12 05:43:14,141 [IPC Server handler 27 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2023-01-12 05:43:14,141 [IPC Server handler 27 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2023-01-12 05:43:14,142 [IPC Server handler 27 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1.org_1   | 2023-01-12 05:43:14,142 [IPC Server handler 27 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1.org_1   | 2023-01-12 05:43:14,145 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1.org_1   | 2023-01-12 05:43:14,207 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c-GrpcLogAppender: send 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1->df8756e7-ed3f-4e48-9521-edf82d63369c#0-t2,notify:(t:1, i:0)
scm1.org_1   | 2023-01-12 05:43:14,209 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for df8756e7-ed3f-4e48-9521-edf82d63369c
scm1.org_1   | 2023-01-12 05:43:21,879 [grpc-default-executor-0] INFO server.GrpcLogAppender: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c-InstallSnapshotResponseHandler: received the first reply 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1<-df8756e7-ed3f-4e48-9521-edf82d63369c#0:OK-t0,ALREADY_INSTALLED
scm1.org_1   | 2023-01-12 05:43:21,895 [grpc-default-executor-0] INFO server.GrpcLogAppender: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm3.org_1   | 2023-01-12 05:43:09,433 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-067D6DACB49F,id=df8756e7-ed3f-4e48-9521-edf82d63369c
scm3.org_1   | 2023-01-12 05:43:09,443 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3.org_1   | 2023-01-12 05:43:09,443 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3.org_1   | 2023-01-12 05:43:09,446 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3.org_1   | 2023-01-12 05:43:09,449 [df8756e7-ed3f-4e48-9521-edf82d63369c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3.org_1   | 2023-01-12 05:43:09,470 [Listener at 0.0.0.0/9860] INFO server.RaftServer: df8756e7-ed3f-4e48-9521-edf82d63369c: start RPC server
scm3.org_1   | 2023-01-12 05:43:09,513 [Listener at 0.0.0.0/9860] INFO server.GrpcService: df8756e7-ed3f-4e48-9521-edf82d63369c: GrpcService started, listening on 9894
scm3.org_1   | 2023-01-12 05:43:09,544 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-df8756e7-ed3f-4e48-9521-edf82d63369c: Started
scm3.org_1   | 2023-01-12 05:43:09,681 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863, nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863]
scm3.org_1   | 2023-01-12 05:43:19,248 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: receive installSnapshot: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1->df8756e7-ed3f-4e48-9521-edf82d63369c#0-t2,notify:(t:1, i:0)
scm3.org_1   | 2023-01-12 05:43:19,313 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3.org_1   | 2023-01-12 05:43:19,314 [grpc-default-executor-0] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: change Leader from null to 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1 at term 2 for installSnapshot, leader elected after 16013ms
scm3.org_1   | 2023-01-12 05:43:19,375 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: Received notification to install snapshot at index 0
scm3.org_1   | 2023-01-12 05:43:19,409 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm3.org_1   | 2023-01-12 05:43:21,489 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |     startupRole: FOLLOWER
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "52d7416a-527f-45fb-b83c-77d6ce7a6ad0"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |     startupRole: FOLLOWER
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2023-01-12 05:43:21,526 [grpc-default-executor-0] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: set configuration 9: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-12 05:43:21,633 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: reply installSnapshot: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1<-df8756e7-ed3f-4e48-9521-edf82d63369c#0:OK-t0,ALREADY_INSTALLED
scm3.org_1   | 2023-01-12 05:43:21,896 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: df8756e7-ed3f-4e48-9521-edf82d63369c: Completed INSTALL_SNAPSHOT, lastRequest: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1->df8756e7-ed3f-4e48-9521-edf82d63369c#0-t2,notify:(t:1, i:0)
scm3.org_1   | 2023-01-12 05:43:22,805 [df8756e7-ed3f-4e48-9521-edf82d63369c-server-thread1] INFO impl.RoleInfo: df8756e7-ed3f-4e48-9521-edf82d63369c: start df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-FollowerState
scm3.org_1   | 2023-01-12 05:43:22,814 [df8756e7-ed3f-4e48-9521-edf82d63369c-server-thread1] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3.org_1   | 2023-01-12 05:43:22,835 [df8756e7-ed3f-4e48-9521-edf82d63369c-server-thread1] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: inconsistency entries. Reply:0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1<-df8756e7-ed3f-4e48-9521-edf82d63369c#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3.org_1   | 2023-01-12 05:43:22,928 [df8756e7-ed3f-4e48-9521-edf82d63369c-server-thread1] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: set configuration 0: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-12 05:43:22,939 [df8756e7-ed3f-4e48-9521-edf82d63369c-server-thread1] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: set configuration 1: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-12 05:43:22,980 [df8756e7-ed3f-4e48-9521-edf82d63369c-server-thread1] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: set configuration 7: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-01-12 05:43:23,009 [df8756e7-ed3f-4e48-9521-edf82d63369c-server-thread1] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: set configuration 9: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-12 05:43:23,119 [df8756e7-ed3f-4e48-9521-edf82d63369c-server-thread1] INFO segmented.SegmentedRaftLogWorker: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-SegmentedRaftLogWorker: Starting segment from index:0
scm3.org_1   | 2023-01-12 05:43:23,911 [df8756e7-ed3f-4e48-9521-edf82d63369c-server-thread1] INFO segmented.SegmentedRaftLogWorker: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm3.org_1   | 2023-01-12 05:43:24,608 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_inprogress_0
scm3.org_1   | 2023-01-12 05:43:24,661 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_inprogress_0 to /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_0-0
scm3.org_1   | 2023-01-12 05:43:24,783 [df8756e7-ed3f-4e48-9521-edf82d63369c-server-thread3] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: set configuration 13: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, df8756e7-ed3f-4e48-9521-edf82d63369c|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3.org_1   | 2023-01-12 05:43:24,790 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_inprogress_1
scm3.org_1   | 2023-01-12 05:43:24,871 [df8756e7-ed3f-4e48-9521-edf82d63369c-server-thread3] INFO server.RaftServer$Division: df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F: set configuration 15: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, df8756e7-ed3f-4e48-9521-edf82d63369c|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3.org_1   | 2023-01-12 05:43:24,912 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:43:24,921 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3.org_1   | 2023-01-12 05:43:24,927 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2023-01-12 05:43:24,930 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3.org_1   | 2023-01-12 05:43:24,940 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-01-12 05:43:25,042 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3.org_1   | 2023-01-12 05:43:25,390 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-067D6DACB49F:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, df8756e7-ed3f-4e48-9521-edf82d63369c|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3.org_1   | 2023-01-12 05:43:25,392 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3.org_1   | 2023-01-12 05:43:25,566 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Starting token manager
scm3.org_1   | 2023-01-12 05:43:25,566 [Listener at 0.0.0.0/9860] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
scm3.org_1   | 2023-01-12 05:43:26,778 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3.org_1   | 2023-01-12 05:43:27,177 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm3.org_1   | 2023-01-12 05:43:27,178 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3.org_1   | 2023-01-12 05:43:27,697 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:43:27,698 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3.org_1   | 2023-01-12 05:43:27,698 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3.org_1   | 2023-01-12 05:43:28,178 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:43:28,444 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:43:31,154 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3.org_1   | 2023-01-12 05:43:31,183 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-01-12 05:43:31,188 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm3.org_1   | 2023-01-12 05:43:31,367 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-df8756e7-ed3f-4e48-9521-edf82d63369c: Detected pause in JVM or host machine (eg GC): pause of approximately 140174263ns.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=106ms
scm3.org_1   | 2023-01-12 05:43:31,946 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3.org_1   | 2023-01-12 05:43:31,956 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm3.org_1   | 2023-01-12 05:43:31,988 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-01-12 05:43:31,988 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm3.org_1   | 2023-01-12 05:43:32,241 [Listener at 0.0.0.0/9860] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm3.org_1   | 2023-01-12 05:43:32,276 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2023-01-12 05:43:32,277 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm3.org_1   | 2023-01-12 05:43:32,281 [Listener at 0.0.0.0/9860] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm3.org_1   | 2023-01-12 05:43:33,024 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.client.port appended with serviceId and nodeId
scm3.org_1   | 2023-01-12 05:43:33,028 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.block.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.block.client.port appended with serviceId and nodeId
scm3.org_1   | 2023-01-12 05:43:33,031 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.datanode.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.datanode.port appended with serviceId and nodeId
recon_1      | 2023-01-12 05:45:26,742 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om1 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 177 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:45:27,829 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 178 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:45:28,333 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:32876
recon_1      | 2023-01-12 05:45:28,530 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-12 05:45:28,532 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 reported by 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-01-12 05:45:28,533 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=48a05780-a7a4-4d83-bfcd-444d5ac91262. Trying to get from SCM.
recon_1      | 2023-01-12 05:45:28,683 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:41034
recon_1      | 2023-01-12 05:45:28,901 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2023-01-12 05:45:29,032 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 48a05780-a7a4-4d83-bfcd-444d5ac91262, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:75176c82-cf64-45b0-aad5-7780ce7245eb, CreationTimestamp2023-01-12T05:44:41.114Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-01-12 05:45:29,033 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 48a05780-a7a4-4d83-bfcd-444d5ac91262, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:75176c82-cf64-45b0-aad5-7780ce7245eb, CreationTimestamp2023-01-12T05:44:41.114Z[UTC]].
recon_1      | 2023-01-12 05:45:29,034 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=48a05780-a7a4-4d83-bfcd-444d5ac91262 reported by 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-01-12 05:45:29,034 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 48a05780-a7a4-4d83-bfcd-444d5ac91262, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:75176c82-cf64-45b0-aad5-7780ce7245eb, CreationTimestamp2023-01-12T05:44:41.114Z[UTC]] moved to OPEN state
recon_1      | 2023-01-12 05:45:29,034 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44. Trying to get from SCM.
recon_1      | 2023-01-12 05:45:29,144 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: e22252e4-1bd7-4367-9a96-52a62dfb0a44, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:41.274Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-01-12 05:45:29,145 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e22252e4-1bd7-4367-9a96-52a62dfb0a44, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:41.274Z[UTC]].
recon_1      | 2023-01-12 05:45:29,145 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44 reported by 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-01-12 05:45:29,146 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 reported by 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-01-12 05:45:29,146 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44 reported by 08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-01-12 05:45:29,146 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 reported by 08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-01-12 05:45:29,151 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om3 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 179 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:45:29,621 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44 reported by a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-01-12 05:45:29,621 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 reported by a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)
recon_1      | 2023-01-12 05:45:30,217 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44 reported by 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-01-12 05:45:30,218 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 reported by 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
recon_1      | 2023-01-12 05:45:30,218 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9debe22a-e51c-422a-aaf2-acb2c75a2877, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:75176c82-cf64-45b0-aad5-7780ce7245eb, CreationTimestamp2023-01-12T05:44:40.913Z[UTC]] moved to OPEN state
recon_1      | 2023-01-12 05:45:31,131 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44 reported by 08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)
recon_1      | 2023-01-12 05:45:31,131 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=003546ba-ec7b-4d1e-b502-60ceae25f148. Trying to get from SCM.
recon_1      | 2023-01-12 05:45:31,162 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om1 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
scm1.org_1   | 2023-01-12 05:43:21,896 [grpc-default-executor-0] INFO leader.FollowerInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c: snapshotIndex: setUnconditionally 0 -> 0
scm1.org_1   | 2023-01-12 05:43:21,897 [grpc-default-executor-0] INFO leader.FollowerInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c: matchIndex: setUnconditionally 0 -> 0
scm1.org_1   | 2023-01-12 05:43:21,899 [grpc-default-executor-0] INFO leader.FollowerInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c: nextIndex: setUnconditionally 0 -> 1
scm1.org_1   | 2023-01-12 05:43:21,899 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c acknowledged installing snapshot
scm1.org_1   | 2023-01-12 05:43:21,901 [grpc-default-executor-0] INFO leader.FollowerInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c: nextIndex: updateToMax old=1, new=1, updated? false
scm1.org_1   | 2023-01-12 05:43:22,246 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: Detected pause in JVM or host machine (eg GC): pause of approximately 139255950ns.
scm1.org_1   | GC pool 'ParNew' had collection(s): count=1 time=235ms
scm1.org_1   | 2023-01-12 05:43:22,883 [grpc-default-executor-0] INFO leader.FollowerInfo: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F->df8756e7-ed3f-4e48-9521-edf82d63369c: nextIndex: updateUnconditionally 13 -> 0
scm1.org_1   | 2023-01-12 05:43:24,681 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderStateImpl] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: set configuration 13: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, df8756e7-ed3f-4e48-9521-edf82d63369c|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1.org_1   | 2023-01-12 05:43:24,774 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-LeaderStateImpl] INFO server.RaftServer$Division: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F: set configuration 15: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|priority:0|startupRole:FOLLOWER, df8756e7-ed3f-4e48-9521-edf82d63369c|rpc:scm3.org:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1.org_1   | 2023-01-12 05:43:24,893 [IPC Server handler 27 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: df8756e7-ed3f-4e48-9521-edf82d63369c.
scm1.org_1   | 2023-01-12 05:43:34,316 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:33138
scm1.org_1   | 2023-01-12 05:43:34,399 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:43:40,646 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:59154
scm1.org_1   | 2023-01-12 05:43:40,981 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-12 05:43:45,942 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:54256
scm1.org_1   | 2023-01-12 05:43:46,213 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-12 05:43:46,301 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:41016
scm1.org_1   | 2023-01-12 05:43:46,468 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-12 05:43:47,633 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:41387
scm1.org_1   | 2023-01-12 05:43:47,687 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-12 05:43:49,102 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:47666
scm1.org_1   | 2023-01-12 05:43:49,232 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-12 05:43:54,586 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:45886
scm1.org_1   | 2023-01-12 05:43:54,779 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:43:54,790 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 789a46e3b060, UUID: 75176c82-cf64-45b0-aad5-7780ce7245eb
scm1.org_1   | 2023-01-12 05:43:54,906 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:55646
scm1.org_1   | 2023-01-12 05:43:55,040 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:43:55,067 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn fb1a7b8e0c66, UUID: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
scm1.org_1   | 2023-01-12 05:43:55,666 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:36872
scm1.org_1   | 2023-01-12 05:43:55,710 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:43:55,874 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:43:55,880 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn e522f50beb6a, UUID: 08ecee04-dee1-41b4-94f3-4660f61ac91b
scm1.org_1   | 2023-01-12 05:43:56,190 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:43:56,796 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:42:39,975 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm2.org_1   | 2023-01-12 05:42:39,976 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2.org_1   | 2023-01-12 05:42:39,977 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2.org_1   | 2023-01-12 05:42:39,997 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0: start RPC server
scm2.org_1   | 2023-01-12 05:42:40,033 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0: GrpcService started, listening on 9894
scm2.org_1   | 2023-01-12 05:42:40,051 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-52d7416a-527f-45fb-b83c-77d6ce7a6ad0: Started
scm2.org_1   | 2023-01-12 05:42:40,117 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863, nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863]
scm2.org_1   | 2023-01-12 05:42:43,449 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: receive installSnapshot: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1->52d7416a-527f-45fb-b83c-77d6ce7a6ad0#0-t2,notify:(t:1, i:0)
scm2.org_1   | 2023-01-12 05:42:43,470 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm2.org_1   | 2023-01-12 05:42:43,472 [grpc-default-executor-0] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: change Leader from null to 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1 at term 2 for installSnapshot, leader elected after 11120ms
scm2.org_1   | 2023-01-12 05:42:43,488 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: Received notification to install snapshot at index 0
scm2.org_1   | 2023-01-12 05:42:43,490 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm2.org_1   | 2023-01-12 05:42:43,937 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |     startupRole: FOLLOWER
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2023-01-12 05:42:43,956 [grpc-default-executor-0] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: set configuration 1: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-01-12 05:42:43,979 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: reply installSnapshot: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1<-52d7416a-527f-45fb-b83c-77d6ce7a6ad0#0:OK-t0,ALREADY_INSTALLED
scm2.org_1   | 2023-01-12 05:42:44,067 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0: Completed INSTALL_SNAPSHOT, lastRequest: 0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1->52d7416a-527f-45fb-b83c-77d6ce7a6ad0#0-t2,notify:(t:1, i:0)
scm2.org_1   | 2023-01-12 05:42:44,504 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread1] INFO impl.RoleInfo: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0: start 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-FollowerState
scm2.org_1   | 2023-01-12 05:42:44,527 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread1] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2.org_1   | 2023-01-12 05:42:44,553 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread1] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: inconsistency entries. Reply:0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1<-52d7416a-527f-45fb-b83c-77d6ce7a6ad0#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm2.org_1   | 2023-01-12 05:42:44,643 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread1] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2.org_1   | 2023-01-12 05:42:44,643 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread1] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: inconsistency entries. Reply:0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1<-52d7416a-527f-45fb-b83c-77d6ce7a6ad0#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm2.org_1   | 2023-01-12 05:42:44,691 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread1] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: set configuration 0: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-01-12 05:42:44,702 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread1] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: set configuration 1: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-01-12 05:42:44,788 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread1] INFO segmented.SegmentedRaftLogWorker: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-SegmentedRaftLogWorker: Starting segment from index:0
scm2.org_1   | 2023-01-12 05:42:45,095 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread1] INFO segmented.SegmentedRaftLogWorker: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm2.org_1   | 2023-01-12 05:42:45,265 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread2] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: set configuration 0: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-01-12 05:42:45,265 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread2] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: set configuration 1: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-01-12 05:42:45,588 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_inprogress_0
scm2.org_1   | 2023-01-12 05:42:45,640 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_inprogress_0 to /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_0-0
scm2.org_1   | 2023-01-12 05:42:45,754 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/dd9a252d-f4dd-4162-963a-067d6dacb49f/current/log_inprogress_1
scm2.org_1   | 2023-01-12 05:42:45,842 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread1] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: set configuration 7: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2.org_1   | 2023-01-12 05:42:45,902 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:42:45,911 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread1] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: set configuration 9: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-01-12 05:42:45,913 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2.org_1   | 2023-01-12 05:42:45,949 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2023-01-12 05:42:45,951 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2.org_1   | 2023-01-12 05:42:45,969 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-01-12 05:42:45,982 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm2.org_1   | 2023-01-12 05:42:46,487 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-067D6DACB49F:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2.org_1   | 2023-01-12 05:42:46,495 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm2.org_1   | 2023-01-12 05:42:46,549 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Starting token manager
scm2.org_1   | 2023-01-12 05:42:46,550 [Listener at 0.0.0.0/9860] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
scm2.org_1   | 2023-01-12 05:42:46,729 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:42:46,733 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2.org_1   | 2023-01-12 05:42:46,737 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2.org_1   | 2023-01-12 05:42:46,816 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:42:47,124 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2.org_1   | 2023-01-12 05:42:47,199 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2.org_1   | 2023-01-12 05:42:47,226 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2.org_1   | 2023-01-12 05:42:48,382 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2.org_1   | 2023-01-12 05:42:48,389 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-01-12 05:42:48,395 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2.org_1   | 2023-01-12 05:42:48,793 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2.org_1   | 2023-01-12 05:42:48,794 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2.org_1   | 2023-01-12 05:42:48,794 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-01-12 05:42:48,795 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2.org_1   | 2023-01-12 05:42:49,112 [Listener at 0.0.0.0/9860] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm2.org_1   | 2023-01-12 05:42:49,121 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2023-01-12 05:42:49,124 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm2.org_1   | 2023-01-12 05:42:49,133 [Listener at 0.0.0.0/9860] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm2.org_1   | 2023-01-12 05:42:49,268 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.client.port appended with serviceId and nodeId
scm2.org_1   | 2023-01-12 05:42:49,270 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.block.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.block.client.port appended with serviceId and nodeId
scm2.org_1   | 2023-01-12 05:42:49,270 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.datanode.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.datanode.port appended with serviceId and nodeId
scm2.org_1   | 2023-01-12 05:42:49,962 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node 52d7416a-527f-45fb-b83c-77d6ce7a6ad0
scm2.org_1   | 2023-01-12 05:42:49,977 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Persist certificate serialId 1212956981891 on Scm Bootstrap Node 52d7416a-527f-45fb-b83c-77d6ce7a6ad0
scm2.org_1   | 2023-01-12 05:42:50,043 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@28debadc] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm2.org_1   | 2023-01-12 05:42:50,111 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2.org_1   | 2023-01-12 05:42:50,111 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm2.org_1   | 2023-01-12 05:42:50,119 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm2.org_1   | 2023-01-12 05:42:50,249 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @29486ms to org.eclipse.jetty.util.log.Slf4jLog
scm2.org_1   | 2023-01-12 05:42:50,708 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2.org_1   | 2023-01-12 05:42:50,729 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2.org_1   | 2023-01-12 05:42:50,734 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm2.org_1   | 2023-01-12 05:42:50,734 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm2.org_1   | 2023-01-12 05:42:50,734 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm2.org_1   | 2023-01-12 05:42:50,740 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm2.org_1   | 2023-01-12 05:42:50,886 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm2.org_1   | 2023-01-12 05:42:50,890 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm2.org_1   | 2023-01-12 05:42:51,036 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm2.org_1   | 2023-01-12 05:42:51,040 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm2.org_1   | 2023-01-12 05:42:51,043 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm2.org_1   | 2023-01-12 05:42:51,139 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-01-12 05:42:51,161 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5bdde886{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2.org_1   | 2023-01-12 05:42:51,166 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6804a46a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm2.org_1   | 2023-01-12 05:42:51,676 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2023-01-12 05:42:51,729 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@156d47c5{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-849066512229155139/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm2.org_1   | 2023-01-12 05:42:51,760 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@1a57bb34{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm2.org_1   | 2023-01-12 05:42:51,761 [Listener at 0.0.0.0/9860] INFO server.Server: Started @30998ms
scm2.org_1   | 2023-01-12 05:42:51,779 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2.org_1   | 2023-01-12 05:42:51,779 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2.org_1   | 2023-01-12 05:42:51,781 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2.org_1   | 2023-01-12 05:42:53,457 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:43:24,690 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread2] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: set configuration 13: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, df8756e7-ed3f-4e48-9521-edf82d63369c|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2.org_1   | 2023-01-12 05:43:24,827 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0-server-thread2] INFO server.RaftServer$Division: 52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F: set configuration 15: peers:[0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 52d7416a-527f-45fb-b83c-77d6ce7a6ad0|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, df8756e7-ed3f-4e48-9521-edf82d63369c|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2.org_1   | 2023-01-12 05:43:55,703 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:43:56,272 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:43:56,819 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:44:02,396 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:44:03,865 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:44:04,812 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:44:34,194 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:56854
scm2.org_1   | 2023-01-12 05:44:34,226 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-12 05:44:34,787 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:44046
scm2.org_1   | 2023-01-12 05:44:35,138 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-12 05:44:35,541 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:41138
scm2.org_1   | 2023-01-12 05:44:35,567 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-12 05:44:39,162 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/08ecee04-dee1-41b4-94f3-4660f61ac91b
scm3.org_1   | 2023-01-12 05:43:35,025 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node df8756e7-ed3f-4e48-9521-edf82d63369c
scm3.org_1   | 2023-01-12 05:43:35,062 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Persist certificate serialId 1212956981891 on Scm Bootstrap Node df8756e7-ed3f-4e48-9521-edf82d63369c
scm3.org_1   | 2023-01-12 05:43:35,305 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3593a38d] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm3.org_1   | 2023-01-12 05:43:35,616 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3.org_1   | 2023-01-12 05:43:35,616 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm3.org_1   | 2023-01-12 05:43:35,640 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm3.org_1   | 2023-01-12 05:43:36,238 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @42106ms to org.eclipse.jetty.util.log.Slf4jLog
scm3.org_1   | 2023-01-12 05:43:37,847 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3.org_1   | 2023-01-12 05:43:38,041 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3.org_1   | 2023-01-12 05:43:38,069 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm3.org_1   | 2023-01-12 05:43:38,070 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm3.org_1   | 2023-01-12 05:43:38,070 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm3.org_1   | 2023-01-12 05:43:38,123 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm3.org_1   | 2023-01-12 05:43:38,784 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm3.org_1   | 2023-01-12 05:43:38,804 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm3.org_1   | 2023-01-12 05:43:39,294 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm3.org_1   | 2023-01-12 05:43:39,310 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm3.org_1   | 2023-01-12 05:43:39,336 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm3.org_1   | 2023-01-12 05:43:39,614 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2023-01-12 05:43:39,653 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@626a783b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3.org_1   | 2023-01-12 05:43:39,661 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3df3e1e9{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm3.org_1   | 2023-01-12 05:43:41,142 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2023-01-12 05:43:41,287 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6d5934f6{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-4082738820563903759/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm3.org_1   | 2023-01-12 05:43:41,395 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@3e0e2a80{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3.org_1   | 2023-01-12 05:43:41,395 [Listener at 0.0.0.0/9860] INFO server.Server: Started @47268ms
scm3.org_1   | 2023-01-12 05:43:41,442 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm3.org_1   | 2023-01-12 05:43:41,444 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm3.org_1   | 2023-01-12 05:43:41,446 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm3.org_1   | 2023-01-12 05:43:55,695 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:43:56,225 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:43:56,835 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:44:02,406 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:44:03,879 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:44:04,814 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:44:34,032 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:33488
scm3.org_1   | 2023-01-12 05:44:34,103 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-12 05:44:34,851 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59148
scm3.org_1   | 2023-01-12 05:44:35,135 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-12 05:44:35,657 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:36328
scm3.org_1   | 2023-01-12 05:44:35,681 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-12 05:44:39,218 [IPC Server handler 70 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
scm3.org_1   | 2023-01-12 05:44:39,294 [IPC Server handler 70 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1332434143678, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-01-12 05:44:01,551 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:60828
scm1.org_1   | 2023-01-12 05:44:01,577 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:44:01,600 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om2, UUID: 8f6450af-1412-4cd4-a332-cc9196201372
scm1.org_1   | 2023-01-12 05:44:02,375 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:44:03,403 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:36514
scm1.org_1   | 2023-01-12 05:44:03,445 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:44:03,448 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om3, UUID: ca9b232d-0797-4348-8c10-47e141bc4ff3
scm1.org_1   | 2023-01-12 05:44:03,834 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:44:04,363 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:43900
scm1.org_1   | 2023-01-12 05:44:04,382 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:44:04,425 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om1, UUID: c4ec61fc-6775-46e8-b7aa-9399a8c6c162
scm1.org_1   | 2023-01-12 05:44:04,796 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:44:10,842 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:49014
scm1.org_1   | 2023-01-12 05:44:10,896 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:44:11,067 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:45948
scm1.org_1   | 2023-01-12 05:44:11,107 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:44:12,102 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:40628
scm1.org_1   | 2023-01-12 05:44:12,152 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:44:33,878 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:41296
scm1.org_1   | 2023-01-12 05:44:33,925 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-12 05:44:34,754 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:36196
scm1.org_1   | 2023-01-12 05:44:35,145 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-12 05:44:35,535 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:47466
scm1.org_1   | 2023-01-12 05:44:35,608 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-12 05:44:39,226 [IPC Server handler 73 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
scm1.org_1   | 2023-01-12 05:44:39,284 [IPC Server handler 73 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1332434143678, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-01-12 05:44:39,302 [IPC Server handler 69 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/08ecee04-dee1-41b4-94f3-4660f61ac91b
scm1.org_1   | 2023-01-12 05:44:39,366 [IPC Server handler 69 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 08ecee04-dee1-41b4-94f3-4660f61ac91b{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1332863307027, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-01-12 05:44:39,498 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1.org_1   | 2023-01-12 05:44:39,671 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1.org_1   | 2023-01-12 05:44:39,671 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-01-12 05:44:39,675 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-01-12 05:44:39,731 [IPC Server handler 42 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/75176c82-cf64-45b0-aad5-7780ce7245eb
scm1.org_1   | 2023-01-12 05:44:39,741 [IPC Server handler 42 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 75176c82-cf64-45b0-aad5-7780ce7245eb{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1331503348608, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2023-01-12 05:44:39,744 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1.org_1   | 2023-01-12 05:44:39,745 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1.org_1   | 2023-01-12 05:44:39,745 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1.org_1   | 2023-01-12 05:44:39,745 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1.org_1   | 2023-01-12 05:44:39,745 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-01-12 05:44:39,792 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2023-01-12 05:44:39,838 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=86cb76d7-d1bd-4dd6-8116-a6bdfd03825d to datanode:a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
scm1.org_1   | 2023-01-12 05:44:40,706 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 86cb76d7-d1bd-4dd6-8116-a6bdfd03825d, Nodes: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:39.825Z[UTC]].
scm1.org_1   | 2023-01-12 05:44:40,729 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:44:40,913 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 to datanode:75176c82-cf64-45b0-aad5-7780ce7245eb
scm1.org_1   | 2023-01-12 05:44:40,925 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 to datanode:08ecee04-dee1-41b4-94f3-4660f61ac91b
scm1.org_1   | 2023-01-12 05:44:40,929 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 to datanode:a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
scm1.org_1   | 2023-01-12 05:44:41,090 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9debe22a-e51c-422a-aaf2-acb2c75a2877, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:40.913Z[UTC]].
scm1.org_1   | 2023-01-12 05:44:41,103 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:44:41,114 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=48a05780-a7a4-4d83-bfcd-444d5ac91262 to datanode:75176c82-cf64-45b0-aad5-7780ce7245eb
scm1.org_1   | 2023-01-12 05:44:41,249 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 48a05780-a7a4-4d83-bfcd-444d5ac91262, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:41.114Z[UTC]].
scm1.org_1   | 2023-01-12 05:44:41,249 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:44:41,274 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44 to datanode:75176c82-cf64-45b0-aad5-7780ce7245eb
scm1.org_1   | 2023-01-12 05:44:41,292 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44 to datanode:a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
scm1.org_1   | 2023-01-12 05:44:41,297 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44 to datanode:08ecee04-dee1-41b4-94f3-4660f61ac91b
scm1.org_1   | 2023-01-12 05:44:41,412 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e22252e4-1bd7-4367-9a96-52a62dfb0a44, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:41.274Z[UTC]].
scm1.org_1   | 2023-01-12 05:44:41,414 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:44:41,420 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44 contains same datanodes as previous pipelines: PipelineID=9debe22a-e51c-422a-aaf2-acb2c75a2877 nodeIds: 75176c82-cf64-45b0-aad5-7780ce7245eb, a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3, 08ecee04-dee1-41b4-94f3-4660f61ac91b
scm1.org_1   | 2023-01-12 05:44:41,477 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=003546ba-ec7b-4d1e-b502-60ceae25f148 to datanode:08ecee04-dee1-41b4-94f3-4660f61ac91b
scm1.org_1   | 2023-01-12 05:44:41,571 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 003546ba-ec7b-4d1e-b502-60ceae25f148, Nodes: 08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:41.477Z[UTC]].
scm1.org_1   | 2023-01-12 05:44:41,574 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:44:41,577 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1.org_1   | 2023-01-12 05:44:41,592 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1.org_1   | 2023-01-12 05:44:42,627 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:60112
scm1.org_1   | 2023-01-12 05:44:42,689 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-12 05:44:43,391 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:58550
scm3.org_1   | 2023-01-12 05:44:39,294 [IPC Server handler 54 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/08ecee04-dee1-41b4-94f3-4660f61ac91b
scm3.org_1   | 2023-01-12 05:44:39,340 [IPC Server handler 54 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 08ecee04-dee1-41b4-94f3-4660f61ac91b{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1332863307027, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-01-12 05:44:39,408 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm3.org_1   | 2023-01-12 05:44:39,448 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm3.org_1   | 2023-01-12 05:44:39,448 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-01-12 05:44:39,503 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-01-12 05:44:39,707 [IPC Server handler 30 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/75176c82-cf64-45b0-aad5-7780ce7245eb
scm3.org_1   | 2023-01-12 05:44:39,708 [IPC Server handler 30 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 75176c82-cf64-45b0-aad5-7780ce7245eb{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1331503348608, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2023-01-12 05:44:39,709 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-01-12 05:44:39,709 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3.org_1   | 2023-01-12 05:44:39,709 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3.org_1   | 2023-01-12 05:44:39,709 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm3.org_1   | 2023-01-12 05:44:39,709 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3.org_1   | 2023-01-12 05:44:39,722 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2023-01-12 05:44:41,207 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 86cb76d7-d1bd-4dd6-8116-a6bdfd03825d, Nodes: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:39.825Z[UTC]].
scm3.org_1   | 2023-01-12 05:44:41,417 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:44:41,467 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9debe22a-e51c-422a-aaf2-acb2c75a2877, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:40.913Z[UTC]].
scm3.org_1   | 2023-01-12 05:44:41,481 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:44:41,509 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 48a05780-a7a4-4d83-bfcd-444d5ac91262, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:41.114Z[UTC]].
scm3.org_1   | 2023-01-12 05:44:41,525 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:44:41,754 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e22252e4-1bd7-4367-9a96-52a62dfb0a44, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:41.274Z[UTC]].
scm3.org_1   | 2023-01-12 05:44:41,762 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:44:41,795 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 003546ba-ec7b-4d1e-b502-60ceae25f148, Nodes: 08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:41.477Z[UTC]].
scm3.org_1   | 2023-01-12 05:44:41,795 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:45:10,419 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:57958
scm3.org_1   | 2023-01-12 05:45:10,510 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-12 05:45:11,336 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:45956
scm3.org_1   | 2023-01-12 05:45:11,410 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-12 05:45:11,695 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39828
scm3.org_1   | 2023-01-12 05:45:11,817 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-12 05:44:39,231 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 08ecee04-dee1-41b4-94f3-4660f61ac91b{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1332863307027, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-01-12 05:44:39,260 [IPC Server handler 31 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3
scm2.org_1   | 2023-01-12 05:44:39,271 [IPC Server handler 31 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1332434143678, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-01-12 05:44:39,412 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2.org_1   | 2023-01-12 05:44:39,487 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2.org_1   | 2023-01-12 05:44:39,480 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-01-12 05:44:39,526 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-01-12 05:44:39,711 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/75176c82-cf64-45b0-aad5-7780ce7245eb
scm2.org_1   | 2023-01-12 05:44:39,716 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 75176c82-cf64-45b0-aad5-7780ce7245eb{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 1331503348608, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2023-01-12 05:44:39,716 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2.org_1   | 2023-01-12 05:44:39,716 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2.org_1   | 2023-01-12 05:44:39,716 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2.org_1   | 2023-01-12 05:44:39,716 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2.org_1   | 2023-01-12 05:44:39,726 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-01-12 05:44:39,727 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2023-01-12 05:44:41,017 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-52d7416a-527f-45fb-b83c-77d6ce7a6ad0: Detected pause in JVM or host machine (eg GC): pause of approximately 117579155ns.
scm2.org_1   | GC pool 'ParNew' had collection(s): count=1 time=184ms
scm2.org_1   | 2023-01-12 05:44:41,334 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 86cb76d7-d1bd-4dd6-8116-a6bdfd03825d, Nodes: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:39.825Z[UTC]].
scm2.org_1   | 2023-01-12 05:44:41,358 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:44:41,425 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9debe22a-e51c-422a-aaf2-acb2c75a2877, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:40.913Z[UTC]].
scm2.org_1   | 2023-01-12 05:44:41,434 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:44:41,456 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 48a05780-a7a4-4d83-bfcd-444d5ac91262, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:41.114Z[UTC]].
scm2.org_1   | 2023-01-12 05:44:41,477 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:44:41,506 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e22252e4-1bd7-4367-9a96-52a62dfb0a44, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:41.274Z[UTC]].
scm2.org_1   | 2023-01-12 05:44:41,516 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:44:41,648 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 003546ba-ec7b-4d1e-b502-60ceae25f148, Nodes: 08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:44:41.477Z[UTC]].
scm2.org_1   | 2023-01-12 05:44:41,652 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:45:10,466 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:34824
scm2.org_1   | 2023-01-12 05:45:10,525 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-12 05:45:11,337 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:59830
scm2.org_1   | 2023-01-12 05:45:11,444 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 180 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:45:31,173 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 181 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:45:31,177 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om3 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 182 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:45:31,268 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 003546ba-ec7b-4d1e-b502-60ceae25f148, Nodes: 08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:08ecee04-dee1-41b4-94f3-4660f61ac91b, CreationTimestamp2023-01-12T05:44:41.477Z[UTC]] to Recon pipeline metadata.
recon_1      | 2023-01-12 05:45:31,271 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 003546ba-ec7b-4d1e-b502-60ceae25f148, Nodes: 08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:08ecee04-dee1-41b4-94f3-4660f61ac91b, CreationTimestamp2023-01-12T05:44:41.477Z[UTC]].
recon_1      | 2023-01-12 05:45:33,236 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om1 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 183 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:45:33,268 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 184 failover attempts. Trying to failover immediately.
recon_1      | 2023-01-12 05:45:33,280 [pool-30-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMLeaderNotReadyException): om3 is Leader but not ready to process request yet.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderNotReadyException(OzoneManagerProtocolServerSideTranslatorPB.java:259)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:237)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 185 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2023-01-12 05:45:33,889 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e22252e4-1bd7-4367-9a96-52a62dfb0a44 reported by 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)
scm1.org_1   | 2023-01-12 05:44:43,449 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-12 05:44:43,683 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:54306
scm1.org_1   | 2023-01-12 05:44:43,782 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2023-01-12 05:44:46,071 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:43570
scm1.org_1   | 2023-01-12 05:44:46,286 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-12 05:44:54,553 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:58278
scm1.org_1   | 2023-01-12 05:44:54,659 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:44:54,937 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:33138
scm1.org_1   | 2023-01-12 05:44:54,948 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:44:55,809 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:39728
scm1.org_1   | 2023-01-12 05:44:55,827 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2023-01-12 05:45:10,624 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:33048
scm1.org_1   | 2023-01-12 05:45:10,638 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-12 05:45:11,356 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39484
scm1.org_1   | 2023-01-12 05:45:11,417 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-12 05:45:11,622 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1.org_1   | 2023-01-12 05:45:11,673 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:43974
scm1.org_1   | 2023-01-12 05:45:11,829 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-12 05:45:13,477 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:45097
scm1.org_1   | 2023-01-12 05:45:13,529 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-12 05:45:14,841 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 86cb76d7-d1bd-4dd6-8116-a6bdfd03825d, Nodes: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3, CreationTimestamp2023-01-12T05:44:39.825Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-01-12 05:45:14,983 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:45:15,198 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-12 05:45:16,000 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-12 05:45:21,106 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: Detected pause in JVM or host machine (eg GC): pause of approximately 104661432ns. No GCs detected.
scm1.org_1   | 2023-01-12 05:45:21,158 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-12 05:45:26,261 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1: Detected pause in JVM or host machine (eg GC): pause of approximately 103121028ns.
scm1.org_1   | GC pool 'ParNew' had collection(s): count=1 time=137ms
scm1.org_1   | 2023-01-12 05:45:28,535 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:35348
scm1.org_1   | 2023-01-12 05:45:28,575 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:55770
scm1.org_1   | 2023-01-12 05:45:28,618 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-12 05:45:28,629 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 48a05780-a7a4-4d83-bfcd-444d5ac91262, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:75176c82-cf64-45b0-aad5-7780ce7245eb, CreationTimestamp2023-01-12T05:44:41.114Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-01-12 05:45:28,798 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:37391
scm1.org_1   | 2023-01-12 05:45:28,807 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-12 05:45:28,943 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:45:28,992 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:35760
scm3.org_1   | 2023-01-12 05:45:14,747 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 86cb76d7-d1bd-4dd6-8116-a6bdfd03825d, Nodes: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3, CreationTimestamp2023-01-12T05:44:39.825Z[UTC]] moved to OPEN state
scm3.org_1   | 2023-01-12 05:45:15,123 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:45:15,985 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-01-12 05:45:17,440 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-df8756e7-ed3f-4e48-9521-edf82d63369c: Detected pause in JVM or host machine (eg GC): pause of approximately 233341296ns.
scm3.org_1   | GC pool 'ParNew' had collection(s): count=1 time=220ms
scm3.org_1   | 2023-01-12 05:45:21,090 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-01-12 05:45:28,316 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:58162
scm3.org_1   | 2023-01-12 05:45:28,373 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-12 05:45:28,375 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 48a05780-a7a4-4d83-bfcd-444d5ac91262, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:75176c82-cf64-45b0-aad5-7780ce7245eb, CreationTimestamp2023-01-12T05:44:41.114Z[UTC]] moved to OPEN state
scm3.org_1   | 2023-01-12 05:45:28,726 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:60436
scm3.org_1   | 2023-01-12 05:45:28,761 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 48a05780-a7a4-4d83-bfcd-444d5ac91262, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:75176c82-cf64-45b0-aad5-7780ce7245eb, CreationTimestamp2023-01-12T05:44:41.114Z[UTC]] moved to OPEN state
scm3.org_1   | 2023-01-12 05:45:29,003 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2023-01-12 05:45:29,069 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:45:29,642 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-01-12 05:45:30,212 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9debe22a-e51c-422a-aaf2-acb2c75a2877, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:75176c82-cf64-45b0-aad5-7780ce7245eb, CreationTimestamp2023-01-12T05:44:40.913Z[UTC]] moved to OPEN state
scm3.org_1   | 2023-01-12 05:45:30,218 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2023-01-12 05:45:30,290 [df8756e7-ed3f-4e48-9521-edf82d63369c@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm3.org_1   | 2023-01-12 05:45:31,128 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 003546ba-ec7b-4d1e-b502-60ceae25f148, Nodes: 08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:08ecee04-dee1-41b4-94f3-4660f61ac91b, CreationTimestamp2023-01-12T05:44:41.477Z[UTC]] moved to OPEN state
scm3.org_1   | 2023-01-12 05:45:31,129 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm3.org_1   | 2023-01-12 05:45:31,129 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm3.org_1   | 2023-01-12 05:45:31,130 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm3.org_1   | 2023-01-12 05:45:31,130 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm3.org_1   | 2023-01-12 05:45:31,131 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm2.org_1   | 2023-01-12 05:45:11,667 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:56412
scm2.org_1   | 2023-01-12 05:45:11,822 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-12 05:45:14,831 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 86cb76d7-d1bd-4dd6-8116-a6bdfd03825d, Nodes: a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3, CreationTimestamp2023-01-12T05:44:39.825Z[UTC]] moved to OPEN state
scm2.org_1   | 2023-01-12 05:45:15,069 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:45:15,992 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-01-12 05:45:21,133 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-01-12 05:45:28,334 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:56906
scm2.org_1   | 2023-01-12 05:45:28,397 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-12 05:45:28,399 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 48a05780-a7a4-4d83-bfcd-444d5ac91262, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:75176c82-cf64-45b0-aad5-7780ce7245eb, CreationTimestamp2023-01-12T05:44:41.114Z[UTC]] moved to OPEN state
scm2.org_1   | 2023-01-12 05:45:28,759 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 48a05780-a7a4-4d83-bfcd-444d5ac91262, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:75176c82-cf64-45b0-aad5-7780ce7245eb, CreationTimestamp2023-01-12T05:44:41.114Z[UTC]] moved to OPEN state
scm2.org_1   | 2023-01-12 05:45:28,907 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:39894
scm2.org_1   | 2023-01-12 05:45:29,117 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2023-01-12 05:45:29,164 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:45:29,642 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-01-12 05:45:30,215 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9debe22a-e51c-422a-aaf2-acb2c75a2877, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:75176c82-cf64-45b0-aad5-7780ce7245eb, CreationTimestamp2023-01-12T05:44:40.913Z[UTC]] moved to OPEN state
scm2.org_1   | 2023-01-12 05:45:30,226 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2023-01-12 05:45:30,342 [52d7416a-527f-45fb-b83c-77d6ce7a6ad0@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm2.org_1   | 2023-01-12 05:45:31,134 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm2.org_1   | 2023-01-12 05:45:31,134 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm2.org_1   | 2023-01-12 05:45:31,135 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm2.org_1   | 2023-01-12 05:45:31,135 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm2.org_1   | 2023-01-12 05:45:31,134 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 003546ba-ec7b-4d1e-b502-60ceae25f148, Nodes: 08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:08ecee04-dee1-41b4-94f3-4660f61ac91b, CreationTimestamp2023-01-12T05:44:41.477Z[UTC]] moved to OPEN state
scm2.org_1   | 2023-01-12 05:45:31,136 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1.org_1   | 2023-01-12 05:45:29,001 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-12 05:45:29,006 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-12 05:45:29,102 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2023-01-12 05:45:29,134 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2023-01-12 05:45:29,653 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-12 05:45:30,224 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9debe22a-e51c-422a-aaf2-acb2c75a2877, Nodes: 75176c82-cf64-45b0-aad5-7780ce7245eb(ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net/172.25.0.104)08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103)a64dab3e-1ea0-48b5-bca9-ca962e3a3cb3(ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net/172.25.0.102), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:75176c82-cf64-45b0-aad5-7780ce7245eb, CreationTimestamp2023-01-12T05:44:40.913Z[UTC]] moved to OPEN state
scm1.org_1   | 2023-01-12 05:45:30,262 [0c3562c7-ba3f-4f72-bb46-d91f8e22e3f1@group-067D6DACB49F-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1.org_1   | 2023-01-12 05:45:30,265 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1.org_1   | 2023-01-12 05:45:30,274 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1.org_1   | 2023-01-12 05:45:30,274 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1.org_1   | 2023-01-12 05:45:30,279 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1.org_1   | 2023-01-12 05:45:30,279 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1.org_1   | 2023-01-12 05:45:30,279 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm1.org_1   | 2023-01-12 05:45:30,279 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm1.org_1   | 2023-01-12 05:45:30,279 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm1.org_1   | 2023-01-12 05:45:30,375 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm1.org_1   | 2023-01-12 05:45:31,172 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 003546ba-ec7b-4d1e-b502-60ceae25f148, Nodes: 08ecee04-dee1-41b4-94f3-4660f61ac91b(ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net/172.25.0.103), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:08ecee04-dee1-41b4-94f3-4660f61ac91b, CreationTimestamp2023-01-12T05:44:41.477Z[UTC]] moved to OPEN state
