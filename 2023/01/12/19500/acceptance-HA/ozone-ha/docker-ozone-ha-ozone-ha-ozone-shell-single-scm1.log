Attaching to ozone-ha_recon_1, ozone-ha_om3_1, ozone-ha_om2_1, ozone-ha_scm3_1, ozone-ha_om1_1, ozone-ha_datanode_2, ozone-ha_s3g_1, ozone-ha_datanode_1, ozone-ha_scm1_1, ozone-ha_datanode_3, ozone-ha_scm2_1
datanode_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1  | 2023-01-12 05:24:41,435 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1  | /************************************************************
datanode_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_1  | STARTUP_MSG:   host = d9a6a467ccde/172.18.0.6
datanode_1  | STARTUP_MSG:   args = []
datanode_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
datanode_1  | STARTUP_MSG:   java = 11.0.14.1
datanode_1  | ************************************************************/
datanode_1  | 2023-01-12 05:24:41,466 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1  | 2023-01-12 05:24:41,772 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1  | 2023-01-12 05:24:42,235 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1  | 2023-01-12 05:24:43,072 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1  | 2023-01-12 05:24:43,077 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1  | 2023-01-12 05:24:43,591 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:d9a6a467ccde ip:172.18.0.6
datanode_1  | 2023-01-12 05:24:44,990 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_1  | 2023-01-12 05:24:45,975 [main] INFO reflections.Reflections: Reflections took 764 ms to scan 2 urls, producing 97 keys and 217 values 
datanode_1  | 2023-01-12 05:24:46,433 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode_1  | 2023-01-12 05:24:47,327 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1  | 2023-01-12 05:24:47,366 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1  | 2023-01-12 05:24:47,375 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1  | 2023-01-12 05:24:47,385 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1  | 2023-01-12 05:24:47,471 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2023-01-12 05:24:47,511 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1  | 2023-01-12 05:24:47,519 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1  | 2023-01-12 05:24:47,520 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1  | 2023-01-12 05:24:47,520 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1  | 2023-01-12 05:24:47,535 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1  | 2023-01-12 05:24:47,747 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1  | 2023-01-12 05:24:47,747 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1  | 2023-01-12 05:24:55,141 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1  | 2023-01-12 05:24:55,785 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1  | 2023-01-12 05:24:56,111 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1  | 2023-01-12 05:24:56,660 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1  | 2023-01-12 05:24:56,669 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1  | 2023-01-12 05:24:56,676 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1  | 2023-01-12 05:24:56,676 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1  | 2023-01-12 05:24:56,679 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1  | 2023-01-12 05:24:56,680 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1  | 2023-01-12 05:24:56,685 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1  | 2023-01-12 05:24:56,694 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2023-01-12 05:24:56,704 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1  | 2023-01-12 05:24:56,712 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1  | 2023-01-12 05:24:56,772 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1  | 2023-01-12 05:24:56,821 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1  | 2023-01-12 05:24:56,844 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1  | 2023-01-12 05:24:58,850 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode_1  | 2023-01-12 05:24:59,035 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode_1  | 2023-01-12 05:24:59,035 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode_1  | 2023-01-12 05:24:59,062 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode_1  | 2023-01-12 05:24:59,072 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode_1  | 2023-01-12 05:24:59,132 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode_1  | 2023-01-12 05:24:59,133 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode_1  | 2023-01-12 05:24:59,232 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode_1  | 2023-01-12 05:24:59,260 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
datanode_1  | 2023-01-12 05:24:59,295 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode_1  | 2023-01-12 05:24:59,299 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode_1  | 2023-01-12 05:24:59,833 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1  | 2023-01-12 05:24:59,841 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1  | 2023-01-12 05:24:59,850 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1  | 2023-01-12 05:24:59,867 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1  | 2023-01-12 05:24:59,871 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2  | 2023-01-12 05:24:41,445 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2  | /************************************************************
datanode_2  | STARTUP_MSG: Starting HddsDatanodeService
datanode_2  | STARTUP_MSG:   host = 4c4cfcfeca58/172.18.0.11
datanode_2  | STARTUP_MSG:   args = []
datanode_2  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_2  | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
datanode_2  | STARTUP_MSG:   java = 11.0.14.1
datanode_2  | ************************************************************/
datanode_2  | 2023-01-12 05:24:41,467 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2  | 2023-01-12 05:24:41,771 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2  | 2023-01-12 05:24:42,271 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2  | 2023-01-12 05:24:42,943 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2  | 2023-01-12 05:24:42,944 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2  | 2023-01-12 05:24:43,424 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:4c4cfcfeca58 ip:172.18.0.11
datanode_2  | 2023-01-12 05:24:44,715 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_2  | 2023-01-12 05:24:45,529 [main] INFO reflections.Reflections: Reflections took 616 ms to scan 2 urls, producing 97 keys and 217 values 
datanode_2  | 2023-01-12 05:24:46,028 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode_2  | 2023-01-12 05:24:46,980 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2  | 2023-01-12 05:24:47,079 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2  | 2023-01-12 05:24:47,082 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2  | 2023-01-12 05:24:47,093 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2  | 2023-01-12 05:24:47,215 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2023-01-12 05:24:47,308 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2  | 2023-01-12 05:24:47,309 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2  | 2023-01-12 05:24:47,317 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2  | 2023-01-12 05:24:47,317 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2  | 2023-01-12 05:24:47,330 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2  | 2023-01-12 05:24:47,481 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2  | 2023-01-12 05:24:47,481 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2  | 2023-01-12 05:24:55,378 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2  | 2023-01-12 05:24:55,971 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2  | 2023-01-12 05:24:56,414 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2  | 2023-01-12 05:24:57,102 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2  | 2023-01-12 05:24:57,104 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2  | 2023-01-12 05:24:57,112 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2  | 2023-01-12 05:24:57,119 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2  | 2023-01-12 05:24:57,122 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2  | 2023-01-12 05:24:57,122 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2  | 2023-01-12 05:24:57,160 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2  | 2023-01-12 05:24:57,163 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-12 05:24:57,168 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2  | 2023-01-12 05:24:57,169 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2023-01-12 05:24:57,231 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2  | 2023-01-12 05:24:57,258 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2  | 2023-01-12 05:24:57,277 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2  | 2023-01-12 05:24:59,079 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode_3  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3  | 2023-01-12 05:24:40,951 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3  | /************************************************************
datanode_3  | STARTUP_MSG: Starting HddsDatanodeService
om2_1       | Waiting for the service scm3:9894
datanode_1  | 2023-01-12 05:25:00,154 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xa996647d] REGISTERED
datanode_1  | 2023-01-12 05:25:00,228 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xa996647d] BIND: 0.0.0.0/0.0.0.0:0
datanode_1  | 2023-01-12 05:25:00,290 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xa996647d, L:/0.0.0.0:39267] ACTIVE
datanode_2  | 2023-01-12 05:24:59,302 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
om2_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1  | 2023-01-12 05:25:00,309 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2  | 2023-01-12 05:24:59,323 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode_2  | 2023-01-12 05:24:59,339 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode_3  | STARTUP_MSG:   host = d407c6d45367/172.18.0.4
datanode_1  | 2023-01-12 05:25:01,102 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
om2_1       | 2023-01-12 05:25:54,725 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
datanode_2  | 2023-01-12 05:24:59,341 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode_3  | STARTUP_MSG:   args = []
datanode_3  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1       | /************************************************************
datanode_2  | 2023-01-12 05:24:59,359 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode_1  | 2023-01-12 05:25:01,241 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1  | 2023-01-12 05:25:01,605 [main] INFO util.log: Logging initialized @26029ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1       | STARTUP_MSG: Starting OzoneManager
datanode_2  | 2023-01-12 05:24:59,375 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
om1_1       | Waiting for the service scm3:9894
datanode_1  | 2023-01-12 05:25:02,615 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_3  | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
om2_1       | STARTUP_MSG:   host = om2/172.18.0.7
om1_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1       | 2023-01-12 05:25:54,706 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
datanode_2  | 2023-01-12 05:24:59,444 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode_2  | 2023-01-12 05:24:59,475 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
datanode_2  | 2023-01-12 05:24:59,491 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode_1  | 2023-01-12 05:25:02,700 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1  | 2023-01-12 05:25:02,766 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1  | 2023-01-12 05:25:02,782 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1  | 2023-01-12 05:25:02,808 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1  | 2023-01-12 05:25:02,811 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1       | STARTUP_MSG:   args = [--init]
om2_1       | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1       | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
datanode_3  | STARTUP_MSG:   java = 11.0.14.1
datanode_3  | ************************************************************/
datanode_3  | 2023-01-12 05:24:40,994 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | 2023-01-12 05:24:41,318 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2  | 2023-01-12 05:24:59,500 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
om1_1       | /************************************************************
om1_1       | STARTUP_MSG: Starting OzoneManager
om1_1       | STARTUP_MSG:   host = om1/172.18.0.5
om1_1       | STARTUP_MSG:   args = [--init]
datanode_2  | 2023-01-12 05:24:59,880 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2  | 2023-01-12 05:24:59,892 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3  | 2023-01-12 05:24:41,686 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3  | 2023-01-12 05:24:42,553 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3  | 2023-01-12 05:24:42,553 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3  | 2023-01-12 05:24:43,093 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:d407c6d45367 ip:172.18.0.4
om2_1       | STARTUP_MSG:   java = 11.0.14.1
om2_1       | ************************************************************/
datanode_1  | 2023-01-12 05:25:03,276 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1  | 2023-01-12 05:25:03,279 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_2  | 2023-01-12 05:24:59,896 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3  | 2023-01-12 05:24:44,510 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
om2_1       | 2023-01-12 05:25:54,779 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1       | Waiting for the service scm3:9894
om1_1       | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1  | 2023-01-12 05:25:03,634 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1  | 2023-01-12 05:25:03,652 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2  | 2023-01-12 05:24:59,898 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3  | 2023-01-12 05:24:45,376 [main] INFO reflections.Reflections: Reflections took 692 ms to scan 2 urls, producing 97 keys and 217 values 
om2_1       | 2023-01-12 05:26:05,344 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
datanode_2  | 2023-01-12 05:24:59,902 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2023-01-12 05:24:45,845 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode_3  | 2023-01-12 05:24:46,659 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
om2_1       | 2023-01-12 05:26:09,078 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1       | 2023-01-12 05:26:09,357 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1       | 2023-01-12 05:26:09,357 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1       | 2023-01-12 05:26:09,387 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1       | 2023-01-12 05:26:10,598 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/172.18.0.3:9863, nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863, nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863]
om2_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-b40a0ffb-806c-459e-acb9-4d43ff37764e;layoutVersion=3
om2_1       | 2023-01-12 05:26:12,003 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1       | /************************************************************
om2_1       | SHUTDOWN_MSG: Shutting down OzoneManager at om2/172.18.0.7
om1_1       | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
datanode_1  | 2023-01-12 05:25:03,655 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1  | 2023-01-12 05:25:03,912 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@329dc214{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1  | 2023-01-12 05:25:03,916 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@296edc75{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1  | 2023-01-12 05:25:04,905 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@46a8c2b4{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-15924757831301547327/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1  | 2023-01-12 05:25:05,010 [main] INFO server.AbstractConnector: Started ServerConnector@3db65c0d{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1  | 2023-01-12 05:25:05,020 [main] INFO server.Server: Started @29434ms
datanode_1  | 2023-01-12 05:25:05,049 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1  | 2023-01-12 05:25:05,049 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1  | 2023-01-12 05:25:05,071 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2  | 2023-01-12 05:25:00,044 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x8b429ae7] REGISTERED
datanode_2  | 2023-01-12 05:25:00,045 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x8b429ae7] BIND: 0.0.0.0/0.0.0.0:0
datanode_2  | 2023-01-12 05:25:00,073 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x8b429ae7, L:/0.0.0.0:39845] ACTIVE
datanode_3  | 2023-01-12 05:24:46,732 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3  | 2023-01-12 05:24:46,741 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3  | 2023-01-12 05:24:46,751 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
om3_1       | 2023-01-12 05:25:55,140 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1       | STARTUP_MSG:   java = 11.0.14.1
om1_1       | ************************************************************/
datanode_2  | 2023-01-12 05:25:00,112 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
om3_1       | /************************************************************
om3_1       | STARTUP_MSG: Starting OzoneManager
datanode_2  | 2023-01-12 05:25:01,560 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
om1_1       | 2023-01-12 05:25:54,757 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1       | STARTUP_MSG:   host = om3/172.18.0.10
datanode_3  | 2023-01-12 05:24:46,842 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
om2_1       | ************************************************************/
datanode_2  | 2023-01-12 05:25:01,809 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1       | 2023-01-12 05:26:05,093 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1       | STARTUP_MSG:   args = [--init]
datanode_1  | 2023-01-12 05:25:05,107 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3  | 2023-01-12 05:24:46,997 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2  | 2023-01-12 05:25:02,045 [main] INFO util.log: Logging initialized @26085ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2  | 2023-01-12 05:25:02,778 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2  | 2023-01-12 05:25:02,883 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2  | 2023-01-12 05:25:02,913 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1       | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1       | 2023-01-12 05:26:09,102 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1       | 2023-01-12 05:26:09,615 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1       | 2023-01-12 05:26:09,617 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1       | 2023-01-12 05:26:09,651 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1       | 2023-01-12 05:26:11,047 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/172.18.0.3:9863, nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863, nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863]
om1_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-b40a0ffb-806c-459e-acb9-4d43ff37764e;layoutVersion=3
datanode_1  | 2023-01-12 05:25:05,381 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@42f4741] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3  | 2023-01-12 05:24:46,998 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3  | 2023-01-12 05:24:47,003 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3  | 2023-01-12 05:24:47,022 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3  | 2023-01-12 05:24:47,045 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
om3_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
datanode_1  | 2023-01-12 05:25:05,896 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.18.0.12:9891
om2_1       | 2023-01-12 05:26:19,164 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1       | /************************************************************
datanode_2  | 2023-01-12 05:25:02,923 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2  | 2023-01-12 05:25:02,931 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2  | 2023-01-12 05:25:02,932 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2  | 2023-01-12 05:25:03,308 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2  | 2023-01-12 05:25:03,310 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_2  | 2023-01-12 05:25:03,611 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1       | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
datanode_1  | 2023-01-12 05:25:06,409 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1  | 2023-01-12 05:25:08,677 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:08,678 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:08,679 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:08,680 [EndpointStateMachine task thread for recon/172.18.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.12:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:09,389 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
om3_1       | STARTUP_MSG:   java = 11.0.14.1
datanode_3  | 2023-01-12 05:24:47,171 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3  | 2023-01-12 05:24:47,176 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
om1_1       | 2023-01-12 05:26:12,121 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1       | /************************************************************
om1_1       | SHUTDOWN_MSG: Shutting down OzoneManager at om1/172.18.0.5
om3_1       | ************************************************************/
om2_1       | STARTUP_MSG: Starting OzoneManager
om2_1       | STARTUP_MSG:   host = om2/172.18.0.7
om2_1       | STARTUP_MSG:   args = []
om2_1       | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1       | 2023-01-12 05:25:55,191 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | 2023-01-12 05:24:54,674 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3  | 2023-01-12 05:24:55,238 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3  | 2023-01-12 05:24:55,608 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3  | 2023-01-12 05:24:56,147 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3  | 2023-01-12 05:24:56,197 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3  | 2023-01-12 05:24:56,197 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3  | 2023-01-12 05:24:56,204 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3  | 2023-01-12 05:24:56,204 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1       | 2023-01-12 05:26:04,643 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
datanode_2  | 2023-01-12 05:25:03,611 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:309)
om3_1       | 2023-01-12 05:26:08,603 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1       | 2023-01-12 05:26:08,937 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1       | 2023-01-12 05:26:08,938 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om1_1       | ************************************************************/
om1_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1       | 2023-01-12 05:26:18,916 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1       | /************************************************************
om1_1       | STARTUP_MSG: Starting OzoneManager
om1_1       | STARTUP_MSG:   host = om1/172.18.0.5
om2_1       | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
om2_1       | STARTUP_MSG:   java = 11.0.14.1
om2_1       | ************************************************************/
datanode_2  | 2023-01-12 05:25:03,638 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2  | 2023-01-12 05:25:03,768 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@329dc214{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2  | 2023-01-12 05:25:03,789 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@296edc75{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2  | 2023-01-12 05:25:04,777 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@46a8c2b4{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-8986761300403745635/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2  | 2023-01-12 05:25:04,853 [main] INFO server.AbstractConnector: Started ServerConnector@3db65c0d{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
om3_1       | 2023-01-12 05:26:08,959 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1       | 2023-01-12 05:26:10,393 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/172.18.0.3:9863, nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863, nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863]
om2_1       | 2023-01-12 05:26:19,200 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1       | 2023-01-12 05:26:24,046 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1       | 2023-01-12 05:26:26,946 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1       | 2023-01-12 05:26:27,138 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
datanode_3  | 2023-01-12 05:24:56,205 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
om3_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-b40a0ffb-806c-459e-acb9-4d43ff37764e;layoutVersion=3
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:509)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1  | Caused by: java.util.concurrent.TimeoutException
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om1_1       | STARTUP_MSG:   args = []
datanode_2  | 2023-01-12 05:25:04,854 [main] INFO server.Server: Started @28893ms
datanode_2  | 2023-01-12 05:25:04,879 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2  | 2023-01-12 05:25:04,880 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1       | 2023-01-12 05:26:11,717 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1       | /************************************************************
om3_1       | SHUTDOWN_MSG: Shutting down OzoneManager at om3/172.18.0.10
om3_1       | ************************************************************/
om3_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3  | 2023-01-12 05:24:56,205 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
om1_1       | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1       | 2023-01-12 05:26:27,145 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1       | 2023-01-12 05:26:27,167 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1       | 2023-01-12 05:26:27,283 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om2_1       | 2023-01-12 05:26:28,790 [main] INFO reflections.Reflections: Reflections took 1305 ms to scan 1 urls, producing 115 keys and 335 values [using 2 cores]
om2_1       | 2023-01-12 05:26:28,890 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1       | 2023-01-12 05:26:30,215 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/172.18.0.3:9863, nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863, nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863]
om2_1       | 2023-01-12 05:26:30,468 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/172.18.0.3:9863, nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863, nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863]
datanode_3  | 2023-01-12 05:24:56,232 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2023-01-12 05:24:56,233 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3  | 2023-01-12 05:24:56,236 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3  | 2023-01-12 05:24:56,312 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2  | 2023-01-12 05:25:04,892 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2  | 2023-01-12 05:25:04,953 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2  | 2023-01-12 05:25:05,100 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5a1b9c93] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2  | 2023-01-12 05:25:05,538 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.18.0.12:9891
datanode_2  | 2023-01-12 05:25:05,836 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2  | 2023-01-12 05:25:08,527 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
datanode_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 2023-01-12 05:26:18,165 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1       | /************************************************************
om3_1       | STARTUP_MSG: Starting OzoneManager
om3_1       | STARTUP_MSG:   host = om3/172.18.0.10
om3_1       | STARTUP_MSG:   args = []
om1_1       | STARTUP_MSG:   java = 11.0.14.1
om1_1       | ************************************************************/
om1_1       | 2023-01-12 05:26:19,009 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | 2023-01-12 05:24:56,319 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3  | 2023-01-12 05:24:56,344 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3  | 2023-01-12 05:24:58,298 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode_3  | 2023-01-12 05:24:58,533 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode_3  | 2023-01-12 05:24:58,534 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode_3  | 2023-01-12 05:24:58,560 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	... 1 more
datanode_1  | 2023-01-12 05:25:09,679 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:26:34,312 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1       | 2023-01-12 05:26:35,283 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
datanode_2  | 2023-01-12 05:25:08,543 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:08,544 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:08,545 [EndpointStateMachine task thread for recon/172.18.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.12:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:26:23,824 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1       | 2023-01-12 05:26:26,278 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1       | 2023-01-12 05:26:35,287 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om2_1       | 2023-01-12 05:26:36,052 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1       | 2023-01-12 05:26:36,277 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om2_1       | 2023-01-12 05:26:36,518 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1       | 2023-01-12 05:26:36,535 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1       | 2023-01-12 05:26:36,617 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
datanode_2  | 2023-01-12 05:25:09,530 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:09,545 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:09,680 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:09,680 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1       | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
om3_1       | STARTUP_MSG:   java = 11.0.14.1
om3_1       | ************************************************************/
datanode_3  | 2023-01-12 05:24:58,561 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode_3  | 2023-01-12 05:24:58,585 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
om1_1       | 2023-01-12 05:26:26,997 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1       | 2023-01-12 05:26:26,998 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
datanode_1  | 2023-01-12 05:25:09,681 [EndpointStateMachine task thread for recon/172.18.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.12:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:10,680 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:09,545 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:09,546 [EndpointStateMachine task thread for recon/172.18.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.12:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:10,531 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:26:37,530 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1       | 2023-01-12 05:26:37,599 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1       | 2023-01-12 05:26:37,872 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1       | 2023-01-12 05:26:37,954 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
datanode_3  | 2023-01-12 05:24:58,585 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
recon_1     | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2  | 2023-01-12 05:25:10,545 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:26:38,173 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3  | 2023-01-12 05:24:58,659 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
om1_1       | 2023-01-12 05:26:27,014 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1       | 2023-01-12 05:26:18,208 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1  | 2023-01-12 05:25:10,681 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:10,681 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:10,546 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:26:38,912 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1       | 2023-01-12 05:26:38,923 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1       | 2023-01-12 05:26:38,927 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1       | 2023-01-12 05:26:38,931 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1       | 2023-01-12 05:26:27,178 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
recon_1     | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1     | 2023-01-12 05:24:41,690 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1     | /************************************************************
recon_1     | STARTUP_MSG: Starting ReconServer
om3_1       | 2023-01-12 05:26:22,203 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
datanode_3  | 2023-01-12 05:24:58,668 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
datanode_3  | 2023-01-12 05:24:58,704 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode_1  | 2023-01-12 05:25:11,392 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
om1_1       | 2023-01-12 05:26:28,057 [main] INFO reflections.Reflections: Reflections took 702 ms to scan 1 urls, producing 115 keys and 335 values [using 2 cores]
recon_1     | STARTUP_MSG:   host = 07cc1c5ffbcd/172.18.0.12
s3g_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1       | 2023-01-12 05:26:24,205 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
datanode_2  | 2023-01-12 05:25:10,547 [EndpointStateMachine task thread for recon/172.18.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.12:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:24:58,705 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode_3  | 2023-01-12 05:24:59,177 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3  | 2023-01-12 05:24:59,177 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3  | 2023-01-12 05:24:59,177 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3  | 2023-01-12 05:24:59,178 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1     | STARTUP_MSG:   args = []
s3g_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1       | 2023-01-12 05:24:40,920 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
scm1_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2  | 2023-01-12 05:25:11,532 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:11,546 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:11,547 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:12,533 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:12,547 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:26:28,148 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1       | 2023-01-12 05:26:29,309 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/172.18.0.3:9863, nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863, nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863]
recon_1     | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1       | 2023-01-12 05:26:24,728 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
scm1_1      | 2023-01-12 05:24:44,129 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1_1      | /************************************************************
scm1_1      | STARTUP_MSG: Starting StorageContainerManager
datanode_2  | 2023-01-12 05:25:12,548 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:26:29,516 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/172.18.0.3:9863, nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863, nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863]
om1_1       | 2023-01-12 05:26:32,228 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1       | 2023-01-12 05:26:33,143 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm3_1      | Waiting for the service scm2:9894
s3g_1       | 2023-01-12 05:24:40,927 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1     | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.23.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.3.23.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.23.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.23.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.23.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
om3_1       | 2023-01-12 05:26:24,740 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1       | 2023-01-12 05:26:24,797 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
om2_1       | 2023-01-12 05:26:38,933 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2  | 2023-01-12 05:25:13,534 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:26:33,158 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
datanode_3  | 2023-01-12 05:24:59,201 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2023-01-12 05:24:59,493 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xddbc1b2e] REGISTERED
datanode_3  | 2023-01-12 05:24:59,572 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xddbc1b2e] BIND: 0.0.0.0/0.0.0.0:0
datanode_3  | 2023-01-12 05:24:59,574 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xddbc1b2e, L:/0.0.0.0:46157] ACTIVE
datanode_3  | 2023-01-12 05:24:59,680 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3  | 2023-01-12 05:25:00,852 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
om3_1       | 2023-01-12 05:26:25,126 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
om2_1       | 2023-01-12 05:26:38,934 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
datanode_2  | 2023-01-12 05:25:13,548 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:26:34,918 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
recon_1     | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
datanode_3  | 2023-01-12 05:25:00,972 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3  | 2023-01-12 05:25:01,277 [main] INFO util.log: Logging initialized @27290ms to org.eclipse.jetty.util.log.Slf4jLog
scm3_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1       | 2023-01-12 05:26:26,876 [main] INFO reflections.Reflections: Reflections took 1409 ms to scan 1 urls, producing 115 keys and 335 values [using 2 cores]
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
om2_1       | 2023-01-12 05:26:38,939 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
datanode_2  | 2023-01-12 05:25:13,548 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:26:35,219 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om1_1       | 2023-01-12 05:26:35,410 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
recon_1     | STARTUP_MSG:   java = 11.0.14.1
scm3_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1       | 2023-01-12 05:26:27,044 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1      | STARTUP_MSG:   host = 7f47dfd12ecd/172.18.0.2
scm1_1      | STARTUP_MSG:   args = [--init]
scm1_1      | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2  | 2023-01-12 05:25:14,535 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:26:38,955 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1       | 2023-01-12 05:24:41,047 [main] INFO util.log: Logging initialized @7534ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1     | ************************************************************/
scm3_1      | 2023-01-12 05:25:35,547 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
om3_1       | 2023-01-12 05:26:28,602 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/172.18.0.3:9863, nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863, nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863]
om3_1       | 2023-01-12 05:26:28,938 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/172.18.0.3:9863, nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863, nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863]
datanode_2  | 2023-01-12 05:25:14,549 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:14,550 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
om2_1       | 2023-01-12 05:26:38,960 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1       | 2023-01-12 05:26:38,962 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
recon_1     | 2023-01-12 05:24:41,768 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1     | WARNING: An illegal reflective access operation has occurred
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:309)
om3_1       | 2023-01-12 05:26:32,168 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1       | 2023-01-12 05:26:32,817 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om3_1       | 2023-01-12 05:26:32,853 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om3_1       | 2023-01-12 05:26:34,481 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
scm1_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
om1_1       | 2023-01-12 05:26:35,429 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1       | 2023-01-12 05:26:35,485 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om1_1       | 2023-01-12 05:26:36,264 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2  | 2023-01-12 05:25:15,536 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1      | Waiting for the service scm1:9894
scm2_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:509)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1  | Caused by: java.util.concurrent.TimeoutException
s3g_1       | 2023-01-12 05:24:41,845 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1       | 2023-01-12 05:24:41,998 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
scm1_1      | STARTUP_MSG:   java = 11.0.14.1
om1_1       | 2023-01-12 05:26:36,327 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3  | 2023-01-12 05:25:02,144 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2  | 2023-01-12 05:25:15,550 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:15,551 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1     | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
om2_1       | 2023-01-12 05:26:39,005 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1       | 2023-01-12 05:26:39,016 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1       | 2023-01-12 05:26:39,017 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1       | 2023-01-12 05:26:40,780 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
s3g_1       | 2023-01-12 05:24:42,037 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1       | 2023-01-12 05:24:42,060 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1       | 2023-01-12 05:24:42,066 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3  | 2023-01-12 05:25:02,206 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2  | 2023-01-12 05:25:15,612 [EndpointStateMachine task thread for recon/172.18.0.12:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
scm2_1      | 2023-01-12 05:25:05,797 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3_1      | /************************************************************
recon_1     | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1      | ************************************************************/
scm1_1      | 2023-01-12 05:24:44,232 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1_1      | 2023-01-12 05:24:44,713 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1      | 2023-01-12 05:24:45,010 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
datanode_3  | 2023-01-12 05:25:02,309 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2  | java.net.SocketTimeoutException: Call From 4c4cfcfeca58/172.18.0.11 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.11:43926 remote=recon/172.18.0.12:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
scm2_1      | /************************************************************
scm2_1      | STARTUP_MSG: Starting StorageContainerManager
recon_1     | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1       | 2023-01-12 05:24:42,067 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1       | 2023-01-12 05:26:40,925 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
om2_1       | 2023-01-12 05:26:40,928 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
om2_1       | 2023-01-12 05:26:40,932 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm1_1      | 2023-01-12 05:24:45,176 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2_1      | STARTUP_MSG:   host = f2edb4bd00ab/172.18.0.3
scm2_1      | STARTUP_MSG:   args = [--bootstrap]
recon_1     | WARNING: All illegal access operations will be denied in a future release
s3g_1       | 2023-01-12 05:24:42,384 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1       | /************************************************************
om1_1       | 2023-01-12 05:26:36,576 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1       | 2023-01-12 05:26:36,643 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om1_1       | 2023-01-12 05:26:36,846 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1       | 2023-01-12 05:26:37,217 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3  | 2023-01-12 05:25:02,345 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
scm1_1      | 2023-01-12 05:24:45,547 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
scm2_1      | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm3_1      | STARTUP_MSG: Starting StorageContainerManager
recon_1     | 2023-01-12 05:24:44,956 [main] INFO reflections.Reflections: Reflections took 506 ms to scan 1 urls, producing 17 keys and 54 values 
datanode_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
s3g_1       | STARTUP_MSG: Starting Gateway
om3_1       | 2023-01-12 05:26:34,777 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om3_1       | 2023-01-12 05:26:34,976 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1       | 2023-01-12 05:26:40,938 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
om2_1       | 2023-01-12 05:26:40,978 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode_3  | 2023-01-12 05:25:02,345 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1       | STARTUP_MSG:   host = 04f97e40b747/172.18.0.8
recon_1     | 2023-01-12 05:24:48,548 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1     | 2023-01-12 05:24:49,856 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
om2_1       | 2023-01-12 05:26:40,988 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
om2_1       | 2023-01-12 05:26:41,021 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode_3  | 2023-01-12 05:25:02,345 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1      | 2023-01-12 05:24:45,552 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
scm2_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm2_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om3_1       | 2023-01-12 05:26:34,978 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1       | 2023-01-12 05:26:41,027 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
om1_1       | 2023-01-12 05:26:37,239 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
datanode_3  | 2023-01-12 05:25:02,665 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1      | 2023-01-12 05:24:47,010 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1      | 2023-01-12 05:24:48,211 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1_1      | 2023-01-12 05:24:48,251 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
s3g_1       | STARTUP_MSG:   args = []
recon_1     | 2023-01-12 05:24:56,684 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1     | 2023-01-12 05:24:58,334 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
om3_1       | 2023-01-12 05:26:35,046 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om2_1       | 2023-01-12 05:26:41,033 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
om2_1       | 2023-01-12 05:26:41,034 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode_3  | 2023-01-12 05:25:02,683 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1      | 2023-01-12 05:24:48,268 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
s3g_1       | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1       | 2023-01-12 05:26:36,055 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1       | 2023-01-12 05:26:41,323 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1       | 2023-01-12 05:26:37,242 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3  | 2023-01-12 05:25:02,797 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1  | 	... 1 more
scm1_1      | 2023-01-12 05:24:48,288 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
s3g_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
om3_1       | 2023-01-12 05:26:36,156 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1       | 2023-01-12 05:26:41,333 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1       | 2023-01-12 05:26:41,336 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1       | 2023-01-12 05:26:37,243 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1       | 2023-01-12 05:26:37,243 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1_1      | 2023-01-12 05:24:48,288 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
s3g_1       | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
om3_1       | 2023-01-12 05:26:36,391 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om2_1       | 2023-01-12 05:26:41,338 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1       | 2023-01-12 05:26:41,364 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1       | 2023-01-12 05:26:41,398 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@39ab5ef7[Not completed]
om2_1       | 2023-01-12 05:26:41,399 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
scm1_1      | 2023-01-12 05:24:48,300 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
datanode_2  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
s3g_1       | STARTUP_MSG:   java = 11.0.14.1
s3g_1       | ************************************************************/
recon_1     | 2023-01-12 05:24:58,352 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.001 seconds to initialized 0 records to KEY_CONTAINER table
recon_1     | 2023-01-12 05:24:58,428 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1     | 2023-01-12 05:24:58,554 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
om3_1       | 2023-01-12 05:26:36,456 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
datanode_1  | 2023-01-12 05:25:11,681 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:11,682 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:26:37,243 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1       | 2023-01-12 05:26:41,428 [om2-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xbca59d38] REGISTERED
scm1_1      | 2023-01-12 05:24:48,301 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1_1      | 2023-01-12 05:24:48,348 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1      | STARTUP_MSG:   host = 7d0c64aff313/172.18.0.9
om3_1       | 2023-01-12 05:26:36,705 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
recon_1     | 2023-01-12 05:24:58,608 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1     | 2023-01-12 05:25:04,413 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1     | 2023-01-12 05:25:04,481 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1       | 2023-01-12 05:26:37,245 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
datanode_1  | 2023-01-12 05:25:11,683 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:12,682 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1      | 2023-01-12 05:24:48,351 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
s3g_1       | 2023-01-12 05:24:42,417 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
scm2_1      | STARTUP_MSG:   java = 11.0.14.1
scm2_1      | ************************************************************/
om3_1       | 2023-01-12 05:26:37,190 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
recon_1     | 2023-01-12 05:25:04,573 [main] INFO util.log: Logging initialized @28777ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1     | 2023-01-12 05:25:05,344 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om1_1       | 2023-01-12 05:26:37,253 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1       | 2023-01-12 05:26:41,446 [om2-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xbca59d38] BIND: 0.0.0.0/0.0.0.0:0
datanode_1  | 2023-01-12 05:25:12,683 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:12,683 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1      | 2023-01-12 05:24:48,352 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
s3g_1       | 2023-01-12 05:24:42,525 [main] INFO s3.Gateway: Starting Ozone S3 gateway
scm2_1      | 2023-01-12 05:25:05,846 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3_1      | STARTUP_MSG:   args = [--bootstrap]
recon_1     | 2023-01-12 05:25:05,389 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1     | 2023-01-12 05:25:05,419 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1       | 2023-01-12 05:26:37,254 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
datanode_3  | 2023-01-12 05:25:02,803 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1  | 2023-01-12 05:25:13,683 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:13,685 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1      | 2023-01-12 05:24:48,490 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
s3g_1       | 2023-01-12 05:24:43,023 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1       | 2023-01-12 05:24:43,808 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1       | 2023-01-12 05:24:43,810 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
recon_1     | 2023-01-12 05:25:05,422 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
om2_1       | 2023-01-12 05:26:41,448 [om2-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xbca59d38, L:/0.0.0.0:36443] ACTIVE
om1_1       | 2023-01-12 05:26:37,256 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
datanode_3  | 2023-01-12 05:25:02,817 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1  | 2023-01-12 05:25:13,685 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:14,684 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1      | 2023-01-12 05:24:48,644 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1_1      | 2023-01-12 05:24:48,655 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm2_1      | 2023-01-12 05:25:06,013 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1      | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1       | 2023-01-12 05:26:41,519 [main] INFO om.OzoneManager: Creating RPC Server
om1_1       | 2023-01-12 05:26:37,317 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3  | 2023-01-12 05:25:02,869 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@329dc214{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1  | 2023-01-12 05:25:14,685 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
scm1_1      | 2023-01-12 05:24:51,467 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
scm2_1      | 2023-01-12 05:25:06,090 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
om3_1       | 2023-01-12 05:26:37,201 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1       | 2023-01-12 05:26:37,207 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
recon_1     | 2023-01-12 05:25:05,430 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1       | 2023-01-12 05:26:41,545 [pool-26-thread-1] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1       | 2023-01-12 05:26:41,574 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1       | 2023-01-12 05:26:37,331 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3  | 2023-01-12 05:25:02,876 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@296edc75{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1  | 2023-01-12 05:25:14,686 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
scm1_1      | 2023-01-12 05:24:51,627 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2_1      | 2023-01-12 05:25:06,105 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
om3_1       | 2023-01-12 05:26:37,209 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1       | 2023-01-12 05:26:37,211 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1     | 2023-01-12 05:25:05,430 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1     | 2023-01-12 05:25:05,987 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
om2_1       | 2023-01-12 05:26:41,579 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1       | 2023-01-12 05:26:37,333 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3  | 2023-01-12 05:25:03,935 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@46a8c2b4{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-13035282012048062454/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3  | 2023-01-12 05:25:03,964 [main] INFO server.AbstractConnector: Started ServerConnector@3db65c0d{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3  | 2023-01-12 05:25:03,967 [main] INFO server.Server: Started @29980ms
datanode_2  | 	at com.sun.proxy.$Proxy42.submitRequest(Unknown Source)
scm1_1      | 2023-01-12 05:24:53,435 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
scm2_1      | 2023-01-12 05:25:06,224 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
recon_1     | 2023-01-12 05:25:06,824 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1     | 2023-01-12 05:25:06,868 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1     | 2023-01-12 05:25:06,888 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1     | 2023-01-12 05:25:06,955 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1       | 2023-01-12 05:26:39,032 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode_1  | 2023-01-12 05:25:14,739 [EndpointStateMachine task thread for recon/172.18.0.12:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1  | java.net.SocketTimeoutException: Call From d9a6a467ccde/172.18.0.6 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.6:59664 remote=recon/172.18.0.12:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm1_1      | 2023-01-12 05:24:53,449 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
scm2_1      | 2023-01-12 05:25:06,230 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
recon_1     | 2023-01-12 05:25:06,955 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
s3g_1       | 2023-01-12 05:24:44,048 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1       | 2023-01-12 05:24:44,051 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1       | 2023-01-12 05:24:44,198 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1       | 2023-01-12 05:26:39,107 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
scm1_1      | 2023-01-12 05:24:53,466 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm1_1      | 2023-01-12 05:24:53,496 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
recon_1     | 2023-01-12 05:25:08,250 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2023-01-12 05:25:08,502 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2023-01-12 05:25:08,659 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1     | 2023-01-12 05:25:08,667 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1     | 2023-01-12 05:25:08,817 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2023-01-12 05:25:09,131 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
om1_1       | 2023-01-12 05:26:39,108 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om3_1       | 2023-01-12 05:26:37,212 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1       | 2023-01-12 05:26:37,214 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
recon_1     | 2023-01-12 05:25:09,225 [main] INFO reflections.Reflections: Reflections took 80 ms to scan 3 urls, producing 121 keys and 272 values 
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.11:43926 remote=recon/172.18.0.12:9891]
datanode_1  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
om3_1       | 2023-01-12 05:26:37,241 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1       | 2023-01-12 05:26:37,244 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
s3g_1       | 2023-01-12 05:24:44,199 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3  | 2023-01-12 05:25:03,996 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3  | 2023-01-12 05:25:03,996 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3  | 2023-01-12 05:25:03,998 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
recon_1     | 2023-01-12 05:25:09,302 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
om1_1       | 2023-01-12 05:26:39,109 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
om3_1       | 2023-01-12 05:26:37,255 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1       | 2023-01-12 05:26:37,403 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
s3g_1       | 2023-01-12 05:24:44,206 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1       | 2023-01-12 05:24:44,375 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a9f4771{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1       | 2023-01-12 05:24:44,381 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c0d7c83{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1       | WARNING: An illegal reflective access operation has occurred
recon_1     | 2023-01-12 05:25:09,372 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3_1      | STARTUP_MSG:   java = 11.0.14.1
om1_1       | 2023-01-12 05:26:39,112 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
om1_1       | 2023-01-12 05:26:39,246 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
om1_1       | 2023-01-12 05:26:39,247 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
om3_1       | 2023-01-12 05:26:37,437 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1       | 2023-01-12 05:26:37,444 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
s3g_1       | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
datanode_3  | 2023-01-12 05:25:04,033 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3  | 2023-01-12 05:25:04,173 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@30261a1d] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3  | 2023-01-12 05:25:04,741 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.18.0.12:9891
recon_1     | 2023-01-12 05:25:09,384 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
scm3_1      | ************************************************************/
om1_1       | 2023-01-12 05:26:39,290 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
om1_1       | 2023-01-12 05:26:39,298 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
scm2_1      | 2023-01-12 05:25:06,557 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863, nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863]
om3_1       | 2023-01-12 05:26:39,438 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
om3_1       | 2023-01-12 05:26:39,741 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
s3g_1       | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
s3g_1       | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
datanode_2  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
recon_1     | 2023-01-12 05:25:09,390 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm3_1      | 2023-01-12 05:25:35,574 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
om1_1       | 2023-01-12 05:26:39,307 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
om1_1       | 2023-01-12 05:26:39,310 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm2_1      | 2023-01-12 05:25:09,246 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f2edb4bd00ab/172.18.0.3 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1       | 2023-01-12 05:26:39,744 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
om3_1       | 2023-01-12 05:26:39,749 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode_3  | 2023-01-12 05:25:05,297 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3  | 2023-01-12 05:25:07,504 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:07,510 [EndpointStateMachine task thread for recon/172.18.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.12:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:07,510 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 2023-01-12 05:25:09,445 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
om1_1       | 2023-01-12 05:26:39,443 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm2_1      | 2023-01-12 05:25:11,248 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f2edb4bd00ab/172.18.0.3 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1       | 2023-01-12 05:26:39,756 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
om3_1       | 2023-01-12 05:26:39,794 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode_3  | 2023-01-12 05:25:07,510 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:26:41,579 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1       | 2023-01-12 05:26:41,580 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1       | 2023-01-12 05:26:41,580 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1     | 2023-01-12 05:25:09,496 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
om1_1       | 2023-01-12 05:26:39,445 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3_1      | 2023-01-12 05:25:35,731 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1      | 2023-01-12 05:25:13,250 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f2edb4bd00ab/172.18.0.3 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
datanode_3  | 2023-01-12 05:25:08,511 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:08,512 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:08,513 [EndpointStateMachine task thread for recon/172.18.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.12:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:08,514 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 2023-01-12 05:25:09,595 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
datanode_1  | 	at com.sun.proxy.$Proxy42.submitRequest(Unknown Source)
om1_1       | 2023-01-12 05:26:39,448 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
scm2_1      | 2023-01-12 05:25:15,251 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f2edb4bd00ab/172.18.0.3 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1      | 2023-01-12 05:25:17,253 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f2edb4bd00ab/172.18.0.3 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1      | 2023-01-12 05:25:19,463 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f98693f9-b2e2-4085-a3fc-32934ea8db14 is not the leader. Could not determine the leader node.
datanode_3  | 2023-01-12 05:25:09,512 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
recon_1     | 2023-01-12 05:25:09,673 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
om1_1       | 2023-01-12 05:26:39,451 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1      | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm1_1      | 2023-01-12 05:24:53,546 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm3_1      | 2023-01-12 05:25:35,828 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
datanode_2  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1     | 2023-01-12 05:25:09,824 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om1_1       | 2023-01-12 05:26:39,479 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm2_1      | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
scm1_1      | 2023-01-12 05:24:53,548 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
scm3_1      | 2023-01-12 05:25:35,838 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
datanode_3  | 2023-01-12 05:25:09,513 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
om2_1       | 2023-01-12 05:26:41,581 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1       | 2023-01-12 05:26:41,619 [pool-26-thread-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1     | 2023-01-12 05:25:09,824 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
om1_1       | 2023-01-12 05:26:39,544 [om1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xfc19d9a8] REGISTERED
scm2_1      | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
scm1_1      | 2023-01-12 05:24:53,623 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode_3  | 2023-01-12 05:25:09,513 [EndpointStateMachine task thread for recon/172.18.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.12:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:09,514 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | 2023-01-12 05:26:39,802 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
om3_1       | 2023-01-12 05:26:39,867 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
s3g_1       | WARNING: All illegal access operations will be denied in a future release
om2_1       | 2023-01-12 05:26:41,633 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1       | 2023-01-12 05:26:41,846 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1     | 2023-01-12 05:25:09,964 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
om1_1       | 2023-01-12 05:26:39,548 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@39ab5ef7[Not completed]
scm2_1      | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
scm1_1      | 2023-01-12 05:24:53,644 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
datanode_3  | 2023-01-12 05:25:10,513 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:10,514 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1       | Jan 12, 2023 5:25:03 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1       | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1       | 
om2_1       | 2023-01-12 05:26:41,856 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1       | 2023-01-12 05:26:42,034 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1       | 2023-01-12 05:26:42,061 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1       | 2023-01-12 05:26:42,072 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1      | 2023-01-12 05:24:53,705 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode_3  | 2023-01-12 05:25:10,514 [EndpointStateMachine task thread for recon/172.18.0.12:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.12:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:10,517 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
s3g_1       | 2023-01-12 05:25:03,515 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@577bfadb{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-2985207324116826395/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1       | 2023-01-12 05:25:03,599 [main] INFO server.AbstractConnector: Started ServerConnector@54504ecd{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1       | 2023-01-12 05:25:03,599 [main] INFO server.Server: Started @30087ms
om2_1       | 2023-01-12 05:26:42,872 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1       | 2023-01-12 05:26:42,876 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1      | 2023-01-12 05:24:53,710 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode_3  | 2023-01-12 05:25:11,514 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:11,517 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | 2023-01-12 05:26:39,876 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
datanode_2  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
s3g_1       | 2023-01-12 05:25:03,601 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1       | 2023-01-12 05:25:03,601 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om2_1       | 2023-01-12 05:26:42,884 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1       | 2023-01-12 05:26:42,900 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1       | 2023-01-12 05:26:42,908 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1       | 2023-01-12 05:26:44,276 [main] INFO reflections.Reflections: Reflections took 2659 ms to scan 8 urls, producing 23 keys and 545 values [using 2 cores]
om2_1       | 2023-01-12 05:26:44,852 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1       | 2023-01-12 05:26:39,884 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2  | 2023-01-12 05:25:16,537 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 2023-01-12 05:26:44,893 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1       | 2023-01-12 05:26:46,743 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1_1      | 2023-01-12 05:24:54,524 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1_1      | 2023-01-12 05:24:54,531 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1_1      | 2023-01-12 05:24:54,531 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om3_1       | 2023-01-12 05:26:39,888 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode_2  | 2023-01-12 05:25:16,551 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 2023-01-12 05:25:10,036 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
scm2_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
scm2_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm2_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
scm1_1      | 2023-01-12 05:24:54,532 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1      | 2023-01-12 05:24:54,699 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1      | 2023-01-12 05:24:55,066 [main] INFO server.RaftServer: f98693f9-b2e2-4085-a3fc-32934ea8db14: addNew group-4D43FF37764E:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|priority:0|startupRole:FOLLOWER] returns group-4D43FF37764E:java.util.concurrent.CompletableFuture@1c65121[Not completed]
om3_1       | 2023-01-12 05:26:40,160 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2  | 2023-01-12 05:25:17,538 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:17,553 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.6:59664 remote=recon/172.18.0.12:9891]
datanode_1  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
om1_1       | 2023-01-12 05:26:39,559 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1       | 2023-01-12 05:26:39,563 [om1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xfc19d9a8] BIND: 0.0.0.0/0.0.0.0:0
om2_1       | 2023-01-12 05:26:46,835 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1       | 2023-01-12 05:26:46,835 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1       | 2023-01-12 05:26:47,146 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/172.18.0.7:9862
datanode_2  | 2023-01-12 05:25:18,540 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 2023-01-12 05:25:10,037 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
s3g_1       | 2023-01-12 05:25:03,616 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
s3g_1       | 2023-01-12 05:33:58,596 [qtp384515747-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1       | 2023-01-12 05:33:58,624 [qtp384515747-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
scm2_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_3  | 2023-01-12 05:25:11,518 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:12,515 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | 2023-01-12 05:26:40,169 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2  | 2023-01-12 05:25:18,554 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:26:39,614 [om1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xfc19d9a8, L:/0.0.0.0:38739] ACTIVE
recon_1     | 2023-01-12 05:25:10,532 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
datanode_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
om2_1       | 2023-01-12 05:26:47,150 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1       | 2023-01-12 05:26:47,163 [om2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om3_1       | 2023-01-12 05:26:40,170 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
scm3_1      | 2023-01-12 05:25:36,001 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
s3g_1       | 2023-01-12 05:33:58,635 [qtp384515747-20] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
s3g_1       | 2023-01-12 05:33:58,636 [qtp384515747-20] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
scm2_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1      | , while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1      | 2023-01-12 05:25:21,473 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f2edb4bd00ab/172.18.0.3 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
datanode_2  | 2023-01-12 05:25:19,541 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | 2023-01-12 05:26:40,170 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1       | 2023-01-12 05:26:40,195 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1       | 2023-01-12 05:26:39,650 [main] INFO om.OzoneManager: Creating RPC Server
s3g_1       | 2023-01-12 05:34:00,650 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-8718359029, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:12,518 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 2023-01-12 05:25:10,541 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om2_1       | 2023-01-12 05:26:47,177 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om2
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2  | 2023-01-12 05:25:19,555 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:20,542 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:20,556 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:26:39,737 [pool-26-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
s3g_1       | 2023-01-12 05:34:04,588 [qtp384515747-19] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-2923742260, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:12,519 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 2023-01-12 05:25:10,715 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1       | 2023-01-12 05:26:47,222 [om2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
datanode_1  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
scm1_1      | 2023-01-12 05:24:55,107 [f98693f9-b2e2-4085-a3fc-32934ea8db14-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x76dd0264] REGISTERED
scm1_1      | 2023-01-12 05:24:55,236 [f98693f9-b2e2-4085-a3fc-32934ea8db14-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x76dd0264] BIND: 0.0.0.0/0.0.0.0:0
datanode_2  | 2023-01-12 05:25:20,562 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
datanode_2  | java.net.SocketTimeoutException: Call From 4c4cfcfeca58/172.18.0.11 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.11:39284 remote=scm1/172.18.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
om3_1       | 2023-01-12 05:26:40,261 [om3-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x377e89f4] REGISTERED
om1_1       | 2023-01-12 05:26:39,756 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
s3g_1       | 2023-01-12 05:34:05,719 [qtp384515747-21] WARN impl.MetricsSystemImpl: S3Gateway metrics system already initialized!
datanode_3  | 2023-01-12 05:25:13,516 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 2023-01-12 05:25:10,716 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
om2_1       | 2023-01-12 05:26:47,268 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1       | 2023-01-12 05:26:47,356 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1      | 2023-01-12 05:24:55,249 [f98693f9-b2e2-4085-a3fc-32934ea8db14-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x76dd0264, L:/0.0.0.0:42821] ACTIVE
scm1_1      | 2023-01-12 05:24:55,377 [pool-2-thread-1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14: new RaftServerImpl for group-4D43FF37764E:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om3_1       | 2023-01-12 05:26:40,284 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@57bd6a8f[Not completed]
om3_1       | 2023-01-12 05:26:40,319 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1       | 2023-01-12 05:26:39,766 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
s3g_1       | 2023-01-12 05:34:06,055 [qtp384515747-21] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3  | 2023-01-12 05:25:13,519 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 2023-01-12 05:25:10,719 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
om2_1       | 2023-01-12 05:26:47,356 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
scm1_1      | 2023-01-12 05:24:55,394 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm2_1      | 2023-01-12 05:25:23,517 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-b40a0ffb-806c-459e-acb9-4d43ff37764e, SCMID 296a2a14-8256-49c4-bc3a-84aaa5fb66c6
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om3_1       | 2023-01-12 05:26:40,321 [om3-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x377e89f4] BIND: 0.0.0.0/0.0.0.0:0
om3_1       | 2023-01-12 05:26:40,339 [om3-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x377e89f4, L:/0.0.0.0:39727] ACTIVE
om1_1       | 2023-01-12 05:26:39,766 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
s3g_1       | 2023-01-12 05:34:16,542 [qtp384515747-19] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-7498891659, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:13,520 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 2023-01-12 05:25:10,766 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2fd39436{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1       | 2023-01-12 05:26:47,372 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1       | 2023-01-12 05:26:47,378 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1      | 2023-01-12 05:24:55,395 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1      | 2023-01-12 05:24:55,402 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om3_1       | 2023-01-12 05:26:40,476 [pool-26-thread-1] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1       | 2023-01-12 05:26:40,486 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1       | 2023-01-12 05:26:39,767 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
s3g_1       | 2023-01-12 05:34:17,284 [qtp384515747-19] INFO rpc.RpcClient: Creating Bucket: s3v/ozone-test-qwhhyijwyi, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:14,517 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 2023-01-12 05:25:10,767 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@251d7fdd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
om2_1       | 2023-01-12 05:26:47,397 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1      | 2023-01-12 05:25:23,517 [main] INFO server.StorageContainerManager: Primary SCM Node ID f98693f9-b2e2-4085-a3fc-32934ea8db14
scm2_1      | 2023-01-12 05:25:23,527 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
datanode_2  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
om3_1       | 2023-01-12 05:26:40,501 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1       | 2023-01-12 05:26:40,502 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1       | 2023-01-12 05:26:39,767 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
s3g_1       | 2023-01-12 05:34:20,159 [qtp384515747-18] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ufntzifvoh, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:14,520 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:14,520 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:15,518 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1  | 2023-01-12 05:25:15,685 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 2023-01-12 05:25:14,140 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@dc3eda6{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-10542295327182920073/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
scm3_1      | 2023-01-12 05:25:36,001 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
scm3_1      | 2023-01-12 05:25:36,314 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2/172.18.0.3:9863, nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863]
om1_1       | 2023-01-12 05:26:39,768 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1       | 2023-01-12 05:26:39,822 [pool-26-thread-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1       | 2023-01-12 05:26:39,823 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1       | 2023-01-12 05:26:39,985 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1       | 2023-01-12 05:26:39,991 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1       | 2023-01-12 05:26:40,161 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1       | 2023-01-12 05:26:40,220 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
recon_1     | 2023-01-12 05:25:14,164 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@66f16742{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1     | 2023-01-12 05:25:14,164 [Listener at 0.0.0.0/9891] INFO server.Server: Started @38368ms
recon_1     | 2023-01-12 05:25:14,171 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1     | 2023-01-12 05:25:14,171 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1       | 2023-01-12 05:26:40,220 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1      | /************************************************************
scm2_1      | SHUTDOWN_MSG: Shutting down StorageContainerManager at f2edb4bd00ab/172.18.0.3
scm2_1      | ************************************************************/
scm1_1      | 2023-01-12 05:24:55,403 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1      | 2023-01-12 05:24:55,427 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
s3g_1       | 2023-01-12 05:34:31,999 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-7112304483, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm3_1      | 2023-01-12 05:25:37,003 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-b40a0ffb-806c-459e-acb9-4d43ff37764e, SCMID 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf
scm3_1      | 2023-01-12 05:25:37,003 [main] INFO server.StorageContainerManager: Primary SCM Node ID f98693f9-b2e2-4085-a3fc-32934ea8db14
scm3_1      | 2023-01-12 05:25:37,009 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
om1_1       | 2023-01-12 05:26:40,921 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1  | 2023-01-12 05:25:15,686 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:15,687 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:16,686 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1      | 2023-01-12 05:24:55,432 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1       | 2023-01-12 05:26:40,502 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1       | 2023-01-12 05:26:40,502 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1       | 2023-01-12 05:26:40,503 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1       | 2023-01-12 05:26:40,511 [main] INFO om.OzoneManager: Creating RPC Server
om1_1       | 2023-01-12 05:26:40,932 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1       | 2023-01-12 05:26:47,416 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1       | 2023-01-12 05:26:47,416 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1       | 2023-01-12 05:26:47,440 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
datanode_3  | 2023-01-12 05:25:15,520 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:15,521 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/172.18.0.2:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:15,554 [EndpointStateMachine task thread for recon/172.18.0.12:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
om3_1       | 2023-01-12 05:26:40,570 [pool-26-thread-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1       | 2023-01-12 05:26:40,580 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1       | 2023-01-12 05:26:40,639 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
om1_1       | 2023-01-12 05:26:40,934 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1       | 2023-01-12 05:26:40,938 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1       | 2023-01-12 05:26:40,941 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
s3g_1       | 2023-01-12 05:34:32,941 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-3900182019, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
om3_1       | 2023-01-12 05:26:40,642 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
scm2_1      | 2023-01-12 05:25:25,170 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2_1      | /************************************************************
om2_1       | 2023-01-12 05:26:47,444 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1       | 2023-01-12 05:26:47,445 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3_1      | /************************************************************
scm3_1      | SHUTDOWN_MSG: Shutting down StorageContainerManager at 7d0c64aff313/172.18.0.9
scm3_1      | ************************************************************/
scm3_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1       | 2023-01-12 05:26:40,704 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
s3g_1       | 2023-01-12 05:34:33,875 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-3726753216, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:34:34,774 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-3726753216, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm3_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1_1      | 2023-01-12 05:24:55,483 [pool-2-thread-1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: ConfigurationManager, init=-1: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1_1      | 2023-01-12 05:24:55,518 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1      | 2023-01-12 05:24:55,592 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1      | 2023-01-12 05:24:55,593 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1      | 2023-01-12 05:24:55,779 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
om3_1       | 2023-01-12 05:26:40,729 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
s3g_1       | 2023-01-12 05:34:37,028 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-5026327384, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:34:45,354 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-7662104206, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm3_1      | 2023-01-12 05:25:39,970 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
datanode_1  | 2023-01-12 05:25:16,687 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:17,688 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | java.net.SocketTimeoutException: Call From d407c6d45367/172.18.0.4 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.4:48678 remote=recon/172.18.0.12:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om3_1       | 2023-01-12 05:26:40,744 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1       | 2023-01-12 05:26:47,456 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1       | 2023-01-12 05:26:47,459 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3_1      | /************************************************************
scm2_1      | STARTUP_MSG: Starting StorageContainerManager
datanode_1  | 2023-01-12 05:25:17,688 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:18,689 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:18,689 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:19,689 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | 2023-01-12 05:26:41,385 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1       | 2023-01-12 05:26:47,476 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
s3g_1       | 2023-01-12 05:34:46,355 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-0967037321, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:34:52,427 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-5351982924, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | STARTUP_MSG:   host = f2edb4bd00ab/172.18.0.3
om1_1       | 2023-01-12 05:26:42,228 [main] INFO reflections.Reflections: Reflections took 2420 ms to scan 8 urls, producing 23 keys and 545 values [using 2 cores]
scm1_1      | 2023-01-12 05:24:55,805 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
datanode_3  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
om1_1       | 2023-01-12 05:26:42,815 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1       | 2023-01-12 05:26:47,483 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1       | 2023-01-12 05:26:41,392 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1       | 2023-01-12 05:26:41,405 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1       | 2023-01-12 05:26:41,417 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1       | 2023-01-12 05:26:41,422 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1      | STARTUP_MSG:   args = []
s3g_1       | 2023-01-12 05:34:58,340 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-2191502675, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
recon_1     | 2023-01-12 05:25:14,175 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
om1_1       | 2023-01-12 05:26:42,879 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1       | 2023-01-12 05:26:45,387 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1       | 2023-01-12 05:26:45,437 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1       | 2023-01-12 05:26:45,437 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
scm1_1      | 2023-01-12 05:24:55,806 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1      | 2023-01-12 05:24:56,533 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1      | 2023-01-12 05:24:56,546 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
scm2_1      | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1       | Jan 12, 2023 5:35:00 AM org.glassfish.jersey.internal.Errors logErrors
recon_1     | 2023-01-12 05:25:14,175 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1     | 2023-01-12 05:25:14,187 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
datanode_1  | 2023-01-12 05:25:19,690 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | 2023-01-12 05:26:43,140 [main] INFO reflections.Reflections: Reflections took 2436 ms to scan 8 urls, producing 23 keys and 545 values [using 2 cores]
om3_1       | 2023-01-12 05:26:43,786 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1       | 2023-01-12 05:26:43,864 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1       | 2023-01-12 05:26:46,014 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3_1      | STARTUP_MSG: Starting StorageContainerManager
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2  | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
s3g_1       | WARNING: The following warnings have been detected: WARNING: Unknown HK2 failure detected:
recon_1     | 2023-01-12 05:25:14,197 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
om2_1       | 2023-01-12 05:26:47,486 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2023-01-12 05:25:20,691 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:20,691 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:20,700 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
datanode_1  | java.net.SocketTimeoutException: Call From d9a6a467ccde/172.18.0.6 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.6:56398 remote=scm1/172.18.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
scm3_1      | STARTUP_MSG:   host = 7d0c64aff313/172.18.0.9
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
scm2_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
s3g_1       | MultiException stack 1 of 1
recon_1     | 2023-01-12 05:25:14,198 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1     | 2023-01-12 05:25:14,198 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
scm1_1      | 2023-01-12 05:24:56,551 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1      | 2023-01-12 05:24:56,558 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
s3g_1       | javax.ws.rs.WebApplicationException: The authorization header you provided is invalid.
recon_1     | 2023-01-12 05:25:14,198 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1     | 2023-01-12 05:25:14,207 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
scm3_1      | STARTUP_MSG:   args = []
om1_1       | 2023-01-12 05:26:45,654 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/172.18.0.5:9862
om1_1       | 2023-01-12 05:26:45,657 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1       | 2023-01-12 05:26:45,668 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
om1_1       | 2023-01-12 05:26:45,691 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 8@om1
om1_1       | 2023-01-12 05:26:45,786 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
scm2_1      | STARTUP_MSG:   java = 11.0.14.1
s3g_1       | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.wrapOS3Exception(OzoneClientProducer.java:141)
recon_1     | 2023-01-12 05:25:16,328 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm2,nodeAddress=scm2/172.18.0.3:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-12 05:25:18,330 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm3,nodeAddress=scm3/172.18.0.9:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
scm3_1      | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1       | 2023-01-12 05:26:46,100 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1       | 2023-01-12 05:26:46,100 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1       | 2023-01-12 05:26:46,442 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/172.18.0.10:9862
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om1_1       | 2023-01-12 05:26:45,822 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1      | ************************************************************/
scm2_1      | 2023-01-12 05:25:25,180 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2_1      | 2023-01-12 05:25:25,227 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2023-01-12 05:25:20,371 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:f98693f9-b2e2-4085-a3fc-32934ea8db14 is not the leader. Could not determine the leader node.
scm3_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1_1      | 2023-01-12 05:24:56,568 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3  | 	at com.sun.proxy.$Proxy42.submitRequest(Unknown Source)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om1_1       | 2023-01-12 05:26:45,877 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1       | 2023-01-12 05:26:47,487 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1      | 2023-01-12 05:25:25,255 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
s3g_1       | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.getSignature(OzoneClientProducer.java:102)
recon_1     | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1     | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
scm1_1      | 2023-01-12 05:24:56,570 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e does not exist. Creating ...
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 2023-01-12 05:26:45,881 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
scm2_1      | 2023-01-12 05:25:25,265 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
scm1_1      | 2023-01-12 05:24:56,599 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/in_use.lock acquired by nodename 13@7f47dfd12ecd
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 2023-01-12 05:26:45,892 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
om2_1       | 2023-01-12 05:26:47,538 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2_1      | 2023-01-12 05:25:25,304 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
scm2_1      | 2023-01-12 05:25:25,305 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
scm2_1      | 2023-01-12 05:25:26,193 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62741)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
datanode_2  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.11:39284 remote=scm1/172.18.0.2:9861]
om1_1       | 2023-01-12 05:26:45,898 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
scm3_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
scm3_1      | STARTUP_MSG:   java = 11.0.14.1
scm2_1      | 2023-01-12 05:25:26,600 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
scm1_1      | 2023-01-12 05:24:56,729 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e has been successfully formatted.
om3_1       | 2023-01-12 05:26:46,447 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1       | 2023-01-12 05:26:46,460 [om3-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
datanode_2  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
scm3_1      | ************************************************************/
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm2_1      | 2023-01-12 05:25:27,002 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
scm1_1      | 2023-01-12 05:24:56,769 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1       | 2023-01-12 05:26:46,475 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@om3
om3_1       | 2023-01-12 05:26:46,560 [om3-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
datanode_2  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1  | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
scm3_1      | 2023-01-12 05:25:39,978 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1       | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:88)
scm2_1      | 2023-01-12 05:25:27,003 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm1_1      | 2023-01-12 05:24:56,892 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
scm3_1      | 2023-01-12 05:25:40,022 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
s3g_1       | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:78)
scm2_1      | 2023-01-12 05:25:27,073 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
scm1_1      | 2023-01-12 05:24:56,903 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1       | 2023-01-12 05:26:46,579 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1       | 2023-01-12 05:26:46,638 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
om1_1       | 2023-01-12 05:26:45,917 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
om2_1       | 2023-01-12 05:26:47,540 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1      | 2023-01-12 05:25:40,074 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
s3g_1       | 	at org.jboss.weld.injection.producer.ProducerMethodProducer.produce(ProducerMethodProducer.java:100)
scm2_1      | 2023-01-12 05:25:27,088 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:296a2a14-8256-49c4-bc3a-84aaa5fb66c6
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1      | 2023-01-12 05:24:56,905 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1       | 2023-01-12 05:26:46,639 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1       | 2023-01-12 05:26:46,648 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
om1_1       | 2023-01-12 05:26:45,942 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1       | 2023-01-12 05:26:47,621 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1      | 2023-01-12 05:25:40,097 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3_1      | 2023-01-12 05:25:40,135 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
s3g_1       | 	at org.jboss.weld.injection.producer.AbstractMemberProducer.produce(AbstractMemberProducer.java:161)
scm2_1      | 2023-01-12 05:25:27,189 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1      | 2023-01-12 05:24:56,947 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1       | 2023-01-12 05:26:46,650 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
scm3_1      | 2023-01-12 05:25:40,136 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
scm3_1      | 2023-01-12 05:25:40,750 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1      | 2023-01-12 05:25:40,979 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1      | 2023-01-12 05:25:41,311 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
s3g_1       | 	at org.jboss.weld.bean.AbstractProducerBean.create(AbstractProducerBean.java:180)
scm1_1      | 2023-01-12 05:24:56,966 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1      | 2023-01-12 05:24:57,080 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1       | 2023-01-12 05:26:47,622 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1       | 2023-01-12 05:26:47,625 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1       | 2023-01-12 05:26:47,763 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1       | 2023-01-12 05:26:47,764 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1       | 2023-01-12 05:26:47,790 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1       | 2023-01-12 05:26:47,792 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1       | 2023-01-12 05:26:47,829 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1       | 2023-01-12 05:26:47,836 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
s3g_1       | 	at org.jboss.weld.context.unbound.DependentContextImpl.get(DependentContextImpl.java:70)
scm1_1      | 2023-01-12 05:24:57,089 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1      | 2023-01-12 05:24:57,143 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e
scm1_1      | 2023-01-12 05:24:57,149 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1      | 2023-01-12 05:24:57,151 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1       | 2023-01-12 05:26:47,836 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1       | 2023-01-12 05:26:47,870 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1       | 2023-01-12 05:26:47,897 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1       | 2023-01-12 05:26:47,915 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1       | 2023-01-12 05:26:47,922 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
datanode_2  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
scm3_1      | 2023-01-12 05:25:41,313 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
om1_1       | 2023-01-12 05:26:45,946 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1       | 2023-01-12 05:26:47,931 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1       | 2023-01-12 05:26:47,968 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
om2_1       | 2023-01-12 05:26:48,010 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
scm3_1      | 2023-01-12 05:25:41,421 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1       | 2023-01-12 05:26:46,016 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1       | 2023-01-12 05:26:46,023 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1       | 2023-01-12 05:26:46,024 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2_1      | 2023-01-12 05:25:27,263 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1      | 2023-01-12 05:25:41,440 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf
s3g_1       | 	at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
s3g_1       | 	at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
s3g_1       | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:785)
scm2_1      | 2023-01-12 05:25:27,265 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
datanode_3  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.4:48678 remote=recon/172.18.0.12:9891]
datanode_3  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
om3_1       | 2023-01-12 05:26:46,658 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1       | 2023-01-12 05:26:46,687 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1       | 2023-01-12 05:26:46,688 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1      | 2023-01-12 05:24:57,159 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1      | 2023-01-12 05:25:41,501 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2_1      | 2023-01-12 05:25:27,266 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2  | 2023-01-12 05:25:21,543 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | 2023-01-12 05:26:46,727 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
scm1_1      | 2023-01-12 05:24:57,163 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm3_1      | 2023-01-12 05:25:41,571 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1       | 2023-01-12 05:26:48,041 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
scm2_1      | 2023-01-12 05:25:27,266 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
om3_1       | 2023-01-12 05:26:46,733 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1       | 2023-01-12 05:26:46,736 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1       | 2023-01-12 05:26:46,747 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1       | 2023-01-12 05:26:46,750 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1      | 2023-01-12 05:24:57,163 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1       | 2023-01-12 05:26:48,048 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1       | 2023-01-12 05:26:48,243 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1       | 2023-01-12 05:26:48,245 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2  | 2023-01-12 05:25:21,557 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1      | 2023-01-12 05:24:57,170 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1      | 2023-01-12 05:25:41,573 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
om2_1       | 2023-01-12 05:26:48,422 [Listener at om2/9862] INFO util.log: Logging initialized @35414ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.6:56398 remote=scm1/172.18.0.2:9861]
om3_1       | 2023-01-12 05:26:46,764 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1       | 2023-01-12 05:26:46,778 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2_1      | 2023-01-12 05:25:27,267 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1     | , while invoking $Proxy43.submitRequest over nodeId=scm1,nodeAddress=scm1/172.18.0.2:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
scm1_1      | 2023-01-12 05:24:57,170 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1      | 2023-01-12 05:24:57,171 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1       | 2023-01-12 05:26:48,830 [Listener at om2/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
s3g_1       | 	at org.jboss.weld.manager.BeanManagerImpl.getInjectableReference(BeanManagerImpl.java:885)
om1_1       | 2023-01-12 05:26:46,032 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1       | 2023-01-12 05:26:46,033 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1       | 2023-01-12 05:26:46,046 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1       | 2023-01-12 05:26:46,069 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1       | 2023-01-12 05:26:46,071 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1      | 2023-01-12 05:25:41,573 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
om2_1       | 2023-01-12 05:26:48,864 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
datanode_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
s3g_1       | 	at org.jboss.weld.injection.FieldInjectionPoint.inject(FieldInjectionPoint.java:92)
om1_1       | 2023-01-12 05:26:46,073 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1     | 2023-01-12 05:25:22,373 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm2,nodeAddress=scm2/172.18.0.3:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-12 05:25:24,374 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm3,nodeAddress=scm3/172.18.0.9:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-12 05:25:26,604 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 4 pipelines from SCM.
scm1_1      | 2023-01-12 05:24:57,221 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3_1      | 2023-01-12 05:25:41,574 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
om2_1       | 2023-01-12 05:26:48,918 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
s3g_1       | 	at org.jboss.weld.util.Beans.injectBoundFields(Beans.java:358)
om1_1       | 2023-01-12 05:26:46,137 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
recon_1     | 2023-01-12 05:25:26,605 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
scm1_1      | 2023-01-12 05:24:57,221 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1      | 2023-01-12 05:25:27,267 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
om3_1       | 2023-01-12 05:26:46,780 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1       | 2023-01-12 05:26:46,784 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1       | 2023-01-12 05:26:46,848 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1       | 2023-01-12 05:26:48,927 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om2_1       | 2023-01-12 05:26:48,928 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1       | 	at org.jboss.weld.util.Beans.injectFieldsAndInitializers(Beans.java:369)
s3g_1       | 	at org.jboss.weld.injection.producer.ResourceInjector$1.proceed(ResourceInjector.java:70)
s3g_1       | 	at org.jboss.weld.injection.InjectionContextImpl.run(InjectionContextImpl.java:48)
s3g_1       | 	at org.jboss.weld.injection.producer.ResourceInjector.inject(ResourceInjector.java:72)
s3g_1       | 	at org.jboss.weld.injection.producer.BasicInjectionTarget.inject(BasicInjectionTarget.java:117)
s3g_1       | 	at org.glassfish.jersey.ext.cdi1x.internal.CdiComponentProvider$InjectionManagerInjectedCdiTarget.inject(CdiComponentProvider.java:665)
s3g_1       | 	at org.jboss.weld.bean.ManagedBean.create(ManagedBean.java:159)
scm3_1      | 2023-01-12 05:25:41,574 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2  | 2023-01-12 05:25:21,677 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-b40a0ffb-806c-459e-acb9-4d43ff37764e/DS-aece7091-cc10-4ef1-9213-ce2591ad7b7b/container.db to cache
om1_1       | 2023-01-12 05:26:46,140 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
om2_1       | 2023-01-12 05:26:48,928 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
scm3_1      | 2023-01-12 05:25:41,575 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
s3g_1       | 	at org.jboss.weld.context.unbound.DependentContextImpl.get(DependentContextImpl.java:70)
om1_1       | 2023-01-12 05:26:46,269 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1       | 2023-01-12 05:26:46,269 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
om2_1       | 2023-01-12 05:26:49,120 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
om2_1       | 2023-01-12 05:26:49,127 [Listener at om2/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm1_1      | 2023-01-12 05:24:57,395 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1      | 2023-01-12 05:24:57,452 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1      | 2023-01-12 05:24:57,454 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
datanode_3  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
scm3_1      | 2023-01-12 05:25:41,575 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
s3g_1       | 	at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
om1_1       | 2023-01-12 05:26:46,274 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
datanode_2  | 2023-01-12 05:25:21,679 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-b40a0ffb-806c-459e-acb9-4d43ff37764e/DS-aece7091-cc10-4ef1-9213-ce2591ad7b7b/container.db for volume DS-aece7091-cc10-4ef1-9213-ce2591ad7b7b
datanode_1  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
recon_1     | 2023-01-12 05:25:26,607 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 from SCM.
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
scm3_1      | 2023-01-12 05:25:41,578 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1      | 2023-01-12 05:25:41,579 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
s3g_1       | 	at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
om1_1       | 2023-01-12 05:26:46,320 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2023-01-12 05:25:21,681 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2  | 2023-01-12 05:25:21,688 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
scm1_1      | 2023-01-12 05:24:57,584 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO segmented.SegmentedRaftLogWorker: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1      | 2023-01-12 05:24:57,585 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO segmented.SegmentedRaftLogWorker: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
recon_1     | 2023-01-12 05:25:26,648 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 7b2ccc1c-57aa-4786-a2ba-0da58dd390a6, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6)34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11)cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:23.439Z[UTC]].
recon_1     | 2023-01-12 05:25:26,659 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=cd62190c-0472-48bc-9601-9e784e6bba4e from SCM.
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
scm3_1      | 2023-01-12 05:25:41,580 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3_1      | 2023-01-12 05:25:41,592 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1       | 2023-01-12 05:26:46,320 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1       | 2023-01-12 05:26:46,333 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1       | 2023-01-12 05:26:46,334 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om1_1       | 2023-01-12 05:26:46,343 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1       | 2023-01-12 05:26:46,352 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
recon_1     | 2023-01-12 05:25:26,662 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: cd62190c-0472-48bc-9601-9e784e6bba4e, Nodes: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:23.217Z[UTC]].
recon_1     | 2023-01-12 05:25:26,663 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc from SCM.
recon_1     | 2023-01-12 05:25:26,665 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:23.423Z[UTC]].
recon_1     | 2023-01-12 05:25:26,666 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=fb7937dd-8506-409c-a1e6-3cdc4fafc0d4 from SCM.
s3g_1       | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:785)
scm3_1      | 2023-01-12 05:25:41,595 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
s3g_1       | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:808)
s3g_1       | 	at org.jboss.weld.util.ForwardingBeanManager.getReference(ForwardingBeanManager.java:61)
s3g_1       | 	at org.jboss.weld.bean.builtin.BeanManagerProxy.getReference(BeanManagerProxy.java:85)
scm2_1      | 2023-01-12 05:25:27,268 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2_1      | 2023-01-12 05:25:27,270 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
recon_1     | 2023-01-12 05:25:26,668 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: fb7937dd-8506-409c-a1e6-3cdc4fafc0d4, Nodes: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:22.284Z[UTC]].
om2_1       | 2023-01-12 05:26:49,322 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
scm3_1      | 2023-01-12 05:25:41,596 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1       | 2023-01-12 05:26:46,352 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1       | 2023-01-12 05:26:46,368 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om3_1       | 2023-01-12 05:26:46,849 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1       | 	at org.glassfish.jersey.ext.cdi1x.internal.CdiUtil.getBeanReference(CdiUtil.java:129)
scm2_1      | 2023-01-12 05:25:27,271 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2_1      | 2023-01-12 05:25:27,272 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1      | 2023-01-12 05:25:27,283 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1     | 2023-01-12 05:25:26,668 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
datanode_3  | 2023-01-12 05:25:16,519 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:26:49,324 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
om2_1       | 2023-01-12 05:26:49,337 [Listener at om2/9862] INFO server.session: node0 Scavenging every 600000ms
datanode_2  | 2023-01-12 05:25:21,714 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
om1_1       | 2023-01-12 05:26:46,376 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1       | 2023-01-12 05:26:46,985 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
s3g_1       | 	at org.glassfish.jersey.ext.cdi1x.internal.AbstractCdiBeanSupplier$1.getInstance(AbstractCdiBeanSupplier.java:72)
scm1_1      | 2023-01-12 05:24:57,613 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: start as a follower, conf=-1: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1      | 2023-01-12 05:24:57,641 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2023-01-12 05:25:21,692 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 2023-01-12 05:25:26,670 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
datanode_3  | 2023-01-12 05:25:16,521 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:26:49,412 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@204b0f07{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1       | 2023-01-12 05:26:49,413 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7ffcb232{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2  | 2023-01-12 05:25:21,716 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
om1_1       | 2023-01-12 05:26:46,377 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1       | 2023-01-12 05:26:46,990 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
s3g_1       | 	at org.glassfish.jersey.ext.cdi1x.internal.AbstractCdiBeanSupplier._provide(AbstractCdiBeanSupplier.java:112)
s3g_1       | 	at org.glassfish.jersey.ext.cdi1x.internal.RequestScopedCdiBeanSupplier.get(RequestScopedCdiBeanSupplier.java:46)
s3g_1       | 	at org.glassfish.jersey.inject.hk2.InstanceSupplierFactoryBridge.provide(InstanceSupplierFactoryBridge.java:53)
s3g_1       | 	at org.jvnet.hk2.internal.FactoryCreator.create(FactoryCreator.java:129)
recon_1     | 2023-01-12 05:25:26,675 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_3  | 2023-01-12 05:25:17,521 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:17,522 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:18,522 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:26:46,380 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1       | 2023-01-12 05:26:46,380 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1       | 2023-01-12 05:26:46,996 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
s3g_1       | 	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)
scm1_1      | 2023-01-12 05:24:57,667 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO impl.RoleInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14: start f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState
scm1_1      | 2023-01-12 05:24:57,703 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1      | 2023-01-12 05:24:57,704 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-12 05:25:18,522 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:19,524 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:19,524 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:26:46,400 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
om3_1       | 2023-01-12 05:26:47,067 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2_1      | 2023-01-12 05:25:27,287 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
s3g_1       | 	at org.jvnet.hk2.internal.PerLookupContext.findOrCreate(PerLookupContext.java:46)
scm1_1      | 2023-01-12 05:24:57,705 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4D43FF37764E,id=f98693f9-b2e2-4085-a3fc-32934ea8db14
scm1_1      | 2023-01-12 05:24:57,735 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1      | 2023-01-12 05:24:57,751 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3_1      | 2023-01-12 05:25:42,123 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode_3  | 2023-01-12 05:25:20,525 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:26:50,042 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7c69e1e1{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-7908592301346847400/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om2_1       | 2023-01-12 05:26:50,111 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@4b7ab7ab{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
datanode_2  | 2023-01-12 05:25:21,770 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO server.RaftServer: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: start RPC server
om1_1       | 2023-01-12 05:26:46,452 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om3_1       | 2023-01-12 05:26:47,081 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1       | 2023-01-12 05:26:47,097 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1       | 	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)
scm1_1      | 2023-01-12 05:24:57,751 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
recon_1     | 2023-01-12 05:25:26,682 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1     | 2023-01-12 05:25:27,023 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.11:43926: output error
scm3_1      | 2023-01-12 05:25:42,467 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode_1  | 2023-01-12 05:25:21,693 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:26:50,112 [Listener at om2/9862] INFO server.Server: Started @37106ms
datanode_2  | 2023-01-12 05:25:21,772 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO server.GrpcService: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: GrpcService started, listening on 9858
om1_1       | 2023-01-12 05:26:46,460 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1       | 2023-01-12 05:26:47,102 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1       | 2023-01-12 05:26:47,133 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
s3g_1       | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:758)
scm1_1      | 2023-01-12 05:24:57,756 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1      | 2023-01-12 05:24:57,844 [main] INFO server.RaftServer: f98693f9-b2e2-4085-a3fc-32934ea8db14: start RPC server
scm3_1      | 2023-01-12 05:25:42,469 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode_1  | 2023-01-12 05:25:21,759 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-b40a0ffb-806c-459e-acb9-4d43ff37764e/DS-e5694648-5720-42a9-987c-c74e0f459bb6/container.db to cache
datanode_1  | 2023-01-12 05:25:21,761 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-b40a0ffb-806c-459e-acb9-4d43ff37764e/DS-e5694648-5720-42a9-987c-c74e0f459bb6/container.db for volume DS-e5694648-5720-42a9-987c-c74e0f459bb6
om2_1       | 2023-01-12 05:26:50,116 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1       | 2023-01-12 05:26:50,116 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1       | 2023-01-12 05:26:46,468 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om3_1       | 2023-01-12 05:26:47,148 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1      | 2023-01-12 05:25:27,288 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
s3g_1       | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:721)
scm1_1      | 2023-01-12 05:24:57,930 [main] INFO server.GrpcService: f98693f9-b2e2-4085-a3fc-32934ea8db14: GrpcService started, listening on 9894
scm1_1      | 2023-01-12 05:24:58,000 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f98693f9-b2e2-4085-a3fc-32934ea8db14: Started
scm3_1      | 2023-01-12 05:25:42,474 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
recon_1     | 2023-01-12 05:25:27,031 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.11:51394: output error
recon_1     | 2023-01-12 05:25:27,030 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.6:59670: output error
om2_1       | 2023-01-12 05:26:50,120 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
datanode_2  | 2023-01-12 05:25:21,774 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO server.GrpcService: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: GrpcService started, listening on 9856
om1_1       | 2023-01-12 05:26:46,607 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
s3g_1       | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:691)
scm2_1      | 2023-01-12 05:25:27,559 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
scm2_1      | 2023-01-12 05:25:27,678 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
scm3_1      | 2023-01-12 05:25:42,475 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
recon_1     | 2023-01-12 05:25:27,030 [IPC Server handler 11 on default port 9891] WARN ipc.Server: IPC Server handler 11 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.6:59664: output error
recon_1     | 2023-01-12 05:25:27,029 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.4:48678: output error
datanode_1  | 2023-01-12 05:25:21,770 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2  | 2023-01-12 05:25:21,778 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO server.GrpcService: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: GrpcService started, listening on 9857
datanode_2  | 2023-01-12 05:25:21,789 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b is started using port 9858 for RATIS
om3_1       | 2023-01-12 05:26:47,161 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-12 05:25:20,525 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:20,529 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
scm3_1      | 2023-01-12 05:25:42,480 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm3_1      | 2023-01-12 05:25:42,480 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
recon_1     | 2023-01-12 05:25:27,040 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
datanode_2  | 2023-01-12 05:25:21,792 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b is started using port 9857 for RATIS_ADMIN
datanode_2  | 2023-01-12 05:25:21,793 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b is started using port 9856 for RATIS_SERVER
om3_1       | 2023-01-12 05:26:47,181 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
datanode_3  | java.net.SocketTimeoutException: Call From d407c6d45367/172.18.0.4 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.4:43224 remote=scm1/172.18.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
scm2_1      | 2023-01-12 05:25:27,679 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
scm2_1      | 2023-01-12 05:25:27,680 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm3_1      | 2023-01-12 05:25:42,494 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
scm3_1      | 2023-01-12 05:25:42,496 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
datanode_2  | 2023-01-12 05:25:21,794 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: Started
om1_1       | 2023-01-12 05:26:46,610 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om3_1       | 2023-01-12 05:26:47,233 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
scm2_1      | 2023-01-12 05:25:27,681 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
scm2_1      | 2023-01-12 05:25:27,685 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm2_1      | 2023-01-12 05:25:27,685 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
om2_1       | 2023-01-12 05:26:50,138 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
datanode_2  | 2023-01-12 05:25:22,550 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:22,551 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
om3_1       | 2023-01-12 05:26:47,234 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1       | 2023-01-12 05:26:47,235 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
s3g_1       | 	at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getInstance(AbstractHk2InjectionManager.java:160)
scm2_1      | 2023-01-12 05:25:27,692 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
scm2_1      | 2023-01-12 05:25:27,693 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
scm1_1      | 2023-01-12 05:25:02,734 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO impl.FollowerState: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5091020577ns, electionTimeout:5028ms
om2_1       | 2023-01-12 05:26:50,121 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1       | 2023-01-12 05:26:46,681 [Listener at om1/9862] INFO util.log: Logging initialized @33517ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2  | java.net.ConnectException: Call From 4c4cfcfeca58/172.18.0.11 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
datanode_3  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
scm3_1      | 2023-01-12 05:25:42,506 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
scm3_1      | 2023-01-12 05:25:42,507 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm1_1      | 2023-01-12 05:25:02,735 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO impl.RoleInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14: shutdown f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState
om2_1       | 2023-01-12 05:26:50,912 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
datanode_1  | 2023-01-12 05:25:21,779 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_1  | 2023-01-12 05:25:21,819 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
om1_1       | 2023-01-12 05:26:47,285 [Listener at om1/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
s3g_1       | 	at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getInstance(ImmediateHk2InjectionManager.java:30)
om3_1       | 2023-01-12 05:26:47,239 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1       | 2023-01-12 05:26:47,255 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
om3_1       | 2023-01-12 05:26:47,371 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
scm3_1      | 2023-01-12 05:25:42,616 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1_1      | 2023-01-12 05:25:02,744 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1_1      | 2023-01-12 05:25:02,777 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1  | 2023-01-12 05:25:21,828 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis f23956f4-f232-4ae2-8f3e-cd8efc9f95bf
om2_1       | 2023-01-12 05:26:51,244 [Listener at om2/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om1_1       | 2023-01-12 05:26:47,313 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
s3g_1       | 	at org.glassfish.jersey.internal.inject.Injections.getOrCreate(Injections.java:105)
om3_1       | 2023-01-12 05:26:47,379 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1       | 2023-01-12 05:26:47,393 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1       | 2023-01-12 05:26:47,559 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1       | 2023-01-12 05:26:47,560 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm3_1      | 2023-01-12 05:25:42,617 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1  | 2023-01-12 05:25:21,887 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO server.RaftServer: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: start RPC server
datanode_1  | 2023-01-12 05:25:21,894 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO server.GrpcService: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: GrpcService started, listening on 9858
datanode_1  | 2023-01-12 05:25:21,896 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO server.GrpcService: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: GrpcService started, listening on 9856
datanode_1  | 2023-01-12 05:25:21,896 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO server.GrpcService: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: GrpcService started, listening on 9857
datanode_2  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
s3g_1       | 	at org.glassfish.jersey.server.model.MethodHandler$ClassBasedMethodHandler.getInstance(MethodHandler.java:260)
scm2_1      | 2023-01-12 05:25:27,695 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
scm2_1      | 2023-01-12 05:25:27,695 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm2_1      | 2023-01-12 05:25:27,765 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm2_1      | 2023-01-12 05:25:27,766 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2_1      | 2023-01-12 05:25:27,766 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_1  | 2023-01-12 05:25:21,908 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f23956f4-f232-4ae2-8f3e-cd8efc9f95bf is started using port 9858 for RATIS
om1_1       | 2023-01-12 05:26:47,345 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1       | 2023-01-12 05:26:51,258 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@c5a2d5] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
s3g_1       | 	at org.glassfish.jersey.server.internal.routing.PushMethodHandlerRouter.apply(PushMethodHandlerRouter.java:51)
scm2_1      | 2023-01-12 05:25:27,767 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1      | 2023-01-12 05:25:02,779 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO impl.RoleInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14: start f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1
scm1_1      | 2023-01-12 05:25:02,851 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO impl.LeaderElection: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1      | 2023-01-12 05:25:02,854 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO impl.LeaderElection: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
datanode_1  | 2023-01-12 05:25:21,908 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f23956f4-f232-4ae2-8f3e-cd8efc9f95bf is started using port 9857 for RATIS_ADMIN
datanode_1  | 2023-01-12 05:25:21,908 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f23956f4-f232-4ae2-8f3e-cd8efc9f95bf is started using port 9856 for RATIS_SERVER
datanode_1  | 2023-01-12 05:25:21,915 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: Started
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
om1_1       | 2023-01-12 05:26:47,357 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om1_1       | 2023-01-12 05:26:47,357 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1      | 2023-01-12 05:25:27,771 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1      | 2023-01-12 05:25:42,618 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
datanode_1  | 2023-01-12 05:25:22,693 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:22,695 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
om1_1       | 2023-01-12 05:26:47,357 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm3_1      | 2023-01-12 05:25:42,618 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1      | 2023-01-12 05:25:02,854 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO impl.RoleInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14: shutdown f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1
datanode_1  | java.net.ConnectException: Call From d9a6a467ccde/172.18.0.6 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
s3g_1       | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
datanode_2  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
om1_1       | 2023-01-12 05:26:47,524 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
scm3_1      | 2023-01-12 05:25:42,621 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1      | 2023-01-12 05:25:02,861 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om2_1       | 2023-01-12 05:26:52,611 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 1, (t:0, i:~))
om2_1       | 2023-01-12 05:26:52,644 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-FOLLOWER: accept ELECTION from om1: our priority 0 <= candidate's priority 0
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
om3_1       | 2023-01-12 05:26:47,742 [Listener at om3/9862] INFO util.log: Logging initialized @35107ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1       | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
om2_1       | 2023-01-12 05:26:52,648 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:om1
om2_1       | 2023-01-12 05:26:52,648 [grpc-default-executor-0] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
scm2_1      | 2023-01-12 05:25:27,788 [main] INFO server.RaftServer: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6: addNew group-4D43FF37764E:[] returns group-4D43FF37764E:java.util.concurrent.CompletableFuture@15d0849[Not completed]
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
om1_1       | 2023-01-12 05:26:47,530 [Listener at om1/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
scm1_1      | 2023-01-12 05:25:02,861 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: change Leader from null to f98693f9-b2e2-4085-a3fc-32934ea8db14 at term 1 for becomeLeader, leader elected after 7110ms
scm1_1      | 2023-01-12 05:25:02,941 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
s3g_1       | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
scm2_1      | 2023-01-12 05:25:27,814 [pool-16-thread-1] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6: new RaftServerImpl for group-4D43FF37764E:[] with SCMStateMachine:uninitialized
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
om1_1       | 2023-01-12 05:26:47,676 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1       | 2023-01-12 05:26:47,678 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
scm1_1      | 2023-01-12 05:25:03,093 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
datanode_3  | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
s3g_1       | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
om2_1       | 2023-01-12 05:26:52,656 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState was interrupted
om3_1       | 2023-01-12 05:26:48,390 [Listener at om3/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm2_1      | 2023-01-12 05:25:27,816 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x9a10fd57] REGISTERED
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om1_1       | 2023-01-12 05:26:47,680 [Listener at om1/9862] INFO server.session: node0 Scavenging every 600000ms
om1_1       | 2023-01-12 05:26:47,731 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@204b0f07{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3_1      | 2023-01-12 05:25:42,656 [main] INFO server.RaftServer: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: addNew group-4D43FF37764E:[] returns group-4D43FF37764E:java.util.concurrent.CompletableFuture@15d0849[Not completed]
scm3_1      | 2023-01-12 05:25:42,658 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x3cc18d44] REGISTERED
s3g_1       | 	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
om2_1       | 2023-01-12 05:26:52,657 [grpc-default-executor-0] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om3_1       | 2023-01-12 05:26:48,420 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
scm2_1      | 2023-01-12 05:25:27,817 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x9a10fd57] BIND: 0.0.0.0/0.0.0.0:0
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
datanode_2  | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
datanode_1  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
scm1_1      | 2023-01-12 05:25:03,128 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om1_1       | 2023-01-12 05:26:47,734 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7ffcb232{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
scm3_1      | 2023-01-12 05:25:42,677 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x3cc18d44] BIND: 0.0.0.0/0.0.0.0:0
scm3_1      | 2023-01-12 05:25:42,696 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x3cc18d44, L:/0.0.0.0:43885] ACTIVE
om2_1       | 2023-01-12 05:26:52,677 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1       | 2023-01-12 05:26:48,480 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1       | 2023-01-12 05:26:48,488 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
scm2_1      | 2023-01-12 05:25:27,818 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
scm1_1      | 2023-01-12 05:25:03,360 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om1_1       | 2023-01-12 05:26:48,581 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7c69e1e1{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-367308183765111194/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
scm3_1      | 2023-01-12 05:25:42,721 [pool-16-thread-1] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: new RaftServerImpl for group-4D43FF37764E:[] with SCMStateMachine:uninitialized
om2_1       | 2023-01-12 05:26:52,677 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1       | 2023-01-12 05:26:48,492 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om3_1       | 2023-01-12 05:26:48,493 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
scm1_1      | 2023-01-12 05:25:03,367 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om1_1       | 2023-01-12 05:26:48,644 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@4b7ab7ab{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
s3g_1       | 	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
scm3_1      | 2023-01-12 05:25:42,724 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3_1      | 2023-01-12 05:25:42,725 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1       | 2023-01-12 05:26:52,744 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om1<-om2#0:OK-t1. Peer's state: om2@group-D66704EFC61C:t1, leader=null, voted=om1, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1      | 2023-01-12 05:25:27,818 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2_1      | 2023-01-12 05:25:27,819 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1     | 2023-01-12 05:25:27,028 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.6:47312: output error
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
scm1_1      | 2023-01-12 05:25:03,368 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1_1      | 2023-01-12 05:25:03,470 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om1_1       | 2023-01-12 05:26:48,647 [Listener at om1/9862] INFO server.Server: Started @35483ms
om1_1       | 2023-01-12 05:26:48,675 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1       | 2023-01-12 05:26:48,675 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm3_1      | 2023-01-12 05:25:42,727 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1       | 2023-01-12 05:26:53,102 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 1, (t:0, i:~))
om2_1       | 2023-01-12 05:26:53,102 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-FOLLOWER: reject ELECTION from om3: already has voted for om1 at current term 1
scm2_1      | 2023-01-12 05:25:27,819 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2_1      | 2023-01-12 05:25:27,819 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1      | 2023-01-12 05:25:27,819 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1      | 2023-01-12 05:25:03,533 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1_1      | 2023-01-12 05:25:03,581 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO impl.RoleInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14: start f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderStateImpl
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
s3g_1       | 	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
scm3_1      | 2023-01-12 05:25:42,727 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om2_1       | 2023-01-12 05:26:53,103 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om3<-om2#0:FAIL-t1. Peer's state: om2@group-D66704EFC61C:t1, leader=null, voted=om1, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1      | 2023-01-12 05:25:27,822 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x9a10fd57, L:/0.0.0.0:38811] ACTIVE
scm2_1      | 2023-01-12 05:25:27,828 [pool-16-thread-1] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
scm1_1      | 2023-01-12 05:25:04,062 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
scm3_1      | 2023-01-12 05:25:42,730 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1       | 2023-01-12 05:26:53,593 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: change Leader from null to om1 at term 1 for appendEntries, leader elected after 11576ms
recon_1     | 2023-01-12 05:25:27,028 [IPC Server handler 12 on default port 9891] WARN ipc.Server: IPC Server handler 12 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.4:45764: output error
scm1_1      | 2023-01-12 05:25:04,294 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-f98693f9-b2e2-4085-a3fc-32934ea8db14: Detected pause in JVM or host machine (eg GC): pause of approximately 148898159ns. No GCs detected.
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 2023-01-12 05:26:48,682 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1       | 2023-01-12 05:26:48,694 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1       | 2023-01-12 05:26:48,710 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
scm3_1      | 2023-01-12 05:25:42,732 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1       | 2023-01-12 05:26:53,617 [om2-server-thread2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1     | 2023-01-12 05:25:27,028 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.4:45748: output error
om3_1       | 2023-01-12 05:26:48,634 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
om3_1       | 2023-01-12 05:26:48,646 [Listener at om3/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om3_1       | 2023-01-12 05:26:48,835 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
scm1_1      | 2023-01-12 05:25:04,934 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: set configuration 0: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.4:43224 remote=scm1/172.18.0.2:9861]
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
scm3_1      | 2023-01-12 05:25:42,750 [pool-16-thread-1] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1       | 2023-01-12 05:26:53,634 [om2-server-thread2] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
recon_1     | 2023-01-12 05:25:27,042 [IPC Server handler 12 on default port 9891] INFO ipc.Server: IPC Server handler 12 on default port 9891 caught an exception
om3_1       | 2023-01-12 05:26:48,837 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
scm1_1      | 2023-01-12 05:25:05,322 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_inprogress_0
scm1_1      | 2023-01-12 05:25:06,033 [main] INFO server.RaftServer: f98693f9-b2e2-4085-a3fc-32934ea8db14: close
datanode_1  | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
scm3_1      | 2023-01-12 05:25:42,752 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1      | 2023-01-12 05:25:42,765 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1     | java.nio.channels.ClosedChannelException
om3_1       | 2023-01-12 05:26:48,848 [Listener at om3/9862] INFO server.session: node0 Scavenging every 600000ms
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1      | 2023-01-12 05:25:06,035 [main] INFO server.GrpcService: f98693f9-b2e2-4085-a3fc-32934ea8db14: shutdown server GrpcServerProtocolService now
datanode_3  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
scm3_1      | 2023-01-12 05:25:42,768 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om3_1       | 2023-01-12 05:26:48,946 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@389a1e34{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1      | 2023-01-12 05:25:06,039 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: shutdown
datanode_3  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
om2_1       | 2023-01-12 05:26:53,862 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
datanode_3  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
om3_1       | 2023-01-12 05:26:48,950 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@9bf63d2{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
scm1_1      | 2023-01-12 05:25:06,052 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-4D43FF37764E,id=f98693f9-b2e2-4085-a3fc-32934ea8db14
scm1_1      | 2023-01-12 05:25:06,052 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO impl.RoleInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14: shutdown f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderStateImpl
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
om2_1       | 2023-01-12 05:26:57,759 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om2_1       | [id: "om1"
om2_1       | address: "om1:9872"
om1_1       | 2023-01-12 05:26:49,203 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
scm1_1      | 2023-01-12 05:25:06,073 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO impl.PendingRequests: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-PendingRequests: sendNotLeaderResponses
scm2_1      | 2023-01-12 05:25:27,829 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1      | 2023-01-12 05:25:27,834 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1       | startupRole: FOLLOWER
om1_1       | 2023-01-12 05:26:49,937 [Listener at om1/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om1_1       | 2023-01-12 05:26:49,970 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2e4d4d22] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1       | 2023-01-12 05:26:51,508 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5165619844ns, electionTimeout:5155ms
datanode_2  | Caused by: java.net.ConnectException: Connection refused
datanode_2  | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
datanode_2  | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
om3_1       | 2023-01-12 05:26:49,516 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3f45dfec{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-10747882286787132186/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om3_1       | 2023-01-12 05:26:49,580 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@5ddb302{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
scm1_1      | 2023-01-12 05:25:06,124 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO impl.StateMachineUpdater: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater: set stopIndex = 0
datanode_3  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
om2_1       | , id: "om3"
om2_1       | address: "om3:9872"
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
scm3_1      | 2023-01-12 05:25:42,797 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm3_1      | 2023-01-12 05:25:42,803 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3_1      | 2023-01-12 05:25:42,804 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1       | 2023-01-12 05:26:49,582 [Listener at om3/9862] INFO server.Server: Started @36947ms
scm2_1      | 2023-01-12 05:25:27,835 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1      | 2023-01-12 05:25:06,124 [main] INFO server.GrpcService: f98693f9-b2e2-4085-a3fc-32934ea8db14: shutdown server GrpcServerProtocolService successfully
om1_1       | 2023-01-12 05:26:51,510 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
om1_1       | 2023-01-12 05:26:51,511 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1       | 2023-01-12 05:26:51,515 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1       | 2023-01-12 05:26:51,515 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
scm3_1      | 2023-01-12 05:25:42,884 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1      | 2023-01-12 05:25:42,885 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om3_1       | 2023-01-12 05:26:49,603 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2_1      | 2023-01-12 05:25:27,844 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1_1      | 2023-01-12 05:25:06,133 [f98693f9-b2e2-4085-a3fc-32934ea8db14-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x76dd0264, L:/0.0.0.0:42821] CLOSE
scm1_1      | 2023-01-12 05:25:06,133 [f98693f9-b2e2-4085-a3fc-32934ea8db14-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x76dd0264, L:/0.0.0.0:42821] INACTIVE
scm1_1      | 2023-01-12 05:25:06,133 [f98693f9-b2e2-4085-a3fc-32934ea8db14-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x76dd0264, L:/0.0.0.0:42821] UNREGISTERED
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
om2_1       | startupRole: FOLLOWER
om2_1       | , id: "om2"
datanode_2  | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
datanode_2  | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
om3_1       | 2023-01-12 05:26:49,604 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1       | 2023-01-12 05:26:49,607 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
scm1_1      | 2023-01-12 05:25:06,141 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO impl.StateMachineUpdater: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater: Took a snapshot at index 0
scm1_1      | 2023-01-12 05:25:06,204 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO impl.StateMachineUpdater: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm1_1      | 2023-01-12 05:25:06,231 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: closes. applyIndex: 0
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
om3_1       | 2023-01-12 05:26:49,616 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1      | 2023-01-12 05:25:27,847 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2_1      | 2023-01-12 05:25:27,848 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
scm1_1      | 2023-01-12 05:25:06,233 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 2023-01-12 05:26:51,526 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1       | 2023-01-12 05:26:51,556 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
om3_1       | 2023-01-12 05:26:49,685 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
scm2_1      | 2023-01-12 05:25:27,889 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1      | 2023-01-12 05:25:27,890 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2_1      | 2023-01-12 05:25:27,891 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1      | 2023-01-12 05:25:06,243 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO segmented.SegmentedRaftLogWorker: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker close()
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 2023-01-12 05:26:51,563 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
datanode_2  | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
om3_1       | 2023-01-12 05:26:49,990 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
scm2_1      | 2023-01-12 05:25:27,891 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1       | address: "om2:9872"
om2_1       | startupRole: FOLLOWER
scm1_1      | 2023-01-12 05:25:06,244 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f98693f9-b2e2-4085-a3fc-32934ea8db14: Stopped
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
datanode_2  | 	... 12 more
datanode_2  | 2023-01-12 05:25:22,557 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1      | 2023-01-12 05:25:27,892 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1      | 2023-01-12 05:25:27,893 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
om2_1       | ]
om2_1       | 2023-01-12 05:27:08,604 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:23008-without-scheme for user:hadoop
scm1_1      | 2023-01-12 05:25:06,252 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
datanode_2  | 2023-01-12 05:25:22,560 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
om3_1       | 2023-01-12 05:26:50,492 [Listener at om3/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
scm2_1      | 2023-01-12 05:25:27,894 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2_1      | 2023-01-12 05:25:27,894 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm1_1      | 2023-01-12 05:25:06,262 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-b40a0ffb-806c-459e-acb9-4d43ff37764e; layoutVersion=4; scmId=f98693f9-b2e2-4085-a3fc-32934ea8db14
om1_1       | 2023-01-12 05:26:51,563 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om1_1       | 2023-01-12 05:26:51,568 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
datanode_2  | java.net.ConnectException: Call From 4c4cfcfeca58/172.18.0.11 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
om3_1       | 2023-01-12 05:26:50,547 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@185339ed] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1       | 2023-01-12 05:27:30,746 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bb1 of layout LEGACY in volume: 23008-without-scheme
om2_1       | 2023-01-12 05:30:16,471 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:23008-without-scheme for user:hadoop
om2_1       | 2023-01-12 05:30:28,986 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bb1 of layout LEGACY in volume: 23008-without-scheme
om2_1       | 2023-01-12 05:30:58,279 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:09780-source for user:hadoop
om2_1       | 2023-01-12 05:31:02,407 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:09780-target for user:hadoop
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1  | Caused by: java.net.ConnectException: Connection refused
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om2_1       | 2023-01-12 05:31:06,710 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: dangling-link of layout LEGACY in volume: 09780-target
om2_1       | 2023-01-12 05:31:15,089 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: link1 of layout LEGACY in volume: 09780-target
scm2_1      | 2023-01-12 05:25:27,921 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm2_1      | 2023-01-12 05:25:28,134 [main] INFO reflections.Reflections: Reflections took 152 ms to scan 3 urls, producing 121 keys and 272 values 
scm1_1      | 2023-01-12 05:25:07,233 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
datanode_3  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1      | 2023-01-12 05:25:42,889 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
om2_1       | 2023-01-12 05:31:19,107 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: 09780-source
om2_1       | 2023-01-12 05:32:39,482 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: link2 of layout LEGACY in volume: 09780-target
om2_1       | 2023-01-12 05:32:43,289 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:link2 in volume:09780-target
om2_1       | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
scm2_1      | 2023-01-12 05:25:28,194 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
om1_1       | 2023-01-12 05:26:52,900 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
om1_1       | 2023-01-12 05:26:52,901 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om2#0:OK-t1
datanode_3  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
om1_1       | 2023-01-12 05:26:52,903 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result PASSED
scm3_1      | 2023-01-12 05:25:42,891 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-12 05:25:27,040 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
scm2_1      | 2023-01-12 05:25:28,195 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm1_1      | /************************************************************
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
om1_1       | 2023-01-12 05:26:52,903 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-LeaderElection1
scm3_1      | 2023-01-12 05:25:42,892 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1      | 2023-01-12 05:25:28,198 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
om2_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om1_1       | 2023-01-12 05:26:52,904 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om1_1       | 2023-01-12 05:26:52,906 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 12750ms
om1_1       | 2023-01-12 05:26:52,931 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm3_1      | 2023-01-12 05:25:42,894 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
datanode_2  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
scm3_1      | 2023-01-12 05:25:42,894 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3_1      | 2023-01-12 05:25:42,895 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3_1      | 2023-01-12 05:25:42,960 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1  | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
om3_1       | 2023-01-12 05:26:52,286 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5158024582ns, electionTimeout:5123ms
om3_1       | 2023-01-12 05:26:52,289 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
om3_1       | 2023-01-12 05:26:52,292 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm3_1      | 2023-01-12 05:25:43,344 [main] INFO reflections.Reflections: Reflections took 222 ms to scan 3 urls, producing 121 keys and 272 values 
scm3_1      | 2023-01-12 05:25:43,450 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm3_1      | 2023-01-12 05:25:43,450 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
scm3_1      | 2023-01-12 05:25:43,454 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm3_1      | 2023-01-12 05:25:43,455 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm2_1      | 2023-01-12 05:25:28,199 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3  | 2023-01-12 05:25:21,525 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
datanode_1  | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
scm3_1      | 2023-01-12 05:25:43,590 [main] INFO node.SCMNodeManager: Entering startup safe mode.
om1_1       | 2023-01-12 05:26:52,956 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om1_1       | 2023-01-12 05:26:52,958 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3  | 2023-01-12 05:25:21,526 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:21,528 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
scm1_1      | SHUTDOWN_MSG: Shutting down StorageContainerManager at 7f47dfd12ecd/172.18.0.2
om3_1       | 2023-01-12 05:26:52,297 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
scm3_1      | 2023-01-12 05:25:43,616 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
om1_1       | 2023-01-12 05:26:52,985 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om1_1       | 2023-01-12 05:26:52,987 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2  | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
scm1_1      | ************************************************************/
scm1_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1  | 	... 12 more
scm3_1      | 2023-01-12 05:25:43,620 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
om1_1       | 2023-01-12 05:26:52,989 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om1_1       | 2023-01-12 05:26:53,007 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
datanode_3  | java.net.ConnectException: Call From d407c6d45367/172.18.0.4 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
scm1_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1_1      | 2023-01-12 05:25:09,963 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
datanode_1  | 2023-01-12 05:25:22,695 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      | 2023-01-12 05:25:43,630 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
om1_1       | 2023-01-12 05:26:53,013 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om1_1       | 2023-01-12 05:26:53,114 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1      | 2023-01-12 05:25:28,307 [main] INFO node.SCMNodeManager: Entering startup safe mode.
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
scm3_1      | 2023-01-12 05:25:43,694 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
om1_1       | 2023-01-12 05:26:53,115 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1       | 2023-01-12 05:26:53,117 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1       | 2023-01-12 05:26:53,138 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm2_1      | 2023-01-12 05:25:28,332 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
datanode_1  | 2023-01-12 05:25:22,696 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1       | 2023-01-12 05:32:47,274 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket3 of layout LEGACY in volume: 09780-target
om2_1       | 2023-01-12 05:32:50,984 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket3 in volume:09780-target
om2_1       | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1      | /************************************************************
om3_1       | 2023-01-12 05:26:52,297 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderElection1
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
om2_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
datanode_3  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1      | STARTUP_MSG: Starting StorageContainerManager
om3_1       | 2023-01-12 05:26:52,318 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3_1      | 2023-01-12 05:25:43,694 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1      | STARTUP_MSG:   host = 7f47dfd12ecd/172.18.0.2
om3_1       | 2023-01-12 05:26:52,483 [om3@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
datanode_3  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1      | STARTUP_MSG:   args = []
om3_1       | 2023-01-12 05:26:52,487 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | java.net.ConnectException: Call From d9a6a467ccde/172.18.0.6 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
datanode_2  | Caused by: java.net.ConnectException: Connection refused
datanode_2  | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
om3_1       | 2023-01-12 05:26:52,487 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1       | 2023-01-12 05:26:53,140 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2  | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
scm1_1      | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
om3_1       | 2023-01-12 05:26:52,493 [om3@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
datanode_1  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
scm3_1      | 2023-01-12 05:25:43,709 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm3_1      | 2023-01-12 05:25:43,710 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3_1      | 2023-01-12 05:25:43,720 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3_1      | 2023-01-12 05:25:43,724 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm3_1      | 2023-01-12 05:25:43,735 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
datanode_2  | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
scm1_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/9d5cfd6934cc6ec34d3c78b99af90888e05f21af ; compiled by 'runner' on 2023-01-12T05:10Z
scm1_1      | STARTUP_MSG:   java = 11.0.14.1
om3_1       | 2023-01-12 05:26:52,784 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 1, (t:0, i:~))
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
scm3_1      | 2023-01-12 05:25:43,742 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm3_1      | 2023-01-12 05:25:43,848 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3_1      | 2023-01-12 05:25:43,889 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2_1      | 2023-01-12 05:25:28,336 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2_1      | 2023-01-12 05:25:28,346 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
om3_1       | 2023-01-12 05:26:52,791 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: reject ELECTION from om1: already has voted for om3 at current term 1
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm3_1      | 2023-01-12 05:25:43,981 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm2_1      | 2023-01-12 05:25:28,416 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2_1      | 2023-01-12 05:25:28,417 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1      | ************************************************************/
scm1_1      | 2023-01-12 05:25:09,997 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1_1      | 2023-01-12 05:25:10,074 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1       | 2023-01-12 05:26:52,830 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om1<-om3#0:FAIL-t1. Peer's state: om3@group-D66704EFC61C:t1, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1       | 2023-01-12 05:26:53,389 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1      | 2023-01-12 05:25:44,030 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2_1      | 2023-01-12 05:25:28,435 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2_1      | 2023-01-12 05:25:28,435 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1      | 2023-01-12 05:25:10,131 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
datanode_2  | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
om3_1       | 2023-01-12 05:26:53,390 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om1#0:FAIL-t1
om3_1       | 2023-01-12 05:26:53,390 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om3<-om2#0:FAIL-t1
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm3_1      | 2023-01-12 05:25:44,034 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
om3_1       | 2023-01-12 05:26:53,390 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result REJECTED
datanode_1  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
scm1_1      | 2023-01-12 05:25:10,146 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
datanode_3  | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1       | 2023-01-12 05:32:54,308 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: loop2 of layout LEGACY in volume: 09780-target
datanode_2  | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
datanode_2  | 	... 12 more
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
om1_1       | 2023-01-12 05:26:53,150 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1       | 2023-01-12 05:26:53,151 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1       | 2023-01-12 05:26:53,155 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om1_1       | 2023-01-12 05:26:53,172 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1       | 2023-01-12 05:26:53,172 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1      | 2023-01-12 05:25:28,444 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm2_1      | 2023-01-12 05:25:28,448 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
om3_1       | 2023-01-12 05:26:53,392 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
om1_1       | 2023-01-12 05:26:53,175 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm3_1      | WARNING: An illegal reflective access operation has occurred
scm3_1      | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm3_1      | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm3_1      | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm3_1      | WARNING: All illegal access operations will be denied in a future release
scm3_1      | 2023-01-12 05:25:44,051 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2_1      | 2023-01-12 05:25:28,459 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
om2_1       | 2023-01-12 05:32:58,275 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: loop3 of layout LEGACY in volume: 09780-target
om1_1       | 2023-01-12 05:26:53,175 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1       | 2023-01-12 05:26:53,175 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
scm3_1      | 2023-01-12 05:25:44,059 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1      | 2023-01-12 05:25:44,065 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3_1      | 2023-01-12 05:25:45,530 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1      | 2023-01-12 05:25:45,566 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1      | 2023-01-12 05:25:45,604 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2_1      | 2023-01-12 05:25:28,464 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm2_1      | 2023-01-12 05:25:28,563 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
scm2_1      | 2023-01-12 05:25:28,602 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1_1      | 2023-01-12 05:25:10,205 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
datanode_2  | 2023-01-12 05:25:23,558 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:23,562 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:24,559 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:26:53,184 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1       | 2023-01-12 05:26:53,184 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1       | 2023-01-12 05:26:53,185 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om1_1       | 2023-01-12 05:26:53,206 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderStateImpl
om1_1       | 2023-01-12 05:26:53,295 [om1@group-D66704EFC61C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
om1_1       | 2023-01-12 05:26:53,301 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 1, (t:0, i:~))
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
datanode_2  | 2023-01-12 05:25:24,563 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:25,560 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:25,564 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:26,561 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:26,565 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:27,563 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:27,565 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:28,566 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:28,566 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:29,566 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:29,567 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:30,567 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
scm3_1      | 2023-01-12 05:25:45,836 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1      | 2023-01-12 05:25:45,862 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1      | 2023-01-12 05:25:45,865 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3_1      | 2023-01-12 05:25:45,916 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1      | 2023-01-12 05:25:45,925 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1      | 2023-01-12 05:25:45,926 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3_1      | 2023-01-12 05:25:46,005 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
om1_1       | 2023-01-12 05:26:53,362 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1       | 2023-01-12 05:26:53,369 [grpc-default-executor-1] INFO impl.VoteContext: om1@group-D66704EFC61C-LEADER: reject ELECTION from om3: already has voted for om1 at current term 1
om1_1       | 2023-01-12 05:26:53,370 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om3<-om1#0:FAIL-t1. Peer's state: om1@group-D66704EFC61C:t1, leader=om1, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1       | 2023-01-12 05:26:53,804 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
datanode_2  | 2023-01-12 05:25:30,568 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:31,569 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:32,570 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:33,574 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      | 2023-01-12 05:25:46,009 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
om1_1       | 2023-01-12 05:26:53,987 [om1@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
datanode_2  | 2023-01-12 05:25:34,574 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:35,576 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:36,577 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:36,828 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode_2  | 2023-01-12 05:25:37,578 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:38,580 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:39,581 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      | Container Balancer status:
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
om1_1       | [id: "om1"
om1_1       | address: "om1:9872"
om1_1       | startupRole: FOLLOWER
om1_1       | , id: "om3"
om1_1       | address: "om3:9872"
om1_1       | startupRole: FOLLOWER
om1_1       | , id: "om2"
om1_1       | address: "om2:9872"
om1_1       | startupRole: FOLLOWER
om1_1       | ]
om1_1       | 2023-01-12 05:26:55,979 [qtp1075390175-49] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
datanode_2  | 2023-01-12 05:25:40,582 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:41,583 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:42,584 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:43,585 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:44,586 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      | Key                            Value
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
datanode_2  | 2023-01-12 05:25:45,587 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:46,588 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:56,826 [Command processor thread] INFO server.RaftServer: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: addNew group-9E784E6BBA4E:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER] returns group-9E784E6BBA4E:java.util.concurrent.CompletableFuture@3c5d5225[Not completed]
datanode_2  | 2023-01-12 05:25:57,057 [pool-22-thread-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: new RaftServerImpl for group-9E784E6BBA4E:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2  | 2023-01-12 05:25:57,070 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2023-01-12 05:25:57,075 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2023-01-12 05:25:57,079 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3_1      | Running                        true
scm3_1      | Container Balancer Configuration values:
scm3_1      | Key                                                Value
om1_1       | 2023-01-12 05:26:56,010 [qtp1075390175-49] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1673501215990 in 19 milliseconds
recon_1     | 2023-01-12 05:25:27,040 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
datanode_2  | 2023-01-12 05:25:57,094 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2  | 2023-01-12 05:25:57,097 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2  | 2023-01-12 05:25:57,104 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2  | 2023-01-12 05:25:57,205 [pool-22-thread-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E: ConfigurationManager, init=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2  | 2023-01-12 05:25:57,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2023-01-12 05:25:57,310 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2023-01-12 05:25:57,323 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1      | Threshold                                          10
om1_1       | 2023-01-12 05:26:56,197 [qtp1075390175-49] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 173 milliseconds
recon_1     | java.nio.channels.ClosedChannelException
scm3_1      | Max Datanodes to Involve per Iteration(percent)    20
datanode_2  | 2023-01-12 05:25:57,398 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2  | 2023-01-12 05:25:57,462 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om3_1       | 2023-01-12 05:26:53,392 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-LeaderElection1
om3_1       | 2023-01-12 05:26:53,393 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1       | 2023-01-12 05:26:53,396 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1       | 2023-01-12 05:26:56,197 [qtp1075390175-49] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1673501215990
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_2  | 2023-01-12 05:25:57,476 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2  | 2023-01-12 05:25:57,801 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2023-01-12 05:25:57,818 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2  | 2023-01-12 05:25:57,822 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om1_1       | 2023-01-12 05:27:08,446 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:23008-without-scheme for user:hadoop
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
om2_1       | 2023-01-12 05:33:02,014 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: loop1 of layout LEGACY in volume: 09780-target
om2_1       | 2023-01-12 05:33:09,018 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: link3 of layout LEGACY in volume: 09780-target
om2_1       | 2023-01-12 05:33:38,744 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: link4 of layout LEGACY in volume: 09780-target
om2_1       | 2023-01-12 05:33:42,508 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketSetPropertyRequest: Setting bucket property failed for bucket:link4 in volume:09780-target
scm1_1      | 2023-01-12 05:25:10,206 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
om1_1       | 2023-01-12 05:27:30,751 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bb1 of layout LEGACY in volume: 23008-without-scheme
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
om3_1       | 2023-01-12 05:26:53,396 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
scm2_1      | 2023-01-12 05:25:28,705 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1      | 2023-01-12 05:25:11,239 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1       | 2023-01-12 05:30:16,463 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:23008-without-scheme for user:hadoop
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
om3_1       | 2023-01-12 05:26:53,488 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: change Leader from null to om1 at term 1 for appendEntries, leader elected after 12783ms
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm2_1      | 2023-01-12 05:25:28,751 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2_1      | 2023-01-12 05:25:28,752 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
datanode_2  | 2023-01-12 05:25:57,825 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2  | 2023-01-12 05:25:57,827 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1      | 2023-01-12 05:25:11,523 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1      | Max Size to Move per Iteration                     500GB
om1_1       | 2023-01-12 05:30:28,977 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bb1 of layout LEGACY in volume: 23008-without-scheme
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
scm2_1      | WARNING: An illegal reflective access operation has occurred
datanode_2  | 2023-01-12 05:25:57,828 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/cd62190c-0472-48bc-9601-9e784e6bba4e does not exist. Creating ...
datanode_2  | 2023-01-12 05:25:57,845 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/cd62190c-0472-48bc-9601-9e784e6bba4e/in_use.lock acquired by nodename 7@4c4cfcfeca58
datanode_2  | 2023-01-12 05:25:57,896 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/cd62190c-0472-48bc-9601-9e784e6bba4e has been successfully formatted.
scm3_1      | Max Size Entering Target per Iteration             26GB
om1_1       | 2023-01-12 05:30:58,271 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:09780-source for user:hadoop
s3g_1       | Caused by: org.apache.hadoop.ozone.s3.exception.OS3Exception
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm3_1      | Max Size Leaving Source per Iteration              26GB
om1_1       | 2023-01-12 05:31:02,379 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:09780-target for user:hadoop
s3g_1       | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.newError(S3ErrorTable.java:139)
scm1_1      | 2023-01-12 05:25:11,930 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
om3_1       | 2023-01-12 05:26:53,590 [om3-server-thread2] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1       | 2023-01-12 05:26:53,629 [om3-server-thread2] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
om3_1       | 2023-01-12 05:26:53,850 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1      | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm2_1      | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm3_1      | 
om1_1       | 2023-01-12 05:31:06,701 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: dangling-link of layout LEGACY in volume: 09780-target
s3g_1       | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.newError(S3ErrorTable.java:126)
om2_1       | NOT_SUPPORTED_OPERATION org.apache.hadoop.ozone.om.exceptions.OMException: Cannot set property on link
scm1_1      | 2023-01-12 05:25:11,934 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
om3_1       | 2023-01-12 05:26:57,782 [om3@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
datanode_3  | Caused by: java.net.ConnectException: Connection refused
datanode_3  | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
datanode_2  | 2023-01-12 05:25:58,003 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-9E784E6BBA4E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm2_1      | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm3_1      | 2023-01-12 05:25:46,009 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3_1      | 2023-01-12 05:25:46,030 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
s3g_1       | 	at org.apache.hadoop.ozone.s3.signature.AWSSignatureProcessor.parseSignature(AWSSignatureProcessor.java:86)
om2_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketSetPropertyRequest.validateAndUpdateCache(OMBucketSetPropertyRequest.java:147)
scm1_1      | 2023-01-12 05:25:12,055 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1       | [id: "om1"
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
datanode_2  | 2023-01-12 05:25:58,045 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1      | WARNING: All illegal access operations will be denied in a future release
scm3_1      | 2023-01-12 05:25:46,038 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3_1      | 2023-01-12 05:25:46,040 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e does not exist. Creating ...
s3g_1       | 	at org.apache.hadoop.ozone.s3.signature.AWSSignatureProcessor$Proxy$_$$_WeldClientProxy.parseSignature(Unknown Source)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
scm1_1      | 2023-01-12 05:25:12,074 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:f98693f9-b2e2-4085-a3fc-32934ea8db14
datanode_3  | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
datanode_3  | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
scm3_1      | 2023-01-12 05:25:46,061 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/in_use.lock acquired by nodename 7@7d0c64aff313
om1_1       | 2023-01-12 05:31:15,072 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: link1 of layout LEGACY in volume: 09780-target
scm2_1      | 2023-01-12 05:25:28,792 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
s3g_1       | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.getSignature(OzoneClientProducer.java:81)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
scm1_1      | 2023-01-12 05:25:12,157 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1       | address: "om1:9872"
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
datanode_2  | 2023-01-12 05:25:58,081 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1      | 2023-01-12 05:25:46,113 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e has been successfully formatted.
om1_1       | 2023-01-12 05:31:19,093 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: 09780-source
scm2_1      | 2023-01-12 05:25:28,801 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
s3g_1       | 	... 114 more
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
scm1_1      | 2023-01-12 05:25:12,250 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1_1      | 2023-01-12 05:25:12,251 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm3_1      | 2023-01-12 05:25:46,131 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1       | 2023-01-12 05:32:39,478 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: link2 of layout LEGACY in volume: 09780-target
om1_1       | 2023-01-12 05:32:43,265 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:link2 in volume:09780-target
scm2_1      | 2023-01-12 05:25:28,805 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
s3g_1       | 
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
om3_1       | startupRole: FOLLOWER
scm1_1      | 2023-01-12 05:25:12,253 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm3_1      | 2023-01-12 05:25:46,181 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1      | 2023-01-12 05:25:46,182 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1       | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
scm2_1      | 2023-01-12 05:25:30,195 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
s3g_1       | 
s3g_1       | 2023-01-12 05:35:10,022 [qtp384515747-86] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg8, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,022 [qtp384515747-80] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg7, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
om3_1       | , id: "om3"
scm3_1      | 2023-01-12 05:25:46,194 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1       | address: "om3:9872"
scm1_1      | 2023-01-12 05:25:12,254 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
scm2_1      | 2023-01-12 05:25:30,254 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
s3g_1       | 2023-01-12 05:35:10,023 [qtp384515747-19] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg3, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm2_1      | 2023-01-12 05:25:30,396 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
s3g_1       | 2023-01-12 05:35:10,023 [qtp384515747-23] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg2, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2  | 2023-01-12 05:25:58,108 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-12 05:25:58,110 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1      | 2023-01-12 05:25:46,204 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1      | 2023-01-12 05:25:46,217 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1      | 2023-01-12 05:25:46,377 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
scm2_1      | 2023-01-12 05:25:30,563 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1  | Caused by: java.net.ConnectException: Connection refused
om3_1       | startupRole: FOLLOWER
om3_1       | , id: "om2"
scm3_1      | 2023-01-12 05:25:46,379 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1      | 2023-01-12 05:25:46,406 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e
scm3_1      | 2023-01-12 05:25:46,413 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1      | 2023-01-12 05:25:30,573 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1       | 2023-01-12 05:34:00,684 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-8718359029 of layout LEGACY in volume: s3v
datanode_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
datanode_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
datanode_2  | 2023-01-12 05:25:58,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2  | 2023-01-12 05:25:58,138 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2023-01-12 05:25:58,185 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2023-01-12 05:25:58,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1      | 2023-01-12 05:25:46,417 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3_1      | 2023-01-12 05:25:46,428 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1      | 2023-01-12 05:25:30,587 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2_1      | 2023-01-12 05:25:30,641 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1      | 2023-01-12 05:25:30,652 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1       | address: "om2:9872"
om3_1       | startupRole: FOLLOWER
om3_1       | ]
datanode_3  | 	... 12 more
datanode_3  | 2023-01-12 05:25:21,527 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
s3g_1       | 2023-01-12 05:35:10,023 [qtp384515747-21] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg9, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,024 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg5, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,024 [qtp384515747-82] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg1, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,024 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg0, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,024 [qtp384515747-22] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg6, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | 2023-01-12 05:25:30,654 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 2023-01-12 05:34:04,626 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-2923742260 of layout LEGACY in volume: s3v
datanode_3  | java.net.ConnectException: Call From d407c6d45367/172.18.0.4 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm3_1      | 2023-01-12 05:25:46,435 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3_1      | 2023-01-12 05:25:46,446 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1  | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
datanode_3  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
datanode_3  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1      | 2023-01-12 05:25:46,455 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1      | 2023-01-12 05:25:46,462 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3  | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
om2_1       | 2023-01-12 05:34:16,561 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-7498891659 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:34:17,312 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ozone-test-qwhhyijwyi of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:34:20,190 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ufntzifvoh of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:34:32,022 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-7112304483 of layout LEGACY in volume: s3v
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
om2_1       | 2023-01-12 05:34:32,961 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-3900182019 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:34:33,894 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-3726753216 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:34:34,794 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket-ozone-test-3726753216 in volume:s3v
om2_1       | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
datanode_3  | Caused by: java.net.ConnectException: Connection refused
datanode_3  | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
datanode_3  | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
datanode_3  | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
scm2_1      | 2023-01-12 05:25:30,746 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2_1      | 2023-01-12 05:25:30,748 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
scm3_1      | 2023-01-12 05:25:46,469 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1      | 2023-01-12 05:25:12,254 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1_1      | 2023-01-12 05:25:12,255 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1_1      | 2023-01-12 05:25:12,256 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
scm2_1      | Container Balancer status:
s3g_1       | 2023-01-12 05:35:10,024 [qtp384515747-18] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg4, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,338 [qtp384515747-82] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg18, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,341 [qtp384515747-21] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg19, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,344 [qtp384515747-86] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg13, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 2023-01-12 05:32:47,267 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket3 of layout LEGACY in volume: 09780-target
scm3_1      | 2023-01-12 05:25:46,532 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1_1      | 2023-01-12 05:25:12,258 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1       | 2023-01-12 05:27:08,510 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:23008-without-scheme for user:hadoop
om3_1       | 2023-01-12 05:27:30,789 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bb1 of layout LEGACY in volume: 23008-without-scheme
datanode_1  | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
scm2_1      | Key                            Value
s3g_1       | 2023-01-12 05:35:10,350 [qtp384515747-84] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg14, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,361 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg16, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,389 [qtp384515747-19] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg12, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
datanode_3  | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
scm3_1      | 2023-01-12 05:25:46,534 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1      | 2023-01-12 05:25:12,259 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1_1      | 2023-01-12 05:25:12,260 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1      | 2023-01-12 05:25:12,272 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
scm2_1      | Running                        true
s3g_1       | 2023-01-12 05:35:10,402 [qtp384515747-85] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg10, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
datanode_3  | 	... 12 more
datanode_3  | 2023-01-12 05:25:21,763 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-b40a0ffb-806c-459e-acb9-4d43ff37764e/DS-dbac957a-3ae3-4034-a52b-b365a2be2c4c/container.db to cache
scm3_1      | 2023-01-12 05:25:46,881 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1      | 2023-01-12 05:25:12,277 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2  | 2023-01-12 05:25:58,231 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/cd62190c-0472-48bc-9601-9e784e6bba4e
om3_1       | 2023-01-12 05:30:16,484 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:23008-without-scheme for user:hadoop
datanode_1  | 	... 12 more
s3g_1       | 2023-01-12 05:35:10,412 [qtp384515747-83] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg11, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,402 [qtp384515747-80] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg15, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,479 [qtp384515747-21] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg20, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | Container Balancer Configuration values:
datanode_3  | 2023-01-12 05:25:21,763 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-b40a0ffb-806c-459e-acb9-4d43ff37764e/DS-dbac957a-3ae3-4034-a52b-b365a2be2c4c/container.db for volume DS-dbac957a-3ae3-4034-a52b-b365a2be2c4c
om1_1       | 2023-01-12 05:32:50,969 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket3 in volume:09780-target
om1_1       | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
scm3_1      | 2023-01-12 05:25:46,882 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1      | 2023-01-12 05:25:12,277 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2  | 2023-01-12 05:25:58,234 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
om3_1       | 2023-01-12 05:30:28,993 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bb1 of layout LEGACY in volume: 23008-without-scheme
s3g_1       | 2023-01-12 05:35:10,519 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg17, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
recon_1     | 2023-01-12 05:25:27,039 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1      | Key                                                Value
datanode_3  | 2023-01-12 05:25:21,770 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
om1_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
scm3_1      | 2023-01-12 05:25:46,883 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1      | 2023-01-12 05:25:12,682 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode_2  | 2023-01-12 05:25:58,241 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om3_1       | 2023-01-12 05:30:58,301 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:09780-source for user:hadoop
s3g_1       | 2023-01-12 05:35:10,544 [qtp384515747-22] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg21, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,596 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg23, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,607 [qtp384515747-84] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg25, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,664 [qtp384515747-83] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg26, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,667 [qtp384515747-18] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg29, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:21,777 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm3_1      | 2023-01-12 05:25:46,913 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1      | 2023-01-12 05:25:12,864 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode_2  | 2023-01-12 05:25:58,252 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om3_1       | 2023-01-12 05:31:02,372 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:09780-target for user:hadoop
s3g_1       | 2023-01-12 05:35:10,671 [qtp384515747-22] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg27, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,673 [qtp384515747-82] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg24, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,681 [qtp384515747-21] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg28, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
om2_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
scm2_1      | Threshold                                          10
datanode_3  | 2023-01-12 05:25:21,821 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3_1      | 2023-01-12 05:25:46,915 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3_1      | 2023-01-12 05:25:46,927 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
datanode_2  | 2023-01-12 05:25:58,252 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm1_1      | 2023-01-12 05:25:12,867 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
datanode_3  | 2023-01-12 05:25:21,829 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1      | Max Datanodes to Involve per Iteration(percent)    20
scm3_1      | 2023-01-12 05:25:46,928 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: changes role from      null to FOLLOWER at term 0 for startInitializing
scm3_1      | 2023-01-12 05:25:46,933 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4D43FF37764E,id=6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf
datanode_2  | 2023-01-12 05:25:58,254 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
datanode_3  | 2023-01-12 05:25:21,890 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO server.RaftServer: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start RPC server
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1      | Max Size to Move per Iteration                     500GB
scm3_1      | 2023-01-12 05:25:46,943 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1      | 2023-01-12 05:25:46,947 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
datanode_2  | 2023-01-12 05:25:58,266 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1      | 2023-01-12 05:25:12,867 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode_1  | 2023-01-12 05:25:23,698 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
datanode_3  | 2023-01-12 05:25:21,897 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO server.GrpcService: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: GrpcService started, listening on 9858
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1      | Max Size Entering Target per Iteration             26GB
scm3_1      | 2023-01-12 05:25:46,948 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3_1      | 2023-01-12 05:25:46,950 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2  | 2023-01-12 05:25:58,266 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
datanode_1  | 2023-01-12 05:25:23,698 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 2023-01-12 05:25:21,905 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO server.GrpcService: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: GrpcService started, listening on 9856
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1      | 2023-01-12 05:25:46,975 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: start RPC server
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
datanode_2  | 2023-01-12 05:25:58,268 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2023-01-12 05:25:58,327 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2023-01-12 05:25:58,334 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2023-01-12 05:25:24,698 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:32:54,303 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: loop2 of layout LEGACY in volume: 09780-target
datanode_3  | 2023-01-12 05:25:21,906 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO server.GrpcService: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: GrpcService started, listening on 9857
om2_1       | 2023-01-12 05:34:37,042 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-5026327384 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:34:45,366 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-7662104206 of layout LEGACY in volume: s3v
scm3_1      | 2023-01-12 05:25:47,012 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: GrpcService started, listening on 9894
scm3_1      | 2023-01-12 05:25:47,035 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: Started
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_2  | 2023-01-12 05:25:58,437 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2  | 2023-01-12 05:25:58,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2  | 2023-01-12 05:25:58,440 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2023-01-12 05:25:24,699 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:32:58,251 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: loop3 of layout LEGACY in volume: 09780-target
datanode_3  | 2023-01-12 05:25:21,918 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis cefdd7f0-5c68-4866-b1db-ab1e8bc5045a is started using port 9858 for RATIS
om2_1       | 2023-01-12 05:34:46,376 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-0967037321 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:34:48,353 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketDeleteRequest: Delete bucket failed for bucket:nosuchbucket-ozone-test-7546334706 in volume:s3v
om2_1       | BUCKET_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Bucket not exists
scm3_1      | 2023-01-12 05:25:47,085 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2/172.18.0.3:9863, nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863]
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1       | 2023-01-12 05:35:10,696 [qtp384515747-80] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg22, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_2  | 2023-01-12 05:25:58,525 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2023-01-12 05:25:25,699 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:33:02,001 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: loop1 of layout LEGACY in volume: 09780-target
datanode_3  | 2023-01-12 05:25:21,918 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis cefdd7f0-5c68-4866-b1db-ab1e8bc5045a is started using port 9857 for RATIS_ADMIN
om2_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketDeleteRequest.validateAndUpdateCache(OMBucketDeleteRequest.java:129)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-12 05:25:27,039 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
datanode_3  | 2023-01-12 05:25:21,918 [EndpointStateMachine task thread for scm1/172.18.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis cefdd7f0-5c68-4866-b1db-ab1e8bc5045a is started using port 9856 for RATIS_SERVER
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3_1      | 2023-01-12 05:25:49,814 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: receive installSnapshot: f98693f9-b2e2-4085-a3fc-32934ea8db14->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf#0-t2,notify:(t:1, i:0)
scm3_1      | 2023-01-12 05:25:49,874 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3_1      | 2023-01-12 05:25:49,875 [grpc-default-executor-0] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: change Leader from null to f98693f9-b2e2-4085-a3fc-32934ea8db14 at term 2 for installSnapshot, leader elected after 7078ms
s3g_1       | 2023-01-12 05:35:10,706 [qtp384515747-23] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg30, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | Max Size Leaving Source per Iteration              26GB
datanode_2  | 2023-01-12 05:25:58,525 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1       | 2023-01-12 05:33:09,005 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: link3 of layout LEGACY in volume: 09780-target
om1_1       | 2023-01-12 05:33:38,729 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: link4 of layout LEGACY in volume: 09780-target
om1_1       | 2023-01-12 05:33:42,498 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketSetPropertyRequest: Setting bucket property failed for bucket:link4 in volume:09780-target
datanode_3  | 2023-01-12 05:25:21,919 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: Started
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1       | 2023-01-12 05:35:10,707 [qtp384515747-85] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg31, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
om3_1       | 2023-01-12 05:31:06,721 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: dangling-link of layout LEGACY in volume: 09780-target
om3_1       | 2023-01-12 05:31:15,086 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: link1 of layout LEGACY in volume: 09780-target
om3_1       | 2023-01-12 05:31:19,130 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: 09780-source
datanode_3  | 2023-01-12 05:25:22,530 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      | 2023-01-12 05:25:49,927 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: Received notification to install snapshot at index 0
scm3_1      | 2023-01-12 05:25:49,936 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
s3g_1       | 2023-01-12 05:35:10,871 [qtp384515747-18] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg33, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,882 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg32, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
om3_1       | 2023-01-12 05:32:39,509 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: link2 of layout LEGACY in volume: 09780-target
scm2_1      | 
scm2_1      | 2023-01-12 05:25:30,751 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
om1_1       | NOT_SUPPORTED_OPERATION org.apache.hadoop.ozone.om.exceptions.OMException: Cannot set property on link
om1_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketSetPropertyRequest.validateAndUpdateCache(OMBucketSetPropertyRequest.java:147)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
datanode_3  | 2023-01-12 05:25:22,533 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      | 2023-01-12 05:25:50,869 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: set new configuration index: 13
scm3_1      | configurationEntry {
scm3_1      |   peers {
s3g_1       | 2023-01-12 05:35:10,894 [qtp384515747-83] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg34, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,899 [qtp384515747-86] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg37, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,934 [qtp384515747-19] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg41, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | 2023-01-12 05:25:30,759 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
datanode_2  | 2023-01-12 05:25:58,536 [pool-22-thread-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E: start as a follower, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 2023-01-12 05:25:23,531 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      |     id: "296a2a14-8256-49c4-bc3a-84aaa5fb66c6"
scm3_1      |     address: "scm2:9894"
scm3_1      |     startupRole: FOLLOWER
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
s3g_1       | 2023-01-12 05:35:10,953 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg35, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:10,975 [qtp384515747-85] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg36, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm1_1      | 2023-01-12 05:25:12,870 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
scm1_1      | 2023-01-12 05:25:12,877 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm1_1      | 2023-01-12 05:25:12,878 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode_3  | 2023-01-12 05:25:23,533 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      |   }
om2_1       | 2023-01-12 05:34:52,462 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-5351982924 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:34:58,362 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-2191502675 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1      | 2023-01-12 05:25:30,765 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
datanode_2  | 2023-01-12 05:25:58,537 [pool-22-thread-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1      | 2023-01-12 05:25:12,908 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode_1  | 2023-01-12 05:25:25,700 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:26,700 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:24,532 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      |   peers {
scm3_1      |     id: "f98693f9-b2e2-4085-a3fc-32934ea8db14"
scm3_1      |     address: "scm1:9894"
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
om3_1       | 2023-01-12 05:32:43,287 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:link2 in volume:09780-target
scm2_1      | 2023-01-12 05:25:30,769 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e does not exist. Creating ...
scm2_1      | 2023-01-12 05:25:30,781 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/in_use.lock acquired by nodename 6@f2edb4bd00ab
scm1_1      | 2023-01-12 05:25:12,919 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 2023-01-12 05:25:24,536 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:35:10,040 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg7 of layout LEGACY in volume: s3v
scm3_1      |     startupRole: FOLLOWER
datanode_1  | 2023-01-12 05:25:26,701 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
s3g_1       | 2023-01-12 05:35:10,996 [qtp384515747-84] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg39, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | 2023-01-12 05:25:30,805 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e has been successfully formatted.
datanode_2  | 2023-01-12 05:25:58,542 [pool-22-thread-1] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: start 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-FollowerState
datanode_2  | 2023-01-12 05:25:58,574 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9E784E6BBA4E,id=34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
scm1_1      | 2023-01-12 05:25:12,937 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 2023-01-12 05:25:25,533 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:35:10,095 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg0 of layout LEGACY in volume: s3v
scm3_1      |   }
datanode_1  | 2023-01-12 05:25:27,702 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
s3g_1       | 2023-01-12 05:35:11,000 [qtp384515747-82] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg38, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | 2023-01-12 05:25:30,820 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2023-01-12 05:25:58,584 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2  | 2023-01-12 05:25:58,587 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1      | 2023-01-12 05:25:12,939 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3  | 2023-01-12 05:25:25,537 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:26,534 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:35:10,192 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg5 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,193 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg8 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,194 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg3 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:11,008 [qtp384515747-22] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg44, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | 2023-01-12 05:25:30,840 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2023-01-12 05:25:58,588 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-12 05:25:58,591 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1      | 2023-01-12 05:25:13,229 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1_1      | 2023-01-12 05:25:13,242 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3_1      | }
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
s3g_1       | 2023-01-12 05:35:11,026 [qtp384515747-80] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg40, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | 2023-01-12 05:25:30,840 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-12 05:25:58,593 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om3_1       | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
scm1_1      | 2023-01-12 05:25:13,242 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
datanode_3  | 2023-01-12 05:25:26,538 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:35:10,196 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg1 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,197 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg2 of layout LEGACY in volume: s3v
scm3_1      |  from snapshot
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
s3g_1       | 2023-01-12 05:35:11,105 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg45, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | 2023-01-12 05:25:30,842 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2_1      | 2023-01-12 05:25:30,844 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
scm1_1      | 2023-01-12 05:25:13,242 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3  | 2023-01-12 05:25:27,536 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:35:10,198 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg6 of layout LEGACY in volume: s3v
scm3_1      | 2023-01-12 05:25:50,891 [grpc-default-executor-0] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: set configuration 13: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_1  | 2023-01-12 05:25:27,702 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:34:00,677 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-8718359029 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:11,193 [qtp384515747-86] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg43, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_2  | 2023-01-12 05:25:58,594 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2  | 2023-01-12 05:25:58,770 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=cd62190c-0472-48bc-9601-9e784e6bba4e
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
scm1_1      | 2023-01-12 05:25:13,258 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om2_1       | 2023-01-12 05:35:10,209 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg4 of layout LEGACY in volume: s3v
datanode_3  | 2023-01-12 05:25:27,538 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      | 2023-01-12 05:25:50,965 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: reply installSnapshot: f98693f9-b2e2-4085-a3fc-32934ea8db14<-6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf#0:OK-t0,ALREADY_INSTALLED
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
datanode_1  | 2023-01-12 05:25:28,703 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:34:04,602 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-2923742260 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:11,197 [qtp384515747-21] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg51, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | 2023-01-12 05:25:30,848 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1      | 2023-01-12 05:25:30,858 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
scm1_1      | 2023-01-12 05:25:13,302 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServer: f98693f9-b2e2-4085-a3fc-32934ea8db14: found a subdirectory /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e
om2_1       | 2023-01-12 05:35:10,227 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg9 of layout LEGACY in volume: s3v
datanode_3  | 2023-01-12 05:25:28,537 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      | 2023-01-12 05:25:51,029 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: Completed INSTALL_SNAPSHOT, lastRequest: f98693f9-b2e2-4085-a3fc-32934ea8db14->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf#0-t2,notify:(t:1, i:0)
recon_1     | 2023-01-12 05:25:27,039 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.11:51392: output error
recon_1     | 2023-01-12 05:25:27,042 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
datanode_1  | 2023-01-12 05:25:28,703 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:29,704 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1       | 2023-01-12 05:35:11,201 [qtp384515747-23] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg50, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | 2023-01-12 05:25:30,858 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1      | 2023-01-12 05:25:30,867 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e
datanode_2  | 2023-01-12 05:25:58,797 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=cd62190c-0472-48bc-9601-9e784e6bba4e.
om2_1       | 2023-01-12 05:35:10,379 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg19 of layout LEGACY in volume: s3v
scm1_1      | 2023-01-12 05:25:13,305 [f98693f9-b2e2-4085-a3fc-32934ea8db14-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x404ecdc7] REGISTERED
datanode_3  | 2023-01-12 05:25:28,546 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      | 2023-01-12 05:25:51,314 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread1] INFO impl.RoleInfo: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: start 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-FollowerState
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_1  | 2023-01-12 05:25:29,705 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1       | 2023-01-12 05:35:11,202 [qtp384515747-83] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg48, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm2_1      | 2023-01-12 05:25:30,868 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2_1      | 2023-01-12 05:25:30,868 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1       | 2023-01-12 05:35:10,414 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg16 of layout LEGACY in volume: s3v
scm1_1      | 2023-01-12 05:25:13,308 [f98693f9-b2e2-4085-a3fc-32934ea8db14-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x404ecdc7] BIND: 0.0.0.0/0.0.0.0:0
scm3_1      | 2023-01-12 05:25:51,336 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread1] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
datanode_3  | 2023-01-12 05:25:29,538 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
datanode_1  | 2023-01-12 05:25:30,705 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 2023-01-12 05:34:16,554 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-7498891659 of layout LEGACY in volume: s3v
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm2_1      | 2023-01-12 05:25:30,874 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1      | 2023-01-12 05:25:30,875 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1       | 2023-01-12 05:35:10,453 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg13 of layout LEGACY in volume: s3v
scm1_1      | 2023-01-12 05:25:13,317 [main] INFO server.RaftServer: f98693f9-b2e2-4085-a3fc-32934ea8db14: addNew group-4D43FF37764E:[] returns group-4D43FF37764E:java.util.concurrent.CompletableFuture@15d0849[Not completed]
datanode_3  | 2023-01-12 05:25:29,546 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1      | 2023-01-12 05:25:51,378 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread1] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: inconsistency entries. Reply:f98693f9-b2e2-4085-a3fc-32934ea8db14<-6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
s3g_1       | 2023-01-12 05:35:11,202 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg47, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm1_1      | 2023-01-12 05:25:13,328 [f98693f9-b2e2-4085-a3fc-32934ea8db14-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x404ecdc7, L:/0.0.0.0:33413] ACTIVE
scm3_1      | 2023-01-12 05:25:51,447 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread2] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3_1      | 2023-01-12 05:25:51,447 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread2] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: inconsistency entries. Reply:f98693f9-b2e2-4085-a3fc-32934ea8db14<-6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
om2_1       | 2023-01-12 05:35:10,472 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg14 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,475 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg18 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,492 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg12 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:11,202 [qtp384515747-80] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg52, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm1_1      | 2023-01-12 05:25:13,356 [pool-16-thread-1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14: new RaftServerImpl for group-4D43FF37764E:[] with SCMStateMachine:uninitialized
datanode_3  | 2023-01-12 05:25:30,539 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/172.18.0.3:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:30,547 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:35:10,495 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg10 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,539 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg15 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,737 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg20 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:11,208 [qtp384515747-18] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg42, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,210 [qtp384515747-82] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg49, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm1_1      | 2023-01-12 05:25:13,361 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1_1      | 2023-01-12 05:25:13,361 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1      | 2023-01-12 05:25:13,362 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1      | 2023-01-12 05:25:13,367 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1      | 2023-01-12 05:25:13,367 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1      | 2023-01-12 05:25:51,547 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread2] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: set configuration 0: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1       | 2023-01-12 05:35:10,762 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg11 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,802 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg17 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,808 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg21 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:11,212 [qtp384515747-84] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg46, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,300 [qtp384515747-22] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg54, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:31,548 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1      | 2023-01-12 05:25:13,368 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1       | 2023-01-12 05:34:17,296 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ozone-test-qwhhyijwyi of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:34:20,192 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ufntzifvoh of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:34:32,009 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-7112304483 of layout LEGACY in volume: s3v
scm3_1      | 2023-01-12 05:25:51,547 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread2] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: set configuration 1: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1       | 2023-01-12 05:35:10,840 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg23 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,856 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg25 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,875 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg29 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:11,343 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg53, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,345 [qtp384515747-83] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg56, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:32,549 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1      | 2023-01-12 05:25:13,386 [pool-16-thread-1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1_1      | 2023-01-12 05:25:13,388 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om1_1       | 2023-01-12 05:34:32,953 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-3900182019 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:34:33,887 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-3726753216 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:11,346 [qtp384515747-86] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg55, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,388 [qtp384515747-21] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg58, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:33,552 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:58,810 [Command processor thread] INFO server.RaftServer: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: addNew group-0DA58DD390A6:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER] returns group-0DA58DD390A6:java.util.concurrent.CompletableFuture@413a763b[Not completed]
scm1_1      | 2023-01-12 05:25:13,393 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1       | 2023-01-12 05:34:34,781 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket-ozone-test-3726753216 in volume:s3v
om1_1       | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
s3g_1       | 2023-01-12 05:35:11,459 [qtp384515747-85] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg62, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,461 [qtp384515747-19] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg59, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,464 [qtp384515747-23] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg57, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,489 [qtp384515747-82] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg61, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm1_1      | 2023-01-12 05:25:13,396 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om2_1       | 2023-01-12 05:35:10,887 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg26 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,892 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg24 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:11,496 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg60, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,550 [qtp384515747-80] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg63, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,552 [qtp384515747-22] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg64, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,718 [qtp384515747-22] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg71, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm1_1      | 2023-01-12 05:25:13,414 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm3_1      | 2023-01-12 05:25:51,573 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread2] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: set configuration 11: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1      | 2023-01-12 05:25:51,573 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread2] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: set configuration 13: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1       | 2023-01-12 05:35:10,899 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg27 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,906 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg28 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,913 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg22 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:11,723 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg65, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1      | 2023-01-12 05:25:13,421 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1      | 2023-01-12 05:25:13,422 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1      | 2023-01-12 05:25:51,612 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread2] INFO segmented.SegmentedRaftLogWorker: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-SegmentedRaftLogWorker: Starting segment from index:0
scm2_1      | 2023-01-12 05:25:30,876 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1       | 2023-01-12 05:35:10,920 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg31 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,932 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg30 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,943 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg33 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:11,722 [qtp384515747-18] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg69, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,722 [qtp384515747-86] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg66, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,722 [qtp384515747-84] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg72, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,728 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg73, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,745 [qtp384515747-19] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg70, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm1_1      | 2023-01-12 05:25:13,540 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1      | 2023-01-12 05:25:51,851 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread2] INFO segmented.SegmentedRaftLogWorker: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm2_1      | 2023-01-12 05:25:30,879 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
s3g_1       | 2023-01-12 05:35:11,746 [qtp384515747-83] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg67, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,760 [qtp384515747-23] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg74, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,777 [qtp384515747-85] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg68, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,965 [qtp384515747-80] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg81, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,970 [qtp384515747-21] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg75, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1      | 2023-01-12 05:25:13,541 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2_1      | 2023-01-12 05:25:30,879 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1      | 2023-01-12 05:25:51,938 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread1] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: set configuration 0: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1      | 2023-01-12 05:25:51,956 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread1] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: set configuration 1: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-12 05:25:27,042 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
scm1_1      | 2023-01-12 05:25:13,543 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1      | 2023-01-12 05:25:30,880 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1      | 2023-01-12 05:25:30,897 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1       | 2023-01-12 05:35:10,953 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg32 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:10,965 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg37 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,030 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg41 of layout LEGACY in volume: s3v
scm3_1      | 2023-01-12 05:25:51,957 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread1] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: set configuration 11: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1      | 2023-01-12 05:25:51,958 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread1] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: set configuration 13: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1      | 2023-01-12 05:25:51,974 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread1] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: set configuration 15: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1      | 2023-01-12 05:25:51,992 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-server-thread1] INFO server.RaftServer$Division: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E: set configuration 17: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1      | 2023-01-12 05:25:13,545 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1      | 2023-01-12 05:25:30,898 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1      | 2023-01-12 05:25:52,159 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-4D43FF37764E:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1      | 2023-01-12 05:25:30,916 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1      | 2023-01-12 05:25:52,188 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
datanode_2  | 2023-01-12 05:25:58,916 [pool-22-thread-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: new RaftServerImpl for group-0DA58DD390A6:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2  | 2023-01-12 05:25:58,960 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 2023-01-12 05:35:11,064 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg35 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,092 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg34 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,096 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg36 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,128 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg39 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,134 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg38 of layout LEGACY in volume: s3v
scm2_1      | 2023-01-12 05:25:30,916 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1      | 2023-01-12 05:25:52,648 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_inprogress_0
s3g_1       | 2023-01-12 05:35:11,970 [qtp384515747-86] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg82, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:11,969 [qtp384515747-82] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg76, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:12,013 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg78, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:12,015 [qtp384515747-22] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg84, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:12,015 [qtp384515747-84] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg80, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_1  | 2023-01-12 05:25:30,707 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:35:11,138 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg44 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,141 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg40 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,263 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg45 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,291 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg43 of layout LEGACY in volume: s3v
scm2_1      | 2023-01-12 05:25:30,917 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1      | 2023-01-12 05:25:13,546 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 2023-01-12 05:34:37,035 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-5026327384 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:34:45,368 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-7662104206 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:34:46,370 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-0967037321 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,294 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg48 of layout LEGACY in volume: s3v
datanode_3  | 2023-01-12 05:25:34,552 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1      | 2023-01-12 05:25:30,996 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3_1      | 2023-01-12 05:25:52,734 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_inprogress_0 to /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_0-0
scm1_1      | 2023-01-12 05:25:13,554 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
om1_1       | 2023-01-12 05:34:48,349 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketDeleteRequest: Delete bucket failed for bucket:nosuchbucket-ozone-test-7546334706 in volume:s3v
datanode_1  | 2023-01-12 05:25:31,707 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:32,709 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:33,710 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:34,711 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | 2023-01-12 05:32:47,276 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket3 of layout LEGACY in volume: 09780-target
s3g_1       | 2023-01-12 05:35:12,024 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg77, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:12,033 [qtp384515747-19] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg83, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm2_1      | 2023-01-12 05:25:30,997 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3_1      | 2023-01-12 05:25:52,896 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_inprogress_1
scm1_1      | 2023-01-12 05:25:13,554 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1       | BUCKET_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Bucket not exists
datanode_1  | 2023-01-12 05:25:35,712 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:36,713 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:36,833 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
om3_1       | 2023-01-12 05:32:50,976 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket3 in volume:09780-target
om3_1       | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
datanode_3  | 2023-01-12 05:25:35,554 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1      | 2023-01-12 05:25:31,008 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm2_1      | 2023-01-12 05:25:31,009 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: changes role from      null to FOLLOWER at term 0 for startInitializing
om1_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketDeleteRequest.validateAndUpdateCache(OMBucketDeleteRequest.java:129)
datanode_1  | 2023-01-12 05:25:37,714 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
datanode_2  | 2023-01-12 05:25:58,962 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2023-01-12 05:25:58,964 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
s3g_1       | 2023-01-12 05:35:12,057 [qtp384515747-18] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg79, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:36,555 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1      | 2023-01-12 05:25:31,017 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4D43FF37764E,id=296a2a14-8256-49c4-bc3a-84aaa5fb66c6
scm2_1      | 2023-01-12 05:25:31,042 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1       | 2023-01-12 05:35:11,296 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg51 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,336 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg50 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,337 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg42 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:12,128 [qtp384515747-83] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg85, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_2  | 2023-01-12 05:25:58,965 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm3_1      | 2023-01-12 05:25:53,001 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1      | 2023-01-12 05:25:53,006 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3_1      | 2023-01-12 05:25:53,011 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2_1      | 2023-01-12 05:25:31,049 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
s3g_1       | 2023-01-12 05:35:12,200 [qtp384515747-80] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg86, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:36,816 [EndpointStateMachine task thread for scm2/172.18.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
om2_1       | 2023-01-12 05:35:11,359 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg49 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,385 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg47 of layout LEGACY in volume: s3v
scm3_1      | 2023-01-12 05:25:53,012 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3_1      | 2023-01-12 05:25:53,370 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3_1      | 2023-01-12 05:25:53,527 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2_1      | 2023-01-12 05:25:31,052 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2_1      | 2023-01-12 05:25:31,054 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
s3g_1       | 2023-01-12 05:35:12,214 [qtp384515747-82] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg88, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:12,258 [qtp384515747-86] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg87, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:12,262 [qtp384515747-22] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg89, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:37,557 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:38,558 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1      | 2023-01-12 05:25:31,086 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6: start RPC server
datanode_1  | 2023-01-12 05:25:38,716 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
om2_1       | 2023-01-12 05:35:11,405 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg46 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,420 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg52 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:12,261 [qtp384515747-85] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg90, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:12,327 [qtp384515747-18] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg94, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_1  | 2023-01-12 05:25:39,717 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:40,718 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1       | 2023-01-12 05:35:11,462 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg54 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,484 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg53 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:35:12,328 [qtp384515747-23] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg92, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm1_1      | 2023-01-12 05:25:13,555 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm1_1      | 2023-01-12 05:25:13,585 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_1  | 2023-01-12 05:25:41,719 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:42,720 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | 2023-01-12 05:32:54,330 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: loop2 of layout LEGACY in volume: 09780-target
om3_1       | 2023-01-12 05:32:58,263 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: loop3 of layout LEGACY in volume: 09780-target
datanode_2  | 2023-01-12 05:25:58,966 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2  | 2023-01-12 05:25:58,967 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1       | 2023-01-12 05:35:11,494 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg55 of layout LEGACY in volume: s3v
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1       | 2023-01-12 05:35:12,329 [qtp384515747-16] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg93, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm1_1      | 2023-01-12 05:25:14,057 [main] INFO reflections.Reflections: Reflections took 414 ms to scan 3 urls, producing 121 keys and 272 values 
scm1_1      | 2023-01-12 05:25:14,186 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
om3_1       | 2023-01-12 05:33:02,011 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: loop1 of layout LEGACY in volume: 09780-target
om3_1       | 2023-01-12 05:33:09,014 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: link3 of layout LEGACY in volume: 09780-target
om2_1       | 2023-01-12 05:35:11,497 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg56 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,532 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg58 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:25:58,968 [pool-22-thread-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: ConfigurationManager, init=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2  | 2023-01-12 05:25:58,972 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2023-01-12 05:25:58,975 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
s3g_1       | 2023-01-12 05:35:12,332 [qtp384515747-21] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg91, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_3  | 2023-01-12 05:25:39,559 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:43,721 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-12 05:25:44,722 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1       | 2023-01-12 05:33:38,735 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: link4 of layout LEGACY in volume: 09780-target
om3_1       | 2023-01-12 05:33:42,504 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketSetPropertyRequest: Setting bucket property failed for bucket:link4 in volume:09780-target
om3_1       | NOT_SUPPORTED_OPERATION org.apache.hadoop.ozone.om.exceptions.OMException: Cannot set property on link
scm3_1      | 2023-01-12 05:25:53,529 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
datanode_2  | 2023-01-12 05:25:58,977 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2  | 2023-01-12 05:25:58,978 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2  | 2023-01-12 05:25:58,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
s3g_1       | 2023-01-12 05:35:12,407 [qtp384515747-83] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg96, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:35:12,422 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg95, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_1  | 2023-01-12 05:25:45,723 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1       | 2023-01-12 05:35:11,561 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg62 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,584 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg61 of layout LEGACY in volume: s3v
scm3_1      | 2023-01-12 05:25:55,805 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
datanode_2  | 2023-01-12 05:25:58,986 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1      | 2023-01-12 05:25:14,187 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm1_1      | 2023-01-12 05:25:14,190 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm2_1      | 2023-01-12 05:25:31,113 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6: GrpcService started, listening on 9894
s3g_1       | 2023-01-12 05:35:12,497 [qtp384515747-80] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg97, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_1  | 2023-01-12 05:25:56,797 [Command processor thread] INFO server.RaftServer: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: addNew group-0EE7B13ECDDC:[f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-0EE7B13ECDDC:java.util.concurrent.CompletableFuture@7f9f6dd[Not completed]
om3_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketSetPropertyRequest.validateAndUpdateCache(OMBucketSetPropertyRequest.java:147)
om2_1       | 2023-01-12 05:35:11,601 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg57 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:34:52,454 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-5351982924 of layout LEGACY in volume: s3v
scm3_1      | 2023-01-12 05:25:55,835 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3_1      | 2023-01-12 05:25:55,884 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
datanode_2  | 2023-01-12 05:25:59,042 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2023-01-12 05:25:59,055 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2  | 2023-01-12 05:25:59,056 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1      | 2023-01-12 05:25:31,123 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-296a2a14-8256-49c4-bc3a-84aaa5fb66c6: Started
s3g_1       | 2023-01-12 05:35:12,512 [qtp384515747-82] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg98, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_1  | 2023-01-12 05:25:56,983 [pool-22-thread-1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: new RaftServerImpl for group-0EE7B13ECDDC:[f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om2_1       | 2023-01-12 05:35:11,606 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg59 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,634 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg63 of layout LEGACY in volume: s3v
recon_1     | 2023-01-12 05:25:27,056 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
scm3_1      | 2023-01-12 05:25:56,077 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_2  | 2023-01-12 05:25:59,061 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2  | 2023-01-12 05:25:59,074 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2  | 2023-01-12 05:25:59,085 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 does not exist. Creating ...
scm2_1      | 2023-01-12 05:25:31,148 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1/172.18.0.2:9863, nodeId=scm3,nodeAddress=scm3/172.18.0.9:9863]
s3g_1       | 2023-01-12 05:35:12,537 [qtp384515747-22] INFO rpc.RpcClient: Creating Bucket: s3v/s3bg99, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_1  | 2023-01-12 05:25:57,021 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1       | 2023-01-12 05:35:11,637 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg64 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:34:58,352 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-2191502675 of layout LEGACY in volume: s3v
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1      | 2023-01-12 05:25:14,197 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm1_1      | 2023-01-12 05:25:14,275 [main] INFO node.SCMNodeManager: Entering startup safe mode.
datanode_2  | 2023-01-12 05:25:59,095 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6/in_use.lock acquired by nodename 7@4c4cfcfeca58
scm2_1      | 2023-01-12 05:25:34,198 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: receive installSnapshot: f98693f9-b2e2-4085-a3fc-32934ea8db14->296a2a14-8256-49c4-bc3a-84aaa5fb66c6#0-t2,notify:(t:1, i:0)
s3g_1       | 2023-01-12 05:35:17,134 [qtp384515747-85] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-0191226535, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_1  | 2023-01-12 05:25:57,031 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 2023-01-12 05:35:11,638 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg60 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,032 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg7 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,066 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg0 of layout LEGACY in volume: s3v
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_3  | 2023-01-12 05:25:40,560 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:41,561 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:59,109 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 has been successfully formatted.
datanode_2  | 2023-01-12 05:25:59,132 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-0DA58DD390A6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
s3g_1       | 2023-01-12 05:36:42,821 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-8056751975, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_1  | 2023-01-12 05:25:57,031 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1      | 2023-01-12 05:25:56,078 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm3_1      | 2023-01-12 05:25:56,334 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861: skipped Call#14 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.6:50398
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
datanode_3  | 2023-01-12 05:25:42,564 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:43,565 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:59,177 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1      | 2023-01-12 05:25:34,219 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
s3g_1       | 2023-01-12 05:36:43,730 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/destbucket-12311, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1       | 2023-01-12 05:35:11,759 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg66 of layout LEGACY in volume: s3v
datanode_1  | 2023-01-12 05:25:57,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1  | 2023-01-12 05:25:57,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1  | 2023-01-12 05:25:57,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
datanode_3  | 2023-01-12 05:25:44,566 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-12 05:25:45,567 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-12 05:25:59,177 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2_1      | 2023-01-12 05:25:34,219 [grpc-default-executor-0] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: change Leader from null to f98693f9-b2e2-4085-a3fc-32934ea8db14 at term 2 for installSnapshot, leader elected after 6375ms
scm2_1      | 2023-01-12 05:25:34,246 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: Received notification to install snapshot at index 0
s3g_1       | 2023-01-12 05:36:56,356 [qtp384515747-85] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-5672640421, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1       | 2023-01-12 05:37:15,242 [qtp384515747-85] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-2727023149, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
datanode_1  | 2023-01-12 05:25:57,100 [pool-22-thread-1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC: ConfigurationManager, init=-1: peers:[f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1  | 2023-01-12 05:25:57,100 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2023-01-12 05:25:57,213 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
datanode_3  | 2023-01-12 05:25:46,568 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/172.18.0.9:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1      | 2023-01-12 05:25:14,287 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm1_1      | 2023-01-12 05:25:14,288 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2_1      | 2023-01-12 05:25:34,249 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm2_1      | 2023-01-12 05:25:34,762 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: set new configuration index: 1
om2_1       | 2023-01-12 05:35:11,784 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg73 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,804 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg65 of layout LEGACY in volume: s3v
datanode_1  | 2023-01-12 05:25:57,213 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
s3g_1       | 2023-01-12 05:37:23,349 [qtp384515747-85] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-5928603759, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
om1_1       | 2023-01-12 05:35:10,116 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg5 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
datanode_2  | 2023-01-12 05:25:59,178 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-12 05:25:59,178 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1       | 2023-01-12 05:34:00,691 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-8718359029 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,813 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg72 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,826 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg69 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,848 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg71 of layout LEGACY in volume: s3v
scm3_1      | 2023-01-12 05:25:56,337 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861: skipped Call#13 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.6:50390
om1_1       | 2023-01-12 05:35:10,122 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg8 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1      | configurationEntry {
scm1_1      | 2023-01-12 05:25:14,292 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
om3_1       | 2023-01-12 05:34:04,623 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-2923742260 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,851 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg74 of layout LEGACY in volume: s3v
datanode_1  | 2023-01-12 05:25:57,376 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
s3g_1       | 2023-01-12 05:37:32,442 [qtp384515747-85] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-7720168204, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
scm3_1      | 2023-01-12 05:25:57,109 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
om1_1       | 2023-01-12 05:35:10,126 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg3 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
datanode_3  | 2023-01-12 05:25:54,217 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
scm2_1      |   peers {
scm2_1      |     id: "f98693f9-b2e2-4085-a3fc-32934ea8db14"
scm1_1      | 2023-01-12 05:25:14,337 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
om3_1       | 2023-01-12 05:34:16,559 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-7498891659 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,854 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg67 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,863 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg70 of layout LEGACY in volume: s3v
datanode_1  | 2023-01-12 05:25:57,379 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm3_1      | 2023-01-12 05:25:57,110 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
om1_1       | 2023-01-12 05:35:10,150 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg1 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
datanode_2  | 2023-01-12 05:25:59,178 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2  | 2023-01-12 05:25:59,178 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm1_1      | 2023-01-12 05:25:14,338 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
om3_1       | 2023-01-12 05:34:17,314 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ozone-test-qwhhyijwyi of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,872 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg68 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:37:58,921 [qtp384515747-22] WARN server.HttpChannel: /bucket-ozone-test-7720168204/ozone-test-0170484136/putobject/custom-metadata/key2
datanode_1  | 2023-01-12 05:25:57,379 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1      | 2023-01-12 05:25:57,118 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1       | 2023-01-12 05:35:10,158 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg2 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
datanode_3  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2  | 2023-01-12 05:25:59,179 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2023-01-12 05:25:59,187 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1       | 2023-01-12 05:34:20,188 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ufntzifvoh of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:11,995 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg81 of layout LEGACY in volume: s3v
s3g_1       | javax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
datanode_1  | 2023-01-12 05:25:57,773 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1      | 2023-01-12 05:25:57,118 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
om1_1       | 2023-01-12 05:35:10,169 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg6 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
scm2_1      |     address: "scm1:9894"
scm2_1      |     startupRole: FOLLOWER
om3_1       | 2023-01-12 05:34:32,029 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-7112304483 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,038 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg76 of layout LEGACY in volume: s3v
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:162)
datanode_1  | 2023-01-12 05:25:57,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1      | 2023-01-12 05:25:57,565 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3d8d970e] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1       | 2023-01-12 05:35:10,229 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg4 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
scm2_1      |   }
scm2_1      | }
scm1_1      | 2023-01-12 05:25:14,342 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
om3_1       | 2023-01-12 05:34:32,970 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-3900182019 of layout LEGACY in volume: s3v
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
datanode_1  | 2023-01-12 05:25:57,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1      | 2023-01-12 05:25:57,959 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: fb7937dd-8506-409c-a1e6-3cdc4fafc0d4, Nodes: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:22.284Z[UTC]].
om1_1       | 2023-01-12 05:35:10,255 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg9 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
scm2_1      |  from snapshot
scm2_1      | 2023-01-12 05:25:34,797 [grpc-default-executor-0] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: set configuration 1: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1      | 2023-01-12 05:25:14,342 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
om3_1       | 2023-01-12 05:34:33,897 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-3726753216 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,066 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg82 of layout LEGACY in volume: s3v
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
om1_1       | 2023-01-12 05:35:10,380 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg19 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,400 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg16 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,445 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg13 of layout LEGACY in volume: s3v
scm2_1      | 2023-01-12 05:25:34,807 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: reply installSnapshot: f98693f9-b2e2-4085-a3fc-32934ea8db14<-296a2a14-8256-49c4-bc3a-84aaa5fb66c6#0:OK-t0,ALREADY_INSTALLED
scm2_1      | 2023-01-12 05:25:34,875 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6: Completed INSTALL_SNAPSHOT, lastRequest: f98693f9-b2e2-4085-a3fc-32934ea8db14->296a2a14-8256-49c4-bc3a-84aaa5fb66c6#0-t2,notify:(t:1, i:0)
scm1_1      | 2023-01-12 05:25:14,346 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
om3_1       | 2023-01-12 05:34:34,790 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket-ozone-test-3726753216 in volume:s3v
om3_1       | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
scm3_1      | 2023-01-12 05:25:58,023 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3_1      | 2023-01-12 05:25:58,025 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1       | 2023-01-12 05:35:10,469 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg14 of layout LEGACY in volume: s3v
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
scm1_1      | 2023-01-12 05:25:14,347 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
scm3_1      | 2023-01-12 05:25:58,026 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3_1      | 2023-01-12 05:25:58,026 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
om1_1       | 2023-01-12 05:35:10,482 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg18 of layout LEGACY in volume: s3v
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:309)
datanode_2  | 2023-01-12 05:25:59,188 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6
datanode_2  | 2023-01-12 05:25:59,197 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2  | 2023-01-12 05:25:59,198 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm1_1      | 2023-01-12 05:25:14,354 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
om1_1       | 2023-01-12 05:35:10,507 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg12 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:509)
datanode_2  | 2023-01-12 05:25:59,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2023-01-12 05:25:59,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2023-01-12 05:25:59,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2023-01-12 05:25:59,205 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 2023-01-12 05:25:59,206 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2023-01-12 05:25:59,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2023-01-12 05:25:59,253 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om1_1       | 2023-01-12 05:35:10,517 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg10 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1  | 2023-01-12 05:25:57,775 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1  | 2023-01-12 05:25:57,775 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1  | 2023-01-12 05:25:57,814 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc does not exist. Creating ...
datanode_1  | 2023-01-12 05:25:57,890 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc/in_use.lock acquired by nodename 7@d9a6a467ccde
scm3_1      | 2023-01-12 05:25:58,045 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm3_1      | 2023-01-12 05:25:58,263 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: cd62190c-0472-48bc-9601-9e784e6bba4e, Nodes: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:23.217Z[UTC]].
scm3_1      | 2023-01-12 05:25:58,267 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1      | 2023-01-12 05:25:58,278 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:23.423Z[UTC]].
scm3_1      | 2023-01-12 05:25:58,283 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1       | 2023-01-12 05:35:10,525 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg15 of layout LEGACY in volume: s3v
recon_1     | 2023-01-12 05:25:54,284 [IPC Server handler 14 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
recon_1     | 2023-01-12 05:25:54,299 [IPC Server handler 14 on default port 9891] INFO node.SCMNodeManager: Registered Data node : cefdd7f0-5c68-4866-b1db-ab1e8bc5045a{ip: 172.18.0.4, host: ozone-ha_datanode_3.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1       | 2023-01-12 05:35:12,072 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg75 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,127 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg78 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,143 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg84 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,170 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg80 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,181 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg77 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,191 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg83 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,197 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg79 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,203 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg85 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,242 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg86 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,279 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg88 of layout LEGACY in volume: s3v
datanode_3  | Caused by: java.util.concurrent.TimeoutException
recon_1     | 2023-01-12 05:25:54,480 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node cefdd7f0-5c68-4866-b1db-ab1e8bc5045a to Node DB.
om1_1       | 2023-01-12 05:35:10,566 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg20 of layout LEGACY in volume: s3v
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
recon_1     | 2023-01-12 05:25:55,248 [IPC Server handler 14 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
recon_1     | 2023-01-12 05:25:55,248 [IPC Server handler 14 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b{ip: 172.18.0.11, host: ozone-ha_datanode_2.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1      | 2023-01-12 05:25:14,354 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1_1      | 2023-01-12 05:25:14,397 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1_1      | 2023-01-12 05:25:14,416 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1_1      | 2023-01-12 05:25:14,462 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm1_1      | 2023-01-12 05:25:14,482 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1_1      | 2023-01-12 05:25:14,485 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm1_1      | WARNING: An illegal reflective access operation has occurred
scm1_1      | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm3_1      | 2023-01-12 05:25:58,300 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 7b2ccc1c-57aa-4786-a2ba-0da58dd390a6, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6)34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11)cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:23.439Z[UTC]].
scm3_1      | 2023-01-12 05:25:58,309 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
datanode_1  | 2023-01-12 05:25:57,953 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc has been successfully formatted.
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
scm2_1      | 2023-01-12 05:25:35,037 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread1] INFO impl.RoleInfo: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6: start 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-FollowerState
scm2_1      | 2023-01-12 05:25:35,064 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread1] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3_1      | 2023-01-12 05:25:58,357 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1      | 2023-01-12 05:25:58,590 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 2023-01-12 05:25:58,011 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-0EE7B13ECDDC: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2023-01-12 05:25:58,013 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
scm2_1      | 2023-01-12 05:25:35,069 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread1] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: inconsistency entries. Reply:f98693f9-b2e2-4085-a3fc-32934ea8db14<-296a2a14-8256-49c4-bc3a-84aaa5fb66c6#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
om2_1       | 2023-01-12 05:35:12,298 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg89 of layout LEGACY in volume: s3v
scm3_1      | 2023-01-12 05:25:58,659 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1       | 2023-01-12 05:35:10,597 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg11 of layout LEGACY in volume: s3v
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1  | 2023-01-12 05:25:58,248 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2023-01-12 05:25:58,248 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-12 05:25:59,261 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-12 05:26:00,494 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: Detected pause in JVM or host machine (eg GC): pause of approximately 1136498233ns.
datanode_2  | GC pool 'ParNew' had collection(s): count=1 time=82ms
scm1_1      | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
om2_1       | 2023-01-12 05:35:12,318 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg90 of layout LEGACY in volume: s3v
scm3_1      | 2023-01-12 05:25:59,046 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @21632ms to org.eclipse.jetty.util.log.Slf4jLog
scm3_1      | 2023-01-12 05:26:00,058 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#13 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.4:37398: output error
om1_1       | 2023-01-12 05:35:10,630 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg17 of layout LEGACY in volume: s3v
recon_1     | 2023-01-12 05:25:55,250 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b to Node DB.
recon_1     | 2023-01-12 05:25:55,463 [IPC Server handler 13 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f23956f4-f232-4ae2-8f3e-cd8efc9f95bf
recon_1     | 2023-01-12 05:25:55,464 [IPC Server handler 13 on default port 9891] INFO node.SCMNodeManager: Registered Data node : f23956f4-f232-4ae2-8f3e-cd8efc9f95bf{ip: 172.18.0.6, host: ozone-ha_datanode_1.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_2  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1142ms
datanode_2  | 2023-01-12 05:26:00,495 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode_2  | 2023-01-12 05:26:00,503 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2  | 2023-01-12 05:26:00,504 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2  | 2023-01-12 05:26:00,504 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm1_1      | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm3_1      | 2023-01-12 05:26:00,083 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm3_1      | java.nio.channels.ClosedChannelException
om1_1       | 2023-01-12 05:35:10,636 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg21 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,730 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg23 of layout LEGACY in volume: s3v
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_1  | 2023-01-12 05:25:58,249 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2_1      | 2023-01-12 05:25:35,083 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread1] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2_1      | 2023-01-12 05:25:35,084 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread1] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: inconsistency entries. Reply:f98693f9-b2e2-4085-a3fc-32934ea8db14<-296a2a14-8256-49c4-bc3a-84aaa5fb66c6#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
om2_1       | 2023-01-12 05:35:12,358 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg92 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,374 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg93 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,379 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg87 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,504 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg94 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,510 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg91 of layout LEGACY in volume: s3v
scm1_1      | WARNING: All illegal access operations will be denied in a future release
recon_1     | 2023-01-12 05:25:55,465 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node f23956f4-f232-4ae2-8f3e-cd8efc9f95bf to Node DB.
recon_1     | 2023-01-12 05:25:56,996 [IPC Server handler 10 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozone-ha_datanode_3.ozone-ha_default
recon_1     | 2023-01-12 05:25:56,997 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=fb7937dd-8506-409c-a1e6-3cdc4fafc0d4 reported by cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4)
scm2_1      | 2023-01-12 05:25:35,173 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread1] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: set configuration 0: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1      | 2023-01-12 05:25:35,175 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread1] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: set configuration 1: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1       | 2023-01-12 05:34:37,053 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-5026327384 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:34:45,373 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-7662104206 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:00,505 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2023-01-12 05:26:00,505 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1      | 2023-01-12 05:25:14,498 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
recon_1     | 2023-01-12 05:25:56,997 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: fb7937dd-8506-409c-a1e6-3cdc4fafc0d4, Nodes: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, CreationTimestamp2023-01-12T05:25:22.284Z[UTC]] moved to OPEN state
recon_1     | 2023-01-12 05:25:58,041 [IPC Server handler 3 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozone-ha_datanode_2.ozone-ha_default
datanode_1  | 2023-01-12 05:25:58,267 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1       | Caused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:410)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 2023-01-12 05:25:58,042 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=cd62190c-0472-48bc-9601-9e784e6bba4e reported by 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11)
recon_1     | 2023-01-12 05:25:58,042 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: cd62190c-0472-48bc-9601-9e784e6bba4e, Nodes: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, CreationTimestamp2023-01-12T05:25:23.217Z[UTC]] moved to OPEN state
recon_1     | 2023-01-12 05:25:58,234 [IPC Server handler 14 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozone-ha_datanode_1.ozone-ha_default
recon_1     | 2023-01-12 05:25:58,235 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc reported by f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6)
recon_1     | 2023-01-12 05:25:58,235 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f23956f4-f232-4ae2-8f3e-cd8efc9f95bf, CreationTimestamp2023-01-12T05:25:23.423Z[UTC]] moved to OPEN state
recon_1     | 2023-01-12 05:25:58,823 [IPC Server handler 8 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozone-ha_datanode_3.ozone-ha_default
recon_1     | 2023-01-12 05:25:58,825 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 reported by cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4)
recon_1     | 2023-01-12 05:25:59,228 [IPC Server handler 14 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozone-ha_datanode_2.ozone-ha_default
recon_1     | 2023-01-12 05:25:59,229 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 reported by 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11)
recon_1     | 2023-01-12 05:26:00,290 [IPC Server handler 10 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozone-ha_datanode_1.ozone-ha_default
scm1_1      | 2023-01-12 05:25:14,503 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1      | 2023-01-12 05:25:14,505 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1_1      | 2023-01-12 05:25:15,350 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1      | 2023-01-12 05:25:15,378 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1      | 2023-01-12 05:25:15,415 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1_1      | 2023-01-12 05:25:15,496 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1      | 2023-01-12 05:25:15,505 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1      | 2023-01-12 05:25:15,507 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2_1      | 2023-01-12 05:25:35,199 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread1] INFO segmented.SegmentedRaftLogWorker: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-SegmentedRaftLogWorker: Starting segment from index:0
scm2_1      | 2023-01-12 05:25:35,260 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread1] INFO segmented.SegmentedRaftLogWorker: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm2_1      | 2023-01-12 05:25:35,280 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread2] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: set configuration 0: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1      | 2023-01-12 05:25:35,280 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread2] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: set configuration 1: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1      | 2023-01-12 05:25:35,488 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_inprogress_0
scm2_1      | 2023-01-12 05:25:35,512 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_inprogress_0 to /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_0-0
om3_1       | 2023-01-12 05:34:46,378 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-0967037321 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:34:48,354 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketDeleteRequest: Delete bucket failed for bucket:nosuchbucket-ozone-test-7546334706 in volume:s3v
recon_1     | 2023-01-12 05:26:00,290 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 reported by f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6)
recon_1     | 2023-01-12 05:26:03,346 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 reported by cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4)
recon_1     | 2023-01-12 05:26:03,737 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 reported by 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11)
datanode_1  | 2023-01-12 05:25:58,297 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm3_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om2_1       | 2023-01-12 05:35:12,547 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg96 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,577 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg95 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,594 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg97 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,602 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg98 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:12,606 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg99 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:17,158 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-0191226535 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:35:37,142 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload: /s3v/bucket-ozone-test-0191226535/ozone-test-8175329106/multipartKey2 Part number: 1 size 6  is less than minimum part size 5242880
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	... 1 more
scm3_1      | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm3_1      | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
om3_1       | BUCKET_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Bucket not exists
om3_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketDeleteRequest.validateAndUpdateCache(OMBucketDeleteRequest.java:129)
datanode_2  | 2023-01-12 05:26:00,505 [pool-22-thread-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: start as a follower, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-12 05:26:00,506 [pool-22-thread-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1      | 2023-01-12 05:25:15,547 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1      | 2023-01-12 05:25:15,567 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3  | 2023-01-12 05:25:55,671 [Command processor thread] INFO server.RaftServer: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: addNew group-3CDC4FAFC0D4:[cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:1|startupRole:FOLLOWER] returns group-3CDC4FAFC0D4:java.util.concurrent.CompletableFuture@79e7b40f[Not completed]
datanode_1  | 2023-01-12 05:25:58,384 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2023-01-12 05:25:58,385 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1  | 2023-01-12 05:25:58,464 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc
scm3_1      | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm3_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm3_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
datanode_3  | 2023-01-12 05:25:55,800 [pool-22-thread-1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: new RaftServerImpl for group-3CDC4FAFC0D4:[cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3  | 2023-01-12 05:25:55,816 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2023-01-12 05:25:55,821 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2023-01-12 05:25:55,826 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1       | 2023-01-12 05:35:10,754 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg25 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,768 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg29 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,775 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg26 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,799 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg24 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,811 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg27 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,831 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg28 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,838 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg22 of layout LEGACY in volume: s3v
scm3_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm3_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om1_1       | 2023-01-12 05:35:10,847 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg31 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,855 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg30 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,905 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg33 of layout LEGACY in volume: s3v
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 2023-01-12 05:26:00,506 [pool-22-thread-1] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: start 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState
datanode_2  | 2023-01-12 05:26:00,508 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0DA58DD390A6,id=34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
datanode_2  | 2023-01-12 05:26:00,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2023-01-12 05:26:00,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2023-01-12 05:26:00,509 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2023-01-12 05:26:00,509 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2  | 2023-01-12 05:26:00,510 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1      | 2023-01-12 05:26:00,105 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#13 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.11:50104: output error
scm1_1      | 2023-01-12 05:25:15,572 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
datanode_1  | 2023-01-12 05:25:58,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
om2_1       | 2023-01-12 05:35:37,142 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-8175329106/multipartKey2 in Volume/Bucket s3v/bucket-ozone-test-0191226535
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1       | 2023-01-12 05:34:52,452 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-5351982924 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:34:58,357 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-2191502675 of layout LEGACY in volume: s3v
scm3_1      | 2023-01-12 05:26:00,087 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#14 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.11:50118: output error
scm3_1      | 2023-01-12 05:26:00,216 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
datanode_3  | 2023-01-12 05:25:55,848 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm1_1      | 2023-01-12 05:25:15,659 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2_1      | 2023-01-12 05:25:35,596 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_inprogress_1
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
om2_1       | ENTITY_TOO_SMALL org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-8175329106/multipartKey2. Entity too small.
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:534)
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:198)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
scm1_1      | 2023-01-12 05:25:15,660 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm3_1      | java.nio.channels.ClosedChannelException
datanode_3  | 2023-01-12 05:25:55,849 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1  | 2023-01-12 05:25:58,472 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2023-01-12 05:25:58,480 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2023-01-12 05:25:58,485 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2023-01-12 05:25:58,485 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1       | 2023-01-12 05:35:10,921 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg32 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:10,956 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg37 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,031 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg41 of layout LEGACY in volume: s3v
scm1_1      | Container Balancer status:
scm3_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_3  | 2023-01-12 05:25:55,851 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1  | 2023-01-12 05:25:58,507 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 2023-01-12 05:26:00,516 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-12 05:26:00,516 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6
datanode_2  | 2023-01-12 05:26:03,602 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-FollowerState] INFO impl.FollowerState: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5062281618ns, electionTimeout:5001ms
datanode_2  | 2023-01-12 05:26:03,607 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-FollowerState] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: shutdown 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-FollowerState
om1_1       | 2023-01-12 05:35:11,036 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg35 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,049 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg34 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,077 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg36 of layout LEGACY in volume: s3v
scm1_1      | Key                            Value
scm1_1      | Running                        true
scm1_1      | Container Balancer Configuration values:
datanode_1  | 2023-01-12 05:25:58,514 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2023-01-12 05:25:58,515 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2023-01-12 05:25:58,683 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2023-01-12 05:25:58,695 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2023-01-12 05:25:58,868 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1  | 2023-01-12 05:25:58,948 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1  | 2023-01-12 05:25:58,948 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2023-01-12 05:25:59,075 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_3  | 2023-01-12 05:25:55,902 [pool-22-thread-1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4: ConfigurationManager, init=-1: peers:[cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1      | 2023-01-12 05:25:35,619 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1      | 2023-01-12 05:25:35,632 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2_1      | 2023-01-12 05:25:35,633 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2_1      | 2023-01-12 05:25:35,636 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2_1      | 2023-01-12 05:25:35,664 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
datanode_1  | 2023-01-12 05:25:59,084 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1  | 2023-01-12 05:25:59,156 [pool-22-thread-1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC: start as a follower, conf=-1: peers:[f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-12 05:25:59,171 [pool-22-thread-1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm3_1      | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
datanode_3  | 2023-01-12 05:25:55,903 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1       | 2023-01-12 05:35:10,044 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg7 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,094 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg0 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,119 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg5 of layout LEGACY in volume: s3v
s3g_1       | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
datanode_1  | 2023-01-12 05:25:59,215 [pool-22-thread-1] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: start f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-FollowerState
scm1_1      | Key                                                Value
scm1_1      | Threshold                                          10
scm3_1      | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
datanode_3  | 2023-01-12 05:25:55,941 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2_1      | 2023-01-12 05:25:35,674 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1      | 2023-01-12 05:25:35,703 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread2] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: set configuration 11: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2_1      | 2023-01-12 05:25:35,820 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread2] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: set configuration 13: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1      | 2023-01-12 05:25:36,232 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-4D43FF37764E:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2_1      | 2023-01-12 05:25:36,251 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm2_1      | 2023-01-12 05:25:36,438 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: fb7937dd-8506-409c-a1e6-3cdc4fafc0d4, Nodes: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:22.284Z[UTC]].
scm2_1      | 2023-01-12 05:25:36,457 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1     | 2023-01-12 05:26:04,741 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 reported by f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6)
scm1_1      | Max Datanodes to Involve per Iteration(percent)    20
datanode_1  | 2023-01-12 05:25:59,254 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
datanode_3  | 2023-01-12 05:25:55,948 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1       | 2023-01-12 05:35:10,185 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg8 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,198 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg3 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,200 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg1 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,203 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg2 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,214 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg6 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,233 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg4 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,245 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg9 of layout LEGACY in volume: s3v
recon_1     | 2023-01-12 05:26:14,199 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
datanode_3  | 2023-01-12 05:25:56,043 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
om3_1       | 2023-01-12 05:35:10,379 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg19 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,409 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg16 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,452 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg13 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,467 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg14 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,474 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg18 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,495 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg12 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,503 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg10 of layout LEGACY in volume: s3v
scm3_1      | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 2023-01-12 05:26:14,200 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
datanode_3  | 2023-01-12 05:25:56,058 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om3_1       | 2023-01-12 05:35:10,541 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg15 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,561 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg20 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,580 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg11 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,714 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg17 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,717 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg21 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,776 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg23 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,788 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg25 of layout LEGACY in volume: s3v
scm3_1      | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
datanode_1  | 2023-01-12 05:25:59,280 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1     | 2023-01-12 05:26:14,271 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 1 failover attempts. Trying to failover immediately.
recon_1     | 2023-01-12 05:26:14,279 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
datanode_3  | 2023-01-12 05:25:56,060 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm3_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 2023-01-12 05:26:16,280 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 3 failover attempts. Trying to failover immediately.
recon_1     | 2023-01-12 05:26:16,281 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 4 failover attempts. Trying to failover immediately.
datanode_3  | 2023-01-12 05:25:56,421 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1       | 2023-01-12 05:35:11,097 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg39 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,105 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg38 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,127 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg44 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,141 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg40 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,236 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg45 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,250 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg43 of layout LEGACY in volume: s3v
scm3_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm3_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
datanode_2  | 2023-01-12 05:26:03,614 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-FollowerState] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3  | 2023-01-12 05:25:56,424 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1       | 2023-01-12 05:35:11,253 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg48 of layout LEGACY in volume: s3v
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1       | 2023-01-12 05:35:39,081 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 1
om2_1       | partName: "etag1"
om2_1       | , partNumber: 2
recon_1     | 2023-01-12 05:26:16,282 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-12 05:26:18,284 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 6 failover attempts. Trying to failover immediately.
scm3_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_3  | 2023-01-12 05:25:56,455 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1       | 2023-01-12 05:35:11,274 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg51 of layout LEGACY in volume: s3v
datanode_1  | 2023-01-12 05:25:59,297 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0EE7B13ECDDC,id=f23956f4-f232-4ae2-8f3e-cd8efc9f95bf
om2_1       | partName: "etag2"
scm1_1      | Max Size to Move per Iteration                     500GB
scm1_1      | Max Size Entering Target per Iteration             26GB
scm1_1      | Max Size Leaving Source per Iteration              26GB
scm1_1      | 
scm1_1      | 2023-01-12 05:25:15,661 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
datanode_2  | 2023-01-12 05:26:03,627 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm3_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_3  | 2023-01-12 05:25:56,469 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1       | 2023-01-12 05:35:11,317 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg50 of layout LEGACY in volume: s3v
datanode_1  | 2023-01-12 05:25:59,359 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1       | ]
om2_1       | 2023-01-12 05:35:39,093 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
om1_1       | 2023-01-12 05:35:11,319 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg42 of layout LEGACY in volume: s3v
datanode_1  | 2023-01-12 05:25:59,372 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm2_1      | 2023-01-12 05:25:36,457 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2_1      | 2023-01-12 05:25:36,457 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
datanode_3  | 2023-01-12 05:25:56,477 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
datanode_2  | 2023-01-12 05:26:03,628 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-FollowerState] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: start 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1
datanode_2  | 2023-01-12 05:26:03,677 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO impl.LeaderElection: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-12 05:26:03,690 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO impl.LeaderElection: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2  | 2023-01-12 05:26:03,700 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: shutdown 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1
scm2_1      | 2023-01-12 05:25:36,555 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: cd62190c-0472-48bc-9601-9e784e6bba4e, Nodes: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:23.217Z[UTC]].
scm2_1      | 2023-01-12 05:25:36,566 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_3  | 2023-01-12 05:25:56,488 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fb7937dd-8506-409c-a1e6-3cdc4fafc0d4 does not exist. Creating ...
scm1_1      | 2023-01-12 05:25:15,665 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1_1      | 2023-01-12 05:25:15,674 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1_1      | 2023-01-12 05:25:15,682 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/in_use.lock acquired by nodename 7@7f47dfd12ecd
scm1_1      | 2023-01-12 05:25:15,697 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=f98693f9-b2e2-4085-a3fc-32934ea8db14} from /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/raft-meta
scm1_1      | 2023-01-12 05:25:15,725 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: set configuration 0: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-12 05:26:03,702 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2  | 2023-01-12 05:26:03,707 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9E784E6BBA4E with new leaderId: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
scm1_1      | 2023-01-12 05:25:15,728 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1      | 2023-01-12 05:25:15,737 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1      | 2023-01-12 05:25:15,737 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1      | 2023-01-12 05:25:15,739 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2  | 2023-01-12 05:26:03,709 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E: change Leader from null to 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b at term 1 for becomeLeader, leader elected after 6310ms
datanode_2  | 2023-01-12 05:26:03,831 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2  | 2023-01-12 05:26:03,926 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1     | 2023-01-12 05:26:18,284 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 7 failover attempts. Trying to failover immediately.
om2_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3
datanode_1  | 2023-01-12 05:25:59,385 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2023-01-12 05:25:59,408 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om1_1       | 2023-01-12 05:35:11,362 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg49 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,375 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg47 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,397 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg46 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:03,946 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2  | 2023-01-12 05:26:03,993 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm2_1      | 2023-01-12 05:25:36,585 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:23.423Z[UTC]].
scm2_1      | 2023-01-12 05:25:36,590 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1     | 2023-01-12 05:26:18,287 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-12 05:26:20,289 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 9 failover attempts. Trying to failover immediately.
recon_1     | 2023-01-12 05:26:20,290 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 10 failover attempts. Trying to failover immediately.
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
scm1_1      | 2023-01-12 05:25:15,740 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1       | 2023-01-12 05:35:11,423 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg52 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:04,001 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2  | 2023-01-12 05:26:04,002 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2  | 2023-01-12 05:26:04,089 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2  | 2023-01-12 05:26:04,158 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1  | 2023-01-12 05:25:59,919 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc
scm3_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1      | 2023-01-12 05:25:15,743 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1       | 2023-01-12 05:35:10,791 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg29 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,450 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg54 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,507 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg53 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,534 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg55 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,587 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg56 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,618 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg58 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,641 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg62 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,655 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg61 of layout LEGACY in volume: s3v
datanode_1  | 2023-01-12 05:25:59,930 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc.
datanode_1  | 2023-01-12 05:25:59,941 [Command processor thread] INFO server.RaftServer: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: addNew group-0DA58DD390A6:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER] returns group-0DA58DD390A6:java.util.concurrent.CompletableFuture@55d27248[Not completed]
datanode_1  | 2023-01-12 05:26:00,041 [pool-22-thread-1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: new RaftServerImpl for group-0DA58DD390A6:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1  | 2023-01-12 05:26:00,048 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1      | 2023-01-12 05:25:15,750 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1      | 2023-01-12 05:25:15,750 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1      | 2023-01-12 05:25:15,757 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e
scm1_1      | 2023-01-12 05:25:15,757 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1      | 2023-01-12 05:25:15,758 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1      | 2023-01-12 05:25:15,759 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1      | 2023-01-12 05:25:15,759 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1      | 2023-01-12 05:25:15,760 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2023-01-12 05:26:00,049 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2023-01-12 05:26:00,068 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1  | 2023-01-12 05:26:00,086 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1  | 2023-01-12 05:26:00,087 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1      | 2023-01-12 05:25:15,761 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1      | 2023-01-12 05:25:15,761 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1      | 2023-01-12 05:25:15,761 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1      | 2023-01-12 05:25:15,784 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1_1      | 2023-01-12 05:25:15,785 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1      | 2023-01-12 05:25:15,796 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1      | 2023-01-12 05:25:15,797 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1      | 2023-01-12 05:25:15,797 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2_1      | 2023-01-12 05:25:36,609 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 7b2ccc1c-57aa-4786-a2ba-0da58dd390a6, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6)34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11)cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:23.439Z[UTC]].
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:187)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
recon_1     | 2023-01-12 05:26:20,292 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-12 05:26:22,294 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 12 failover attempts. Trying to failover immediately.
recon_1     | 2023-01-12 05:26:22,298 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 13 failover attempts. Trying to failover immediately.
datanode_1  | 2023-01-12 05:26:00,088 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2_1      | 2023-01-12 05:25:36,612 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1      | 2023-01-12 05:25:36,813 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#7 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.4:34544: output error
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1       | 2023-01-12 05:35:40,001 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 2
datanode_1  | 2023-01-12 05:26:00,090 [pool-22-thread-1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: ConfigurationManager, init=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1      | 2023-01-12 05:25:36,826 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm2_1      | java.nio.channels.ClosedChannelException
om2_1       | partName: "etag1"
om2_1       | , partNumber: 1
datanode_2  | 2023-01-12 05:26:04,220 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: start 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderStateImpl
datanode_2  | 2023-01-12 05:26:04,494 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-SegmentedRaftLogWorker: Starting segment from index:0
recon_1     | 2023-01-12 05:26:22,300 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-12 05:26:24,301 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 15 failover attempts. Trying to failover immediately.
datanode_1  | 2023-01-12 05:26:00,099 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
scm2_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1      | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
om3_1       | 2023-01-12 05:35:10,821 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg26 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,837 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg24 of layout LEGACY in volume: s3v
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
om2_1       | partName: "etag2"
om2_1       | ]
om2_1       | 2023-01-12 05:35:40,002 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
om2_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:187)
datanode_1  | 2023-01-12 05:26:00,102 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm2_1      | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
om3_1       | 2023-01-12 05:35:10,844 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg27 of layout LEGACY in volume: s3v
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
datanode_3  | 2023-01-12 05:25:56,567 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fb7937dd-8506-409c-a1e6-3cdc4fafc0d4/in_use.lock acquired by nodename 6@d407c6d45367
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
datanode_2  | 2023-01-12 05:26:05,316 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-LeaderElection1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E: set configuration 0: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-12 05:26:05,588 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO impl.FollowerState: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5081255319ns, electionTimeout:5071ms
datanode_1  | 2023-01-12 05:26:00,112 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2023-01-12 05:26:00,115 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
om3_1       | 2023-01-12 05:35:10,854 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg28 of layout LEGACY in volume: s3v
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
datanode_3  | 2023-01-12 05:25:56,708 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fb7937dd-8506-409c-a1e6-3cdc4fafc0d4 has been successfully formatted.
recon_1     | 2023-01-12 05:26:24,302 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 16 failover attempts. Trying to failover immediately.
recon_1     | 2023-01-12 05:26:24,303 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 17 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-12 05:26:24,362 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 reported by 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11)
recon_1     | 2023-01-12 05:26:24,364 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7b2ccc1c-57aa-4786-a2ba-0da58dd390a6, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6)34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11)cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, CreationTimestamp2023-01-12T05:25:23.439Z[UTC]] moved to OPEN state
recon_1     | 2023-01-12 05:26:26,306 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 18 failover attempts. Trying to failover immediately.
recon_1     | 2023-01-12 05:26:26,307 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 19 failover attempts. Trying to failover immediately.
recon_1     | 2023-01-12 05:26:26,308 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 20 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-12 05:26:28,310 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 21 failover attempts. Trying to failover immediately.
recon_1     | 2023-01-12 05:26:28,311 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 22 failover attempts. Trying to failover immediately.
datanode_2  | 2023-01-12 05:26:05,590 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: shutdown 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState
om1_1       | 2023-01-12 05:35:11,662 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg57 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,672 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg59 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,677 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg63 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,681 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg64 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,869 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg22 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,876 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg31 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:05,593 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2  | 2023-01-12 05:26:05,594 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 2023-01-12 05:35:11,685 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg60 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,824 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg66 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,846 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg73 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,858 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg65 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,876 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg72 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:05,595 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: start 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	... 17 more
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3  | 2023-01-12 05:25:56,783 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-3CDC4FAFC0D4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
om3_1       | 2023-01-12 05:35:10,879 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg30 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,909 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg33 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,937 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg32 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:10,952 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg37 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,020 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg41 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:05,705 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1       | Caused by: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.getCustomMetadataFromHeaders(EndpointBase.java:291)
om2_1       | 2023-01-12 05:35:45,122 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
datanode_3  | 2023-01-12 05:25:56,823 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1       | 2023-01-12 05:35:11,046 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg35 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,079 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg34 of layout LEGACY in volume: s3v
scm2_1      | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
datanode_2  | 2023-01-12 05:26:05,977 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-12 05:26:00,116 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.put(ObjectEndpoint.java:214)
scm1_1      | 2023-01-12 05:25:15,880 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: set configuration 0: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3. Provided Part info is { etag1, 1}, whereas OM has partName /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3-b33807c3-dfe3-405d-9b03-b3c9292d342c-109674609906155555-1
datanode_3  | 2023-01-12 05:25:56,948 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om3_1       | 2023-01-12 05:35:11,099 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg36 of layout LEGACY in volume: s3v
scm2_1      | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
datanode_2  | 2023-01-12 05:26:05,979 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-12 05:26:00,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
scm1_1      | 2023-01-12 05:25:15,882 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_inprogress_0
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:511)
scm3_1      | 2023-01-12 05:26:00,225 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
om3_1       | 2023-01-12 05:35:11,127 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg39 of layout LEGACY in volume: s3v
scm2_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 2023-01-12 05:26:28,312 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 23 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-12 05:26:30,317 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 24 failover attempts. Trying to failover immediately.
recon_1     | 2023-01-12 05:26:30,319 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 25 failover attempts. Trying to failover immediately.
recon_1     | 2023-01-12 05:26:30,324 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 26 failover attempts. Trying to failover after sleeping for 2000ms.
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1_1      | 2023-01-12 05:25:15,892 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO segmented.SegmentedRaftLogWorker: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:198)
scm3_1      | java.nio.channels.ClosedChannelException
scm3_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om3_1       | 2023-01-12 05:35:11,139 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg38 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,886 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg69 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:06,010 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for f23956f4-f232-4ae2-8f3e-cd8efc9f95bf
datanode_1  | 2023-01-12 05:26:00,132 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm1_1      | 2023-01-12 05:25:15,893 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO segmented.SegmentedRaftLogWorker: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
scm3_1      | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm3_1      | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
om3_1       | 2023-01-12 05:35:11,145 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg44 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,152 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg40 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,236 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg45 of layout LEGACY in volume: s3v
datanode_1  | 2023-01-12 05:26:00,183 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2023-01-12 05:26:00,183 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
datanode_3  | 2023-01-12 05:25:56,953 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1       | 2023-01-12 05:35:11,246 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg43 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,261 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg48 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,894 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg71 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:11,906 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg74 of layout LEGACY in volume: s3v
datanode_1  | 2023-01-12 05:26:00,184 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
datanode_3  | 2023-01-12 05:25:56,967 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm3_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
om3_1       | 2023-01-12 05:35:11,284 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg51 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,431 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg50 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,440 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg42 of layout LEGACY in volume: s3v
scm1_1      | 2023-01-12 05:25:15,990 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: start as a follower, conf=0: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-12 05:26:00,186 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1  | 2023-01-12 05:26:00,186 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 2023-01-12 05:25:57,000 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1     | 2023-01-12 05:26:32,335 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 27 failover attempts. Trying to failover immediately.
scm2_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1      | 2023-01-12 05:25:36,844 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#8 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.6:42200: output error
om3_1       | 2023-01-12 05:35:11,445 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg49 of layout LEGACY in volume: s3v
scm1_1      | 2023-01-12 05:25:15,990 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: changes role from      null to FOLLOWER at term 1 for startAsFollower
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3  | 2023-01-12 05:25:57,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1     | 2023-01-12 05:26:32,340 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 28 failover attempts. Trying to failover immediately.
recon_1     | 2023-01-12 05:26:32,348 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 29 failover attempts. Trying to failover after sleeping for 2000ms.
datanode_2  | 2023-01-12 05:26:06,028 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
datanode_2  | 2023-01-12 05:26:06,997 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: Detected pause in JVM or host machine (eg GC): pause of approximately 897027411ns.
om3_1       | 2023-01-12 05:35:11,448 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg47 of layout LEGACY in volume: s3v
scm1_1      | 2023-01-12 05:25:15,992 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO impl.RoleInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14: start f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState
datanode_1  | 2023-01-12 05:26:00,187 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 does not exist. Creating ...
scm3_1      | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
datanode_3  | 2023-01-12 05:25:57,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1       | 2023-01-12 05:35:46,206 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
om2_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3. Provided Part info is { etag2, 2}, whereas OM has partName /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3-b33807c3-dfe3-405d-9b03-b3c9292d342c-109674609906155555-2
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:511)
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:198)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om3_1       | 2023-01-12 05:35:11,462 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg46 of layout LEGACY in volume: s3v
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
scm1_1      | 2023-01-12 05:25:15,992 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-12 05:26:00,202 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6/in_use.lock acquired by nodename 7@d9a6a467ccde
scm3_1      | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm3_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
recon_1     | 2023-01-12 05:26:34,351 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 30 failover attempts. Trying to failover immediately.
scm2_1      | 2023-01-12 05:25:36,857 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm2_1      | java.nio.channels.ClosedChannelException
datanode_2  | GC pool 'ParNew' had collection(s): count=1 time=912ms
om3_1       | 2023-01-12 05:35:11,480 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg52 of layout LEGACY in volume: s3v
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
scm1_1      | 2023-01-12 05:25:15,993 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-12 05:26:00,209 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 has been successfully formatted.
scm3_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
datanode_3  | 2023-01-12 05:25:57,183 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 2023-01-12 05:35:11,915 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg67 of layout LEGACY in volume: s3v
recon_1     | 2023-01-12 05:26:34,352 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 31 failover attempts. Trying to failover immediately.
datanode_2  | 2023-01-12 05:26:07,244 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-9E784E6BBA4E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/cd62190c-0472-48bc-9601-9e784e6bba4e/current/log_inprogress_0
om3_1       | 2023-01-12 05:35:11,485 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg54 of layout LEGACY in volume: s3v
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
scm1_1      | 2023-01-12 05:25:15,998 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4D43FF37764E,id=f98693f9-b2e2-4085-a3fc-32934ea8db14
datanode_1  | 2023-01-12 05:26:00,211 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-0DA58DD390A6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm3_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_3  | 2023-01-12 05:25:57,237 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fb7937dd-8506-409c-a1e6-3cdc4fafc0d4
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 2023-01-12 05:35:11,924 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg70 of layout LEGACY in volume: s3v
recon_1     | 2023-01-12 05:26:34,354 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 32 failover attempts. Trying to failover after sleeping for 2000ms.
datanode_2  | 2023-01-12 05:26:08,206 [grpc-default-executor-0] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: receive requestVote(ELECTION, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, group-0DA58DD390A6, 1, (t:0, i:0))
om3_1       | 2023-01-12 05:35:11,493 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg53 of layout LEGACY in volume: s3v
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
scm1_1      | 2023-01-12 05:25:16,000 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_3  | 2023-01-12 05:25:57,254 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm2_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om1_1       | 2023-01-12 05:35:11,934 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg68 of layout LEGACY in volume: s3v
recon_1     | 2023-01-12 05:26:36,355 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 33 failover attempts. Trying to failover immediately.
datanode_2  | 2023-01-12 05:26:08,272 [grpc-default-executor-0] INFO impl.VoteContext: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-CANDIDATE: reject ELECTION from cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: already has voted for 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b at current term 1
datanode_2  | 2023-01-12 05:26:08,327 [grpc-default-executor-0] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6 replies to ELECTION vote request: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b#0:FAIL-t1. Peer's state: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6:t1, leader=null, voted=34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, raftlog=Memoized:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
datanode_1  | 2023-01-12 05:26:00,213 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
datanode_3  | 2023-01-12 05:25:57,261 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm2_1      | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 2023-01-12 05:35:11,991 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg81 of layout LEGACY in volume: s3v
recon_1     | 2023-01-12 05:26:36,356 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 34 failover attempts. Trying to failover immediately.
datanode_2  | 2023-01-12 05:26:09,047 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
om3_1       | 2023-01-12 05:35:11,501 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg55 of layout LEGACY in volume: s3v
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
scm1_1      | 2023-01-12 05:25:16,001 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1_1      | 2023-01-12 05:25:16,001 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
datanode_3  | 2023-01-12 05:25:57,335 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm2_1      | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
om2_1       | 2023-01-12 05:35:47,261 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: PartNumber at index 1 is 2, and its previous partNumber at index 0 is 4 for ozonekey is /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3
om1_1       | 2023-01-12 05:35:12,115 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg76 of layout LEGACY in volume: s3v
recon_1     | 2023-01-12 05:26:36,356 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 35 failover attempts. Trying to failover after sleeping for 2000ms.
datanode_2  | 2023-01-12 05:26:09,047 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection:   Response 0: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b<-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf#0:FAIL-t1
om3_1       | 2023-01-12 05:35:11,547 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg56 of layout LEGACY in volume: s3v
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
datanode_1  | 2023-01-12 05:26:00,279 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1      | 2023-01-12 05:25:16,002 [f98693f9-b2e2-4085-a3fc-32934ea8db14-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1      | 2023-01-12 05:26:00,086 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#14 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.4:37414: output error
datanode_3  | 2023-01-12 05:25:57,342 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
om2_1       | 2023-01-12 05:35:47,261 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
om1_1       | 2023-01-12 05:35:12,130 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg82 of layout LEGACY in volume: s3v
recon_1     | 2023-01-12 05:26:38,358 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 36 failover attempts. Trying to failover immediately.
datanode_2  | 2023-01-12 05:26:09,047 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection:   Response 1: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b<-cefdd7f0-5c68-4866-b1db-ab1e8bc5045a#0:FAIL-t1
datanode_2  | 2023-01-12 05:26:09,051 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2 ELECTION round 0: result REJECTED
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
datanode_1  | 2023-01-12 05:26:00,280 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1      | 2023-01-12 05:25:16,010 [Listener at 0.0.0.0/9860] INFO server.RaftServer: f98693f9-b2e2-4085-a3fc-32934ea8db14: start RPC server
scm3_1      | 2023-01-12 05:26:00,229 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
datanode_3  | 2023-01-12 05:25:57,346 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
om2_1       | INVALID_PART_ORDER org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3 because parts are in Invalid order.
om1_1       | 2023-01-12 05:35:12,153 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg75 of layout LEGACY in volume: s3v
recon_1     | 2023-01-12 05:26:38,358 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 37 failover attempts. Trying to failover immediately.
datanode_2  | 2023-01-12 05:26:09,052 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
om3_1       | 2023-01-12 05:35:11,554 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg58 of layout LEGACY in volume: s3v
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
scm1_1      | 2023-01-12 05:25:16,030 [Listener at 0.0.0.0/9860] INFO server.GrpcService: f98693f9-b2e2-4085-a3fc-32934ea8db14: GrpcService started, listening on 9894
scm3_1      | java.nio.channels.ClosedChannelException
datanode_3  | 2023-01-12 05:25:57,373 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
om1_1       | 2023-01-12 05:35:12,177 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg78 of layout LEGACY in volume: s3v
recon_1     | 2023-01-12 05:26:38,359 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 38 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-12 05:26:40,360 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 39 failover attempts. Trying to failover immediately.
om3_1       | 2023-01-12 05:35:11,582 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg62 of layout LEGACY in volume: s3v
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
datanode_1  | 2023-01-12 05:26:00,281 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1  | 2023-01-12 05:26:00,281 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_3  | 2023-01-12 05:25:57,387 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
om1_1       | 2023-01-12 05:35:12,189 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg84 of layout LEGACY in volume: s3v
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
datanode_1  | 2023-01-12 05:26:00,283 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2023-01-12 05:26:00,284 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_3  | 2023-01-12 05:25:57,388 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2023-01-12 05:25:57,486 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2023-01-12 05:25:57,514 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1       | 2023-01-12 05:35:12,199 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg80 of layout LEGACY in volume: s3v
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
scm1_1      | 2023-01-12 05:25:16,032 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f98693f9-b2e2-4085-a3fc-32934ea8db14: Started
datanode_1  | 2023-01-12 05:26:00,286 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1      | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
datanode_3  | 2023-01-12 05:25:57,760 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
om1_1       | 2023-01-12 05:35:12,234 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg77 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:12,243 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg83 of layout LEGACY in volume: s3v
s3g_1       | 	... 51 more
scm1_1      | 2023-01-12 05:25:16,042 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3_1      | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
datanode_1  | 2023-01-12 05:26:00,287 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6
datanode_3  | 2023-01-12 05:25:57,761 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 2023-01-12 05:26:40,361 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 40 failover attempts. Trying to failover immediately.
om1_1       | 2023-01-12 05:35:12,255 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg79 of layout LEGACY in volume: s3v
scm1_1      | 2023-01-12 05:25:16,042 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3_1      | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
datanode_3  | 2023-01-12 05:25:57,772 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm2_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 2023-01-12 05:26:40,362 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 41 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2023-01-12 05:35:12,312 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg85 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:09,053 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: shutdown 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2
datanode_2  | 2023-01-12 05:26:09,061 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection2] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: start 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState
s3g_1       | 2023-01-12 05:37:58,926 [qtp384515747-22] WARN server.HttpChannelState: unhandled due to prior sendError
scm1_1      | 2023-01-12 05:25:16,117 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1  | 2023-01-12 05:26:00,288 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1  | 2023-01-12 05:26:00,292 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2023-01-12 05:25:57,951 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getPartsListSize(S3MultipartUploadCompleteRequest.java:477)
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:194)
recon_1     | 2023-01-12 05:26:42,363 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 42 failover attempts. Trying to failover immediately.
om1_1       | 2023-01-12 05:35:12,385 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg86 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:09,096 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2  | 2023-01-12 05:26:09,096 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
s3g_1       | javax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:162)
scm1_1      | 2023-01-12 05:25:16,131 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1_1      | 2023-01-12 05:25:16,131 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1_1      | 2023-01-12 05:25:16,390 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
datanode_3  | 2023-01-12 05:25:57,955 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
recon_1     | 2023-01-12 05:26:42,365 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 43 failover attempts. Trying to failover immediately.
om1_1       | 2023-01-12 05:35:12,439 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg88 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:10,261 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6.
om3_1       | 2023-01-12 05:35:11,589 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg61 of layout LEGACY in volume: s3v
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
datanode_1  | 2023-01-12 05:26:00,294 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1      | 2023-01-12 05:25:16,391 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1      | 2023-01-12 05:25:16,395 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
recon_1     | 2023-01-12 05:26:42,366 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07cc1c5ffbcd/172.18.0.12 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 44 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1       | 2023-01-12 05:35:12,458 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg89 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:12,469 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg90 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:12,479 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg92 of layout LEGACY in volume: s3v
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
datanode_1  | 2023-01-12 05:26:00,295 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1      | 2023-01-12 05:25:36,834 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#8 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.11:41660: output error
datanode_3  | 2023-01-12 05:25:57,976 [pool-22-thread-1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4: start as a follower, conf=-1: peers:[cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1       | 2023-01-12 05:35:52,588 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadAbortRequest: Abort Multipart request is failed for KeyName ozone-test-7719133293/multipartKey5 in VolumeName/Bucket s3v/bucket-ozone-test-0191226535
om2_1       | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Abort Multipart Upload Failed: volume: s3vbucket: bucket-ozone-test-0191226535key: ozone-test-7719133293/multipartKey5
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadAbortRequest.validateAndUpdateCache(S3MultipartUploadAbortRequest.java:161)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
scm3_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1      | 2023-01-12 05:25:16,413 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1_1      | 2023-01-12 05:25:16,414 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
recon_1     | 2023-01-12 05:26:49,784 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om1 is not the leader. Could not determine the leader node.
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
om3_1       | 2023-01-12 05:35:11,607 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg57 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,631 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg59 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,638 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg63 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,642 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg64 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,654 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg60 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,760 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg66 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,771 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg73 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm2_1      | 2023-01-12 05:25:36,859 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm2_1      | java.nio.channels.ClosedChannelException
scm2_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om3_1       | 2023-01-12 05:35:11,794 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg65 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:13,617 [grpc-default-executor-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: receive requestVote(ELECTION, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, group-0DA58DD390A6, 2, (t:0, i:0))
datanode_2  | 2023-01-12 05:26:13,617 [grpc-default-executor-1] INFO impl.VoteContext: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FOLLOWER: reject ELECTION from cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: our priority 1 > candidate's priority 0
datanode_2  | 2023-01-12 05:26:13,618 [grpc-default-executor-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
datanode_2  | 2023-01-12 05:26:13,618 [grpc-default-executor-1] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: shutdown 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState
datanode_2  | 2023-01-12 05:26:13,618 [grpc-default-executor-1] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: start 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1     | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om3_1       | 2023-01-12 05:35:11,796 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg72 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,816 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg69 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,854 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg71 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,877 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg74 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,890 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg67 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,897 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg70 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:13,618 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO impl.FollowerState: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState was interrupted
datanode_2  | 2023-01-12 05:26:13,635 [grpc-default-executor-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6 replies to ELECTION vote request: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b#0:FAIL-t2. Peer's state: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6:t2, leader=null, voted=null, raftlog=Memoized:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-12 05:26:13,657 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2  | 2023-01-12 05:26:13,661 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-12 05:26:18,828 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO impl.FollowerState: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5209760352ns, electionTimeout:5166ms
datanode_2  | 2023-01-12 05:26:18,828 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: shutdown 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState
scm2_1      | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1      | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
datanode_3  | 2023-01-12 05:25:57,979 [pool-22-thread-1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2023-01-12 05:25:57,994 [pool-22-thread-1] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-FollowerState
datanode_3  | 2023-01-12 05:25:58,030 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-12 05:25:58,030 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-12 05:25:58,064 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3CDC4FAFC0D4,id=cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
datanode_3  | 2023-01-12 05:25:58,081 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_3  | 2023-01-12 05:25:58,083 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2023-01-12 05:25:58,086 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2023-01-12 05:25:58,091 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3  | 2023-01-12 05:25:58,336 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=fb7937dd-8506-409c-a1e6-3cdc4fafc0d4
datanode_3  | 2023-01-12 05:25:58,342 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=fb7937dd-8506-409c-a1e6-3cdc4fafc0d4.
datanode_3  | 2023-01-12 05:25:58,347 [Command processor thread] INFO server.RaftServer: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: addNew group-0DA58DD390A6:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER] returns group-0DA58DD390A6:java.util.concurrent.CompletableFuture@13a3acd0[Not completed]
scm2_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_3  | 2023-01-12 05:25:58,435 [pool-22-thread-1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: new RaftServerImpl for group-0DA58DD390A6:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3  | 2023-01-12 05:25:58,455 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2023-01-12 05:25:58,468 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2023-01-12 05:25:58,471 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3  | 2023-01-12 05:25:58,472 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3  | 2023-01-12 05:25:58,476 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3  | 2023-01-12 05:25:58,479 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3  | 2023-01-12 05:25:58,480 [pool-22-thread-1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: ConfigurationManager, init=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3  | 2023-01-12 05:25:58,485 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2023-01-12 05:25:58,494 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2023-01-12 05:25:58,539 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3  | 2023-01-12 05:25:58,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
om1_1       | 2023-01-12 05:35:12,489 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg93 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:12,508 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg87 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:12,519 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg94 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:12,532 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg91 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:11,908 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg68 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,003 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg81 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,077 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg76 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,101 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg82 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,123 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg75 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
datanode_1  | 2023-01-12 05:26:00,299 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2023-01-12 05:26:18,828 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_2  | 2023-01-12 05:26:18,829 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2  | 2023-01-12 05:26:18,830 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: start 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3
om1_1       | 2023-01-12 05:35:12,548 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg96 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 2023-01-12 05:35:12,129 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg78 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,158 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg84 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,170 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg80 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | , while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 45 failover attempts. Trying to failover immediately.
om1_1       | 2023-01-12 05:35:12,574 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg95 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:12,598 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg97 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:12,612 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg98 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,179 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg77 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,189 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg83 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,205 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg79 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,217 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg85 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,236 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg86 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,282 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg88 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,296 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg89 of layout LEGACY in volume: s3v
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1       | 2023-01-12 05:35:53,520 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyCreateRequest: Key creation failed. Volume:s3v, Bucket:bucket-ozone-test-0191226535, Key:ozone-test-9943555385/multipartKey. 
om2_1       | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No such Multipart upload is with specified uploadId random
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareMultipartFileInfo(OMKeyRequest.java:798)
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareFileInfo(OMKeyRequest.java:685)
om3_1       | 2023-01-12 05:35:12,308 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg90 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,353 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg92 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,378 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg93 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,386 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg87 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,397 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg94 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,411 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg91 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,484 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg96 of layout LEGACY in volume: s3v
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareKeyInfo(OMKeyRequest.java:662)
recon_1     | 2023-01-12 05:26:51,152 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Could not determine the leader node.
om1_1       | 2023-01-12 05:35:12,621 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg99 of layout LEGACY in volume: s3v
datanode_3  | 2023-01-12 05:25:58,547 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm2_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om3_1       | 2023-01-12 05:35:12,489 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg95 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,522 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg97 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,565 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg98 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:12,581 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: s3bg99 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:35:17,153 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-0191226535 of layout LEGACY in volume: s3v
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:279)
datanode_2  | 2023-01-12 05:26:18,844 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3] INFO impl.LeaderElection: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3 ELECTION round 0: submit vote requests at term 3 for -1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
om1_1       | 2023-01-12 05:35:17,146 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-0191226535 of layout LEGACY in volume: s3v
datanode_3  | 2023-01-12 05:25:58,556 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
datanode_2  | 2023-01-12 05:26:18,848 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
recon_1     | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
datanode_3  | 2023-01-12 05:25:58,616 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2023-01-12 05:25:58,632 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3  | 2023-01-12 05:25:58,635 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3  | 2023-01-12 05:25:58,639 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3  | 2023-01-12 05:25:58,642 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3  | 2023-01-12 05:25:58,647 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 does not exist. Creating ...
datanode_3  | 2023-01-12 05:25:58,661 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6/in_use.lock acquired by nodename 6@d407c6d45367
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1       | 2023-01-12 05:36:42,837 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-8056751975 of layout LEGACY in volume: s3v
datanode_3  | 2023-01-12 05:25:58,678 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 has been successfully formatted.
scm1_1      | 2023-01-12 05:25:16,415 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1      | 2023-01-12 05:25:16,415 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1_1      | 2023-01-12 05:25:16,454 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2bfc2f8b] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm1_1      | 2023-01-12 05:25:16,474 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
datanode_1  | 2023-01-12 05:26:00,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2023-01-12 05:26:00,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1       | 2023-01-12 05:36:43,751 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: destbucket-12311 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:36:56,364 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-5672640421 of layout LEGACY in volume: s3v
datanode_2  | 2023-01-12 05:26:18,848 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-12 05:26:18,953 [grpc-default-executor-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: receive requestVote(ELECTION, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf, group-0DA58DD390A6, 3, (t:0, i:0))
datanode_2  | 2023-01-12 05:26:18,953 [grpc-default-executor-1] INFO impl.VoteContext: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-CANDIDATE: reject ELECTION from f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: already has voted for 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b at current term 3
datanode_2  | 2023-01-12 05:26:18,954 [grpc-default-executor-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6 replies to ELECTION vote request: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf<-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b#0:FAIL-t3. Peer's state: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6:t3, leader=null, voted=34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, raftlog=Memoized:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
om1_1       | 2023-01-12 05:35:37,139 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload: /s3v/bucket-ozone-test-0191226535/ozone-test-8175329106/multipartKey2 Part number: 1 size 6  is less than minimum part size 5242880
om1_1       | 2023-01-12 05:35:37,150 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-8175329106/multipartKey2 in Volume/Bucket s3v/bucket-ozone-test-0191226535
om1_1       | ENTITY_TOO_SMALL org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-8175329106/multipartKey2. Entity too small.
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:534)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
om2_1       | 2023-01-12 05:37:01,572 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-ozone-test-5672640421, Key:thereisnosuchfile.
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
datanode_1  | 2023-01-12 05:26:00,302 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2023-01-12 05:26:00,308 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2023-01-12 05:26:00,313 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:198)
datanode_2  | 2023-01-12 05:26:18,974 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3] INFO impl.LeaderElection: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_2  | 2023-01-12 05:26:18,974 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3] INFO impl.LeaderElection:   Response 0: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b<-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf#0:FAIL-t3
datanode_2  | 2023-01-12 05:26:18,974 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3] INFO impl.LeaderElection:   Response 1: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b<-cefdd7f0-5c68-4866-b1db-ab1e8bc5045a#0:FAIL-t3
scm3_1      | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm3_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm3_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1      | 2023-01-12 05:25:16,474 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm1_1      | 2023-01-12 05:25:16,507 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @8642ms to org.eclipse.jetty.util.log.Slf4jLog
scm1_1      | 2023-01-12 05:25:16,631 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm1_1      | 2023-01-12 05:25:16,637 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1_1      | 2023-01-12 05:25:16,644 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1  | 2023-01-12 05:26:01,385 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: Detected pause in JVM or host machine (eg GC): pause of approximately 880969184ns.
datanode_2  | 2023-01-12 05:26:18,974 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3] INFO impl.LeaderElection: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3 ELECTION round 0: result REJECTED
scm3_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1      | 2023-01-12 05:25:16,645 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm2_1      | 2023-01-12 05:25:36,937 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1  | GC pool 'ParNew' had collection(s): count=1 time=88ms
datanode_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=965ms
datanode_1  | 2023-01-12 05:26:01,459 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1  | 2023-01-12 05:26:01,461 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2  | 2023-01-12 05:26:18,974 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
scm3_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm3_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm3_1      | 2023-01-12 05:26:00,515 [IPC Server handler 97 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f23956f4-f232-4ae2-8f3e-cd8efc9f95bf
scm1_1      | 2023-01-12 05:25:16,646 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1  | 2023-01-12 05:26:01,462 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2  | 2023-01-12 05:26:18,975 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: shutdown 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3
scm3_1      | 2023-01-12 05:26:00,599 [IPC Server handler 97 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f23956f4-f232-4ae2-8f3e-cd8efc9f95bf{ip: 172.18.0.6, host: ozone-ha_datanode_1.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 2023-01-12 05:26:01,464 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
s3g_1       | Caused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:410)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
datanode_2  | 2023-01-12 05:26:18,975 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection3] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: start 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState
scm3_1      | 2023-01-12 05:26:00,687 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1      | 2023-01-12 05:25:36,978 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1_1      | 2023-01-12 05:25:16,646 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1      | 2023-01-12 05:25:16,680 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm1_1      | 2023-01-12 05:25:16,681 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm1_1      | 2023-01-12 05:25:16,735 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1  | 2023-01-12 05:26:01,468 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1  | 2023-01-12 05:26:01,482 [pool-22-thread-1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: start as a follower, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-12 05:26:01,483 [pool-22-thread-1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2023-01-12 05:26:01,484 [pool-22-thread-1] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: start f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState
scm3_1      | 2023-01-12 05:26:00,693 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm3_1      | 2023-01-12 05:26:00,816 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1      | 2023-01-12 05:25:36,978 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
datanode_1  | 2023-01-12 05:26:01,497 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0DA58DD390A6,id=f23956f4-f232-4ae2-8f3e-cd8efc9f95bf
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
om3_1       | 2023-01-12 05:35:37,192 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload: /s3v/bucket-ozone-test-0191226535/ozone-test-8175329106/multipartKey2 Part number: 1 size 6  is less than minimum part size 5242880
om3_1       | 2023-01-12 05:35:37,194 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-8175329106/multipartKey2 in Volume/Bucket s3v/bucket-ozone-test-0191226535
om3_1       | ENTITY_TOO_SMALL org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-8175329106/multipartKey2. Entity too small.
scm1_1      | 2023-01-12 05:25:16,735 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm2_1      | 2023-01-12 05:25:37,729 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3_1      | 2023-01-12 05:26:01,194 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm3_1      | 2023-01-12 05:26:01,266 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1  | 2023-01-12 05:26:01,498 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1      | 2023-01-12 05:25:16,737 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm1_1      | 2023-01-12 05:25:16,752 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2154652c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1_1      | 2023-01-12 05:25:16,758 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@21527b8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1      | 2023-01-12 05:25:37,730 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1      | 2023-01-12 05:25:37,756 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2_1      | 2023-01-12 05:25:37,814 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
datanode_2  | 2023-01-12 05:26:19,022 [grpc-default-executor-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: receive requestVote(ELECTION, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, group-0DA58DD390A6, 3, (t:0, i:0))
scm1_1      | 2023-01-12 05:25:16,894 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@67e6eb52{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-16129864339965214165/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1_1      | 2023-01-12 05:25:16,913 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@7e050be1{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:534)
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:198)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
recon_1     | , while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 46 failover attempts. Trying to failover immediately.
om2_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:152)
scm3_1      | 2023-01-12 05:26:01,331 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
datanode_1  | 2023-01-12 05:26:01,498 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1  | 2023-01-12 05:26:01,499 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2023-01-12 05:26:01,499 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1  | 2023-01-12 05:26:01,500 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-12 05:26:01,518 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6
datanode_2  | 2023-01-12 05:26:19,023 [grpc-default-executor-1] INFO impl.VoteContext: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FOLLOWER: reject ELECTION from cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: already has voted for 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b at current term 3
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:98)
om1_1       | 2023-01-12 05:35:39,080 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 1
om1_1       | partName: "etag1"
recon_1     | 2023-01-12 05:26:51,406 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om3 is not the leader. Could not determine the leader node.
datanode_3  | 2023-01-12 05:25:58,687 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-0DA58DD390A6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2023-01-12 05:25:58,756 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2023-01-12 05:25:58,768 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1       | , partNumber: 2
scm1_1      | 2023-01-12 05:25:16,914 [Listener at 0.0.0.0/9860] INFO server.Server: Started @9049ms
datanode_2  | 2023-01-12 05:26:19,024 [grpc-default-executor-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6 replies to ELECTION vote request: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b#0:FAIL-t3. Peer's state: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6:t3, leader=null, voted=34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, raftlog=Memoized:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-12 05:26:19,046 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1       | partName: "etag2"
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
scm1_1      | 2023-01-12 05:25:16,918 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2_1      | 2023-01-12 05:25:37,816 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2_1      | 2023-01-12 05:25:37,820 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1      | 2023-01-12 05:25:37,823 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
datanode_2  | 2023-01-12 05:26:19,048 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-12 05:26:24,099 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO impl.FollowerState: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5124151723ns, electionTimeout:5049ms
datanode_2  | 2023-01-12 05:26:24,099 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: shutdown 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState
datanode_2  | 2023-01-12 05:26:24,100 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
om1_1       | ]
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
scm2_1      | 2023-01-12 05:25:37,954 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@68d6f48e] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm3_1      | 2023-01-12 05:26:01,372 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
scm2_1      | 2023-01-12 05:25:37,992 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2_1      | 2023-01-12 05:25:37,992 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm3_1      | 2023-01-12 05:26:01,377 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1       | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 2023-01-12 05:26:01,530 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-12 05:26:04,350 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-FollowerState] INFO impl.FollowerState: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5153696091ns, electionTimeout:5046ms
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm2_1      | 2023-01-12 05:25:38,046 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @14291ms to org.eclipse.jetty.util.log.Slf4jLog
scm2_1      | 2023-01-12 05:25:38,294 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm3_1      | 2023-01-12 05:26:01,378 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm3_1      | 2023-01-12 05:26:01,879 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
datanode_3  | 2023-01-12 05:25:58,790 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2023-01-12 05:25:58,791 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3  | 2023-01-12 05:25:58,792 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3  | 2023-01-12 05:25:58,794 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2023-01-12 05:25:58,797 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm3_1      | 2023-01-12 05:26:01,882 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm1_1      | 2023-01-12 05:25:16,918 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
datanode_2  | 2023-01-12 05:26:24,100 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1       | 2023-01-12 05:35:39,095 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
om1_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:187)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
scm2_1      | 2023-01-12 05:25:38,316 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3_1      | 2023-01-12 05:26:02,842 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm1_1      | 2023-01-12 05:25:16,921 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
datanode_3  | 2023-01-12 05:25:58,798 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
datanode_2  | 2023-01-12 05:26:24,100 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: start 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4
datanode_2  | 2023-01-12 05:26:24,101 [grpc-default-executor-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: receive requestVote(ELECTION, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, group-0DA58DD390A6, 4, (t:0, i:0))
datanode_1  | 2023-01-12 05:26:04,355 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-FollowerState] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: shutdown f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-FollowerState
datanode_1  | 2023-01-12 05:26:04,358 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-FollowerState] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
recon_1     | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1      | 2023-01-12 05:26:02,842 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm1_1      | 2023-01-12 05:25:21,163 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO impl.FollowerState: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5171624578ns, electionTimeout:5169ms
datanode_3  | 2023-01-12 05:25:58,800 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
datanode_1  | 2023-01-12 05:26:04,389 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1  | 2023-01-12 05:26:04,395 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-FollowerState] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: start f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1
datanode_1  | 2023-01-12 05:26:04,529 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO impl.LeaderElection: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1      | 2023-01-12 05:25:38,344 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3_1      | 2023-01-12 05:26:02,933 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm3_1      | 2023-01-12 05:26:03,298 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@73b74615{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3  | 2023-01-12 05:25:58,801 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
datanode_1  | 2023-01-12 05:26:04,581 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO impl.LeaderElection: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1  | 2023-01-12 05:26:04,602 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: shutdown f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1
datanode_1  | 2023-01-12 05:26:04,616 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm2_1      | 2023-01-12 05:25:38,350 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm3_1      | 2023-01-12 05:26:03,316 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@320ff86f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1_1      | 2023-01-12 05:25:21,164 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO impl.RoleInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14: shutdown f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState
scm1_1      | 2023-01-12 05:25:21,166 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 2023-01-12 05:26:04,616 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0EE7B13ECDDC with new leaderId: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf
datanode_1  | 2023-01-12 05:26:04,748 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC: change Leader from null to f23956f4-f232-4ae2-8f3e-cd8efc9f95bf at term 1 for becomeLeader, leader elected after 7241ms
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1      | 2023-01-12 05:26:03,383 [IPC Server handler 98 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
scm1_1      | 2023-01-12 05:25:21,169 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1       | 2023-01-12 05:35:39,087 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 1
datanode_1  | 2023-01-12 05:26:04,933 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1  | 2023-01-12 05:26:05,053 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1      | 2023-01-12 05:26:03,394 [IPC Server handler 98 on default port 9861] INFO node.SCMNodeManager: Registered Data node : cefdd7f0-5c68-4866-b1db-ab1e8bc5045a{ip: 172.18.0.4, host: ozone-ha_datanode_3.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1      | 2023-01-12 05:25:21,169 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-FollowerState] INFO impl.RoleInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14: start f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1
datanode_3  | 2023-01-12 05:25:58,802 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om3_1       | partName: "etag1"
datanode_2  | 2023-01-12 05:26:24,126 [grpc-default-executor-1] INFO impl.VoteContext: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-CANDIDATE: reject ELECTION from cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: already has voted for 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b at current term 4
scm2_1      | 2023-01-12 05:25:38,351 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1  | 2023-01-12 05:26:05,067 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1      | 2023-01-12 05:26:03,412 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1      | 2023-01-12 05:25:21,187 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO impl.LeaderElection: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-12 05:25:58,817 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om3_1       | , partNumber: 2
om3_1       | partName: "etag2"
scm2_1      | 2023-01-12 05:25:38,352 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1  | 2023-01-12 05:26:05,288 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1  | 2023-01-12 05:26:05,296 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm3_1      | 2023-01-12 05:26:03,418 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1_1      | 2023-01-12 05:25:21,188 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO impl.LeaderElection: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1 ELECTION round 0: result PASSED (term=2)
datanode_3  | 2023-01-12 05:25:58,819 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
om1_1       | 2023-01-12 05:35:39,992 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 2
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1       | ]
om3_1       | 2023-01-12 05:35:39,127 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
om3_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3
scm2_1      | 2023-01-12 05:25:38,445 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
datanode_1  | 2023-01-12 05:26:05,343 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1      | 2023-01-12 05:26:03,423 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1      | 2023-01-12 05:25:21,188 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO impl.RoleInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14: shutdown f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
om1_1       | partName: "etag1"
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_2  | 2023-01-12 05:26:24,127 [grpc-default-executor-1] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6 replies to ELECTION vote request: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b#0:FAIL-t4. Peer's state: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6:t4, leader=null, voted=34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, raftlog=Memoized:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-12 05:26:24,135 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO impl.LeaderElection: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4 ELECTION round 0: submit vote requests at term 4 for -1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-12 05:26:24,189 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1      | 2023-01-12 05:25:38,447 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_1  | 2023-01-12 05:26:05,524 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1      | 2023-01-12 05:26:03,744 [IPC Server handler 97 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
scm1_1      | 2023-01-12 05:25:21,189 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
om1_1       | , partNumber: 1
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om2_1       | 2023-01-12 05:37:05,092 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-ozone-test-5672640421, Key:ozone-test-7616628748/deletetestapidir/key=value/.
datanode_2  | 2023-01-12 05:26:24,189 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:187)
scm2_1      | 2023-01-12 05:25:38,540 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1  | 2023-01-12 05:26:05,540 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm3_1      | 2023-01-12 05:26:03,745 [IPC Server handler 97 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b{ip: 172.18.0.11, host: ozone-ha_datanode_2.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1      | 2023-01-12 05:25:21,189 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm1_1      | 2023-01-12 05:25:21,189 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
datanode_3  | 2023-01-12 05:25:58,821 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
om1_1       | partName: "etag2"
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om2_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
datanode_2  | 2023-01-12 05:26:24,224 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO impl.LeaderElection: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4: ELECTION PASSED received 1 response(s) and 0 exception(s):
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
scm2_1      | 2023-01-12 05:25:38,540 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
datanode_1  | 2023-01-12 05:26:05,573 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: start f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderStateImpl
scm3_1      | 2023-01-12 05:26:03,745 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1      | 2023-01-12 05:25:21,191 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: change Leader from null to f98693f9-b2e2-4085-a3fc-32934ea8db14 at term 2 for becomeLeader, leader elected after 7776ms
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
recon_1     | , while invoking $Proxy42.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 47 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:152)
datanode_2  | 2023-01-12 05:26:24,224 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO impl.LeaderElection:   Response 0: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b<-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf#0:OK-t4
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
scm2_1      | 2023-01-12 05:25:38,546 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
datanode_1  | 2023-01-12 05:26:05,754 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-SegmentedRaftLogWorker: Starting segment from index:0
scm3_1      | 2023-01-12 05:26:03,748 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1_1      | 2023-01-12 05:25:21,199 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3  | 2023-01-12 05:25:58,821 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2023-01-12 05:25:58,828 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
recon_1     | 2023-01-12 05:26:53,413 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMLeaderNotReadyException): om1 is Leader but not ready to process request yet.
datanode_2  | 2023-01-12 05:26:24,224 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO impl.LeaderElection: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4 ELECTION round 0: result PASSED
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm2_1      | 2023-01-12 05:25:38,588 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@192b472d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1  | 2023-01-12 05:26:06,392 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-LeaderElection1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC: set configuration 0: peers:[f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1      | 2023-01-12 05:26:03,748 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3_1      | 2023-01-12 05:26:03,753 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
datanode_3  | 2023-01-12 05:25:58,830 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2023-01-12 05:25:58,835 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:98)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderNotReadyException(OzoneManagerProtocolServerSideTranslatorPB.java:259)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:237)
datanode_1  | 2023-01-12 05:26:07,642 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: Detected pause in JVM or host machine (eg GC): pause of approximately 1217928334ns.
scm3_1      | 2023-01-12 05:26:03,748 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1      | 2023-01-12 05:26:03,755 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3_1      | 2023-01-12 05:26:03,764 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
scm2_1      | 2023-01-12 05:25:38,589 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@379f9555{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1_1      | 2023-01-12 05:25:21,214 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1_1      | 2023-01-12 05:25:21,218 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
datanode_3  | 2023-01-12 05:25:58,849 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-12 05:26:24,224 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: shutdown 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4
datanode_2  | 2023-01-12 05:26:24,224 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
datanode_1  | GC pool 'ParNew' had collection(s): count=1 time=1163ms
scm3_1      | 2023-01-12 05:26:04,739 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm2_1      | 2023-01-12 05:25:39,004 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@13a268cd{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-1331179489877589561/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 2023-01-12 05:25:59,851 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: Detected pause in JVM or host machine (eg GC): pause of approximately 816256376ns.
scm1_1      | 2023-01-12 05:25:21,223 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
datanode_2  | 2023-01-12 05:26:24,224 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0DA58DD390A6 with new leaderId: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
om1_1       | ]
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
datanode_1  | 2023-01-12 05:26:07,639 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] WARN impl.FollowerState: Unexpected long sleep: sleep 5105ms but took extra 1003950102ns (> threshold = 300ms)
scm3_1      | 2023-01-12 05:26:05,302 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3b3546a3{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-18058577223709925865/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm2_1      | 2023-01-12 05:25:39,037 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@25fd6d1e{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
datanode_3  | GC pool 'ParNew' had collection(s): count=1 time=179ms
scm1_1      | 2023-01-12 05:25:21,224 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1      | 2023-01-12 05:25:21,225 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2  | 2023-01-12 05:26:24,228 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: change Leader from null to 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b at term 4 for becomeLeader, leader elected after 25246ms
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
datanode_1  | 2023-01-12 05:26:07,679 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1      | 2023-01-12 05:26:05,460 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@592a1882{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3_1      | 2023-01-12 05:26:05,506 [Listener at 0.0.0.0/9860] INFO server.Server: Started @28126ms
scm3_1      | 2023-01-12 05:26:05,566 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2_1      | 2023-01-12 05:25:39,038 [Listener at 0.0.0.0/9860] INFO server.Server: Started @15283ms
scm2_1      | 2023-01-12 05:25:39,040 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2_1      | 2023-01-12 05:25:39,040 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=814ms
om1_1       | 2023-01-12 05:35:39,994 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
scm1_1      | 2023-01-12 05:25:21,231 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
datanode_2  | 2023-01-12 05:26:24,228 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1     | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
datanode_1  | 2023-01-12 05:26:07,683 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1      | 2023-01-12 05:26:05,567 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 2023-01-12 05:25:59,879 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1      | 2023-01-12 05:25:39,042 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
om1_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3
scm1_1      | 2023-01-12 05:25:21,235 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2  | 2023-01-12 05:26:24,229 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1     | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
datanode_1  | 2023-01-12 05:26:07,862 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@42f4741] INFO util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1211ms
scm3_1      | 2023-01-12 05:26:05,578 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
datanode_3  | 2023-01-12 05:25:59,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1      | 2023-01-12 05:25:51,639 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread2] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: set configuration 15: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:187)
scm1_1      | 2023-01-12 05:25:21,247 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO impl.RoleInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14: start f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderStateImpl
datanode_2  | 2023-01-12 05:26:24,229 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
datanode_1  | GC pool 'ParNew' had collection(s): count=1 time=1163ms
scm3_1      | 2023-01-12 05:26:24,381 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7b2ccc1c-57aa-4786-a2ba-0da58dd390a6, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6)34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11)cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, CreationTimestamp2023-01-12T05:25:23.439Z[UTC]] moved to OPEN state
om3_1       | 2023-01-12 05:35:39,986 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 2
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1       | 2023-01-12 05:37:09,215 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-ozone-test-5672640421, Key:ozone-test-7616628748/deletetestapiprefix/key=value/file.
scm2_1      | 2023-01-12 05:25:51,682 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6-server-thread2] INFO server.RaftServer$Division: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E: set configuration 17: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
scm1_1      | 2023-01-12 05:25:21,255 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
datanode_2  | 2023-01-12 05:26:24,262 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
datanode_1  | 2023-01-12 05:26:08,480 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: receive requestVote(ELECTION, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, group-0DA58DD390A6, 1, (t:0, i:0))
scm3_1      | 2023-01-12 05:26:24,525 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
om3_1       | partName: "etag1"
datanode_3  | 2023-01-12 05:25:59,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
om2_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
scm2_1      | 2023-01-12 05:25:54,244 [IPC Server handler 31 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
scm2_1      | 2023-01-12 05:25:54,300 [IPC Server handler 31 on default port 9861] INFO node.SCMNodeManager: Registered Data node : cefdd7f0-5c68-4866-b1db-ab1e8bc5045a{ip: 172.18.0.4, host: ozone-ha_datanode_3.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1      | 2023-01-12 05:25:54,355 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1      | 2023-01-12 05:25:21,267 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_inprogress_0 to /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_0-0
scm1_1      | 2023-01-12 05:25:21,274 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderElection1] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: set configuration 1: peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
datanode_1  | 2023-01-12 05:26:08,526 [grpc-default-executor-3] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: receive requestVote(ELECTION, 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, group-0DA58DD390A6, 1, (t:0, i:0))
scm3_1      | 2023-01-12 05:26:24,858 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om3_1       | , partNumber: 1
om3_1       | partName: "etag2"
om3_1       | ]
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:152)
scm2_1      | 2023-01-12 05:25:54,375 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1_1      | 2023-01-12 05:25:21,292 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/b40a0ffb-806c-459e-acb9-4d43ff37764e/current/log_inprogress_1
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
datanode_1  | 2023-01-12 05:26:08,547 [grpc-default-executor-0] INFO impl.VoteContext: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FOLLOWER: accept ELECTION from cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: our priority 0 <= candidate's priority 0
scm3_1      | 2023-01-12 05:26:33,393 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
datanode_3  | 2023-01-12 05:25:59,888 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
om3_1       | 2023-01-12 05:35:39,987 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:98)
scm2_1      | 2023-01-12 05:25:55,230 [IPC Server handler 31 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
scm1_1      | 2023-01-12 05:25:21,301 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
datanode_1  | 2023-01-12 05:26:08,552 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
scm3_1      | 2023-01-12 05:26:33,393 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm3_1      | 2023-01-12 05:26:33,394 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
om3_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
scm2_1      | 2023-01-12 05:25:55,230 [IPC Server handler 31 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b{ip: 172.18.0.11, host: ozone-ha_datanode_2.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1      | 2023-01-12 05:25:21,302 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm1_1      | 2023-01-12 05:25:21,304 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_2  | 2023-01-12 05:26:24,262 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1       | 	... 17 more
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:187)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
scm2_1      | 2023-01-12 05:25:55,231 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1      | 2023-01-12 05:25:21,305 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm3_1      | 2023-01-12 05:26:33,394 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
s3g_1       | Caused by: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm2_1      | 2023-01-12 05:25:55,241 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2_1      | 2023-01-12 05:25:55,462 [IPC Server handler 30 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f23956f4-f232-4ae2-8f3e-cd8efc9f95bf
scm2_1      | 2023-01-12 05:25:55,468 [IPC Server handler 30 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f23956f4-f232-4ae2-8f3e-cd8efc9f95bf{ip: 172.18.0.6, host: ozone-ha_datanode_1.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1      | 2023-01-12 05:25:55,469 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1      | 2023-01-12 05:25:55,470 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2_1      | 2023-01-12 05:25:55,471 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
datanode_3  | 2023-01-12 05:25:59,888 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm3_1      | 2023-01-12 05:26:33,394 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.getCustomMetadataFromHeaders(EndpointBase.java:291)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 2023-01-12 05:26:24,262 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2  | 2023-01-12 05:26:24,262 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm2_1      | 2023-01-12 05:25:55,472 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
datanode_3  | 2023-01-12 05:25:59,888 [pool-22-thread-1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: start as a follower, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1      | 2023-01-12 05:25:21,305 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
datanode_1  | 2023-01-12 05:26:08,571 [grpc-default-executor-0] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: shutdown f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState
recon_1     | , while invoking $Proxy42.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 48 failover attempts. Trying to failover after sleeping for 2000ms.
scm3_1      | 2023-01-12 05:26:33,397 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.put(ObjectEndpoint.java:214)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 2023-01-12 05:26:24,262 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2  | 2023-01-12 05:26:24,483 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm2_1      | 2023-01-12 05:25:55,473 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1_1      | 2023-01-12 05:25:21,306 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
datanode_1  | 2023-01-12 05:26:08,572 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO impl.FollowerState: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState was interrupted
recon_1     | 2023-01-12 05:26:56,472 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1673501174200
scm3_1      | 2023-01-12 05:26:33,397 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1      | 2023-01-12 05:25:55,474 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
datanode_3  | 2023-01-12 05:25:59,895 [pool-22-thread-1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1      | 2023-01-12 05:25:21,312 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1_1      | 2023-01-12 05:25:21,313 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1     | 2023-01-12 05:26:56,491 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm3_1      | 2023-01-12 05:26:33,397 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=false, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm3_1      | 2023-01-12 05:28:05,965 [6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf@group-4D43FF37764E-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 2023-01-12 05:26:24,484 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1       | 2023-01-12 05:37:15,256 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-2727023149 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:35:45,114 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
scm2_1      | 2023-01-12 05:25:56,880 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: fb7937dd-8506-409c-a1e6-3cdc4fafc0d4, Nodes: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, CreationTimestamp2023-01-12T05:25:22.284Z[UTC]] moved to OPEN state
datanode_3  | 2023-01-12 05:25:59,895 [pool-22-thread-1] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState
scm1_1      | 2023-01-12 05:25:21,461 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.6:56398: output error
scm1_1      | 2023-01-12 05:25:21,462 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
recon_1     | 2023-01-12 05:26:56,492 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
datanode_2  | 2023-01-12 05:26:24,484 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
om2_1       | 2023-01-12 05:37:23,370 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-5928603759 of layout LEGACY in volume: s3v
om1_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3. Provided Part info is { etag1, 1}, whereas OM has partName /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3-b33807c3-dfe3-405d-9b03-b3c9292d342c-109674609906155555-1
scm2_1      | 2023-01-12 05:25:57,213 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_3  | 2023-01-12 05:25:59,902 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0DA58DD390A6,id=cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
scm1_1      | java.nio.channels.ClosedChannelException
datanode_1  | 2023-01-12 05:26:08,573 [grpc-default-executor-0] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: start f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState
recon_1     | 2023-01-12 05:26:56,866 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1673501174200.
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 2023-01-12 05:26:24,495 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om2_1       | 2023-01-12 05:37:27,888 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-ozone-test-5928603759, Key:ozone-test-8001474398/multidelete/key=value/f4.
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:511)
scm2_1      | 2023-01-12 05:25:58,005 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: cd62190c-0472-48bc-9601-9e784e6bba4e, Nodes: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, CreationTimestamp2023-01-12T05:25:23.217Z[UTC]] moved to OPEN state
datanode_3  | 2023-01-12 05:25:59,902 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
datanode_1  | 2023-01-12 05:26:08,625 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1     | 2023-01-12 05:26:57,325 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
scm3_1      | 2023-01-12 05:30:44,036 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2  | 2023-01-12 05:26:24,495 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
om2_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:198)
scm2_1      | 2023-01-12 05:25:58,185 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f23956f4-f232-4ae2-8f3e-cd8efc9f95bf, CreationTimestamp2023-01-12T05:25:23.423Z[UTC]] moved to OPEN state
datanode_3  | 2023-01-12 05:25:59,902 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_1  | 2023-01-12 05:26:08,640 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1     | 2023-01-12 05:26:57,403 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
scm3_1      | 2023-01-12 05:35:44,037 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
datanode_2  | 2023-01-12 05:26:24,495 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1       | 2023-01-12 05:35:45,128 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
om3_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3. Provided Part info is { etag1, 1}, whereas OM has partName /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3-b33807c3-dfe3-405d-9b03-b3c9292d342c-109674609906155555-1
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:511)
scm2_1      | 2023-01-12 05:25:58,388 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_3  | 2023-01-12 05:25:59,902 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1      | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
datanode_1  | 2023-01-12 05:26:08,952 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6 replies to ELECTION vote request: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf#0:OK-t1. Peer's state: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6:t1, leader=null, voted=cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, raftlog=Memoized:f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1     | 2023-01-12 05:26:57,404 [pool-49-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
datanode_2  | 2023-01-12 05:26:24,495 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2  | 2023-01-12 05:26:24,495 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2  | 2023-01-12 05:26:24,537 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:152)
scm2_1      | 2023-01-12 05:25:58,459 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_3  | 2023-01-12 05:25:59,902 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1      | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
datanode_1  | 2023-01-12 05:26:08,958 [grpc-default-executor-3] INFO impl.VoteContext: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FOLLOWER: reject ELECTION from 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: already has voted for cefdd7f0-5c68-4866-b1db-ab1e8bc5045a at current term 1
recon_1     | 2023-01-12 05:26:58,402 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1     | 2023-01-12 05:26:58,402 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
datanode_2  | 2023-01-12 05:26:24,537 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-12 05:26:24,537 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2  | 2023-01-12 05:26:24,537 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm2_1      | 2023-01-12 05:25:58,763 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
datanode_3  | 2023-01-12 05:25:59,923 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1      | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 2023-01-12 05:26:58,403 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
datanode_1  | 2023-01-12 05:26:09,022 [grpc-default-executor-3] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6 replies to ELECTION vote request: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b<-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf#0:FAIL-t1. Peer's state: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6:t1, leader=null, voted=cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, raftlog=Memoized:f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
datanode_2  | 2023-01-12 05:26:24,537 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:198)
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:98)
scm2_1      | 2023-01-12 05:25:59,180 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
datanode_3  | 2023-01-12 05:25:59,981 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1      | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
datanode_3  | 2023-01-12 05:25:59,983 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
datanode_1  | 2023-01-12 05:26:09,289 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0EE7B13ECDDC-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc/current/log_inprogress_0
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
datanode_2  | 2023-01-12 05:26:24,537 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
scm2_1      | 2023-01-12 05:26:00,260 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1      | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
datanode_3  | 2023-01-12 05:26:00,090 [EndpointStateMachine task thread for scm3/172.18.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
recon_1     | 2023-01-12 05:26:58,408 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
datanode_1  | 2023-01-12 05:26:10,237 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6.
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 2023-01-12 05:26:24,537 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
scm2_1      | 2023-01-12 05:26:03,388 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1      | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
datanode_3  | 2023-01-12 05:26:03,175 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-FollowerState] INFO impl.FollowerState: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5185961107ns, electionTimeout:5135ms
recon_1     | 2023-01-12 05:26:58,425 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
datanode_1  | 2023-01-12 05:26:13,642 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: receive requestVote(ELECTION, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, group-0DA58DD390A6, 2, (t:0, i:0))
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 2023-01-12 05:26:24,537 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm2_1      | 2023-01-12 05:26:03,734 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
datanode_3  | 2023-01-12 05:26:03,178 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-FollowerState] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: shutdown cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-FollowerState
datanode_3  | 2023-01-12 05:26:03,184 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-FollowerState] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
datanode_1  | 2023-01-12 05:26:13,643 [grpc-default-executor-0] INFO impl.VoteContext: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FOLLOWER: accept ELECTION from cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: our priority 0 <= candidate's priority 0
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 2023-01-12 05:26:24,549 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO impl.RoleInfo: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: start 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderStateImpl
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm2_1      | 2023-01-12 05:26:04,709 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1      | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
datanode_3  | 2023-01-12 05:26:03,203 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
recon_1     | 2023-01-12 05:26:58,425 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.022 seconds to process 0 keys.
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
datanode_1  | 2023-01-12 05:26:13,643 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2  | 2023-01-12 05:26:24,550 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-SegmentedRaftLogWorker: Starting segment from index:0
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1      | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
datanode_3  | 2023-01-12 05:26:03,205 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-FollowerState] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1
recon_1     | 2023-01-12 05:26:58,497 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
datanode_1  | 2023-01-12 05:26:13,643 [grpc-default-executor-0] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: shutdown f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState
datanode_1  | 2023-01-12 05:26:13,643 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO impl.FollowerState: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState was interrupted
datanode_2  | 2023-01-12 05:26:24,560 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6/current/log_inprogress_0
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1      | 2023-01-12 05:26:24,505 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7b2ccc1c-57aa-4786-a2ba-0da58dd390a6, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6)34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11)cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, CreationTimestamp2023-01-12T05:25:23.439Z[UTC]] moved to OPEN state
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3  | 2023-01-12 05:26:03,273 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1     | 2023-01-12 05:26:58,500 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
om1_1       | 2023-01-12 05:35:46,189 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
datanode_2  | 2023-01-12 05:26:24,589 [34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServer$Division: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b@group-0DA58DD390A6: set configuration 0: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1       | 2023-01-12 05:35:46,209 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
scm2_1      | 2023-01-12 05:26:24,568 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1       | 2023-01-12 05:37:32,458 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-7720168204 of layout LEGACY in volume: s3v
scm1_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
datanode_3  | 2023-01-12 05:26:03,296 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1 ELECTION round 0: result PASSED (term=1)
recon_1     | 2023-01-12 05:28:09,564 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #1 got from ozone-ha_datanode_2.ozone-ha_default.
datanode_1  | 2023-01-12 05:26:13,643 [grpc-default-executor-0] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: start f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
om1_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3. Provided Part info is { etag2, 2}, whereas OM has partName /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3-b33807c3-dfe3-405d-9b03-b3c9292d342c-109674609906155555-2
om3_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3. Provided Part info is { etag2, 2}, whereas OM has partName /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3-b33807c3-dfe3-405d-9b03-b3c9292d342c-109674609906155555-2
scm2_1      | 2023-01-12 05:26:24,584 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
om2_1       | 2023-01-12 05:37:35,381 [OM StateMachine ApplyTransaction Thread - 0] INFO key.OMKeyRequest: Detect allocated but uncommitted blocks [{blockID={containerID=2, localID=111677748019200056}, length=268435456, offset=0, token=null, pipeline=null, createVersion=0, partNumber=0}] in key ozone-test-0170484136/putobject/key=value/zerobyte.
scm1_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
datanode_3  | 2023-01-12 05:26:03,302 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: shutdown cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1
recon_1     | 2023-01-12 05:28:09,609 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #1 got from ozone-ha_datanode_3.ozone-ha_default.
datanode_1  | 2023-01-12 05:26:13,649 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6 replies to ELECTION vote request: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf#0:OK-t2. Peer's state: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6:t2, leader=null, voted=cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, raftlog=Memoized:f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:511)
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:511)
scm2_1      | 2023-01-12 05:26:33,402 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
om2_1       | 2023-01-12 05:38:10,604 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-8035968912 of layout LEGACY in volume: s3v
om2_1       | 2023-01-12 05:38:49,758 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
datanode_3  | 2023-01-12 05:26:03,310 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
recon_1     | 2023-01-12 05:28:09,676 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
datanode_1  | 2023-01-12 05:26:13,661 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:198)
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:198)
scm2_1      | 2023-01-12 05:26:33,402 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1       | 2023-01-12 05:38:49,846 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
datanode_3  | 2023-01-12 05:26:03,312 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3CDC4FAFC0D4 with new leaderId: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
recon_1     | 2023-01-12 05:28:09,725 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
datanode_1  | 2023-01-12 05:26:13,661 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
scm2_1      | 2023-01-12 05:26:33,402 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1       | 2023-01-12 05:38:57,366 [OM StateMachine ApplyTransaction Thread - 0] INFO key.OMKeyRequest: Detect allocated but uncommitted blocks [{blockID={containerID=3, localID=111677748019200060}, length=268435456, offset=0, token=null, pipeline=null, createVersion=0, partNumber=0}] in key omkg/0.
recon_1     | 2023-01-12 05:28:25,038 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2 got from ozone-ha_datanode_2.ozone-ha_default.
datanode_3  | 2023-01-12 05:26:03,314 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4: change Leader from null to cefdd7f0-5c68-4866-b1db-ab1e8bc5045a at term 1 for becomeLeader, leader elected after 7281ms
datanode_3  | 2023-01-12 05:26:03,555 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1  | 2023-01-12 05:26:14,836 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: Detected pause in JVM or host machine (eg GC): pause of approximately 151773966ns. No GCs detected.
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
scm2_1      | 2023-01-12 05:26:33,402 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om2_1       | 2023-01-12 05:39:02,357 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ombg0 of layout LEGACY in volume: vol1
recon_1     | 2023-01-12 05:28:25,079 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #2 got from ozone-ha_datanode_3.ozone-ha_default.
datanode_3  | 2023-01-12 05:26:03,641 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3  | 2023-01-12 05:26:03,654 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1  | 2023-01-12 05:26:18,745 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO impl.FollowerState: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5096192913ns, electionTimeout:5084ms
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm2_1      | 2023-01-12 05:26:33,402 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
recon_1     | 2023-01-12 05:28:25,092 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2 got from ozone-ha_datanode_1.ozone-ha_default.
datanode_3  | 2023-01-12 05:26:03,759 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3  | 2023-01-12 05:26:03,763 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1  | 2023-01-12 05:26:18,746 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: shutdown f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 2023-01-12 05:28:25,098 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
datanode_3  | 2023-01-12 05:26:03,778 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3  | 2023-01-12 05:26:03,913 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1  | 2023-01-12 05:26:18,746 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1      | 2023-01-12 05:28:06,016 [296a2a14-8256-49c4-bc3a-84aaa5fb66c6@group-4D43FF37764E-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm2_1      | 2023-01-12 05:30:28,754 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
recon_1     | 2023-01-12 05:28:25,124 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
datanode_3  | 2023-01-12 05:26:03,974 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3  | 2023-01-12 05:26:04,030 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderStateImpl
datanode_1  | 2023-01-12 05:26:18,746 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1      | 2023-01-12 05:25:21,462 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.4:43224: output error
recon_1     | 2023-01-12 05:28:25,130 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
s3g_1       | 	... 51 more
datanode_3  | 2023-01-12 05:26:04,311 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2023-01-12 05:26:18,746 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: start f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2
scm2_1      | 2023-01-12 05:35:28,754 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm2_1      | 2023-01-12 05:40:28,755 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
om3_1       | 2023-01-12 05:35:47,255 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: PartNumber at index 1 is 2, and its previous partNumber at index 0 is 4 for ozonekey is /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3
om2_1       | 2023-01-12 05:39:07,608 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:voltest for user:hadoop
scm1_1      | 2023-01-12 05:25:21,467 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
recon_1     | 2023-01-12 05:28:26,817 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
s3g_1       | 2023-01-12 05:37:59,078 [qtp384515747-20] WARN server.HttpChannel: /bucket-ozone-test-7720168204/ozone-test-0170484136/putobject/custom-metadata/key2
datanode_3  | 2023-01-12 05:26:04,994 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-LeaderElection1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4: set configuration 0: peers:[cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-12 05:26:18,755 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2 ELECTION round 0: submit vote requests at term 3 for -1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1       | 2023-01-12 05:35:47,254 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: PartNumber at index 1 is 2, and its previous partNumber at index 0 is 4 for ozonekey is /s3v/bucket-ozone-test-0191226535/ozone-test-3829597967/multipartKey3
om3_1       | 2023-01-12 05:35:47,257 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
om2_1       | 2023-01-12 05:39:07,669 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: buckettest of layout LEGACY in volume: voltest
scm1_1      | java.nio.channels.ClosedChannelException
recon_1     | 2023-01-12 05:28:26,819 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
s3g_1       | javax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
datanode_3  | 2023-01-12 05:26:05,094 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO impl.FollowerState: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5198907071ns, electionTimeout:5113ms
datanode_3  | 2023-01-12 05:26:05,127 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: shutdown cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState
om1_1       | 2023-01-12 05:35:47,257 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: ozone-test-3829597967/multipartKey3 in Volume/Bucket s3v/bucket-ozone-test-0191226535
om1_1       | INVALID_PART_ORDER org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3 because parts are in Invalid order.
om2_1       | 2023-01-12 05:40:11,314 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ombr0 of layout LEGACY in volume: vol1
recon_1     | 2023-01-12 05:28:26,858 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:162)
datanode_1  | 2023-01-12 05:26:18,762 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-12 05:26:05,128 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3  | 2023-01-12 05:26:05,128 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getPartsListSize(S3MultipartUploadCompleteRequest.java:477)
recon_1     | 2023-01-12 05:28:26,863 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
datanode_3  | 2023-01-12 05:26:05,128 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2
om3_1       | INVALID_PART_ORDER org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-ozone-test-0191226535 key: ozone-test-3829597967/multipartKey3 because parts are in Invalid order.
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:194)
datanode_1  | 2023-01-12 05:26:18,762 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-12 05:26:18,765 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
scm1_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 2023-01-12 05:28:26,886 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 4 pipelines in house.
recon_1     | 2023-01-12 05:28:26,931 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 63 milliseconds.
datanode_3  | 2023-01-12 05:26:05,154 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getPartsListSize(S3MultipartUploadCompleteRequest.java:477)
datanode_1  | 2023-01-12 05:26:18,774 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
scm1_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1      | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1      | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 2023-01-12 05:28:26,963 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 143 milliseconds to process 0 existing database records.
datanode_3  | 2023-01-12 05:26:05,193 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-12 05:26:05,193 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-12 05:26:18,862 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: receive requestVote(ELECTION, 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, group-0DA58DD390A6, 3, (t:0, i:0))
scm1_1      | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
recon_1     | 2023-01-12 05:28:27,075 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 111 milliseconds for processing 2 containers.
datanode_3  | 2023-01-12 05:26:05,242 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:194)
datanode_1  | 2023-01-12 05:26:18,862 [grpc-default-executor-0] INFO impl.VoteContext: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-CANDIDATE: reject ELECTION from 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: already has voted for f23956f4-f232-4ae2-8f3e-cd8efc9f95bf at current term 3
scm1_1      | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
recon_1     | 2023-01-12 05:28:49,178 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #3 got from ozone-ha_datanode_3.ozone-ha_default.
recon_1     | 2023-01-12 05:28:49,208 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
datanode_3  | 2023-01-12 05:26:05,266 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for f23956f4-f232-4ae2-8f3e-cd8efc9f95bf
scm1_1      | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
datanode_1  | 2023-01-12 05:26:18,862 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6 replies to ELECTION vote request: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b<-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf#0:FAIL-t3. Peer's state: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6:t3, leader=null, voted=f23956f4-f232-4ae2-8f3e-cd8efc9f95bf, raftlog=Memoized:f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
recon_1     | 2023-01-12 05:33:27,009 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 4 pipelines in house.
datanode_3  | 2023-01-12 05:26:05,473 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-3CDC4FAFC0D4-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fb7937dd-8506-409c-a1e6-3cdc4fafc0d4/current/log_inprogress_0
scm1_1      | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
datanode_1  | 2023-01-12 05:26:18,918 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: receive requestVote(ELECTION, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, group-0DA58DD390A6, 3, (t:0, i:0))
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
recon_1     | 2023-01-12 05:33:27,012 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 17 milliseconds.
datanode_3  | 2023-01-12 05:26:06,533 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: Detected pause in JVM or host machine (eg GC): pause of approximately 663048096ns.
scm1_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 2023-01-12 05:33:27,076 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 1 milliseconds to process 0 existing database records.
datanode_3  | GC pool 'ParNew' had collection(s): count=1 time=812ms
scm1_1      | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
datanode_1  | 2023-01-12 05:26:18,918 [grpc-default-executor-0] INFO impl.VoteContext: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-CANDIDATE: reject ELECTION from cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: already has voted for f23956f4-f232-4ae2-8f3e-cd8efc9f95bf at current term 3
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 2023-01-12 05:33:27,080 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 4 milliseconds for processing 3 containers.
datanode_3  | 2023-01-12 05:26:08,091 [grpc-default-executor-0] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: receive requestVote(ELECTION, 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, group-0DA58DD390A6, 1, (t:0, i:0))
scm1_1      | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 2023-01-12 05:35:52,586 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadAbortRequest: Abort Multipart request is failed for KeyName ozone-test-7719133293/multipartKey5 in VolumeName/Bucket s3v/bucket-ozone-test-0191226535
recon_1     | 2023-01-12 05:36:58,528 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
datanode_3  | 2023-01-12 05:26:08,130 [grpc-default-executor-0] INFO impl.VoteContext: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-CANDIDATE: reject ELECTION from 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: already has voted for cefdd7f0-5c68-4866-b1db-ab1e8bc5045a at current term 1
scm1_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
datanode_1  | 2023-01-12 05:26:18,918 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6 replies to ELECTION vote request: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf#0:FAIL-t3. Peer's state: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6:t3, leader=null, voted=f23956f4-f232-4ae2-8f3e-cd8efc9f95bf, raftlog=Memoized:f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 2023-01-12 05:36:58,531 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
datanode_3  | 2023-01-12 05:26:08,201 [grpc-default-executor-0] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6 replies to ELECTION vote request: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b<-cefdd7f0-5c68-4866-b1db-ab1e8bc5045a#0:FAIL-t1. Peer's state: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6:t1, leader=null, voted=cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, raftlog=Memoized:cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
datanode_1  | 2023-01-12 05:26:18,970 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1       | 2023-01-12 05:35:52,584 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadAbortRequest: Abort Multipart request is failed for KeyName ozone-test-7719133293/multipartKey5 in VolumeName/Bucket s3v/bucket-ozone-test-0191226535
recon_1     | 2023-01-12 05:36:58,532 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
datanode_3  | 2023-01-12 05:26:08,400 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
scm1_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_1  | 2023-01-12 05:26:18,981 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection:   Response 0: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf<-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b#0:FAIL-t3
datanode_1  | 2023-01-12 05:26:18,981 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection:   Response 1: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf<-cefdd7f0-5c68-4866-b1db-ab1e8bc5045a#0:FAIL-t3
datanode_1  | 2023-01-12 05:26:18,981 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1  | 2023-01-12 05:26:18,981 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
recon_1     | 2023-01-12 05:36:58,534 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_3  | 2023-01-12 05:26:08,401 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection:   Response 0: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b#0:FAIL-t1
om3_1       | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Abort Multipart Upload Failed: volume: s3vbucket: bucket-ozone-test-0191226535key: ozone-test-7719133293/multipartKey5
om1_1       | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Abort Multipart Upload Failed: volume: s3vbucket: bucket-ozone-test-0191226535key: ozone-test-7719133293/multipartKey5
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
datanode_1  | 2023-01-12 05:26:18,981 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: shutdown f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2
recon_1     | 2023-01-12 05:36:58,534 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
datanode_3  | 2023-01-12 05:26:08,401 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2 ELECTION round 0: result REJECTED
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadAbortRequest.validateAndUpdateCache(S3MultipartUploadAbortRequest.java:161)
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadAbortRequest.validateAndUpdateCache(S3MultipartUploadAbortRequest.java:161)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
datanode_1  | 2023-01-12 05:26:18,982 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-LeaderElection2] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: start f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState
recon_1     | 2023-01-12 05:36:58,534 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
recon_1     | 2023-01-12 05:36:58,535 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
datanode_3  | 2023-01-12 05:26:08,403 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 2023-01-12 05:26:19,022 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1      | 2023-01-12 05:25:21,469 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.11:39284: output error
scm1_1      | 2023-01-12 05:25:21,508 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
datanode_3  | 2023-01-12 05:26:08,407 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: shutdown cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
datanode_1  | 2023-01-12 05:26:19,022 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1     | 2023-01-12 05:36:58,535 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
scm1_1      | java.nio.channels.ClosedChannelException
scm1_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
datanode_1  | 2023-01-12 05:26:24,157 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: receive requestVote(ELECTION, 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, group-0DA58DD390A6, 4, (t:0, i:0))
recon_1     | 2023-01-12 05:36:58,535 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 2 
datanode_3  | 2023-01-12 05:26:08,410 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection2] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState
datanode_3  | 2023-01-12 05:26:08,434 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 2023-01-12 05:35:53,516 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyCreateRequest: Key creation failed. Volume:s3v, Bucket:bucket-ozone-test-0191226535, Key:ozone-test-9943555385/multipartKey. 
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
datanode_1  | 2023-01-12 05:26:24,157 [grpc-default-executor-0] INFO impl.VoteContext: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FOLLOWER: accept ELECTION from 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: our priority 0 <= candidate's priority 1
recon_1     | 2023-01-12 05:36:58,850 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 341, SequenceNumber diff: 982, SequenceNumber Lag from OM 0.
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No such Multipart upload is with specified uploadId random
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
datanode_1  | 2023-01-12 05:26:24,157 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
recon_1     | 2023-01-12 05:36:58,851 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 982 records
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareMultipartFileInfo(OMKeyRequest.java:798)
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareFileInfo(OMKeyRequest.java:685)
datanode_1  | 2023-01-12 05:26:24,157 [grpc-default-executor-0] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: shutdown f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState
recon_1     | 2023-01-12 05:36:58,859 [pool-28-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
datanode_3  | 2023-01-12 05:26:08,445 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-12 05:26:10,297 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6.
om3_1       | 2023-01-12 05:35:53,537 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyCreateRequest: Key creation failed. Volume:s3v, Bucket:bucket-ozone-test-0191226535, Key:ozone-test-9943555385/multipartKey. 
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareKeyInfo(OMKeyRequest.java:662)
datanode_1  | 2023-01-12 05:26:24,157 [grpc-default-executor-0] INFO impl.RoleInfo: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: start f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState
recon_1     | 2023-01-12 05:36:59,207 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
datanode_3  | 2023-01-12 05:26:13,603 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO impl.FollowerState: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5192585630ns, electionTimeout:5158ms
datanode_3  | 2023-01-12 05:26:13,603 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: shutdown cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState
om3_1       | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No such Multipart upload is with specified uploadId random
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareMultipartFileInfo(OMKeyRequest.java:798)
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:279)
datanode_1  | 2023-01-12 05:26:24,157 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO impl.FollowerState: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState was interrupted
recon_1     | 2023-01-12 05:36:59,257 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 39 OM DB update event(s).
scm1_1      | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1      | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
s3g_1       | Caused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:410)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
recon_1     | 2023-01-12 05:36:59,382 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
datanode_3  | 2023-01-12 05:26:13,603 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm1_1      | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
datanode_1  | 2023-01-12 05:26:24,193 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1     | 2023-01-12 05:36:59,383 [pool-27-thread-1] ERROR tasks.ReconTaskControllerImpl: Unexpected error : 
scm1_1      | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareFileInfo(OMKeyRequest.java:685)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
recon_1     | java.util.concurrent.ExecutionException: java.lang.NullPointerException
scm1_1      | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareKeyInfo(OMKeyRequest.java:662)
datanode_3  | 2023-01-12 05:26:13,603 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
datanode_1  | 2023-01-12 05:26:24,196 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
datanode_3  | 2023-01-12 05:26:13,603 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 2023-01-12 05:26:24,200 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6 replies to ELECTION vote request: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b<-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf#0:OK-t4. Peer's state: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6:t4, leader=null, voted=34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, raftlog=Memoized:f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1     | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:279)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
datanode_3  | 2023-01-12 05:26:13,612 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 2023-01-12 05:26:24,246 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: receive requestVote(ELECTION, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, group-0DA58DD390A6, 4, (t:0, i:0))
recon_1     | 	at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.processTaskResults(ReconTaskControllerImpl.java:247)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
datanode_3  | 2023-01-12 05:26:13,636 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 2023-01-12 05:26:24,249 [grpc-default-executor-0] INFO impl.VoteContext: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-FOLLOWER: reject ELECTION from cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: already has voted for 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b at current term 4
recon_1     | 	at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.consumeOMEvents(ReconTaskControllerImpl.java:118)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 2023-01-12 05:26:13,637 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-12 05:26:13,657 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3  | 2023-01-12 05:26:13,657 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3] INFO impl.LeaderElection:   Response 0: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b#0:FAIL-t2
datanode_1  | 2023-01-12 05:26:24,249 [grpc-default-executor-0] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6 replies to ELECTION vote request: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf#0:FAIL-t4. Peer's state: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6:t4, leader=null, voted=34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, raftlog=Memoized:f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1      | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 2023-01-12 05:26:13,660 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3 ELECTION round 0: result REJECTED
datanode_3  | 2023-01-12 05:26:13,669 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_3  | 2023-01-12 05:26:13,669 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: shutdown cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3
datanode_1  | 2023-01-12 05:26:24,928 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0DA58DD390A6 with new leaderId: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
datanode_1  | 2023-01-12 05:26:24,930 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf-server-thread1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: change Leader from null to 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b at term 4 for appendEntries, leader elected after 24811ms
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 2023-01-12 05:36:42,832 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-8056751975 of layout LEGACY in volume: s3v
scm1_1      | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3  | 2023-01-12 05:26:13,670 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection3] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState
datanode_3  | 2023-01-12 05:26:13,686 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1       | 2023-01-12 05:36:43,740 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: destbucket-12311 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:36:56,362 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-5672640421 of layout LEGACY in volume: s3v
scm1_1      | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1      | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
om3_1       | 2023-01-12 05:36:42,839 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-8056751975 of layout LEGACY in volume: s3v
om3_1       | 2023-01-12 05:36:43,745 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: destbucket-12311 of layout LEGACY in volume: s3v
datanode_3  | 2023-01-12 05:26:13,686 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-12 05:26:18,803 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO impl.FollowerState: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5133755309ns, electionTimeout:5117ms
datanode_3  | 2023-01-12 05:26:18,804 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: shutdown cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState
om3_1       | 2023-01-12 05:36:56,370 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-5672640421 of layout LEGACY in volume: s3v
recon_1     | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:503)
recon_1     | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$startSyncDataFromOM$0(OzoneManagerServiceProviderImpl.java:258)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
om3_1       | 2023-01-12 05:37:01,577 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-ozone-test-5672640421, Key:thereisnosuchfile.
om1_1       | 2023-01-12 05:37:01,560 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-ozone-test-5672640421, Key:thereisnosuchfile.
om1_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
datanode_1  | 2023-01-12 05:26:24,932 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf-server-thread1] INFO server.RaftServer$Division: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6: set configuration 0: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:152)
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:98)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
scm1_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:152)
datanode_3  | 2023-01-12 05:26:18,804 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_3  | 2023-01-12 05:26:18,805 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
scm1_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:98)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm1_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
datanode_3  | 2023-01-12 05:26:18,805 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4
scm1_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1      | 2023-01-12 05:25:22,251 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
datanode_3  | 2023-01-12 05:26:18,856 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4 ELECTION round 0: submit vote requests at term 3 for -1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1     | Caused by: java.lang.NullPointerException
recon_1     | 	at org.apache.hadoop.ozone.recon.tasks.NSSummaryTaskWithLegacy.processWithLegacy(NSSummaryTaskWithLegacy.java:107)
recon_1     | 	at org.apache.hadoop.ozone.recon.tasks.NSSummaryTask.process(NSSummaryTask.java:99)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm1_1      | 2023-01-12 05:25:22,255 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : cefdd7f0-5c68-4866-b1db-ab1e8bc5045a{ip: 172.18.0.4, host: ozone-ha_datanode_3.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_1  | 2023-01-12 05:26:24,933 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf-server-thread1] INFO segmented.SegmentedRaftLogWorker: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2023-01-12 05:26:24,939 [f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf@group-0DA58DD390A6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6/current/log_inprogress_0
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
scm1_1      | 2023-01-12 05:25:22,264 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1      | 2023-01-12 05:25:22,266 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1      | 2023-01-12 05:25:22,281 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1_1      | 2023-01-12 05:25:22,284 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
recon_1     | 	at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.lambda$consumeOMEvents$0(ReconTaskControllerImpl.java:113)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1     | 	... 3 more
om3_1       | 2023-01-12 05:37:05,095 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-ozone-test-5672640421, Key:ozone-test-7616628748/deletetestapidir/key=value/.
om3_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
recon_1     | 2023-01-12 05:38:27,037 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 4 pipelines in house.
recon_1     | 2023-01-12 05:38:27,041 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 14 milliseconds.
scm1_1      | 2023-01-12 05:25:22,285 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=fb7937dd-8506-409c-a1e6-3cdc4fafc0d4 to datanode:cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
scm1_1      | 2023-01-12 05:25:22,411 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: fb7937dd-8506-409c-a1e6-3cdc4fafc0d4, Nodes: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:22.284Z[UTC]].
scm1_1      | 2023-01-12 05:25:22,413 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:152)
recon_1     | 2023-01-12 05:38:27,081 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 1 milliseconds to process 0 existing database records.
scm1_1      | 2023-01-12 05:25:23,213 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
datanode_3  | 2023-01-12 05:26:18,859 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:98)
recon_1     | 2023-01-12 05:38:27,085 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 3 milliseconds for processing 3 containers.
scm1_1      | 2023-01-12 05:25:23,213 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b{ip: 172.18.0.11, host: ozone-ha_datanode_2.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_3  | 2023-01-12 05:26:18,859 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
scm1_1      | 2023-01-12 05:25:23,215 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
datanode_3  | 2023-01-12 05:26:18,926 [grpc-default-executor-2] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: receive requestVote(ELECTION, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf, group-0DA58DD390A6, 3, (t:0, i:0))
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm1_1      | 2023-01-12 05:25:23,215 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1_1      | 2023-01-12 05:25:23,217 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=cd62190c-0472-48bc-9601-9e784e6bba4e to datanode:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
om1_1       | 2023-01-12 05:37:05,078 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-ozone-test-5672640421, Key:ozone-test-7616628748/deletetestapidir/key=value/.
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 2023-01-12 05:26:18,926 [grpc-default-executor-2] INFO impl.VoteContext: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-CANDIDATE: reject ELECTION from f23956f4-f232-4ae2-8f3e-cd8efc9f95bf: already has voted for cefdd7f0-5c68-4866-b1db-ab1e8bc5045a at current term 3
datanode_3  | 2023-01-12 05:26:18,927 [grpc-default-executor-2] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6 replies to ELECTION vote request: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf<-cefdd7f0-5c68-4866-b1db-ab1e8bc5045a#0:FAIL-t3. Peer's state: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6:t3, leader=null, voted=cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, raftlog=Memoized:cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
om1_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:152)
datanode_3  | 2023-01-12 05:26:18,966 [grpc-default-executor-2] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: receive requestVote(ELECTION, 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, group-0DA58DD390A6, 3, (t:0, i:0))
scm1_1      | 2023-01-12 05:25:23,223 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: cd62190c-0472-48bc-9601-9e784e6bba4e, Nodes: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:23.217Z[UTC]].
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:98)
datanode_3  | 2023-01-12 05:26:18,966 [grpc-default-executor-2] INFO impl.VoteContext: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-CANDIDATE: reject ELECTION from 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: already has voted for cefdd7f0-5c68-4866-b1db-ab1e8bc5045a at current term 3
scm1_1      | 2023-01-12 05:25:23,223 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
datanode_3  | 2023-01-12 05:26:18,967 [grpc-default-executor-2] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6 replies to ELECTION vote request: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b<-cefdd7f0-5c68-4866-b1db-ab1e8bc5045a#0:FAIL-t3. Peer's state: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6:t3, leader=null, voted=cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, raftlog=Memoized:cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1      | 2023-01-12 05:25:23,421 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f23956f4-f232-4ae2-8f3e-cd8efc9f95bf
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
datanode_3  | 2023-01-12 05:26:19,031 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4: ELECTION REJECTED received 2 response(s) and 0 exception(s):
scm1_1      | 2023-01-12 05:25:23,422 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f23956f4-f232-4ae2-8f3e-cd8efc9f95bf{ip: 172.18.0.6, host: ozone-ha_datanode_1.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om3_1       | 2023-01-12 05:37:09,224 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-ozone-test-5672640421, Key:ozone-test-7616628748/deletetestapiprefix/key=value/file.
om3_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
datanode_3  | 2023-01-12 05:26:19,031 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4] INFO impl.LeaderElection:   Response 0: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b#0:FAIL-t3
scm1_1      | 2023-01-12 05:25:23,422 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:152)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 2023-01-12 05:26:19,032 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4] INFO impl.LeaderElection:   Response 1: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-f23956f4-f232-4ae2-8f3e-cd8efc9f95bf#0:FAIL-t3
scm1_1      | 2023-01-12 05:25:23,423 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc to datanode:f23956f4-f232-4ae2-8f3e-cd8efc9f95bf
scm1_1      | 2023-01-12 05:25:23,426 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 2023-01-12 05:26:19,032 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4 ELECTION round 0: result REJECTED
scm1_1      | 2023-01-12 05:25:23,426 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:98)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1      | 2023-01-12 05:25:23,426 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
scm1_1      | 2023-01-12 05:25:23,427 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
datanode_3  | 2023-01-12 05:26:19,032 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
s3g_1       | 	... 17 more
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 2023-01-12 05:37:09,213 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-ozone-test-5672640421, Key:ozone-test-7616628748/deletetestapiprefix/key=value/file.
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 2023-01-12 05:26:19,032 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: shutdown cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4
s3g_1       | Caused by: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
scm1_1      | 2023-01-12 05:25:23,427 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
om1_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:152)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:98)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.getCustomMetadataFromHeaders(EndpointBase.java:291)
datanode_3  | 2023-01-12 05:26:19,033 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection4] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.put(ObjectEndpoint.java:214)
datanode_3  | 2023-01-12 05:26:19,038 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1       | 2023-01-12 05:37:15,259 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-2727023149 of layout LEGACY in volume: s3v
scm1_1      | 2023-01-12 05:25:23,429 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:23.423Z[UTC]].
datanode_3  | 2023-01-12 05:26:19,038 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1       | 2023-01-12 05:37:23,375 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-5928603759 of layout LEGACY in volume: s3v
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1_1      | 2023-01-12 05:25:23,429 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode_3  | 2023-01-12 05:26:24,085 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO impl.FollowerState: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5051531982ns, electionTimeout:5046ms
datanode_3  | 2023-01-12 05:26:24,085 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: shutdown cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState
datanode_3  | 2023-01-12 05:26:24,085 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om3_1       | 2023-01-12 05:37:27,892 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-ozone-test-5928603759, Key:ozone-test-8001474398/multidelete/key=value/f4.
om3_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
scm1_1      | 2023-01-12 05:25:23,439 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 to datanode:f23956f4-f232-4ae2-8f3e-cd8efc9f95bf
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:152)
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:98)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
datanode_3  | 2023-01-12 05:26:24,085 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1      | 2023-01-12 05:25:23,440 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 to datanode:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
datanode_3  | 2023-01-12 05:26:24,087 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
datanode_3  | 2023-01-12 05:26:24,092 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5 ELECTION round 0: submit vote requests at term 4 for -1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1       | 2023-01-12 05:37:15,251 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-2727023149 of layout LEGACY in volume: s3v
om1_1       | 2023-01-12 05:37:23,365 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-5928603759 of layout LEGACY in volume: s3v
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
scm1_1      | 2023-01-12 05:25:23,442 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7b2ccc1c-57aa-4786-a2ba-0da58dd390a6 to datanode:cefdd7f0-5c68-4866-b1db-ab1e8bc5045a
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
datanode_3  | 2023-01-12 05:26:24,159 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1       | 2023-01-12 05:37:27,883 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-ozone-test-5928603759, Key:ozone-test-8001474398/multidelete/key=value/f4.
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm1_1      | 2023-01-12 05:25:23,447 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 7b2ccc1c-57aa-4786-a2ba-0da58dd390a6, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6)34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11)cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-12T05:25:23.439Z[UTC]].
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
datanode_3  | 2023-01-12 05:26:24,161 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-12 05:26:24,165 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5: ELECTION REJECTED received 1 response(s) and 0 exception(s):
om1_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm1_1      | 2023-01-12 05:25:23,447 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
datanode_3  | 2023-01-12 05:26:24,165 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5] INFO impl.LeaderElection:   Response 0: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a<-34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b#0:FAIL-t4
datanode_3  | 2023-01-12 05:26:24,166 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5] INFO impl.LeaderElection: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5 ELECTION round 0: result REJECTED
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1      | 2023-01-12 05:25:23,449 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:152)
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:98)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1      | 2023-01-12 05:25:23,452 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
datanode_3  | 2023-01-12 05:26:24,183 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: changes role from CANDIDATE to FOLLOWER at term 4 for REJECTED
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:317)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
datanode_3  | 2023-01-12 05:26:24,184 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: shutdown cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5
om3_1       | 2023-01-12 05:37:32,457 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-7720168204 of layout LEGACY in volume: s3v
scm1_1      | 2023-01-12 05:25:31,394 [IPC Server handler 2 on default port 9863] INFO ha.SCMRatisServerImpl: f98693f9-b2e2-4085-a3fc-32934ea8db14: Submitting SetConfiguration request to Ratis server with new SCM peers list: [f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|priority:0|startupRole:FOLLOWER]
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
datanode_3  | 2023-01-12 05:26:24,187 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-LeaderElection5] INFO impl.RoleInfo: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a: start cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState
om3_1       | 2023-01-12 05:37:35,377 [OM StateMachine ApplyTransaction Thread - 0] INFO key.OMKeyRequest: Detect allocated but uncommitted blocks [{blockID={containerID=2, localID=111677748019200056}, length=268435456, offset=0, token=null, pipeline=null, createVersion=0, partNumber=0}] in key ozone-test-0170484136/putobject/key=value/zerobyte.
scm1_1      | 2023-01-12 05:25:31,399 [IPC Server handler 2 on default port 9863] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: receive setConfiguration SetConfigurationRequest:client-7075FE2157DD->f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E, cid=4, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|priority:0|startupRole:FOLLOWER], listeners:[]
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 2023-01-12 05:38:10,603 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-8035968912 of layout LEGACY in volume: s3v
scm1_1      | 2023-01-12 05:25:31,402 [IPC Server handler 2 on default port 9863] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-7075FE2157DD->f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E, cid=4, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|priority:0|startupRole:FOLLOWER], listeners:[]
datanode_3  | 2023-01-12 05:26:24,221 [grpc-default-executor-2] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: receive requestVote(ELECTION, 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, group-0DA58DD390A6, 4, (t:0, i:0))
datanode_3  | 2023-01-12 05:26:24,235 [grpc-default-executor-2] INFO impl.VoteContext: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FOLLOWER: reject ELECTION from 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b: already has voted for cefdd7f0-5c68-4866-b1db-ab1e8bc5045a at current term 4
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
datanode_3  | 2023-01-12 05:26:24,239 [grpc-default-executor-2] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6 replies to ELECTION vote request: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b<-cefdd7f0-5c68-4866-b1db-ab1e8bc5045a#0:FAIL-t4. Peer's state: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6:t4, leader=null, voted=cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, raftlog=Memoized:cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1       | 2023-01-12 05:38:49,767 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om3_1       | 2023-01-12 05:38:49,844 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
om3_1       | 2023-01-12 05:38:57,369 [OM StateMachine ApplyTransaction Thread - 0] INFO key.OMKeyRequest: Detect allocated but uncommitted blocks [{blockID={containerID=3, localID=111677748019200060}, length=268435456, offset=0, token=null, pipeline=null, createVersion=0, partNumber=0}] in key omkg/0.
scm1_1      | 2023-01-12 05:25:31,437 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3  | 2023-01-12 05:26:24,240 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 2023-01-12 05:39:02,351 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ombg0 of layout LEGACY in volume: vol1
om3_1       | 2023-01-12 05:39:07,598 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:voltest for user:hadoop
scm1_1      | 2023-01-12 05:25:31,438 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2023-01-12 05:26:24,240 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1       | 	... 51 more
scm1_1      | 2023-01-12 05:25:31,438 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om3_1       | 2023-01-12 05:39:07,663 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: buckettest of layout LEGACY in volume: voltest
datanode_3  | 2023-01-12 05:26:24,818 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0DA58DD390A6 with new leaderId: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b
om1_1       | 2023-01-12 05:37:32,450 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-7720168204 of layout LEGACY in volume: s3v
s3g_1       | 2023-01-12 05:37:59,080 [qtp384515747-20] WARN server.HttpChannelState: unhandled due to prior sendError
s3g_1       | javax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
om1_1       | 2023-01-12 05:37:35,375 [OM StateMachine ApplyTransaction Thread - 0] INFO key.OMKeyRequest: Detect allocated but uncommitted blocks [{blockID={containerID=2, localID=111677748019200056}, length=268435456, offset=0, token=null, pipeline=null, createVersion=0, partNumber=0}] in key ozone-test-0170484136/putobject/key=value/zerobyte.
om1_1       | 2023-01-12 05:38:10,602 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket-ozone-test-8035968912 of layout LEGACY in volume: s3v
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:162)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
om3_1       | 2023-01-12 05:40:11,315 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ombr0 of layout LEGACY in volume: vol1
datanode_3  | 2023-01-12 05:26:24,820 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a-server-thread1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: change Leader from null to 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b at term 4 for appendEntries, leader elected after 26275ms
datanode_3  | 2023-01-12 05:26:24,918 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a-server-thread1] INFO server.RaftServer$Division: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6: set configuration 0: peers:[34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b|rpc:172.18.0.11:9856|admin:172.18.0.11:9857|client:172.18.0.11:9858|dataStream:172.18.0.11:9858|priority:1|startupRole:FOLLOWER, f23956f4-f232-4ae2-8f3e-cd8efc9f95bf|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:0|startupRole:FOLLOWER, cefdd7f0-5c68-4866-b1db-ab1e8bc5045a|rpc:172.18.0.4:9856|admin:172.18.0.4:9857|client:172.18.0.4:9858|dataStream:172.18.0.4:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-12 05:26:24,923 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a-server-thread1] INFO segmented.SegmentedRaftLogWorker: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-SegmentedRaftLogWorker: Starting segment from index:0
om1_1       | 2023-01-12 05:38:22,168 [IPC Server handler 63 on default port 9862] WARN ipc.Server: Large response size 1048663 for call Call#0 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 172.18.0.2:49576
datanode_3  | 2023-01-12 05:26:24,930 [cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a@group-0DA58DD390A6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7b2ccc1c-57aa-4786-a2ba-0da58dd390a6/current/log_inprogress_0
om1_1       | 2023-01-12 05:38:31,045 [IPC Server handler 79 on default port 9862] WARN ipc.Server: Large response size 1048663 for call Call#0 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 172.18.0.2:52424
om1_1       | 2023-01-12 05:38:49,743 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
om1_1       | 2023-01-12 05:38:49,841 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
scm1_1      | 2023-01-12 05:25:31,463 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
om1_1       | 2023-01-12 05:38:57,361 [OM StateMachine ApplyTransaction Thread - 0] INFO key.OMKeyRequest: Detect allocated but uncommitted blocks [{blockID={containerID=3, localID=111677748019200060}, length=268435456, offset=0, token=null, pipeline=null, createVersion=0, partNumber=0}] in key omkg/0.
om1_1       | 2023-01-12 05:39:02,347 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ombg0 of layout LEGACY in volume: vol1
scm1_1      | 2023-01-12 05:25:31,465 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1      | 2023-01-12 05:25:31,466 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1      | 2023-01-12 05:25:31,466 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1       | 2023-01-12 05:39:07,603 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:voltest for user:hadoop
scm1_1      | 2023-01-12 05:25:31,467 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
om1_1       | 2023-01-12 05:39:07,654 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: buckettest of layout LEGACY in volume: voltest
scm1_1      | 2023-01-12 05:25:31,499 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
om1_1       | 2023-01-12 05:40:11,309 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ombr0 of layout LEGACY in volume: vol1
scm1_1      | 2023-01-12 05:25:31,624 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6-GrpcLogAppender: send f98693f9-b2e2-4085-a3fc-32934ea8db14->296a2a14-8256-49c4-bc3a-84aaa5fb66c6#0-t2,notify:(t:1, i:0)
scm1_1      | 2023-01-12 05:25:31,629 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 296a2a14-8256-49c4-bc3a-84aaa5fb66c6
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
scm1_1      | 2023-01-12 05:25:34,911 [grpc-default-executor-0] INFO server.GrpcLogAppender: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6-InstallSnapshotResponseHandler: received the first reply f98693f9-b2e2-4085-a3fc-32934ea8db14<-296a2a14-8256-49c4-bc3a-84aaa5fb66c6#0:OK-t0,ALREADY_INSTALLED
scm1_1      | 2023-01-12 05:25:34,923 [grpc-default-executor-0] INFO server.GrpcLogAppender: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1_1      | 2023-01-12 05:25:34,924 [grpc-default-executor-0] INFO leader.FollowerInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6: snapshotIndex: setUnconditionally 0 -> 0
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
scm1_1      | 2023-01-12 05:25:34,924 [grpc-default-executor-0] INFO leader.FollowerInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6: matchIndex: setUnconditionally 0 -> 0
scm1_1      | 2023-01-12 05:25:34,925 [grpc-default-executor-0] INFO leader.FollowerInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6: nextIndex: setUnconditionally 0 -> 1
scm1_1      | 2023-01-12 05:25:34,925 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6 acknowledged installing snapshot
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
scm1_1      | 2023-01-12 05:25:34,926 [grpc-default-executor-0] INFO leader.FollowerInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6: nextIndex: updateToMax old=1, new=1, updated? false
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
scm1_1      | 2023-01-12 05:25:35,110 [grpc-default-executor-0] INFO leader.FollowerInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6: nextIndex: updateUnconditionally 11 -> 0
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
scm1_1      | 2023-01-12 05:25:35,119 [grpc-default-executor-2] INFO leader.FollowerInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->296a2a14-8256-49c4-bc3a-84aaa5fb66c6: nextIndex: updateUnconditionally 11 -> 0
scm1_1      | 2023-01-12 05:25:35,649 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderStateImpl] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: set configuration 11: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1      | 2023-01-12 05:25:35,728 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderStateImpl] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: set configuration 13: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
scm1_1      | 2023-01-12 05:25:35,838 [IPC Server handler 2 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 296a2a14-8256-49c4-bc3a-84aaa5fb66c6.
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
scm1_1      | 2023-01-12 05:25:48,681 [IPC Server handler 2 on default port 9863] INFO ha.SCMRatisServerImpl: f98693f9-b2e2-4085-a3fc-32934ea8db14: Submitting SetConfiguration request to Ratis server with new SCM peers list: [296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf|rpc:scm3:9894|priority:0|startupRole:FOLLOWER]
scm1_1      | 2023-01-12 05:25:48,682 [IPC Server handler 2 on default port 9863] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: receive setConfiguration SetConfigurationRequest:client-7075FE2157DD->f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E, cid=5, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf|rpc:scm3:9894|priority:0|startupRole:FOLLOWER], listeners:[]
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
scm1_1      | 2023-01-12 05:25:48,682 [IPC Server handler 2 on default port 9863] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-7075FE2157DD->f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E, cid=5, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf|rpc:scm3:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1_1      | 2023-01-12 05:25:48,683 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1_1      | 2023-01-12 05:25:48,683 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1      | 2023-01-12 05:25:48,684 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1      | 2023-01-12 05:25:48,688 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1_1      | 2023-01-12 05:25:48,688 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1      | 2023-01-12 05:25:48,688 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
s3g_1       | Caused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
scm1_1      | 2023-01-12 05:25:48,688 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1      | 2023-01-12 05:25:48,688 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1_1      | 2023-01-12 05:25:48,696 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:410)
scm1_1      | 2023-01-12 05:25:48,706 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-GrpcLogAppender: send f98693f9-b2e2-4085-a3fc-32934ea8db14->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf#0-t2,notify:(t:1, i:0)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
scm1_1      | 2023-01-12 05:25:48,706 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
scm1_1      | 2023-01-12 05:25:51,111 [grpc-default-executor-2] INFO server.GrpcLogAppender: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-InstallSnapshotResponseHandler: received the first reply f98693f9-b2e2-4085-a3fc-32934ea8db14<-6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf#0:OK-t0,ALREADY_INSTALLED
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
scm1_1      | 2023-01-12 05:25:51,112 [grpc-default-executor-2] INFO server.GrpcLogAppender: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1_1      | 2023-01-12 05:25:51,112 [grpc-default-executor-2] INFO leader.FollowerInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: snapshotIndex: setUnconditionally 0 -> 0
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
scm1_1      | 2023-01-12 05:25:51,113 [grpc-default-executor-2] INFO leader.FollowerInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: matchIndex: setUnconditionally 0 -> 0
scm1_1      | 2023-01-12 05:25:51,113 [grpc-default-executor-2] INFO leader.FollowerInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: nextIndex: setUnconditionally 0 -> 1
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
scm1_1      | 2023-01-12 05:25:51,114 [grpc-default-executor-2] INFO leader.FollowerInfo: Follower f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf acknowledged installing snapshot
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
scm1_1      | 2023-01-12 05:25:51,115 [grpc-default-executor-2] INFO leader.FollowerInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: nextIndex: updateToMax old=1, new=1, updated? false
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
scm1_1      | 2023-01-12 05:25:51,482 [grpc-default-executor-0] INFO leader.FollowerInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: nextIndex: updateUnconditionally 15 -> 0
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
scm1_1      | 2023-01-12 05:25:51,549 [grpc-default-executor-0] INFO leader.FollowerInfo: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E->6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf: nextIndex: updateUnconditionally 15 -> 0
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm1_1      | 2023-01-12 05:25:51,617 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderStateImpl] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: set configuration 15: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf|rpc:scm3:9894|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
scm1_1      | 2023-01-12 05:25:51,666 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-LeaderStateImpl] INFO server.RaftServer$Division: f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E: set configuration 17: peers:[296a2a14-8256-49c4-bc3a-84aaa5fb66c6|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf|rpc:scm3:9894|priority:0|startupRole:FOLLOWER, f98693f9-b2e2-4085-a3fc-32934ea8db14|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1      | 2023-01-12 05:25:51,698 [IPC Server handler 2 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 6ed50f3f-dbc3-40bc-ac36-41c7a78fc7cf.
s3g_1       | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
scm1_1      | 2023-01-12 05:25:56,924 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: fb7937dd-8506-409c-a1e6-3cdc4fafc0d4, Nodes: cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:cefdd7f0-5c68-4866-b1db-ab1e8bc5045a, CreationTimestamp2023-01-12T05:25:22.284Z[UTC]] moved to OPEN state
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
scm1_1      | 2023-01-12 05:25:57,166 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
scm1_1      | 2023-01-12 05:25:57,336 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm1_1      | 2023-01-12 05:25:58,085 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: cd62190c-0472-48bc-9601-9e784e6bba4e, Nodes: 34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, CreationTimestamp2023-01-12T05:25:23.217Z[UTC]] moved to OPEN state
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
scm1_1      | 2023-01-12 05:25:58,163 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1      | 2023-01-12 05:25:58,380 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
scm1_1      | 2023-01-12 05:25:58,403 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3e9abc36-6d7f-4b91-a5f9-0ee7b13ecddc, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f23956f4-f232-4ae2-8f3e-cd8efc9f95bf, CreationTimestamp2023-01-12T05:25:23.423Z[UTC]] moved to OPEN state
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm1_1      | 2023-01-12 05:25:58,442 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1      | 2023-01-12 05:25:58,478 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1      | 2023-01-12 05:25:58,802 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
scm1_1      | 2023-01-12 05:25:59,188 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
scm1_1      | 2023-01-12 05:26:00,262 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
scm1_1      | 2023-01-12 05:26:03,351 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
scm1_1      | 2023-01-12 05:26:03,721 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
scm1_1      | 2023-01-12 05:26:04,927 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
scm1_1      | 2023-01-12 05:26:24,425 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7b2ccc1c-57aa-4786-a2ba-0da58dd390a6, Nodes: f23956f4-f232-4ae2-8f3e-cd8efc9f95bf(ozone-ha_datanode_1.ozone-ha_default/172.18.0.6)34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b(ozone-ha_datanode_2.ozone-ha_default/172.18.0.11)cefdd7f0-5c68-4866-b1db-ab1e8bc5045a(ozone-ha_datanode_3.ozone-ha_default/172.18.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:34efb4e4-4ba1-4b8b-9eea-b948a0e8a25b, CreationTimestamp2023-01-12T05:25:23.439Z[UTC]] moved to OPEN state
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
scm1_1      | 2023-01-12 05:26:24,477 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1_1      | 2023-01-12 05:26:24,483 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1_1      | 2023-01-12 05:26:24,496 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1_1      | 2023-01-12 05:26:24,496 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1_1      | 2023-01-12 05:26:24,496 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1_1      | 2023-01-12 05:26:24,496 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1_1      | 2023-01-12 05:26:24,496 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm1_1      | 2023-01-12 05:26:24,497 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm1_1      | 2023-01-12 05:26:24,497 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	... 17 more
s3g_1       | Caused by: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.getCustomMetadataFromHeaders(EndpointBase.java:291)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.put(ObjectEndpoint.java:214)
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1       | 	... 51 more
s3g_1       | 2023-01-12 05:38:00,070 [qtp384515747-22] WARN server.HttpChannel: /bucket-ozone-test-7720168204/ozone-test-0170484136/putobject/custom-metadata/key2
s3g_1       | javax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:162)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
scm1_1      | 2023-01-12 05:26:24,531 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm1_1      | 2023-01-12 05:27:23,454 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1_1      | 2023-01-12 05:28:05,836 [IPC Server handler 2 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm1_1      | 2023-01-12 05:28:05,959 [f98693f9-b2e2-4085-a3fc-32934ea8db14@group-4D43FF37764E-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm1_1      | 2023-01-12 05:28:05,996 [IPC Server handler 2 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm1_1      | 2023-01-12 05:28:46,653 [IPC Server handler 29 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
scm1_1      | 2023-01-12 05:29:23,456 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1_1      | 2023-01-12 05:30:14,487 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm1_1      | 2023-01-12 05:31:23,460 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1_1      | 2023-01-12 05:31:44,486 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:31:44,501 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:32:14,486 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:32:14,502 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:32:44,486 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:32:44,503 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:33:14,487 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:33:14,503 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:33:23,463 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1_1      | 2023-01-12 05:33:44,487 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:33:44,503 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:34:14,487 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:34:14,504 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:34:44,493 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:34:44,512 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:35:14,489 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 2 milliseconds for processing 3 containers.
scm1_1      | 2023-01-12 05:35:14,494 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:35:14,512 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:35:23,464 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1_1      | 2023-01-12 05:35:44,494 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:35:44,512 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:36:14,495 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:36:14,513 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:36:44,495 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:36:44,513 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:37:14,496 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:37:14,514 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:37:23,467 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1_1      | 2023-01-12 05:37:44,497 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:37:44,514 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:38:14,499 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:38:14,515 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:38:44,499 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:38:44,515 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:39:14,511 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1       | Caused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:410)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	... 17 more
s3g_1       | Caused by: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.getCustomMetadataFromHeaders(EndpointBase.java:291)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.put(ObjectEndpoint.java:214)
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1       | 	... 51 more
s3g_1       | 2023-01-12 05:38:00,072 [qtp384515747-22] WARN server.HttpChannelState: unhandled due to prior sendError
s3g_1       | javax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:162)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
scm1_1      | 2023-01-12 05:39:14,518 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:39:23,469 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm1_1      | 2023-01-12 05:39:44,513 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:39:44,518 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:40:14,490 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 3 containers.
scm1_1      | 2023-01-12 05:40:14,513 [Under Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
scm1_1      | 2023-01-12 05:40:14,519 [Over Replicated Processor] INFO replication.UnhealthyReplicationProcessor: Processed 0 containers with health state counts {},failed processing 0
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1       | Caused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:410)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	... 17 more
s3g_1       | Caused by: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.getCustomMetadataFromHeaders(EndpointBase.java:291)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.put(ObjectEndpoint.java:214)
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1       | 	... 51 more
s3g_1       | 2023-01-12 05:38:01,266 [qtp384515747-20] WARN server.HttpChannel: /bucket-ozone-test-7720168204/ozone-test-0170484136/putobject/custom-metadata/key2
s3g_1       | javax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:162)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1       | Caused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:410)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	... 17 more
s3g_1       | Caused by: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.getCustomMetadataFromHeaders(EndpointBase.java:291)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.put(ObjectEndpoint.java:214)
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1       | 	... 51 more
s3g_1       | 2023-01-12 05:38:01,266 [qtp384515747-20] WARN server.HttpChannelState: unhandled due to prior sendError
s3g_1       | javax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:162)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1       | Caused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:410)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	... 17 more
s3g_1       | Caused by: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.getCustomMetadataFromHeaders(EndpointBase.java:291)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.put(ObjectEndpoint.java:214)
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1       | 	... 51 more
s3g_1       | 2023-01-12 05:38:07,001 [qtp384515747-22] WARN server.HttpChannel: /bucket-ozone-test-7720168204/ozone-test-0170484136/putobject/custom-metadata/key2
s3g_1       | javax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:162)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1       | Caused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:410)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	... 17 more
s3g_1       | Caused by: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.getCustomMetadataFromHeaders(EndpointBase.java:291)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.put(ObjectEndpoint.java:214)
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1       | 	... 51 more
s3g_1       | 2023-01-12 05:38:07,002 [qtp384515747-22] WARN server.HttpChannelState: unhandled due to prior sendError
s3g_1       | javax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:162)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1       | Caused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:410)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	... 17 more
s3g_1       | Caused by: java.lang.IllegalArgumentException: Illegal user defined metadata. Combined size cannot exceed 2KB.
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.getCustomMetadataFromHeaders(EndpointBase.java:291)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.put(ObjectEndpoint.java:214)
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1       | 	... 51 more
s3g_1       | 2023-01-12 05:38:10,593 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-ozone-test-8035968912, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
