Attaching to ozone-legacy-bucket_datanode_3, ozone-legacy-bucket_datanode_4, ozone-legacy-bucket_datanode_1, ozone-legacy-bucket_datanode_2, ozone-legacy-bucket_datanode_5, ozone-legacy-bucket_scm_1, ozone-legacy-bucket_om_1, ozone-legacy-bucket_s3g_1, ozone-legacy-bucket_recon_1
datanode_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1  | 2023-01-03 10:09:46,010 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1  | /************************************************************
datanode_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_1  | STARTUP_MSG:   host = eafe2f3b9d53/172.18.0.8
datanode_1  | STARTUP_MSG:   args = []
datanode_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/1c830c5bc7b0be75001a5e8bcef69a8810ca74f1 ; compiled by 'runner' on 2023-01-03T09:55Z
datanode_1  | STARTUP_MSG:   java = 11.0.14.1
datanode_1  | ************************************************************/
datanode_1  | 2023-01-03 10:09:46,098 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1  | 2023-01-03 10:09:46,751 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1  | 2023-01-03 10:09:47,875 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1  | 2023-01-03 10:09:49,373 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1  | 2023-01-03 10:09:49,383 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_4  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_4  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_4  | 2023-01-03 10:09:46,295 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_4  | /************************************************************
datanode_4  | STARTUP_MSG: Starting HddsDatanodeService
datanode_4  | STARTUP_MSG:   host = 2706491cd5f6/172.18.0.10
datanode_4  | STARTUP_MSG:   args = []
datanode_4  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_4  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_4  | STARTUP_MSG:   build = https://github.com/apache/ozone/1c830c5bc7b0be75001a5e8bcef69a8810ca74f1 ; compiled by 'runner' on 2023-01-03T09:55Z
datanode_4  | STARTUP_MSG:   java = 11.0.14.1
datanode_4  | ************************************************************/
datanode_4  | 2023-01-03 10:09:46,384 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_4  | 2023-01-03 10:09:46,888 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_4  | 2023-01-03 10:09:47,939 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_4  | 2023-01-03 10:09:49,093 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_4  | 2023-01-03 10:09:49,100 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_4  | 2023-01-03 10:09:50,163 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:2706491cd5f6 ip:172.18.0.10
datanode_4  | 2023-01-03 10:09:53,676 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_4  | 2023-01-03 10:09:55,302 [main] INFO reflections.Reflections: Reflections took 1348 ms to scan 2 urls, producing 97 keys and 217 values 
datanode_4  | 2023-01-03 10:09:56,395 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_4  | 2023-01-03 10:09:58,721 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_4  | 2023-01-03 10:09:59,006 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_4  | 2023-01-03 10:09:59,116 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_4  | 2023-01-03 10:09:59,129 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_4  | 2023-01-03 10:09:59,593 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_4  | 2023-01-03 10:09:59,791 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4  | 2023-01-03 10:09:59,800 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_4  | 2023-01-03 10:09:59,843 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_4  | 2023-01-03 10:09:59,871 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_4  | 2023-01-03 10:09:59,875 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_4  | 2023-01-03 10:10:00,276 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_4  | 2023-01-03 10:10:00,295 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_4  | 2023-01-03 10:10:12,638 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_4  | 2023-01-03 10:10:14,509 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4  | 2023-01-03 10:10:15,083 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_4  | 2023-01-03 10:10:15,625 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_4  | 2023-01-03 10:10:15,648 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_4  | 2023-01-03 10:10:15,649 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_4  | 2023-01-03 10:10:15,650 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_4  | 2023-01-03 10:10:15,651 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_4  | 2023-01-03 10:10:15,654 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_4  | 2023-01-03 10:10:15,655 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_4  | 2023-01-03 10:10:15,664 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4  | 2023-01-03 10:10:15,667 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_4  | 2023-01-03 10:10:15,683 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4  | 2023-01-03 10:10:15,839 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_4  | 2023-01-03 10:10:15,852 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_4  | 2023-01-03 10:10:15,884 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_4  | 2023-01-03 10:10:19,208 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode_4  | 2023-01-03 10:10:19,542 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode_4  | 2023-01-03 10:10:19,606 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode_4  | 2023-01-03 10:10:19,611 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode_4  | 2023-01-03 10:10:19,616 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode_4  | 2023-01-03 10:10:19,636 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode_4  | 2023-01-03 10:10:19,636 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode_4  | 2023-01-03 10:10:19,670 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode_4  | 2023-01-03 10:10:19,741 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
datanode_4  | 2023-01-03 10:10:19,756 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode_4  | 2023-01-03 10:10:19,783 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode_4  | 2023-01-03 10:10:20,446 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_4  | 2023-01-03 10:10:20,464 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_4  | 2023-01-03 10:10:20,489 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4  | 2023-01-03 10:10:20,490 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4  | 2023-01-03 10:10:20,505 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4  | 2023-01-03 10:10:20,935 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x7bf5e74d] REGISTERED
datanode_4  | 2023-01-03 10:10:20,987 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x7bf5e74d] BIND: 0.0.0.0/0.0.0.0:0
datanode_4  | 2023-01-03 10:10:21,012 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x7bf5e74d, L:/0.0.0.0:42087] ACTIVE
datanode_4  | 2023-01-03 10:10:21,267 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_4  | 2023-01-03 10:10:23,499 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_4  | 2023-01-03 10:10:23,609 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_4  | 2023-01-03 10:10:23,854 [main] INFO util.log: Logging initialized @52749ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_4  | 2023-01-03 10:10:24,898 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_4  | 2023-01-03 10:10:24,981 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_4  | 2023-01-03 10:10:25,070 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_4  | 2023-01-03 10:10:25,101 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_4  | 2023-01-03 10:10:25,105 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_4  | 2023-01-03 10:10:25,114 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_4  | 2023-01-03 10:10:25,525 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_4  | 2023-01-03 10:10:25,527 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_4  | 2023-01-03 10:10:26,016 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_4  | 2023-01-03 10:10:26,016 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_4  | 2023-01-03 10:10:26,062 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_4  | 2023-01-03 10:10:26,170 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5e557671{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_4  | 2023-01-03 10:10:26,192 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7962c1d5{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_4  | 2023-01-03 10:10:27,206 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@34588991{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-2271686290530003824/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_4  | 2023-01-03 10:10:27,277 [main] INFO server.AbstractConnector: Started ServerConnector@2dd46693{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_4  | 2023-01-03 10:10:27,287 [main] INFO server.Server: Started @56182ms
datanode_4  | 2023-01-03 10:10:27,319 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_4  | 2023-01-03 10:10:27,319 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_4  | 2023-01-03 10:10:27,342 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_4  | 2023-01-03 10:10:27,351 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_4  | 2023-01-03 10:10:27,458 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@504c9340] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_4  | 2023-01-03 10:10:28,703 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.18.0.3:9891
datanode_4  | 2023-01-03 10:10:29,543 [Datanode State Machine Daemon Thread] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.
datanode_4  | java.util.concurrent.TimeoutException
datanode_4  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.await(InitDatanodeState.java:178)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.await(InitDatanodeState.java:48)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:309)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:509)
datanode_4  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4  | 2023-01-03 10:10:29,544 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_4  | 2023-01-03 10:10:29,571 [Datanode State Machine Task Thread - 1] WARN statemachine.SCMConnectionManager: Trying to add an existing SCM Machine to Machines group. Ignoring the request.
datanode_4  | 2023-01-03 10:10:29,621 [Datanode State Machine Task Thread - 1] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.18.0.3:9891
datanode_4  | 2023-01-03 10:10:29,623 [Datanode State Machine Task Thread - 1] WARN statemachine.SCMConnectionManager: Trying to add an existing SCM Machine to Machines group. Ignoring the request.
datanode_3  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3  | 2023-01-03 10:09:46,445 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3  | /************************************************************
datanode_3  | STARTUP_MSG: Starting HddsDatanodeService
datanode_3  | STARTUP_MSG:   host = ad1ff48b9685/172.18.0.9
datanode_3  | STARTUP_MSG:   args = []
datanode_3  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_3  | STARTUP_MSG:   build = https://github.com/apache/ozone/1c830c5bc7b0be75001a5e8bcef69a8810ca74f1 ; compiled by 'runner' on 2023-01-03T09:55Z
datanode_3  | STARTUP_MSG:   java = 11.0.14.1
datanode_3  | ************************************************************/
datanode_3  | 2023-01-03 10:09:46,489 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | 2023-01-03 10:09:47,028 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3  | 2023-01-03 10:09:47,930 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3  | 2023-01-03 10:09:49,297 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3  | 2023-01-03 10:09:49,298 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3  | 2023-01-03 10:09:50,428 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:ad1ff48b9685 ip:172.18.0.9
datanode_3  | 2023-01-03 10:09:54,140 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_3  | 2023-01-03 10:09:55,725 [main] INFO reflections.Reflections: Reflections took 1253 ms to scan 2 urls, producing 97 keys and 217 values 
datanode_3  | 2023-01-03 10:09:56,839 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3  | 2023-01-03 10:09:59,106 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3  | 2023-01-03 10:09:59,356 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3  | 2023-01-03 10:09:59,482 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3  | 2023-01-03 10:09:59,521 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3  | 2023-01-03 10:10:00,066 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2023-01-03 10:10:00,241 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3  | 2023-01-03 10:10:00,249 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3  | 2023-01-03 10:10:00,279 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3  | 2023-01-03 10:10:00,287 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3  | 2023-01-03 10:10:00,288 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3  | 2023-01-03 10:10:00,744 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3  | 2023-01-03 10:10:00,747 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3  | 2023-01-03 10:10:11,820 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3  | 2023-01-03 10:10:13,486 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3  | 2023-01-03 10:10:14,335 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3  | 2023-01-03 10:10:15,480 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3  | 2023-01-03 10:10:15,513 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3  | 2023-01-03 10:10:15,513 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3  | 2023-01-03 10:10:15,539 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3  | 2023-01-03 10:10:15,540 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3  | 2023-01-03 10:10:15,549 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3  | 2023-01-03 10:10:15,550 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3  | 2023-01-03 10:10:15,610 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2023-01-03 10:10:15,611 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3  | 2023-01-03 10:10:15,612 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3  | 2023-01-03 10:10:15,735 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3  | 2023-01-03 10:10:15,756 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3  | 2023-01-03 10:10:15,757 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3  | 2023-01-03 10:10:18,000 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode_3  | 2023-01-03 10:10:18,417 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode_3  | 2023-01-03 10:10:18,435 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode_3  | 2023-01-03 10:10:18,451 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode_3  | 2023-01-03 10:10:18,491 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode_3  | 2023-01-03 10:10:18,547 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode_3  | 2023-01-03 10:10:18,551 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode_3  | 2023-01-03 10:10:18,626 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode_3  | 2023-01-03 10:10:18,715 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
datanode_3  | 2023-01-03 10:10:18,721 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode_3  | 2023-01-03 10:10:18,740 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode_3  | 2023-01-03 10:10:19,357 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3  | 2023-01-03 10:10:19,416 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3  | 2023-01-03 10:10:19,423 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3  | 2023-01-03 10:10:19,444 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3  | 2023-01-03 10:10:19,511 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2  | 2023-01-03 10:09:46,309 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2  | /************************************************************
datanode_2  | STARTUP_MSG: Starting HddsDatanodeService
datanode_2  | STARTUP_MSG:   host = 88faf1c2abe6/172.18.0.7
datanode_2  | STARTUP_MSG:   args = []
datanode_2  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1  | 2023-01-03 10:09:50,552 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:eafe2f3b9d53 ip:172.18.0.8
datanode_1  | 2023-01-03 10:09:53,833 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_1  | 2023-01-03 10:09:55,379 [main] INFO reflections.Reflections: Reflections took 1193 ms to scan 2 urls, producing 97 keys and 217 values 
datanode_1  | 2023-01-03 10:09:56,387 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1  | 2023-01-03 10:09:58,273 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1  | 2023-01-03 10:09:58,868 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1  | 2023-01-03 10:09:58,882 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1  | 2023-01-03 10:09:58,981 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1  | 2023-01-03 10:09:59,435 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2023-01-03 10:09:59,629 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1  | 2023-01-03 10:09:59,641 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1  | 2023-01-03 10:09:59,678 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1  | 2023-01-03 10:09:59,680 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1  | 2023-01-03 10:09:59,682 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1  | 2023-01-03 10:10:00,092 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1  | 2023-01-03 10:10:00,107 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1  | 2023-01-03 10:10:11,983 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1  | 2023-01-03 10:10:13,940 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1  | 2023-01-03 10:10:14,715 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1  | 2023-01-03 10:10:15,646 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1  | 2023-01-03 10:10:15,690 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1  | 2023-01-03 10:10:15,693 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1  | 2023-01-03 10:10:15,695 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1  | 2023-01-03 10:10:15,696 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1  | 2023-01-03 10:10:15,697 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1  | 2023-01-03 10:10:15,698 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1  | 2023-01-03 10:10:15,700 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2023-01-03 10:10:15,702 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1  | 2023-01-03 10:10:15,708 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1  | 2023-01-03 10:10:15,884 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1  | 2023-01-03 10:10:15,917 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1  | 2023-01-03 10:10:15,942 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1  | 2023-01-03 10:10:19,251 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode_1  | 2023-01-03 10:10:19,485 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode_1  | 2023-01-03 10:10:19,495 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode_1  | 2023-01-03 10:10:19,503 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode_1  | 2023-01-03 10:10:19,526 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode_1  | 2023-01-03 10:10:19,530 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode_1  | 2023-01-03 10:10:19,597 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode_1  | 2023-01-03 10:10:19,665 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode_1  | 2023-01-03 10:10:19,696 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
datanode_4  | 2023-01-03 10:10:29,644 [Datanode State Machine Task Thread - 1] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_4  | 2023-01-03 10:10:33,089 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:33,091 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:34,091 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:34,092 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:35,092 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:35,093 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:36,094 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:37,095 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:38,096 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:39,098 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:40,099 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:40,133 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_4  | java.net.SocketTimeoutException: Call From 2706491cd5f6/172.18.0.10 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.10:45574 remote=recon/172.18.0.3:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_4  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_4  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_4  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_4  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_4  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_4  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_4  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_4  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_4  | 	at com.sun.proxy.$Proxy43.submitRequest(Unknown Source)
datanode_4  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_4  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.10:45574 remote=recon/172.18.0.3:9891]
datanode_4  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_4  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_4  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_4  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_4  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_4  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_4  | 2023-01-03 10:10:41,100 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:42,101 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:43,102 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:44,103 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 2023-01-03 10:10:49,113 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_4  | java.net.SocketTimeoutException: Call From 2706491cd5f6/172.18.0.10 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.10:39804 remote=scm/172.18.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_4  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_4  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_4  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_4  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_2  | STARTUP_MSG:   build = https://github.com/apache/ozone/1c830c5bc7b0be75001a5e8bcef69a8810ca74f1 ; compiled by 'runner' on 2023-01-03T09:55Z
datanode_2  | STARTUP_MSG:   java = 11.0.14.1
datanode_2  | ************************************************************/
datanode_2  | 2023-01-03 10:09:46,407 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2  | 2023-01-03 10:09:46,997 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2  | 2023-01-03 10:09:47,954 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2  | 2023-01-03 10:09:49,207 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2  | 2023-01-03 10:09:49,207 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2  | 2023-01-03 10:09:50,160 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:88faf1c2abe6 ip:172.18.0.7
datanode_2  | 2023-01-03 10:09:53,806 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_2  | 2023-01-03 10:09:55,618 [main] INFO reflections.Reflections: Reflections took 1499 ms to scan 2 urls, producing 97 keys and 217 values 
datanode_2  | 2023-01-03 10:09:56,557 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2  | 2023-01-03 10:09:58,573 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2  | 2023-01-03 10:09:58,900 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2  | 2023-01-03 10:09:58,973 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2  | 2023-01-03 10:09:58,984 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2  | 2023-01-03 10:09:59,605 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2023-01-03 10:09:59,772 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2  | 2023-01-03 10:09:59,807 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2  | 2023-01-03 10:09:59,823 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2  | 2023-01-03 10:09:59,834 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2  | 2023-01-03 10:09:59,835 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2  | 2023-01-03 10:10:00,141 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2  | 2023-01-03 10:10:00,142 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2  | 2023-01-03 10:10:13,648 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2  | 2023-01-03 10:10:14,862 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2  | 2023-01-03 10:10:15,904 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2  | 2023-01-03 10:10:17,008 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2  | 2023-01-03 10:10:17,041 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2  | 2023-01-03 10:10:17,060 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2  | 2023-01-03 10:10:17,068 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2  | 2023-01-03 10:10:17,068 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2  | 2023-01-03 10:10:17,086 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2  | 2023-01-03 10:10:17,091 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2  | 2023-01-03 10:10:17,112 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-03 10:10:17,211 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2  | 2023-01-03 10:10:17,215 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2023-01-03 10:10:17,335 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2  | 2023-01-03 10:10:17,377 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2  | 2023-01-03 10:10:17,409 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2  | 2023-01-03 10:10:20,102 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode_2  | 2023-01-03 10:10:20,684 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode_2  | 2023-01-03 10:10:20,691 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode_2  | 2023-01-03 10:10:20,699 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode_2  | 2023-01-03 10:10:20,716 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode_2  | 2023-01-03 10:10:21,025 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode_2  | 2023-01-03 10:10:21,026 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode_2  | 2023-01-03 10:10:21,163 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode_2  | 2023-01-03 10:10:21,217 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
datanode_2  | 2023-01-03 10:10:21,233 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode_2  | 2023-01-03 10:10:21,234 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode_2  | 2023-01-03 10:10:22,140 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2  | 2023-01-03 10:10:22,160 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2  | 2023-01-03 10:10:22,211 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2  | 2023-01-03 10:10:22,211 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2  | 2023-01-03 10:10:22,301 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2023-01-03 10:10:22,484 [482b5314-1cac-4df1-beda-20c463470dc2-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xbc40a36c] REGISTERED
datanode_2  | 2023-01-03 10:10:22,527 [482b5314-1cac-4df1-beda-20c463470dc2-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xbc40a36c] BIND: 0.0.0.0/0.0.0.0:0
datanode_2  | 2023-01-03 10:10:22,532 [482b5314-1cac-4df1-beda-20c463470dc2-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0xbc40a36c, L:/0.0.0.0:41573] ACTIVE
datanode_2  | 2023-01-03 10:10:22,848 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2  | 2023-01-03 10:10:24,898 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2  | 2023-01-03 10:10:25,026 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2  | 2023-01-03 10:10:25,378 [main] INFO util.log: Logging initialized @54254ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2  | 2023-01-03 10:10:26,510 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2  | 2023-01-03 10:10:26,594 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2  | 2023-01-03 10:10:26,614 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2  | 2023-01-03 10:10:26,641 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2  | 2023-01-03 10:10:26,649 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2  | 2023-01-03 10:10:26,651 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2  | 2023-01-03 10:10:26,969 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2  | 2023-01-03 10:10:26,996 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_2  | 2023-01-03 10:10:27,438 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2  | 2023-01-03 10:10:27,458 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2  | 2023-01-03 10:10:27,465 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2  | 2023-01-03 10:10:27,655 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4f1f2f84{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2  | 2023-01-03 10:10:27,680 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@646c872a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2  | 2023-01-03 10:10:28,781 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@38b7004d{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-3655930191975330965/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2  | 2023-01-03 10:10:28,900 [main] INFO server.AbstractConnector: Started ServerConnector@eb1e868{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2  | 2023-01-03 10:10:28,900 [main] INFO server.Server: Started @57777ms
datanode_2  | 2023-01-03 10:10:28,936 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2  | 2023-01-03 10:10:28,936 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3  | 2023-01-03 10:10:19,572 [3719a142-1617-4d0e-a6d8-f47fd68adc81-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x3e3c70e7] REGISTERED
datanode_3  | 2023-01-03 10:10:19,628 [3719a142-1617-4d0e-a6d8-f47fd68adc81-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x3e3c70e7] BIND: 0.0.0.0/0.0.0.0:0
datanode_3  | 2023-01-03 10:10:19,707 [3719a142-1617-4d0e-a6d8-f47fd68adc81-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x3e3c70e7, L:/0.0.0.0:43547] ACTIVE
datanode_3  | 2023-01-03 10:10:20,179 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3  | 2023-01-03 10:10:22,451 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3  | 2023-01-03 10:10:22,570 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3  | 2023-01-03 10:10:22,830 [main] INFO util.log: Logging initialized @51570ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3  | 2023-01-03 10:10:23,801 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3  | 2023-01-03 10:10:23,887 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3  | 2023-01-03 10:10:23,957 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3  | 2023-01-03 10:10:23,981 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3  | 2023-01-03 10:10:23,981 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3  | 2023-01-03 10:10:23,987 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3  | 2023-01-03 10:10:24,276 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3  | 2023-01-03 10:10:24,280 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_3  | 2023-01-03 10:10:24,520 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3  | 2023-01-03 10:10:24,520 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3  | 2023-01-03 10:10:24,566 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3  | 2023-01-03 10:10:24,733 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@747f0f34{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3  | 2023-01-03 10:10:24,739 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3472f3ab{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3  | 2023-01-03 10:10:25,929 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1771d6a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-16850131587693597811/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3  | 2023-01-03 10:10:26,021 [main] INFO server.AbstractConnector: Started ServerConnector@39451d92{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3  | 2023-01-03 10:10:26,021 [main] INFO server.Server: Started @54762ms
datanode_3  | 2023-01-03 10:10:26,034 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3  | 2023-01-03 10:10:26,034 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3  | 2023-01-03 10:10:26,102 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3  | 2023-01-03 10:10:26,152 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3  | 2023-01-03 10:10:26,413 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3066fd06] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3  | 2023-01-03 10:10:27,465 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.18.0.3:9891
datanode_3  | 2023-01-03 10:10:28,025 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3  | 2023-01-03 10:10:30,689 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:30,731 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:31,690 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:31,732 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:32,691 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:32,733 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:33,692 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:33,734 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:34,693 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:34,734 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:35,694 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:35,736 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:28,957 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2  | 2023-01-03 10:10:29,000 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2  | 2023-01-03 10:10:29,402 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@504c9340] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2  | 2023-01-03 10:10:30,347 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.18.0.3:9891
datanode_2  | 2023-01-03 10:10:30,694 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2  | 2023-01-03 10:10:32,955 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:32,956 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:33,956 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:33,957 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:34,957 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:34,959 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:35,959 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:36,961 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:37,962 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:38,963 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:39,964 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_4  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_4  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_4  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_4  | 	at com.sun.proxy.$Proxy42.submitRequest(Unknown Source)
datanode_4  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_4  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.10:39804 remote=scm/172.18.0.5:9861]
datanode_4  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_4  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_4  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_4  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_4  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_4  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_4  | 2023-01-03 10:10:50,900 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-14143503-c595-4722-b95c-9dd877fc0e68/DS-226d0965-84b8-4130-9086-d1b6d3b5f7ef/container.db to cache
datanode_4  | 2023-01-03 10:10:50,905 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-14143503-c595-4722-b95c-9dd877fc0e68/DS-226d0965-84b8-4130-9086-d1b6d3b5f7ef/container.db for volume DS-226d0965-84b8-4130-9086-d1b6d3b5f7ef
datanode_4  | 2023-01-03 10:10:50,910 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_4  | 2023-01-03 10:10:50,930 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_4  | 2023-01-03 10:10:51,059 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_4  | 2023-01-03 10:10:51,074 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis c950fbed-bbb7-451f-b5e1-adf39d3a6a32
datanode_4  | 2023-01-03 10:10:51,215 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.RaftServer: c950fbed-bbb7-451f-b5e1-adf39d3a6a32: start RPC server
datanode_4  | 2023-01-03 10:10:51,224 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: c950fbed-bbb7-451f-b5e1-adf39d3a6a32: GrpcService started, listening on 9858
datanode_4  | 2023-01-03 10:10:51,234 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: c950fbed-bbb7-451f-b5e1-adf39d3a6a32: GrpcService started, listening on 9856
datanode_4  | 2023-01-03 10:10:51,239 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: c950fbed-bbb7-451f-b5e1-adf39d3a6a32: GrpcService started, listening on 9857
datanode_4  | 2023-01-03 10:10:51,312 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c950fbed-bbb7-451f-b5e1-adf39d3a6a32 is started using port 9858 for RATIS
datanode_4  | 2023-01-03 10:10:51,313 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c950fbed-bbb7-451f-b5e1-adf39d3a6a32 is started using port 9857 for RATIS_ADMIN
datanode_4  | 2023-01-03 10:10:51,313 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c950fbed-bbb7-451f-b5e1-adf39d3a6a32 is started using port 9856 for RATIS_SERVER
datanode_4  | 2023-01-03 10:10:51,315 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-c950fbed-bbb7-451f-b5e1-adf39d3a6a32: Started
datanode_4  | 2023-01-03 10:10:55,745 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_4  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_4  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_4  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:309)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:509)
datanode_4  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4  | Caused by: java.util.concurrent.TimeoutException
datanode_4  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_4  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_4  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_4  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4  | 	... 1 more
datanode_4  | 2023-01-03 10:11:00,398 [Command processor thread] INFO server.RaftServer: c950fbed-bbb7-451f-b5e1-adf39d3a6a32: addNew group-495FB9ADC940:[c950fbed-bbb7-451f-b5e1-adf39d3a6a32|rpc:172.18.0.10:9856|admin:172.18.0.10:9857|client:172.18.0.10:9858|dataStream:172.18.0.10:9858|priority:1|startupRole:FOLLOWER] returns group-495FB9ADC940:java.util.concurrent.CompletableFuture@94ab5f2[Not completed]
datanode_4  | 2023-01-03 10:11:00,549 [pool-22-thread-1] INFO server.RaftServer$Division: c950fbed-bbb7-451f-b5e1-adf39d3a6a32: new RaftServerImpl for group-495FB9ADC940:[c950fbed-bbb7-451f-b5e1-adf39d3a6a32|rpc:172.18.0.10:9856|admin:172.18.0.10:9857|client:172.18.0.10:9858|dataStream:172.18.0.10:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4  | 2023-01-03 10:11:00,553 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4  | 2023-01-03 10:11:00,560 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4  | 2023-01-03 10:11:00,562 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4  | 2023-01-03 10:11:00,575 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4  | 2023-01-03 10:11:00,576 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4  | 2023-01-03 10:11:00,577 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4  | 2023-01-03 10:11:00,642 [pool-22-thread-1] INFO server.RaftServer$Division: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940: ConfigurationManager, init=-1: peers:[c950fbed-bbb7-451f-b5e1-adf39d3a6a32|rpc:172.18.0.10:9856|admin:172.18.0.10:9857|client:172.18.0.10:9858|dataStream:172.18.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4  | 2023-01-03 10:11:00,657 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4  | 2023-01-03 10:11:00,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4  | 2023-01-03 10:11:00,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4  | 2023-01-03 10:11:00,791 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4  | 2023-01-03 10:11:00,851 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4  | 2023-01-03 10:11:00,857 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4  | 2023-01-03 10:11:01,206 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4  | 2023-01-03 10:11:01,206 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1  | 2023-01-03 10:10:19,719 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode_1  | 2023-01-03 10:10:19,735 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode_1  | 2023-01-03 10:10:20,318 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1  | 2023-01-03 10:10:20,319 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1  | 2023-01-03 10:10:20,319 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1  | 2023-01-03 10:10:20,344 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1  | 2023-01-03 10:10:20,490 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2023-01-03 10:10:20,633 [74492474-3f40-4050-829e-f08eb3e4f292-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x776b18a9] REGISTERED
datanode_1  | 2023-01-03 10:10:20,667 [74492474-3f40-4050-829e-f08eb3e4f292-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x776b18a9] BIND: 0.0.0.0/0.0.0.0:0
datanode_1  | 2023-01-03 10:10:20,688 [74492474-3f40-4050-829e-f08eb3e4f292-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x776b18a9, L:/0.0.0.0:35189] ACTIVE
datanode_1  | 2023-01-03 10:10:20,871 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1  | 2023-01-03 10:10:22,758 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1  | 2023-01-03 10:10:22,882 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1  | 2023-01-03 10:10:23,154 [main] INFO util.log: Logging initialized @51983ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1  | 2023-01-03 10:10:24,086 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1  | 2023-01-03 10:10:24,154 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1  | 2023-01-03 10:10:24,316 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1  | 2023-01-03 10:10:24,318 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1  | 2023-01-03 10:10:24,336 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1  | 2023-01-03 10:10:24,343 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1  | 2023-01-03 10:10:24,776 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1  | 2023-01-03 10:10:24,811 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_1  | 2023-01-03 10:10:25,114 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1  | 2023-01-03 10:10:25,114 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1  | 2023-01-03 10:10:25,145 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1  | 2023-01-03 10:10:25,424 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5e557671{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1  | 2023-01-03 10:10:25,434 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7962c1d5{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1  | 2023-01-03 10:10:26,650 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@34588991{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-7229623161601936612/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1  | 2023-01-03 10:10:26,776 [main] INFO server.AbstractConnector: Started ServerConnector@2dd46693{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1  | 2023-01-03 10:10:26,776 [main] INFO server.Server: Started @55605ms
datanode_1  | 2023-01-03 10:10:26,836 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1  | 2023-01-03 10:10:26,836 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1  | 2023-01-03 10:10:26,852 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1  | 2023-01-03 10:10:26,874 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1  | 2023-01-03 10:10:27,304 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@a4a3d02] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1  | 2023-01-03 10:10:28,393 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.18.0.3:9891
datanode_1  | 2023-01-03 10:10:29,047 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1  | 2023-01-03 10:10:31,230 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:31,249 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:32,231 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:32,253 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:33,232 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:33,253 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:34,233 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:34,254 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:35,234 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:35,261 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:36,262 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:37,265 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:40,035 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2  | java.net.SocketTimeoutException: Call From 88faf1c2abe6/172.18.0.7 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.7:44682 remote=recon/172.18.0.3:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2  | 	at com.sun.proxy.$Proxy43.submitRequest(Unknown Source)
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.7:44682 remote=recon/172.18.0.3:9891]
datanode_2  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2  | 2023-01-03 10:10:40,966 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:41,967 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:42,968 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:43,969 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:10:48,984 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2  | java.net.SocketTimeoutException: Call From 88faf1c2abe6/172.18.0.7 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.7:52126 remote=scm/172.18.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2  | 	at com.sun.proxy.$Proxy42.submitRequest(Unknown Source)
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.7:52126 remote=scm/172.18.0.5:9861]
datanode_2  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2  | 2023-01-03 10:10:49,519 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:309)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:509)
datanode_1  | 2023-01-03 10:10:38,266 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:39,267 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:40,268 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:40,288 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1  | java.net.SocketTimeoutException: Call From eafe2f3b9d53/172.18.0.8 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.8:38702 remote=recon/172.18.0.3:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1  | 	at com.sun.proxy.$Proxy43.submitRequest(Unknown Source)
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.8:38702 remote=recon/172.18.0.3:9891]
datanode_1  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1  | 2023-01-03 10:10:41,270 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:42,276 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:43,277 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:44,278 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1  | 2023-01-03 10:10:49,293 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1  | java.net.SocketTimeoutException: Call From eafe2f3b9d53/172.18.0.8 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.8:58594 remote=scm/172.18.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1  | 	at com.sun.proxy.$Proxy42.submitRequest(Unknown Source)
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3  | 2023-01-03 10:10:36,737 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:37,738 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:38,739 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:39,741 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:40,742 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:40,768 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3  | java.net.SocketTimeoutException: Call From ad1ff48b9685/172.18.0.9 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.9:32866 remote=recon/172.18.0.3:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3  | 	at com.sun.proxy.$Proxy43.submitRequest(Unknown Source)
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.9:32866 remote=recon/172.18.0.3:9891]
datanode_3  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_3  | 2023-01-03 10:10:41,743 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:42,745 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:43,746 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3  | 2023-01-03 10:10:48,760 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3  | java.net.SocketTimeoutException: Call From ad1ff48b9685/172.18.0.9 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.9:39234 remote=scm/172.18.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3  | 	at com.sun.proxy.$Proxy42.submitRequest(Unknown Source)
datanode_4  | 2023-01-03 10:11:01,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_4  | 2023-01-03 10:11:01,213 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4  | 2023-01-03 10:11:01,218 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4  | 2023-01-03 10:11:01,222 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d15fb565-ac34-40f7-b405-495fb9adc940 does not exist. Creating ...
datanode_4  | 2023-01-03 10:11:01,274 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d15fb565-ac34-40f7-b405-495fb9adc940/in_use.lock acquired by nodename 7@2706491cd5f6
datanode_4  | 2023-01-03 10:11:01,346 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d15fb565-ac34-40f7-b405-495fb9adc940 has been successfully formatted.
datanode_4  | 2023-01-03 10:11:01,496 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-495FB9ADC940: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4  | 2023-01-03 10:11:01,517 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4  | 2023-01-03 10:11:01,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4  | 2023-01-03 10:11:01,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4  | 2023-01-03 10:11:01,665 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4  | 2023-01-03 10:11:01,680 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4  | 2023-01-03 10:11:01,701 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4  | 2023-01-03 10:11:01,821 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4  | 2023-01-03 10:11:01,825 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4  | 2023-01-03 10:11:01,895 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d15fb565-ac34-40f7-b405-495fb9adc940
datanode_4  | 2023-01-03 10:11:01,897 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4  | 2023-01-03 10:11:01,905 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4  | 2023-01-03 10:11:01,920 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4  | 2023-01-03 10:11:01,927 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4  | 2023-01-03 10:11:01,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4  | 2023-01-03 10:11:01,964 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4  | 2023-01-03 10:11:01,966 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4  | 2023-01-03 10:11:01,995 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4  | 2023-01-03 10:11:02,299 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4  | 2023-01-03 10:11:02,307 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4  | 2023-01-03 10:11:02,564 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_4  | 2023-01-03 10:11:02,612 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4  | 2023-01-03 10:11:02,619 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4  | 2023-01-03 10:11:02,732 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4  | 2023-01-03 10:11:02,732 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_4  | 2023-01-03 10:11:02,760 [pool-22-thread-1] INFO server.RaftServer$Division: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940: start as a follower, conf=-1: peers:[c950fbed-bbb7-451f-b5e1-adf39d3a6a32|rpc:172.18.0.10:9856|admin:172.18.0.10:9857|client:172.18.0.10:9858|dataStream:172.18.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4  | 2023-01-03 10:11:02,760 [pool-22-thread-1] INFO server.RaftServer$Division: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4  | 2023-01-03 10:11:02,778 [pool-22-thread-1] INFO impl.RoleInfo: c950fbed-bbb7-451f-b5e1-adf39d3a6a32: start c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-FollowerState
datanode_4  | 2023-01-03 10:11:02,791 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4  | 2023-01-03 10:11:02,811 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4  | 2023-01-03 10:11:02,827 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-495FB9ADC940,id=c950fbed-bbb7-451f-b5e1-adf39d3a6a32
datanode_4  | 2023-01-03 10:11:02,868 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4  | 2023-01-03 10:11:02,876 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4  | 2023-01-03 10:11:02,881 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4  | 2023-01-03 10:11:02,891 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4  | 2023-01-03 10:11:03,247 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d15fb565-ac34-40f7-b405-495fb9adc940
datanode_4  | 2023-01-03 10:11:03,262 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=d15fb565-ac34-40f7-b405-495fb9adc940.
datanode_4  | 2023-01-03 10:11:07,849 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-FollowerState] INFO impl.FollowerState: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5077403205ns, electionTimeout:5036ms
datanode_4  | 2023-01-03 10:11:07,850 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-FollowerState] INFO impl.RoleInfo: c950fbed-bbb7-451f-b5e1-adf39d3a6a32: shutdown c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-FollowerState
datanode_4  | 2023-01-03 10:11:07,851 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-FollowerState] INFO server.RaftServer$Division: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_4  | 2023-01-03 10:11:07,857 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_4  | 2023-01-03 10:11:07,857 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-FollowerState] INFO impl.RoleInfo: c950fbed-bbb7-451f-b5e1-adf39d3a6a32: start c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1
datanode_4  | 2023-01-03 10:11:07,895 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO impl.LeaderElection: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[c950fbed-bbb7-451f-b5e1-adf39d3a6a32|rpc:172.18.0.10:9856|admin:172.18.0.10:9857|client:172.18.0.10:9858|dataStream:172.18.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4  | 2023-01-03 10:11:07,896 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO impl.LeaderElection: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_4  | 2023-01-03 10:11:07,897 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO impl.RoleInfo: c950fbed-bbb7-451f-b5e1-adf39d3a6a32: shutdown c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1
datanode_4  | 2023-01-03 10:11:07,908 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO server.RaftServer$Division: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_4  | 2023-01-03 10:11:07,909 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-495FB9ADC940 with new leaderId: c950fbed-bbb7-451f-b5e1-adf39d3a6a32
datanode_4  | 2023-01-03 10:11:07,919 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO server.RaftServer$Division: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940: change Leader from null to c950fbed-bbb7-451f-b5e1-adf39d3a6a32 at term 1 for becomeLeader, leader elected after 7118ms
datanode_4  | 2023-01-03 10:11:08,034 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_4  | 2023-01-03 10:11:08,095 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4  | 2023-01-03 10:11:08,105 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_4  | 2023-01-03 10:11:08,151 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_4  | 2023-01-03 10:11:08,156 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_4  | 2023-01-03 10:11:08,196 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_4  | 2023-01-03 10:11:08,280 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4  | 2023-01-03 10:11:08,307 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_4  | 2023-01-03 10:11:08,359 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO impl.RoleInfo: c950fbed-bbb7-451f-b5e1-adf39d3a6a32: start c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderStateImpl
datanode_4  | 2023-01-03 10:11:08,508 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.8:58594 remote=scm/172.18.0.5:9861]
datanode_1  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1  | 2023-01-03 10:10:50,964 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-14143503-c595-4722-b95c-9dd877fc0e68/DS-7aab7e7d-112a-4c95-94e5-059375edbd28/container.db to cache
datanode_1  | 2023-01-03 10:10:50,965 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-14143503-c595-4722-b95c-9dd877fc0e68/DS-7aab7e7d-112a-4c95-94e5-059375edbd28/container.db for volume DS-7aab7e7d-112a-4c95-94e5-059375edbd28
datanode_1  | 2023-01-03 10:10:50,966 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1  | 2023-01-03 10:10:51,021 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_1  | 2023-01-03 10:10:51,128 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_1  | 2023-01-03 10:10:51,150 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 74492474-3f40-4050-829e-f08eb3e4f292
datanode_1  | 2023-01-03 10:10:51,231 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.RaftServer: 74492474-3f40-4050-829e-f08eb3e4f292: start RPC server
datanode_1  | 2023-01-03 10:10:51,236 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: 74492474-3f40-4050-829e-f08eb3e4f292: GrpcService started, listening on 9858
datanode_1  | 2023-01-03 10:10:51,252 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: 74492474-3f40-4050-829e-f08eb3e4f292: GrpcService started, listening on 9856
datanode_1  | 2023-01-03 10:10:51,257 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: 74492474-3f40-4050-829e-f08eb3e4f292: GrpcService started, listening on 9857
datanode_1  | 2023-01-03 10:10:51,341 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 74492474-3f40-4050-829e-f08eb3e4f292 is started using port 9858 for RATIS
datanode_1  | 2023-01-03 10:10:51,343 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 74492474-3f40-4050-829e-f08eb3e4f292 is started using port 9857 for RATIS_ADMIN
datanode_1  | 2023-01-03 10:10:51,344 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 74492474-3f40-4050-829e-f08eb3e4f292 is started using port 9856 for RATIS_SERVER
datanode_1  | 2023-01-03 10:10:51,349 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-74492474-3f40-4050-829e-f08eb3e4f292: Started
datanode_1  | 2023-01-03 10:10:53,740 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:309)
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.9:39234 remote=scm/172.18.0.5:9861]
datanode_3  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_3  | 2023-01-03 10:10:50,748 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-14143503-c595-4722-b95c-9dd877fc0e68/DS-0f08aab3-0d33-4417-b433-6da2f2711065/container.db to cache
datanode_3  | 2023-01-03 10:10:50,755 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-14143503-c595-4722-b95c-9dd877fc0e68/DS-0f08aab3-0d33-4417-b433-6da2f2711065/container.db for volume DS-0f08aab3-0d33-4417-b433-6da2f2711065
datanode_3  | 2023-01-03 10:10:50,767 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3  | 2023-01-03 10:10:50,835 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_3  | 2023-01-03 10:10:50,971 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_3  | 2023-01-03 10:10:50,987 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 3719a142-1617-4d0e-a6d8-f47fd68adc81
datanode_3  | 2023-01-03 10:10:51,221 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.RaftServer: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start RPC server
datanode_3  | 2023-01-03 10:10:51,250 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: 3719a142-1617-4d0e-a6d8-f47fd68adc81: GrpcService started, listening on 9858
datanode_3  | 2023-01-03 10:10:51,265 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: 3719a142-1617-4d0e-a6d8-f47fd68adc81: GrpcService started, listening on 9856
datanode_3  | 2023-01-03 10:10:51,276 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: 3719a142-1617-4d0e-a6d8-f47fd68adc81: GrpcService started, listening on 9857
datanode_3  | 2023-01-03 10:10:51,302 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3719a142-1617-4d0e-a6d8-f47fd68adc81 is started using port 9858 for RATIS
datanode_3  | 2023-01-03 10:10:51,302 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3719a142-1617-4d0e-a6d8-f47fd68adc81 is started using port 9857 for RATIS_ADMIN
datanode_3  | 2023-01-03 10:10:51,302 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3719a142-1617-4d0e-a6d8-f47fd68adc81 is started using port 9856 for RATIS_SERVER
datanode_3  | 2023-01-03 10:10:51,307 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3719a142-1617-4d0e-a6d8-f47fd68adc81: Started
datanode_3  | 2023-01-03 10:11:01,262 [Command processor thread] INFO server.RaftServer: 3719a142-1617-4d0e-a6d8-f47fd68adc81: addNew group-9B236F254454:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER] returns group-9B236F254454:java.util.concurrent.CompletableFuture@77113e27[Not completed]
datanode_3  | 2023-01-03 10:11:01,605 [pool-22-thread-1] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81: new RaftServerImpl for group-9B236F254454:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3  | 2023-01-03 10:11:01,627 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2023-01-03 10:11:01,633 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2023-01-03 10:11:01,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3  | 2023-01-03 10:11:01,642 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3  | 2023-01-03 10:11:01,648 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:509)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1  | Caused by: java.util.concurrent.TimeoutException
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	... 1 more
datanode_1  | 2023-01-03 10:10:56,536 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
datanode_4  | 2023-01-03 10:11:09,044 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-LeaderElection1] INFO server.RaftServer$Division: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940: set configuration 0: peers:[c950fbed-bbb7-451f-b5e1-adf39d3a6a32|rpc:172.18.0.10:9856|admin:172.18.0.10:9857|client:172.18.0.10:9858|dataStream:172.18.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4  | 2023-01-03 10:11:09,513 [c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c950fbed-bbb7-451f-b5e1-adf39d3a6a32@group-495FB9ADC940-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d15fb565-ac34-40f7-b405-495fb9adc940/current/log_inprogress_0
datanode_3  | 2023-01-03 10:11:01,679 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2  | Caused by: java.util.concurrent.TimeoutException
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	... 1 more
datanode_2  | 2023-01-03 10:10:50,930 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-14143503-c595-4722-b95c-9dd877fc0e68/DS-6de75241-ee15-4137-ae93-a7750bb15cef/container.db to cache
datanode_2  | 2023-01-03 10:10:50,930 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-14143503-c595-4722-b95c-9dd877fc0e68/DS-6de75241-ee15-4137-ae93-a7750bb15cef/container.db for volume DS-6de75241-ee15-4137-ae93-a7750bb15cef
datanode_2  | 2023-01-03 10:10:50,938 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2  | 2023-01-03 10:10:50,974 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_2  | 2023-01-03 10:10:51,137 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_2  | 2023-01-03 10:10:51,154 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 482b5314-1cac-4df1-beda-20c463470dc2
datanode_2  | 2023-01-03 10:10:51,369 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.RaftServer: 482b5314-1cac-4df1-beda-20c463470dc2: start RPC server
datanode_2  | 2023-01-03 10:10:51,381 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: 482b5314-1cac-4df1-beda-20c463470dc2: GrpcService started, listening on 9858
datanode_2  | 2023-01-03 10:10:51,400 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: 482b5314-1cac-4df1-beda-20c463470dc2: GrpcService started, listening on 9856
datanode_2  | 2023-01-03 10:10:51,410 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: 482b5314-1cac-4df1-beda-20c463470dc2: GrpcService started, listening on 9857
datanode_2  | 2023-01-03 10:10:51,483 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 482b5314-1cac-4df1-beda-20c463470dc2 is started using port 9858 for RATIS
datanode_2  | 2023-01-03 10:10:51,483 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 482b5314-1cac-4df1-beda-20c463470dc2 is started using port 9857 for RATIS_ADMIN
datanode_2  | 2023-01-03 10:10:51,484 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 482b5314-1cac-4df1-beda-20c463470dc2 is started using port 9856 for RATIS_SERVER
datanode_2  | 2023-01-03 10:10:51,490 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-482b5314-1cac-4df1-beda-20c463470dc2: Started
datanode_2  | 2023-01-03 10:11:01,969 [Command processor thread] INFO server.RaftServer: 482b5314-1cac-4df1-beda-20c463470dc2: addNew group-9B236F254454:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER] returns group-9B236F254454:java.util.concurrent.CompletableFuture@59de49f4[Not completed]
datanode_2  | 2023-01-03 10:11:02,352 [pool-22-thread-1] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2: new RaftServerImpl for group-9B236F254454:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2  | 2023-01-03 10:11:02,391 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2023-01-03 10:11:02,412 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2023-01-03 10:11:02,415 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2  | 2023-01-03 10:11:02,424 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2  | 2023-01-03 10:11:02,433 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2  | 2023-01-03 10:11:02,435 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2  | 2023-01-03 10:11:02,581 [pool-22-thread-1] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: ConfigurationManager, init=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2  | 2023-01-03 10:11:02,585 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2023-01-03 10:11:02,655 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2023-01-03 10:11:02,664 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2  | 2023-01-03 10:11:02,773 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2  | 2023-01-03 10:11:02,826 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2023-01-03 10:11:02,832 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2  | 2023-01-03 10:11:03,193 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-482b5314-1cac-4df1-beda-20c463470dc2: Detected pause in JVM or host machine (eg GC): pause of approximately 120762889ns.
datanode_2  | GC pool 'ParNew' had collection(s): count=1 time=191ms
datanode_2  | 2023-01-03 10:11:03,514 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2023-01-03 10:11:03,523 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2  | 2023-01-03 10:11:03,528 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2  | 2023-01-03 10:11:03,529 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2  | 2023-01-03 10:11:03,532 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2  | 2023-01-03 10:11:03,533 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454 does not exist. Creating ...
datanode_2  | 2023-01-03 10:11:03,578 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454/in_use.lock acquired by nodename 6@88faf1c2abe6
datanode_2  | 2023-01-03 10:11:03,632 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454 has been successfully formatted.
datanode_2  | 2023-01-03 10:11:03,702 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-9B236F254454: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2023-01-03 10:11:03,797 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2023-01-03 10:11:03,921 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2023-01-03 10:11:03,922 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-03 10:11:03,927 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2  | 2023-01-03 10:11:03,928 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2  | 2023-01-03 10:11:03,978 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2023-01-03 10:11:04,049 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2023-01-03 10:11:04,053 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2  | 2023-01-03 10:11:04,084 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454
datanode_2  | 2023-01-03 10:11:04,086 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2  | 2023-01-03 10:11:04,088 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2023-01-03 10:11:04,097 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2023-01-03 10:11:04,099 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2023-01-03 10:11:04,106 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2023-01-03 10:11:04,114 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 2023-01-03 10:11:04,139 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2023-01-03 10:11:04,146 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2023-01-03 10:11:04,319 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2023-01-03 10:11:04,325 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-03 10:11:04,492 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2  | 2023-01-03 10:11:04,493 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2  | 2023-01-03 10:11:04,493 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2  | 2023-01-03 10:11:04,599 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2023-01-03 10:11:04,603 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2  | 2023-01-03 10:11:04,640 [pool-22-thread-1] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: start as a follower, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:309)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:509)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1  | Caused by: java.util.concurrent.TimeoutException
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	... 1 more
datanode_1  | 2023-01-03 10:11:00,476 [Command processor thread] INFO server.RaftServer: 74492474-3f40-4050-829e-f08eb3e4f292: addNew group-AD68F8D0A56D:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-AD68F8D0A56D:java.util.concurrent.CompletableFuture@5ee5a238[Not completed]
datanode_1  | 2023-01-03 10:11:00,653 [pool-22-thread-1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292: new RaftServerImpl for group-AD68F8D0A56D:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1  | 2023-01-03 10:11:00,671 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2023-01-03 10:11:00,672 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2023-01-03 10:11:00,681 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1  | 2023-01-03 10:11:00,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1  | 2023-01-03 10:11:00,692 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1  | 2023-01-03 10:11:00,708 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1  | 2023-01-03 10:11:00,809 [pool-22-thread-1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D: ConfigurationManager, init=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1  | 2023-01-03 10:11:00,811 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2023-01-03 10:11:00,873 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2023-01-03 10:11:00,880 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1  | 2023-01-03 10:11:00,928 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1  | 2023-01-03 10:11:00,949 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2023-01-03 10:11:00,954 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1  | 2023-01-03 10:11:01,201 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2023-01-03 10:11:01,205 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1  | 2023-01-03 10:11:01,209 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1  | 2023-01-03 10:11:01,216 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1  | 2023-01-03 10:11:01,216 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1  | 2023-01-03 10:11:01,222 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e4d2410b-8e21-4447-8fa8-ad68f8d0a56d does not exist. Creating ...
datanode_1  | 2023-01-03 10:11:01,269 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e4d2410b-8e21-4447-8fa8-ad68f8d0a56d/in_use.lock acquired by nodename 8@eafe2f3b9d53
datanode_1  | 2023-01-03 10:11:01,307 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e4d2410b-8e21-4447-8fa8-ad68f8d0a56d has been successfully formatted.
datanode_1  | 2023-01-03 10:11:01,366 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-AD68F8D0A56D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2023-01-03 10:11:01,378 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1  | 2023-01-03 10:11:01,423 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2023-01-03 10:11:01,425 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2023-01-03 10:11:01,428 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1  | 2023-01-03 10:11:01,437 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1  | 2023-01-03 10:11:01,452 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2023-01-03 10:11:01,541 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2023-01-03 10:11:01,549 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1  | 2023-01-03 10:11:01,711 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e4d2410b-8e21-4447-8fa8-ad68f8d0a56d
datanode_1  | 2023-01-03 10:11:01,716 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1  | 2023-01-03 10:11:01,719 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2023-01-03 10:11:01,730 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2023-01-03 10:11:01,735 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2023-01-03 10:11:01,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2023-01-03 10:11:01,755 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2023-01-03 10:11:01,771 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2023-01-03 10:11:01,775 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2023-01-03 10:11:04,645 [pool-22-thread-1] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2023-01-03 10:11:04,654 [pool-22-thread-1] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: start 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState
datanode_2  | 2023-01-03 10:11:04,666 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2  | 2023-01-03 10:11:04,690 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-03 10:11:04,701 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9B236F254454,id=482b5314-1cac-4df1-beda-20c463470dc2
datanode_2  | 2023-01-03 10:11:04,706 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2023-01-03 10:11:04,719 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2023-01-03 10:11:04,735 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2023-01-03 10:11:04,741 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2  | 2023-01-03 10:11:04,913 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454
datanode_2  | 2023-01-03 10:11:09,522 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: receive requestVote(ELECTION, 3719a142-1617-4d0e-a6d8-f47fd68adc81, group-9B236F254454, 1, (t:0, i:0))
datanode_2  | 2023-01-03 10:11:09,543 [grpc-default-executor-0] INFO impl.VoteContext: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FOLLOWER: reject ELECTION from 3719a142-1617-4d0e-a6d8-f47fd68adc81: our priority 1 > candidate's priority 0
datanode_2  | 2023-01-03 10:11:09,555 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:3719a142-1617-4d0e-a6d8-f47fd68adc81
datanode_2  | 2023-01-03 10:11:09,562 [grpc-default-executor-0] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: shutdown 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState
datanode_2  | 2023-01-03 10:11:09,563 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO impl.FollowerState: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState was interrupted
datanode_2  | 2023-01-03 10:11:09,566 [grpc-default-executor-0] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: start 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState
datanode_2  | 2023-01-03 10:11:09,589 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2  | 2023-01-03 10:11:09,611 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-03 10:11:09,635 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454 replies to ELECTION vote request: 3719a142-1617-4d0e-a6d8-f47fd68adc81<-482b5314-1cac-4df1-beda-20c463470dc2#0:FAIL-t1. Peer's state: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454:t1, leader=null, voted=null, raftlog=Memoized:482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-03 10:11:10,655 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: receive requestVote(ELECTION, 74492474-3f40-4050-829e-f08eb3e4f292, group-9B236F254454, 1, (t:0, i:0))
datanode_2  | 2023-01-03 10:11:10,656 [grpc-default-executor-0] INFO impl.VoteContext: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FOLLOWER: reject ELECTION from 74492474-3f40-4050-829e-f08eb3e4f292: our priority 1 > candidate's priority 0
datanode_2  | 2023-01-03 10:11:10,656 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:74492474-3f40-4050-829e-f08eb3e4f292
datanode_2  | 2023-01-03 10:11:10,657 [grpc-default-executor-0] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: shutdown 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState
datanode_2  | 2023-01-03 10:11:10,658 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO impl.FollowerState: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState was interrupted
datanode_2  | 2023-01-03 10:11:10,660 [grpc-default-executor-0] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: start 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState
datanode_2  | 2023-01-03 10:11:10,664 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454 replies to ELECTION vote request: 74492474-3f40-4050-829e-f08eb3e4f292<-482b5314-1cac-4df1-beda-20c463470dc2#0:FAIL-t1. Peer's state: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454:t1, leader=null, voted=null, raftlog=Memoized:482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-03 10:11:10,685 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2  | 2023-01-03 10:11:10,686 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-03 10:11:11,115 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454.
datanode_2  | 2023-01-03 10:11:11,121 [pool-22-thread-1] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2: new RaftServerImpl for group-06CFDBB51D5D:[482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3  | 2023-01-03 10:11:01,802 [pool-22-thread-1] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: ConfigurationManager, init=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3  | 2023-01-03 10:11:01,843 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2023-01-03 10:11:01,900 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2023-01-03 10:11:01,906 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3  | 2023-01-03 10:11:02,084 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3  | 2023-01-03 10:11:02,149 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2023-01-03 10:11:02,149 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3  | 2023-01-03 10:11:02,803 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2023-01-03 10:11:02,815 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3  | 2023-01-03 10:11:02,823 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3  | 2023-01-03 10:11:02,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3  | 2023-01-03 10:11:02,843 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3  | 2023-01-03 10:11:02,849 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454 does not exist. Creating ...
datanode_3  | 2023-01-03 10:11:02,910 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454/in_use.lock acquired by nodename 6@ad1ff48b9685
datanode_3  | 2023-01-03 10:11:02,974 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454 has been successfully formatted.
datanode_3  | 2023-01-03 10:11:03,043 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-9B236F254454: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2023-01-03 10:11:03,064 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2023-01-03 10:11:03,251 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2023-01-03 10:11:03,251 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2023-01-03 10:11:03,272 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3  | 2023-01-03 10:11:03,282 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3  | 2023-01-03 10:11:03,315 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2023-01-03 10:11:03,394 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 2023-01-03 10:11:03,401 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3  | 2023-01-03 10:11:03,480 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454
datanode_3  | 2023-01-03 10:11:03,484 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3  | 2023-01-03 10:11:03,487 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2023-01-03 10:11:03,499 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2023-01-03 10:11:03,502 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2023-01-03 10:11:03,519 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2023-01-03 10:11:03,534 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2023-01-03 10:11:03,538 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2023-01-03 10:11:03,540 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2023-01-03 10:11:03,734 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2023-01-03 10:11:03,783 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2023-01-03 10:11:03,863 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3  | 2023-01-03 10:11:03,864 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3  | 2023-01-03 10:11:03,869 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2023-01-03 10:11:03,948 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2023-01-03 10:11:03,953 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3  | 2023-01-03 10:11:03,975 [pool-22-thread-1] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: start as a follower, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-03 10:11:03,976 [pool-22-thread-1] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2023-01-03 10:11:03,985 [pool-22-thread-1] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState
datanode_3  | 2023-01-03 10:11:03,988 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-03 10:11:03,994 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-03 10:11:04,029 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9B236F254454,id=3719a142-1617-4d0e-a6d8-f47fd68adc81
datanode_3  | 2023-01-03 10:11:04,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2023-01-03 10:11:04,033 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2023-01-03 10:11:04,033 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2023-01-03 10:11:04,034 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3  | 2023-01-03 10:11:04,215 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454
datanode_1  | 2023-01-03 10:11:01,902 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2023-01-03 10:11:01,910 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2023-01-03 10:11:02,163 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1  | 2023-01-03 10:11:02,173 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-74492474-3f40-4050-829e-f08eb3e4f292: Detected pause in JVM or host machine (eg GC): pause of approximately 191631733ns. No GCs detected.
datanode_1  | 2023-01-03 10:11:02,180 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1  | 2023-01-03 10:11:02,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2023-01-03 10:11:02,445 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2023-01-03 10:11:02,445 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1  | 2023-01-03 10:11:02,479 [pool-22-thread-1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D: start as a follower, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-03 10:11:02,480 [pool-22-thread-1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2023-01-03 10:11:02,524 [pool-22-thread-1] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: start 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-FollowerState
datanode_1  | 2023-01-03 10:11:02,550 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-03 10:11:02,550 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-03 10:11:02,600 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-AD68F8D0A56D,id=74492474-3f40-4050-829e-f08eb3e4f292
datanode_1  | 2023-01-03 10:11:02,686 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2023-01-03 10:11:02,694 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1  | 2023-01-03 10:11:02,705 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2023-01-03 10:11:02,729 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1  | 2023-01-03 10:11:03,110 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=e4d2410b-8e21-4447-8fa8-ad68f8d0a56d
datanode_1  | 2023-01-03 10:11:03,126 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=e4d2410b-8e21-4447-8fa8-ad68f8d0a56d.
datanode_1  | 2023-01-03 10:11:03,131 [Command processor thread] INFO server.RaftServer: 74492474-3f40-4050-829e-f08eb3e4f292: addNew group-9B236F254454:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER] returns group-9B236F254454:java.util.concurrent.CompletableFuture@2ac1a8eb[Not completed]
datanode_1  | 2023-01-03 10:11:03,246 [pool-22-thread-1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292: new RaftServerImpl for group-9B236F254454:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1  | 2023-01-03 10:11:03,256 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2023-01-03 10:11:03,260 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2023-01-03 10:11:03,260 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1  | 2023-01-03 10:11:03,260 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1  | 2023-01-03 10:11:03,263 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1  | 2023-01-03 10:11:03,264 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3  | 2023-01-03 10:11:08,108 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-3719a142-1617-4d0e-a6d8-f47fd68adc81: Detected pause in JVM or host machine (eg GC): pause of approximately 129341112ns. No GCs detected.
datanode_3  | 2023-01-03 10:11:09,089 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO impl.FollowerState: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5110474985ns, electionTimeout:5088ms
datanode_3  | 2023-01-03 10:11:09,090 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: shutdown 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState
datanode_3  | 2023-01-03 10:11:09,091 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3  | 2023-01-03 10:11:09,112 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3  | 2023-01-03 10:11:09,112 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1
datanode_3  | 2023-01-03 10:11:09,150 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1] INFO impl.LeaderElection: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-03 10:11:09,157 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-03 10:11:09,157 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-03 10:11:09,168 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 74492474-3f40-4050-829e-f08eb3e4f292
datanode_3  | 2023-01-03 10:11:09,169 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 482b5314-1cac-4df1-beda-20c463470dc2
datanode_3  | 2023-01-03 10:11:09,742 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1] INFO impl.LeaderElection: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3  | 2023-01-03 10:11:09,743 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1] INFO impl.LeaderElection:   Response 0: 3719a142-1617-4d0e-a6d8-f47fd68adc81<-482b5314-1cac-4df1-beda-20c463470dc2#0:FAIL-t1
datanode_3  | 2023-01-03 10:11:09,744 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1] INFO impl.LeaderElection: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1 ELECTION round 0: result REJECTED
datanode_3  | 2023-01-03 10:11:09,745 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_3  | 2023-01-03 10:11:09,745 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: shutdown 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1
datanode_3  | 2023-01-03 10:11:09,747 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection1] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState
datanode_3  | 2023-01-03 10:11:09,805 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-03 10:11:09,805 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-03 10:11:10,956 [grpc-default-executor-0] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: receive requestVote(ELECTION, 74492474-3f40-4050-829e-f08eb3e4f292, group-9B236F254454, 1, (t:0, i:0))
datanode_3  | 2023-01-03 10:11:10,958 [grpc-default-executor-0] INFO impl.VoteContext: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FOLLOWER: reject ELECTION from 74492474-3f40-4050-829e-f08eb3e4f292: already has voted for 3719a142-1617-4d0e-a6d8-f47fd68adc81 at current term 1
datanode_3  | 2023-01-03 10:11:10,965 [grpc-default-executor-0] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454 replies to ELECTION vote request: 74492474-3f40-4050-829e-f08eb3e4f292<-3719a142-1617-4d0e-a6d8-f47fd68adc81#0:FAIL-t1. Peer's state: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454:t1, leader=null, voted=3719a142-1617-4d0e-a6d8-f47fd68adc81, raftlog=Memoized:3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-03 10:11:11,086 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454.
datanode_3  | 2023-01-03 10:11:11,088 [Command processor thread] INFO server.RaftServer: 3719a142-1617-4d0e-a6d8-f47fd68adc81: addNew group-4562A4FBD783:[3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:1|startupRole:FOLLOWER] returns group-4562A4FBD783:java.util.concurrent.CompletableFuture@5c6fbcb4[Not completed]
datanode_1  | 2023-01-03 10:11:03,264 [pool-22-thread-1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: ConfigurationManager, init=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1  | 2023-01-03 10:11:03,265 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2023-01-03 10:11:03,267 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2023-01-03 10:11:03,272 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1  | 2023-01-03 10:11:03,274 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1  | 2023-01-03 10:11:03,328 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2023-01-03 10:11:03,333 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1  | 2023-01-03 10:11:03,354 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2023-01-03 10:11:03,360 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1  | 2023-01-03 10:11:03,361 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1  | 2023-01-03 10:11:03,365 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1  | 2023-01-03 10:11:03,366 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1  | 2023-01-03 10:11:03,367 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454 does not exist. Creating ...
datanode_1  | 2023-01-03 10:11:03,382 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454/in_use.lock acquired by nodename 8@eafe2f3b9d53
datanode_1  | 2023-01-03 10:11:03,395 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454 has been successfully formatted.
datanode_1  | 2023-01-03 10:11:03,404 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-9B236F254454: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2023-01-03 10:11:03,405 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1  | 2023-01-03 10:11:03,407 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2023-01-03 10:11:03,409 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2023-01-03 10:11:03,410 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1  | 2023-01-03 10:11:03,410 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1  | 2023-01-03 10:11:03,412 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2023-01-03 10:11:03,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2023-01-03 10:11:03,421 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1  | 2023-01-03 10:11:03,422 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454
datanode_1  | 2023-01-03 10:11:03,422 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1  | 2023-01-03 10:11:03,423 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2023-01-03 10:11:03,424 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2023-01-03 10:11:03,425 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2023-01-03 10:11:03,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2023-01-03 10:11:11,122 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2023-01-03 10:11:11,122 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2023-01-03 10:11:11,123 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2  | 2023-01-03 10:11:11,123 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2  | 2023-01-03 10:11:11,124 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2  | 2023-01-03 10:11:11,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2  | 2023-01-03 10:11:11,125 [pool-22-thread-1] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D: ConfigurationManager, init=-1: peers:[482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2  | 2023-01-03 10:11:11,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2023-01-03 10:11:11,126 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2023-01-03 10:11:11,126 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2  | 2023-01-03 10:11:11,126 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2  | 2023-01-03 10:11:11,128 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2023-01-03 10:11:11,128 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2  | 2023-01-03 10:11:11,129 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2023-01-03 10:11:11,131 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2  | 2023-01-03 10:11:11,131 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2  | 2023-01-03 10:11:11,131 [Command processor thread] INFO server.RaftServer: 482b5314-1cac-4df1-beda-20c463470dc2: addNew group-06CFDBB51D5D:[482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER] returns group-06CFDBB51D5D:java.util.concurrent.CompletableFuture@5a5dd77[Not completed]
datanode_2  | 2023-01-03 10:11:11,132 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2  | 2023-01-03 10:11:11,132 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2  | 2023-01-03 10:11:11,133 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/22c9380f-296a-4792-92f3-06cfdbb51d5d does not exist. Creating ...
datanode_2  | 2023-01-03 10:11:11,136 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/22c9380f-296a-4792-92f3-06cfdbb51d5d/in_use.lock acquired by nodename 6@88faf1c2abe6
datanode_2  | 2023-01-03 10:11:11,139 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/22c9380f-296a-4792-92f3-06cfdbb51d5d has been successfully formatted.
datanode_2  | 2023-01-03 10:11:11,141 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-06CFDBB51D5D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2023-01-03 10:11:11,141 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2023-01-03 10:11:11,142 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2023-01-03 10:11:11,146 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-03 10:11:11,153 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2  | 2023-01-03 10:11:11,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2  | 2023-01-03 10:11:11,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2023-01-03 10:11:11,163 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2023-01-03 10:11:11,164 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2  | 2023-01-03 10:11:11,164 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/22c9380f-296a-4792-92f3-06cfdbb51d5d
datanode_2  | 2023-01-03 10:11:11,164 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2  | 2023-01-03 10:11:11,165 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2023-01-03 10:11:11,165 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2023-01-03 10:11:11,167 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2023-01-03 10:11:11,167 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2023-01-03 10:11:11,167 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 2023-01-03 10:11:11,174 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2023-01-03 10:11:11,174 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2023-01-03 10:11:11,175 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2023-01-03 10:11:11,188 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-03 10:11:11,253 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2  | 2023-01-03 10:11:11,254 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2  | 2023-01-03 10:11:11,259 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2023-01-03 10:11:03,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2023-01-03 10:11:03,428 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2023-01-03 10:11:03,429 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2023-01-03 10:11:03,437 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2023-01-03 10:11:03,529 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2023-01-03 10:11:04,617 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-74492474-3f40-4050-829e-f08eb3e4f292: Detected pause in JVM or host machine (eg GC): pause of approximately 914590423ns.
datanode_1  | GC pool 'ParNew' had collection(s): count=1 time=60ms
datanode_1  | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1021ms
datanode_1  | 2023-01-03 10:11:04,680 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1  | 2023-01-03 10:11:04,696 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1  | 2023-01-03 10:11:04,696 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2023-01-03 10:11:04,697 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2023-01-03 10:11:04,697 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1  | 2023-01-03 10:11:04,703 [pool-22-thread-1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: start as a follower, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-03 10:11:04,711 [pool-22-thread-1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2023-01-03 10:11:04,716 [pool-22-thread-1] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: start 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState
datanode_1  | 2023-01-03 10:11:04,717 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9B236F254454,id=74492474-3f40-4050-829e-f08eb3e4f292
datanode_1  | 2023-01-03 10:11:04,718 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2023-01-03 10:11:04,718 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1  | 2023-01-03 10:11:04,718 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2023-01-03 10:11:04,718 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1  | 2023-01-03 10:11:04,733 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-03 10:11:04,737 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454
datanode_1  | 2023-01-03 10:11:04,762 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-03 10:11:07,632 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-FollowerState] INFO impl.FollowerState: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5116920306ns, electionTimeout:5080ms
datanode_1  | 2023-01-03 10:11:07,633 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-FollowerState] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: shutdown 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-FollowerState
datanode_1  | 2023-01-03 10:11:07,633 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-FollowerState] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1  | 2023-01-03 10:11:07,643 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1  | 2023-01-03 10:11:07,646 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-FollowerState] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: start 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1
datanode_1  | 2023-01-03 10:11:07,681 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO impl.LeaderElection: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-03 10:11:07,692 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO impl.LeaderElection: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1  | 2023-01-03 10:11:07,693 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: shutdown 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1
datanode_1  | 2023-01-03 10:11:07,697 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1  | 2023-01-03 10:11:07,700 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-AD68F8D0A56D with new leaderId: 74492474-3f40-4050-829e-f08eb3e4f292
datanode_1  | 2023-01-03 10:11:07,701 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D: change Leader from null to 74492474-3f40-4050-829e-f08eb3e4f292 at term 1 for becomeLeader, leader elected after 6782ms
datanode_1  | 2023-01-03 10:11:07,770 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1  | 2023-01-03 10:11:07,832 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1  | 2023-01-03 10:11:07,845 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1  | 2023-01-03 10:11:07,972 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1  | 2023-01-03 10:11:07,973 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1  | 2023-01-03 10:11:07,974 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1  | 2023-01-03 10:11:08,094 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1  | 2023-01-03 10:11:08,153 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1  | 2023-01-03 10:11:08,242 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: start 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderStateImpl
datanode_1  | 2023-01-03 10:11:08,467 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2023-01-03 10:11:09,421 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-LeaderElection1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D: set configuration 0: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-03 10:11:09,787 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO impl.FollowerState: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5071502390ns, electionTimeout:5023ms
datanode_1  | 2023-01-03 10:11:09,852 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: shutdown 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState
datanode_1  | 2023-01-03 10:11:09,853 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1  | 2023-01-03 10:11:09,853 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1  | 2023-01-03 10:11:09,854 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: start 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2
datanode_1  | 2023-01-03 10:11:10,087 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-03 10:11:10,176 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 482b5314-1cac-4df1-beda-20c463470dc2
datanode_1  | 2023-01-03 10:11:10,199 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-03 10:11:10,224 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-03 10:11:10,227 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: receive requestVote(ELECTION, 3719a142-1617-4d0e-a6d8-f47fd68adc81, group-9B236F254454, 1, (t:0, i:0))
datanode_5  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_5  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_5  | 2023-01-03 10:09:44,100 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_5  | /************************************************************
datanode_5  | STARTUP_MSG: Starting HddsDatanodeService
datanode_5  | STARTUP_MSG:   host = 8e31dcea51a8/172.18.0.6
datanode_5  | STARTUP_MSG:   args = []
datanode_5  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2  | 2023-01-03 10:11:11,260 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2023-01-03 10:11:11,260 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2  | 2023-01-03 10:11:11,260 [pool-22-thread-1] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D: start as a follower, conf=-1: peers:[482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1        | 2023-01-03 10:09:44,740 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1        | /************************************************************
om_1        | STARTUP_MSG: Starting OzoneManager
om_1        | STARTUP_MSG:   host = 8d15445c285b/172.18.0.4
om_1        | STARTUP_MSG:   args = [--init]
om_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/1c830c5bc7b0be75001a5e8bcef69a8810ca74f1 ; compiled by 'runner' on 2023-01-03T09:55Z
om_1        | STARTUP_MSG:   java = 11.0.14.1
om_1        | ************************************************************/
om_1        | 2023-01-03 10:09:44,844 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1        | 2023-01-03 10:09:55,740 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1        | 2023-01-03 10:09:59,331 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1        | 2023-01-03 10:09:59,899 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.18.0.4:9862
datanode_3  | 2023-01-03 10:11:11,186 [pool-22-thread-1] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81: new RaftServerImpl for group-4562A4FBD783:[3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3  | 2023-01-03 10:11:11,214 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2023-01-03 10:11:11,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2023-01-03 10:11:11,223 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3  | 2023-01-03 10:11:11,223 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3  | 2023-01-03 10:11:11,224 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3  | 2023-01-03 10:11:11,224 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3  | 2023-01-03 10:11:11,225 [pool-22-thread-1] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783: ConfigurationManager, init=-1: peers:[3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3  | 2023-01-03 10:11:11,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2023-01-03 10:11:11,229 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2023-01-03 10:11:11,229 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3  | 2023-01-03 10:11:11,233 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3  | 2023-01-03 10:11:11,234 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2023-01-03 10:11:11,235 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3  | 2023-01-03 10:11:11,251 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2023-01-03 10:11:11,252 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3  | 2023-01-03 10:11:11,253 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3  | 2023-01-03 10:11:11,253 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3  | 2023-01-03 10:11:11,254 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3  | 2023-01-03 10:11:11,255 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b7fb08b7-ba15-458b-85fd-4562a4fbd783 does not exist. Creating ...
datanode_3  | 2023-01-03 10:11:11,257 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b7fb08b7-ba15-458b-85fd-4562a4fbd783/in_use.lock acquired by nodename 6@ad1ff48b9685
datanode_3  | 2023-01-03 10:11:11,261 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b7fb08b7-ba15-458b-85fd-4562a4fbd783 has been successfully formatted.
datanode_3  | 2023-01-03 10:11:11,299 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-4562A4FBD783: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2023-01-03 10:11:11,299 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2023-01-03 10:11:11,300 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2023-01-03 10:11:11,300 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2023-01-03 10:11:11,300 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3  | 2023-01-03 10:11:11,300 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3  | 2023-01-03 10:11:11,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2023-01-03 10:11:11,303 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 2023-01-03 10:11:11,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3  | 2023-01-03 10:11:11,307 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b7fb08b7-ba15-458b-85fd-4562a4fbd783
datanode_3  | 2023-01-03 10:11:11,315 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3  | 2023-01-03 10:11:11,315 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2023-01-03 10:11:11,315 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2023-01-03 10:11:11,315 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2023-01-03 10:11:11,315 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2023-01-03 10:11:11,315 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2023-01-03 10:11:11,316 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2023-01-03 10:11:11,316 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2023-01-03 10:11:11,323 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2023-01-03 10:11:11,326 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2023-01-03 10:11:11,447 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3  | 2023-01-03 10:11:11,447 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3  | 2023-01-03 10:11:11,447 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2023-01-03 10:11:11,448 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
recon_1     | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1     | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1     | 2023-01-03 10:09:42,809 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1     | /************************************************************
recon_1     | STARTUP_MSG: Starting ReconServer
recon_1     | STARTUP_MSG:   host = 318b4659c1ef/172.18.0.3
recon_1     | STARTUP_MSG:   args = []
recon_1     | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1     | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.23.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.3.23.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.23.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.23.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.23.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1     | STARTUP_MSG:   build = https://github.com/apache/ozone/1c830c5bc7b0be75001a5e8bcef69a8810ca74f1 ; compiled by 'runner' on 2023-01-03T09:55Z
recon_1     | STARTUP_MSG:   java = 11.0.14.1
recon_1     | ************************************************************/
recon_1     | 2023-01-03 10:09:42,994 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1     | 2023-01-03 10:09:49,928 [main] INFO reflections.Reflections: Reflections took 876 ms to scan 1 urls, producing 16 keys and 51 values 
recon_1     | 2023-01-03 10:09:57,392 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1     | 2023-01-03 10:10:00,765 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1     | 2023-01-03 10:10:11,466 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1     | WARNING: An illegal reflective access operation has occurred
recon_1     | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1     | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1     | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1     | WARNING: All illegal access operations will be denied in a future release
recon_1     | 2023-01-03 10:10:14,513 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1     | 2023-01-03 10:10:14,514 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.001 seconds to initialized 0 records to KEY_CONTAINER table
recon_1     | 2023-01-03 10:10:14,601 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1     | 2023-01-03 10:10:14,839 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1     | 2023-01-03 10:10:14,841 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1     | 2023-01-03 10:10:24,438 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1     | 2023-01-03 10:10:24,556 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1     | 2023-01-03 10:10:24,688 [main] INFO util.log: Logging initialized @55775ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1     | 2023-01-03 10:10:25,542 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1     | 2023-01-03 10:10:25,621 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1     | 2023-01-03 10:10:25,721 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1     | 2023-01-03 10:10:25,769 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1     | 2023-01-03 10:10:25,771 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1     | 2023-01-03 10:10:25,771 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1     | 2023-01-03 10:10:27,546 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1     | 2023-01-03 10:10:29,101 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1     | 2023-01-03 10:10:29,164 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1     | 2023-01-03 10:10:29,236 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1     | 2023-01-03 10:10:29,464 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1     | 2023-01-03 10:10:29,464 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1     | 2023-01-03 10:10:32,838 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2023-01-03 10:10:33,366 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2023-01-03 10:10:33,533 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1     | 2023-01-03 10:10:33,544 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1     | 2023-01-03 10:10:33,738 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2023-01-03 10:10:34,152 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1     | 2023-01-03 10:10:34,349 [main] INFO reflections.Reflections: Reflections took 181 ms to scan 3 urls, producing 121 keys and 272 values 
recon_1     | 2023-01-03 10:10:34,563 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1     | 2023-01-03 10:10:34,643 [main] WARN server.ServerUtils: ozone.scm.dead.node.interval value = 45000 is smaller than min = 60000 based on the key value of ozone.scm.stale.node.interval, reset to the min value 60000.
recon_1     | 2023-01-03 10:10:34,655 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1     | 2023-01-03 10:10:34,672 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1     | 2023-01-03 10:10:34,705 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1     | 2023-01-03 10:10:34,783 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1     | 2023-01-03 10:10:34,842 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1     | 2023-01-03 10:10:34,940 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1     | 2023-01-03 10:10:35,038 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1     | 2023-01-03 10:10:35,431 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
scm_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1       | 2023-01-03 10:09:56,314 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1       | /************************************************************
scm_1       | STARTUP_MSG: Starting StorageContainerManager
scm_1       | STARTUP_MSG:   host = 1898d909ae22/172.18.0.5
scm_1       | STARTUP_MSG:   args = [--init]
scm_1       | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1       | STARTUP_MSG:   build = https://github.com/apache/ozone/1c830c5bc7b0be75001a5e8bcef69a8810ca74f1 ; compiled by 'runner' on 2023-01-03T09:55Z
scm_1       | STARTUP_MSG:   java = 11.0.14.1
scm_1       | ************************************************************/
scm_1       | 2023-01-03 10:09:56,692 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1       | 2023-01-03 10:09:58,435 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2023-01-03 10:09:59,298 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1       | 2023-01-03 10:09:59,537 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1       | 2023-01-03 10:10:03,706 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1       | 2023-01-03 10:10:05,780 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1       | 2023-01-03 10:10:05,816 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1       | 2023-01-03 10:10:05,836 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1       | 2023-01-03 10:10:05,836 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1       | 2023-01-03 10:10:05,837 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1       | 2023-01-03 10:10:05,837 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1       | 2023-01-03 10:10:05,888 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1       | 2023-01-03 10:10:06,011 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | 2023-01-03 10:10:06,012 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1       | 2023-01-03 10:10:06,052 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1       | 2023-01-03 10:10:06,317 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1       | 2023-01-03 10:10:06,392 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1       | 2023-01-03 10:10:06,393 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1       | 2023-01-03 10:10:10,686 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
scm_1       | 2023-01-03 10:10:10,913 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1       | 2023-01-03 10:10:13,516 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
scm_1       | 2023-01-03 10:10:13,518 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
scm_1       | 2023-01-03 10:10:13,519 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm_1       | 2023-01-03 10:10:13,520 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
scm_1       | 2023-01-03 10:10:13,628 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm_1       | 2023-01-03 10:10:13,674 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
scm_1       | 2023-01-03 10:10:13,784 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
scm_1       | 2023-01-03 10:10:13,812 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
scm_1       | 2023-01-03 10:10:13,852 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
scm_1       | 2023-01-03 10:10:13,868 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm_1       | 2023-01-03 10:10:15,069 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1       | 2023-01-03 10:10:15,102 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1       | 2023-01-03 10:10:15,159 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1       | 2023-01-03 10:10:15,187 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1       | 2023-01-03 10:10:15,311 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1       | 2023-01-03 10:10:15,586 [8036be97-5432-4d54-a213-e0da532bfc11-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x9b287a69] REGISTERED
scm_1       | 2023-01-03 10:10:15,617 [main] INFO server.RaftServer: 8036be97-5432-4d54-a213-e0da532bfc11: addNew group-9DD877FC0E68:[8036be97-5432-4d54-a213-e0da532bfc11|rpc:1898d909ae22:9894|priority:0|startupRole:FOLLOWER] returns group-9DD877FC0E68:java.util.concurrent.CompletableFuture@4b2e3e8f[Not completed]
scm_1       | 2023-01-03 10:10:15,691 [8036be97-5432-4d54-a213-e0da532bfc11-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x9b287a69] BIND: 0.0.0.0/0.0.0.0:0
scm_1       | 2023-01-03 10:10:15,757 [8036be97-5432-4d54-a213-e0da532bfc11-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x9b287a69, L:/0.0.0.0:34509] ACTIVE
scm_1       | 2023-01-03 10:10:16,256 [pool-2-thread-1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11: new RaftServerImpl for group-9DD877FC0E68:[8036be97-5432-4d54-a213-e0da532bfc11|rpc:1898d909ae22:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1       | 2023-01-03 10:10:16,379 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1       | 2023-01-03 10:10:16,383 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1       | 2023-01-03 10:10:16,383 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1       | 2023-01-03 10:10:16,387 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1       | 2023-01-03 10:10:16,391 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1       | 2023-01-03 10:10:16,391 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1        | 2023-01-03 10:09:59,900 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1        | 2023-01-03 10:09:59,906 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1        | 2023-01-03 10:10:00,389 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1        | 2023-01-03 10:10:02,670 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863]
om_1        | 2023-01-03 10:10:06,972 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:08,973 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:10,976 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:12,977 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:14,979 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:16,981 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:18,983 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:20,985 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:22,987 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:24,989 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:26,990 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:28,995 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:30,999 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:33,002 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:35,005 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:37,012 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:39,014 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:41,017 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 18 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:43,019 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8d15445c285b/172.18.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 19 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | 2023-01-03 10:10:45,986 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:8036be97-5432-4d54-a213-e0da532bfc11 is not the leader. Could not determine the leader node.
om_1        | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1        | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1        | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1        | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1        | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1        | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1        | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1        | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1        | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1        | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1        | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1        | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1        | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1        | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1        | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 20 failover attempts. Trying to failover after sleeping for 2000ms.
datanode_3  | 2023-01-03 10:11:11,448 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3  | 2023-01-03 10:11:11,457 [pool-22-thread-1] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783: start as a follower, conf=-1: peers:[3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm_1       | 2023-01-03 10:10:16,524 [pool-2-thread-1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: ConfigurationManager, init=-1: peers:[8036be97-5432-4d54-a213-e0da532bfc11|rpc:1898d909ae22:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1       | 2023-01-03 10:10:16,563 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1       | 2023-01-03 10:10:16,649 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1       | 2023-01-03 10:10:16,688 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1       | 2023-01-03 10:10:16,933 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1       | 2023-01-03 10:10:17,059 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1       | 2023-01-03 10:10:17,063 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1       | 2023-01-03 10:10:17,798 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1       | 2023-01-03 10:10:17,883 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1       | 2023-01-03 10:10:17,924 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1       | 2023-01-03 10:10:17,935 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1       | 2023-01-03 10:10:17,963 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1       | 2023-01-03 10:10:17,980 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68 does not exist. Creating ...
scm_1       | 2023-01-03 10:10:18,022 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68/in_use.lock acquired by nodename 12@1898d909ae22
scm_1       | 2023-01-03 10:10:18,150 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68 has been successfully formatted.
scm_1       | 2023-01-03 10:10:18,184 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1       | 2023-01-03 10:10:18,377 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1       | 2023-01-03 10:10:18,385 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | 2023-01-03 10:10:18,388 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1       | 2023-01-03 10:10:18,416 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1       | 2023-01-03 10:10:18,468 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1       | 2023-01-03 10:10:18,570 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1       | 2023-01-03 10:10:18,588 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1       | 2023-01-03 10:10:18,674 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68
scm_1       | 2023-01-03 10:10:18,687 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1       | 2023-01-03 10:10:18,695 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1       | 2023-01-03 10:10:18,707 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1       | 2023-01-03 10:10:18,708 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1       | 2023-01-03 10:10:18,716 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1       | 2023-01-03 10:10:18,722 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1       | 2023-01-03 10:10:18,739 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1       | 2023-01-03 10:10:18,740 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1       | 2023-01-03 10:10:18,876 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1       | 2023-01-03 10:10:18,877 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | 2023-01-03 10:10:18,999 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1       | 2023-01-03 10:10:19,046 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1       | 2023-01-03 10:10:19,049 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1       | 2023-01-03 10:10:19,199 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1       | 2023-01-03 10:10:19,200 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1       | 2023-01-03 10:10:19,235 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: start as a follower, conf=-1: peers:[8036be97-5432-4d54-a213-e0da532bfc11|rpc:1898d909ae22:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1       | 2023-01-03 10:10:19,265 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1       | 2023-01-03 10:10:19,267 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO impl.RoleInfo: 8036be97-5432-4d54-a213-e0da532bfc11: start 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState
scm_1       | 2023-01-03 10:10:19,321 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1     | 2023-01-03 10:10:35,432 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1     | 2023-01-03 10:10:35,640 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1     | 2023-01-03 10:10:35,687 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1     | 2023-01-03 10:10:35,687 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1     | 2023-01-03 10:10:36,360 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1     | 2023-01-03 10:10:36,366 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1     | 2023-01-03 10:10:36,425 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1     | 2023-01-03 10:10:36,426 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1     | 2023-01-03 10:10:36,429 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1     | 2023-01-03 10:10:36,457 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7affee54{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1     | 2023-01-03 10:10:36,459 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@744fb110{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1     | 2023-01-03 10:10:40,982 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6fc1a561{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-5791063313405332368/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1     | 2023-01-03 10:10:41,007 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@3d405fe5{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1     | 2023-01-03 10:10:41,009 [Listener at 0.0.0.0/9891] INFO server.Server: Started @72096ms
recon_1     | 2023-01-03 10:10:41,022 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1     | 2023-01-03 10:10:41,023 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1     | 2023-01-03 10:10:41,026 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1     | 2023-01-03 10:10:41,027 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1     | 2023-01-03 10:10:41,059 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
om_1        | 2023-01-03 10:10:48,000 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:8036be97-5432-4d54-a213-e0da532bfc11 is not the leader. Could not determine the leader node.
om_1        | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1        | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1        | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1        | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1        | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1        | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1        | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1        | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1        | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1        | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1        | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1        | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1        | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1        | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1        | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863 after 21 failover attempts. Trying to failover after sleeping for 2000ms.
om_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-14143503-c595-4722-b95c-9dd877fc0e68;layoutVersion=3
om_1        | 2023-01-03 10:10:50,993 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1        | /************************************************************
om_1        | SHUTDOWN_MSG: Shutting down OzoneManager at 8d15445c285b/172.18.0.4
om_1        | ************************************************************/
om_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1        | 2023-01-03 10:11:00,578 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1        | /************************************************************
om_1        | STARTUP_MSG: Starting OzoneManager
om_1        | STARTUP_MSG:   host = 8d15445c285b/172.18.0.4
om_1        | STARTUP_MSG:   args = []
om_1        | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1     | 2023-01-03 10:10:41,093 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1     | 2023-01-03 10:10:41,099 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1     | 2023-01-03 10:10:41,099 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2023-01-03 10:10:41,102 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1     | 2023-01-03 10:10:41,108 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1     | 2023-01-03 10:10:43,351 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 318b4659c1ef/172.18.0.3 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-03 10:10:46,099 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:8036be97-5432-4d54-a213-e0da532bfc11 is not the leader. Could not determine the leader node.
recon_1     | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1     | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1     | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62741)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | , while invoking $Proxy44.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-03 10:10:48,109 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:8036be97-5432-4d54-a213-e0da532bfc11 is not the leader. Could not determine the leader node.
recon_1     | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1     | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1     | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62741)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | , while invoking $Proxy44.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1     | 2023-01-03 10:10:53,331 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1     | 2023-01-03 10:10:53,332 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1     | 2023-01-03 10:10:53,332 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1     | 2023-01-03 10:10:53,334 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1     | 2023-01-03 10:10:53,354 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1     | 2023-01-03 10:10:53,431 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
om_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/1c830c5bc7b0be75001a5e8bcef69a8810ca74f1 ; compiled by 'runner' on 2023-01-03T09:55Z
om_1        | STARTUP_MSG:   java = 11.0.14.1
om_1        | ************************************************************/
om_1        | 2023-01-03 10:11:00,681 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1        | 2023-01-03 10:11:08,203 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1        | 2023-01-03 10:11:10,799 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1        | 2023-01-03 10:11:11,057 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.18.0.4:9862
om_1        | 2023-01-03 10:11:11,057 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1        | 2023-01-03 10:11:11,059 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1        | 2023-01-03 10:11:11,280 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1        | 2023-01-03 10:11:11,407 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om_1        | 2023-01-03 10:11:12,079 [main] INFO reflections.Reflections: Reflections took 485 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om_1        | 2023-01-03 10:11:12,146 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1        | 2023-01-03 10:11:13,389 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863]
om_1        | 2023-01-03 10:11:13,641 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.18.0.5:9863]
om_1        | 2023-01-03 10:11:15,805 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1        | 2023-01-03 10:11:16,404 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
datanode_1  | 2023-01-03 10:11:10,245 [grpc-default-executor-2] INFO impl.VoteContext: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-CANDIDATE: reject ELECTION from 3719a142-1617-4d0e-a6d8-f47fd68adc81: already has voted for 74492474-3f40-4050-829e-f08eb3e4f292 at current term 1
datanode_1  | 2023-01-03 10:11:10,265 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 3719a142-1617-4d0e-a6d8-f47fd68adc81
datanode_1  | 2023-01-03 10:11:10,458 [74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 74492474-3f40-4050-829e-f08eb3e4f292@group-AD68F8D0A56D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e4d2410b-8e21-4447-8fa8-ad68f8d0a56d/current/log_inprogress_0
datanode_1  | 2023-01-03 10:11:10,461 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454 replies to ELECTION vote request: 3719a142-1617-4d0e-a6d8-f47fd68adc81<-74492474-3f40-4050-829e-f08eb3e4f292#0:FAIL-t1. Peer's state: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454:t1, leader=null, voted=74492474-3f40-4050-829e-f08eb3e4f292, raftlog=Memoized:74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-03 10:11:10,869 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1  | 2023-01-03 10:11:10,870 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection:   Response 0: 74492474-3f40-4050-829e-f08eb3e4f292<-482b5314-1cac-4df1-beda-20c463470dc2#0:FAIL-t1
datanode_1  | 2023-01-03 10:11:10,870 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1  | 2023-01-03 10:11:10,873 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_1  | 2023-01-03 10:11:10,874 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: shutdown 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2
datanode_1  | 2023-01-03 10:11:10,874 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection2] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: start 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState
datanode_1  | 2023-01-03 10:11:10,916 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-03 10:11:10,926 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-03 10:11:11,184 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454.
datanode_1  | 2023-01-03 10:11:14,958 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: receive requestVote(ELECTION, 3719a142-1617-4d0e-a6d8-f47fd68adc81, group-9B236F254454, 2, (t:0, i:0))
datanode_1  | 2023-01-03 10:11:14,959 [grpc-default-executor-2] INFO impl.VoteContext: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FOLLOWER: accept ELECTION from 3719a142-1617-4d0e-a6d8-f47fd68adc81: our priority 0 <= candidate's priority 0
datanode_1  | 2023-01-03 10:11:14,959 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:3719a142-1617-4d0e-a6d8-f47fd68adc81
datanode_1  | 2023-01-03 10:11:14,959 [grpc-default-executor-2] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: shutdown 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState
datanode_1  | 2023-01-03 10:11:14,959 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO impl.FollowerState: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState was interrupted
datanode_1  | 2023-01-03 10:11:14,960 [grpc-default-executor-2] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: start 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState
datanode_1  | 2023-01-03 10:11:14,967 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-03 10:11:14,968 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-03 10:11:14,979 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454 replies to ELECTION vote request: 3719a142-1617-4d0e-a6d8-f47fd68adc81<-74492474-3f40-4050-829e-f08eb3e4f292#0:OK-t2. Peer's state: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454:t2, leader=null, voted=3719a142-1617-4d0e-a6d8-f47fd68adc81, raftlog=Memoized:74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-03 10:11:20,102 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-03 10:11:20,103 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-03 10:11:20,212 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: receive requestVote(ELECTION, 3719a142-1617-4d0e-a6d8-f47fd68adc81, group-9B236F254454, 3, (t:0, i:0))
datanode_1  | 2023-01-03 10:11:20,213 [grpc-default-executor-2] INFO impl.VoteContext: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FOLLOWER: accept ELECTION from 3719a142-1617-4d0e-a6d8-f47fd68adc81: our priority 0 <= candidate's priority 0
datanode_1  | 2023-01-03 10:11:20,214 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:3719a142-1617-4d0e-a6d8-f47fd68adc81
datanode_1  | 2023-01-03 10:11:20,214 [grpc-default-executor-2] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: shutdown 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState
datanode_1  | 2023-01-03 10:11:20,214 [grpc-default-executor-2] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: start 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState
datanode_1  | 2023-01-03 10:11:20,214 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO impl.FollowerState: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState was interrupted
datanode_1  | 2023-01-03 10:11:20,219 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454 replies to ELECTION vote request: 3719a142-1617-4d0e-a6d8-f47fd68adc81<-74492474-3f40-4050-829e-f08eb3e4f292#0:OK-t3. Peer's state: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454:t3, leader=null, voted=3719a142-1617-4d0e-a6d8-f47fd68adc81, raftlog=Memoized:74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-03 10:11:20,263 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-03 10:11:20,264 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-03 10:11:20,323 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: receive requestVote(ELECTION, 482b5314-1cac-4df1-beda-20c463470dc2, group-9B236F254454, 3, (t:0, i:0))
datanode_1  | 2023-01-03 10:11:20,325 [grpc-default-executor-2] INFO impl.VoteContext: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FOLLOWER: reject ELECTION from 482b5314-1cac-4df1-beda-20c463470dc2: already has voted for 3719a142-1617-4d0e-a6d8-f47fd68adc81 at current term 3
datanode_1  | 2023-01-03 10:11:20,326 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454 replies to ELECTION vote request: 482b5314-1cac-4df1-beda-20c463470dc2<-74492474-3f40-4050-829e-f08eb3e4f292#0:FAIL-t3. Peer's state: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454:t3, leader=null, voted=3719a142-1617-4d0e-a6d8-f47fd68adc81, raftlog=Memoized:74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-03 10:11:25,281 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO impl.FollowerState: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5062584348ns, electionTimeout:5013ms
datanode_1  | 2023-01-03 10:11:25,282 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: shutdown 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState
datanode_1  | 2023-01-03 10:11:25,282 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
scm_1       | 2023-01-03 10:10:19,324 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1       | 2023-01-03 10:10:19,393 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9DD877FC0E68,id=8036be97-5432-4d54-a213-e0da532bfc11
scm_1       | 2023-01-03 10:10:19,411 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1       | 2023-01-03 10:10:19,412 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1       | 2023-01-03 10:10:19,413 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1       | 2023-01-03 10:10:19,467 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1       | 2023-01-03 10:10:19,504 [main] INFO server.RaftServer: 8036be97-5432-4d54-a213-e0da532bfc11: start RPC server
scm_1       | 2023-01-03 10:10:19,740 [main] INFO server.GrpcService: 8036be97-5432-4d54-a213-e0da532bfc11: GrpcService started, listening on 9894
scm_1       | 2023-01-03 10:10:19,804 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-8036be97-5432-4d54-a213-e0da532bfc11: Started
scm_1       | 2023-01-03 10:10:24,455 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO impl.FollowerState: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5188566344ns, electionTimeout:5115ms
scm_1       | 2023-01-03 10:10:24,456 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO impl.RoleInfo: 8036be97-5432-4d54-a213-e0da532bfc11: shutdown 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState
scm_1       | 2023-01-03 10:10:24,457 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1       | 2023-01-03 10:10:24,495 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm_1       | 2023-01-03 10:10:24,538 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO impl.RoleInfo: 8036be97-5432-4d54-a213-e0da532bfc11: start 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1
scm_1       | 2023-01-03 10:10:24,593 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO impl.LeaderElection: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[8036be97-5432-4d54-a213-e0da532bfc11|rpc:1898d909ae22:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1       | 2023-01-03 10:10:24,600 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO impl.LeaderElection: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1       | 2023-01-03 10:10:24,608 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO impl.RoleInfo: 8036be97-5432-4d54-a213-e0da532bfc11: shutdown 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1
scm_1       | 2023-01-03 10:10:24,630 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1       | 2023-01-03 10:10:24,671 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: change Leader from null to 8036be97-5432-4d54-a213-e0da532bfc11 at term 1 for becomeLeader, leader elected after 7738ms
scm_1       | 2023-01-03 10:10:24,789 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1       | 2023-01-03 10:10:24,888 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1       | 2023-01-03 10:10:24,916 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1       | 2023-01-03 10:10:25,155 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1       | 2023-01-03 10:10:25,169 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1       | 2023-01-03 10:10:25,189 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1       | 2023-01-03 10:10:25,339 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1       | 2023-01-03 10:10:25,376 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1       | 2023-01-03 10:10:25,576 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO impl.RoleInfo: 8036be97-5432-4d54-a213-e0da532bfc11: start 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderStateImpl
scm_1       | 2023-01-03 10:10:26,232 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker: Starting segment from index:0
scm_1       | 2023-01-03 10:10:26,845 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: set configuration 0: peers:[8036be97-5432-4d54-a213-e0da532bfc11|rpc:1898d909ae22:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1       | 2023-01-03 10:10:27,492 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68/current/log_inprogress_0
scm_1       | 2023-01-03 10:10:27,941 [main] INFO server.RaftServer: 8036be97-5432-4d54-a213-e0da532bfc11: close
scm_1       | 2023-01-03 10:10:27,946 [main] INFO server.GrpcService: 8036be97-5432-4d54-a213-e0da532bfc11: shutdown server GrpcServerProtocolService now
scm_1       | 2023-01-03 10:10:27,975 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: shutdown
scm_1       | 2023-01-03 10:10:27,993 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-9DD877FC0E68,id=8036be97-5432-4d54-a213-e0da532bfc11
scm_1       | 2023-01-03 10:10:27,993 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO impl.RoleInfo: 8036be97-5432-4d54-a213-e0da532bfc11: shutdown 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderStateImpl
om_1        | 2023-01-03 10:11:16,410 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1        | 2023-01-03 10:11:17,793 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1        | 2023-01-03 10:11:18,165 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1        | 2023-01-03 10:11:18,586 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1        | 2023-01-03 10:11:18,587 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1        | 2023-01-03 10:11:18,631 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1        | 2023-01-03 10:11:18,642 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1        | 2023-01-03 10:11:18,784 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1        | 2023-01-03 10:11:18,819 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1        | 2023-01-03 10:11:19,054 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1        | 2023-01-03 10:11:19,385 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1        | 2023-01-03 10:11:19,390 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1        | 2023-01-03 10:11:19,391 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1        | 2023-01-03 10:11:19,393 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1        | 2023-01-03 10:11:19,393 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1        | 2023-01-03 10:11:19,394 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1        | 2023-01-03 10:11:19,396 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1        | 2023-01-03 10:11:19,400 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1        | 2023-01-03 10:11:19,404 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1        | 2023-01-03 10:11:19,405 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1        | 2023-01-03 10:11:19,442 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1        | 2023-01-03 10:11:19,463 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1        | 2023-01-03 10:11:19,467 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1        | 2023-01-03 10:11:20,502 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
om_1        | 2023-01-03 10:11:20,625 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
om_1        | 2023-01-03 10:11:20,626 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
om_1        | 2023-01-03 10:11:20,628 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
om_1        | 2023-01-03 10:11:20,633 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
om_1        | 2023-01-03 10:11:20,651 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
om_1        | 2023-01-03 10:11:20,654 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
om_1        | 2023-01-03 10:11:20,687 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
om_1        | 2023-01-03 10:11:20,704 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
om_1        | 2023-01-03 10:11:20,707 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
om_1        | 2023-01-03 10:11:20,708 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
om_1        | 2023-01-03 10:11:20,910 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1        | 2023-01-03 10:11:20,916 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1        | 2023-01-03 10:11:20,919 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1        | 2023-01-03 10:11:20,923 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1        | 2023-01-03 10:11:20,942 [om1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x23cdc9eb] REGISTERED
om_1        | 2023-01-03 10:11:20,951 [om1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x23cdc9eb] BIND: 0.0.0.0/0.0.0.0:0
om_1        | 2023-01-03 10:11:20,960 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1        | 2023-01-03 10:11:20,965 [om1-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x23cdc9eb, L:/0.0.0.0:44969] ACTIVE
om_1        | 2023-01-03 10:11:20,986 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@30f0b75[Not completed]
om_1        | 2023-01-03 10:11:20,986 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
scm_1       | 2023-01-03 10:10:28,126 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO impl.PendingRequests: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-PendingRequests: sendNotLeaderResponses
scm_1       | 2023-01-03 10:10:28,141 [main] INFO server.GrpcService: 8036be97-5432-4d54-a213-e0da532bfc11: shutdown server GrpcServerProtocolService successfully
scm_1       | 2023-01-03 10:10:28,163 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO impl.StateMachineUpdater: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater: set stopIndex = 0
scm_1       | 2023-01-03 10:10:28,154 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO impl.StateMachineUpdater: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater: Took a snapshot at index 0
scm_1       | 2023-01-03 10:10:28,207 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO impl.StateMachineUpdater: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1       | 2023-01-03 10:10:28,249 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: closes. applyIndex: 0
scm_1       | 2023-01-03 10:10:28,257 [8036be97-5432-4d54-a213-e0da532bfc11-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x9b287a69, L:/0.0.0.0:34509] CLOSE
scm_1       | 2023-01-03 10:10:28,262 [8036be97-5432-4d54-a213-e0da532bfc11-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x9b287a69, L:/0.0.0.0:34509] INACTIVE
scm_1       | 2023-01-03 10:10:28,262 [8036be97-5432-4d54-a213-e0da532bfc11-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x9b287a69, L:/0.0.0.0:34509] UNREGISTERED
scm_1       | 2023-01-03 10:10:28,264 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm_1       | 2023-01-03 10:10:28,619 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-8036be97-5432-4d54-a213-e0da532bfc11: Detected pause in JVM or host machine (eg GC): pause of approximately 130491736ns.
scm_1       | GC pool 'ParNew' had collection(s): count=1 time=195ms
scm_1       | 2023-01-03 10:10:28,762 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker close()
scm_1       | 2023-01-03 10:10:28,774 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-8036be97-5432-4d54-a213-e0da532bfc11: Stopped
scm_1       | 2023-01-03 10:10:28,781 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2023-01-03 10:10:28,819 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-14143503-c595-4722-b95c-9dd877fc0e68; layoutVersion=4; scmId=8036be97-5432-4d54-a213-e0da532bfc11
scm_1       | 2023-01-03 10:10:29,077 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1       | /************************************************************
scm_1       | SHUTDOWN_MSG: Shutting down StorageContainerManager at 1898d909ae22/172.18.0.5
scm_1       | ************************************************************/
scm_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1       | 2023-01-03 10:10:36,530 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1       | /************************************************************
scm_1       | STARTUP_MSG: Starting StorageContainerManager
scm_1       | STARTUP_MSG:   host = 1898d909ae22/172.18.0.5
scm_1       | STARTUP_MSG:   args = []
scm_1       | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1       | 2023-01-03 10:09:44,797 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1       | 2023-01-03 10:09:44,798 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1       | 2023-01-03 10:09:45,525 [main] INFO util.log: Logging initialized @16107ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1       | 2023-01-03 10:09:46,912 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1       | 2023-01-03 10:09:47,352 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1       | 2023-01-03 10:09:47,463 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1       | 2023-01-03 10:09:47,515 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1       | 2023-01-03 10:09:47,519 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1       | 2023-01-03 10:09:47,519 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1       | 2023-01-03 10:09:48,648 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1       | /************************************************************
s3g_1       | STARTUP_MSG: Starting Gateway
s3g_1       | STARTUP_MSG:   host = 585031d59482/172.18.0.2
s3g_1       | STARTUP_MSG:   args = []
s3g_1       | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.52.Final.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.52.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1       | STARTUP_MSG:   build = https://github.com/apache/ozone/1c830c5bc7b0be75001a5e8bcef69a8810ca74f1 ; compiled by 'runner' on 2023-01-03T09:55Z
s3g_1       | STARTUP_MSG:   java = 11.0.14.1
s3g_1       | ************************************************************/
s3g_1       | 2023-01-03 10:09:48,716 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1       | 2023-01-03 10:09:49,002 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1       | 2023-01-03 10:09:50,108 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1       | 2023-01-03 10:09:51,435 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1       | 2023-01-03 10:09:51,436 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1       | 2023-01-03 10:09:51,712 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1       | 2023-01-03 10:09:51,759 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1       | 2023-01-03 10:09:52,028 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1       | 2023-01-03 10:09:52,028 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1       | 2023-01-03 10:09:52,074 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1       | 2023-01-03 10:09:52,418 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@63192798{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1       | 2023-01-03 10:09:52,456 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@51850751{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1       | WARNING: An illegal reflective access operation has occurred
s3g_1       | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1       | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1       | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1       | WARNING: All illegal access operations will be denied in a future release
s3g_1       | Jan 03, 2023 10:10:27 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1       | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1       | 
s3g_1       | 2023-01-03 10:10:27,130 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1a43a88e{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-14903955371826172866/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1       | 2023-01-03 10:10:27,246 [main] INFO server.AbstractConnector: Started ServerConnector@1a1da881{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1       | 2023-01-03 10:10:27,253 [main] INFO server.Server: Started @57836ms
s3g_1       | 2023-01-03 10:10:27,262 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1       | 2023-01-03 10:10:27,262 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1       | 2023-01-03 10:10:27,277 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
datanode_3  | 2023-01-03 10:11:11,457 [pool-22-thread-1] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2023-01-03 10:11:11,458 [pool-22-thread-1] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-FollowerState
datanode_3  | 2023-01-03 10:11:11,458 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4562A4FBD783,id=3719a142-1617-4d0e-a6d8-f47fd68adc81
datanode_3  | 2023-01-03 10:11:11,458 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2023-01-03 10:11:11,458 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2023-01-03 10:11:11,459 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2023-01-03 10:11:11,459 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3  | 2023-01-03 10:11:11,459 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=b7fb08b7-ba15-458b-85fd-4562a4fbd783
datanode_3  | 2023-01-03 10:11:11,460 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=b7fb08b7-ba15-458b-85fd-4562a4fbd783.
datanode_3  | 2023-01-03 10:11:11,479 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-03 10:11:11,481 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-03 10:11:14,916 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO impl.FollowerState: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5169085445ns, electionTimeout:5111ms
datanode_3  | 2023-01-03 10:11:14,917 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: shutdown 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState
datanode_3  | 2023-01-03 10:11:14,917 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3  | 2023-01-03 10:11:14,917 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3  | 2023-01-03 10:11:14,918 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2
datanode_3  | 2023-01-03 10:11:14,928 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-03 10:11:14,932 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-03 10:11:14,932 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-03 10:11:14,992 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3  | 2023-01-03 10:11:14,992 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection:   Response 0: 3719a142-1617-4d0e-a6d8-f47fd68adc81<-74492474-3f40-4050-829e-f08eb3e4f292#0:OK-t2
datanode_3  | 2023-01-03 10:11:14,992 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection:   Response 1: 3719a142-1617-4d0e-a6d8-f47fd68adc81<-482b5314-1cac-4df1-beda-20c463470dc2#0:FAIL-t2
datanode_3  | 2023-01-03 10:11:14,993 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2 ELECTION round 0: result REJECTED
datanode_3  | 2023-01-03 10:11:14,993 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_3  | 2023-01-03 10:11:14,993 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: shutdown 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2
datanode_3  | 2023-01-03 10:11:14,994 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection2] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState
datanode_3  | 2023-01-03 10:11:15,001 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-03 10:11:15,030 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-03 10:11:16,632 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-FollowerState] INFO impl.FollowerState: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5173968125ns, electionTimeout:5148ms
datanode_3  | 2023-01-03 10:11:16,632 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-FollowerState] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: shutdown 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-FollowerState
datanode_3  | 2023-01-03 10:11:16,633 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-FollowerState] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3  | 2023-01-03 10:11:16,633 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3  | 2023-01-03 10:11:16,633 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-FollowerState] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3
datanode_3  | 2023-01-03 10:11:16,638 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO impl.LeaderElection: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-03 10:11:16,639 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO impl.LeaderElection: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3 ELECTION round 0: result PASSED (term=1)
datanode_3  | 2023-01-03 10:11:16,639 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: shutdown 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3
datanode_3  | 2023-01-03 10:11:16,639 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3  | 2023-01-03 10:11:16,639 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4562A4FBD783 with new leaderId: 3719a142-1617-4d0e-a6d8-f47fd68adc81
datanode_3  | 2023-01-03 10:11:16,640 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783: change Leader from null to 3719a142-1617-4d0e-a6d8-f47fd68adc81 at term 1 for becomeLeader, leader elected after 5406ms
datanode_3  | 2023-01-03 10:11:16,655 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3  | 2023-01-03 10:11:16,673 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3  | 2023-01-03 10:11:16,675 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3  | 2023-01-03 10:11:16,696 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3  | 2023-01-03 10:11:16,697 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3  | 2023-01-03 10:11:16,700 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3  | 2023-01-03 10:11:16,724 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3  | 2023-01-03 10:11:16,729 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3  | 2023-01-03 10:11:16,738 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderStateImpl
datanode_3  | 2023-01-03 10:11:16,804 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2023-01-03 10:11:16,848 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-LeaderElection3] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783: set configuration 0: peers:[3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-03 10:11:17,053 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-4562A4FBD783-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b7fb08b7-ba15-458b-85fd-4562a4fbd783/current/log_inprogress_0
datanode_3  | 2023-01-03 10:11:20,194 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO impl.FollowerState: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5199845288ns, electionTimeout:5163ms
datanode_3  | 2023-01-03 10:11:20,194 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: shutdown 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState
datanode_3  | 2023-01-03 10:11:20,194 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_3  | 2023-01-03 10:11:20,195 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3  | 2023-01-03 10:11:20,195 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4
datanode_3  | 2023-01-03 10:11:20,204 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4] INFO impl.LeaderElection: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4 ELECTION round 0: submit vote requests at term 3 for -1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-03 10:11:20,205 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-03 10:11:20,206 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-03 10:11:20,267 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4] INFO impl.LeaderElection: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3  | 2023-01-03 10:11:20,267 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4] INFO impl.LeaderElection:   Response 0: 3719a142-1617-4d0e-a6d8-f47fd68adc81<-74492474-3f40-4050-829e-f08eb3e4f292#0:OK-t3
datanode_3  | 2023-01-03 10:11:20,267 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4] INFO impl.LeaderElection:   Response 1: 3719a142-1617-4d0e-a6d8-f47fd68adc81<-482b5314-1cac-4df1-beda-20c463470dc2#0:FAIL-t3
datanode_3  | 2023-01-03 10:11:20,268 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4] INFO impl.LeaderElection: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4 ELECTION round 0: result REJECTED
datanode_3  | 2023-01-03 10:11:20,268 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
recon_1     | 2023-01-03 10:10:53,571 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1     | 2023-01-03 10:10:53,573 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1     | 2023-01-03 10:10:53,852 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1     | 2023-01-03 10:10:53,867 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1     | 2023-01-03 10:10:53,898 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 219 milliseconds.
recon_1     | 2023-01-03 10:10:53,989 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1     | 2023-01-03 10:10:54,677 [IPC Server handler 15 on default port 9891] INFO ipc.Server: IPC Server handler 15 on default port 9891: skipped Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.9:60982
recon_1     | 2023-01-03 10:10:54,677 [IPC Server handler 16 on default port 9891] INFO ipc.Server: IPC Server handler 16 on default port 9891: skipped Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.8:48062
recon_1     | 2023-01-03 10:10:54,678 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891: skipped Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.10:38766
recon_1     | 2023-01-03 10:10:54,907 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 868 milliseconds to process 0 existing database records.
recon_1     | 2023-01-03 10:10:55,013 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 105 milliseconds for processing 0 containers.
recon_1     | 2023-01-03 10:10:55,895 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.10:45574: output error
recon_1     | 2023-01-03 10:10:55,908 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.7:59112: output error
recon_1     | 2023-01-03 10:10:55,908 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.6:60124: output error
recon_1     | 2023-01-03 10:10:55,908 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.10:38758: output error
recon_1     | 2023-01-03 10:10:55,908 [IPC Server handler 17 on default port 9891] WARN ipc.Server: IPC Server handler 17 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.7:46290: output error
recon_1     | 2023-01-03 10:10:55,902 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.9:32866: output error
recon_1     | 2023-01-03 10:10:55,902 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.8:48054: output error
recon_1     | 2023-01-03 10:10:55,902 [IPC Server handler 12 on default port 9891] WARN ipc.Server: IPC Server handler 12 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.10:58734: output error
recon_1     | 2023-01-03 10:10:55,902 [IPC Server handler 13 on default port 9891] WARN ipc.Server: IPC Server handler 13 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.8:36008: output error
recon_1     | 2023-01-03 10:10:55,901 [IPC Server handler 18 on default port 9891] WARN ipc.Server: IPC Server handler 18 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.6:48992: output error
recon_1     | 2023-01-03 10:10:55,901 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.7:59100: output error
recon_1     | 2023-01-03 10:10:55,896 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.8:38702: output error
recon_1     | 2023-01-03 10:10:55,918 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.6:40176: output error
recon_1     | 2023-01-03 10:10:56,034 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
scm_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1       | STARTUP_MSG:   build = https://github.com/apache/ozone/1c830c5bc7b0be75001a5e8bcef69a8810ca74f1 ; compiled by 'runner' on 2023-01-03T09:55Z
scm_1       | STARTUP_MSG:   java = 11.0.14.1
scm_1       | ************************************************************/
scm_1       | 2023-01-03 10:10:36,563 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1       | 2023-01-03 10:10:36,712 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2023-01-03 10:10:36,845 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1       | 2023-01-03 10:10:36,879 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1       | 2023-01-03 10:10:38,249 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2023-01-03 10:10:38,740 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2023-01-03 10:10:39,531 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm_1       | 2023-01-03 10:10:39,533 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1       | 2023-01-03 10:10:39,678 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1       | 2023-01-03 10:10:39,722 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:8036be97-5432-4d54-a213-e0da532bfc11
scm_1       | 2023-01-03 10:10:39,951 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1       | 2023-01-03 10:10:40,158 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1       | 2023-01-03 10:10:40,160 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1       | 2023-01-03 10:10:40,165 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1       | 2023-01-03 10:10:40,168 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1       | 2023-01-03 10:10:40,168 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1       | 2023-01-03 10:10:40,169 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1       | 2023-01-03 10:10:40,171 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1       | 2023-01-03 10:10:40,181 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | 2023-01-03 10:10:40,185 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1       | 2023-01-03 10:10:40,187 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1       | 2023-01-03 10:10:40,209 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1       | 2023-01-03 10:10:40,220 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1       | 2023-01-03 10:10:40,222 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1       | 2023-01-03 10:10:40,699 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode_3  | 2023-01-03 10:11:20,268 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: shutdown 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4
datanode_3  | 2023-01-03 10:11:20,268 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-LeaderElection4] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState
datanode_3  | 2023-01-03 10:11:20,269 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-03 10:11:20,271 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-03 10:11:20,316 [grpc-default-executor-0] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: receive requestVote(ELECTION, 482b5314-1cac-4df1-beda-20c463470dc2, group-9B236F254454, 3, (t:0, i:0))
datanode_3  | 2023-01-03 10:11:20,317 [grpc-default-executor-0] INFO impl.VoteContext: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FOLLOWER: reject ELECTION from 482b5314-1cac-4df1-beda-20c463470dc2: already has voted for 3719a142-1617-4d0e-a6d8-f47fd68adc81 at current term 3
datanode_3  | 2023-01-03 10:11:20,317 [grpc-default-executor-0] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454 replies to ELECTION vote request: 482b5314-1cac-4df1-beda-20c463470dc2<-3719a142-1617-4d0e-a6d8-f47fd68adc81#0:FAIL-t3. Peer's state: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454:t3, leader=null, voted=3719a142-1617-4d0e-a6d8-f47fd68adc81, raftlog=Memoized:3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-03 10:11:25,295 [grpc-default-executor-0] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: receive requestVote(ELECTION, 74492474-3f40-4050-829e-f08eb3e4f292, group-9B236F254454, 4, (t:0, i:0))
datanode_3  | 2023-01-03 10:11:25,296 [grpc-default-executor-0] INFO impl.VoteContext: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FOLLOWER: accept ELECTION from 74492474-3f40-4050-829e-f08eb3e4f292: our priority 0 <= candidate's priority 0
datanode_3  | 2023-01-03 10:11:25,296 [grpc-default-executor-0] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:74492474-3f40-4050-829e-f08eb3e4f292
datanode_3  | 2023-01-03 10:11:25,296 [grpc-default-executor-0] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: shutdown 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState
datanode_3  | 2023-01-03 10:11:25,296 [grpc-default-executor-0] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState
datanode_3  | 2023-01-03 10:11:25,296 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO impl.FollowerState: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState was interrupted
datanode_3  | 2023-01-03 10:11:25,330 [grpc-default-executor-0] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454 replies to ELECTION vote request: 74492474-3f40-4050-829e-f08eb3e4f292<-3719a142-1617-4d0e-a6d8-f47fd68adc81#0:OK-t4. Peer's state: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454:t4, leader=null, voted=74492474-3f40-4050-829e-f08eb3e4f292, raftlog=Memoized:3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-03 10:11:25,334 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-03 10:11:25,335 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-03 10:11:30,416 [grpc-default-executor-0] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: receive requestVote(ELECTION, 482b5314-1cac-4df1-beda-20c463470dc2, group-9B236F254454, 5, (t:0, i:0))
datanode_3  | 2023-01-03 10:11:30,417 [grpc-default-executor-0] INFO impl.VoteContext: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FOLLOWER: accept ELECTION from 482b5314-1cac-4df1-beda-20c463470dc2: our priority 0 <= candidate's priority 1
datanode_3  | 2023-01-03 10:11:30,422 [grpc-default-executor-0] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:482b5314-1cac-4df1-beda-20c463470dc2
datanode_3  | 2023-01-03 10:11:30,422 [grpc-default-executor-0] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: shutdown 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState
datanode_3  | 2023-01-03 10:11:30,423 [grpc-default-executor-0] INFO impl.RoleInfo: 3719a142-1617-4d0e-a6d8-f47fd68adc81: start 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState
datanode_3  | 2023-01-03 10:11:30,428 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3  | 2023-01-03 10:11:30,429 [grpc-default-executor-0] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454 replies to ELECTION vote request: 482b5314-1cac-4df1-beda-20c463470dc2<-3719a142-1617-4d0e-a6d8-f47fd68adc81#0:OK-t5. Peer's state: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454:t5, leader=null, voted=482b5314-1cac-4df1-beda-20c463470dc2, raftlog=Memoized:3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-03 10:11:30,430 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO impl.FollowerState: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState: Stopping now (isRunning? false, role = FOLLOWER)
datanode_3  | 2023-01-03 10:11:30,451 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3  | 2023-01-03 10:11:30,685 [3719a142-1617-4d0e-a6d8-f47fd68adc81-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9B236F254454 with new leaderId: 482b5314-1cac-4df1-beda-20c463470dc2
datanode_3  | 2023-01-03 10:11:30,685 [3719a142-1617-4d0e-a6d8-f47fd68adc81-server-thread1] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: change Leader from null to 482b5314-1cac-4df1-beda-20c463470dc2 at term 5 for appendEntries, leader elected after 28625ms
datanode_3  | 2023-01-03 10:11:30,738 [3719a142-1617-4d0e-a6d8-f47fd68adc81-server-thread1] INFO server.RaftServer$Division: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454: set configuration 0: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3  | 2023-01-03 10:11:30,738 [3719a142-1617-4d0e-a6d8-f47fd68adc81-server-thread1] INFO segmented.SegmentedRaftLogWorker: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2023-01-03 10:11:30,754 [3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3719a142-1617-4d0e-a6d8-f47fd68adc81@group-9B236F254454-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454/current/log_inprogress_0
datanode_1  | 2023-01-03 10:11:25,282 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1  | 2023-01-03 10:11:25,283 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: start 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3
datanode_1  | 2023-01-03 10:11:25,286 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3] INFO impl.LeaderElection: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for -1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-03 10:11:25,288 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-03 10:11:25,288 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-03 10:11:25,341 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3] INFO impl.LeaderElection: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1  | 2023-01-03 10:11:25,341 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3] INFO impl.LeaderElection:   Response 0: 74492474-3f40-4050-829e-f08eb3e4f292<-482b5314-1cac-4df1-beda-20c463470dc2#0:FAIL-t4
datanode_1  | 2023-01-03 10:11:25,341 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3] INFO impl.LeaderElection: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3 ELECTION round 0: result REJECTED
datanode_1  | 2023-01-03 10:11:25,342 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: changes role from CANDIDATE to FOLLOWER at term 4 for REJECTED
datanode_1  | 2023-01-03 10:11:25,344 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: shutdown 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3
datanode_1  | 2023-01-03 10:11:25,345 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-LeaderElection3] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: start 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState
datanode_1  | 2023-01-03 10:11:25,358 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-03 10:11:25,359 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-03 10:11:11,261 [pool-22-thread-1] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2023-01-03 10:11:11,261 [pool-22-thread-1] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: start 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-FollowerState
datanode_2  | 2023-01-03 10:11:11,262 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-06CFDBB51D5D,id=482b5314-1cac-4df1-beda-20c463470dc2
datanode_2  | 2023-01-03 10:11:11,262 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2023-01-03 10:11:11,263 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2023-01-03 10:11:11,264 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2023-01-03 10:11:11,265 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2  | 2023-01-03 10:11:11,266 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2  | 2023-01-03 10:11:11,267 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-03 10:11:11,267 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=22c9380f-296a-4792-92f3-06cfdbb51d5d
datanode_2  | 2023-01-03 10:11:11,267 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=22c9380f-296a-4792-92f3-06cfdbb51d5d.
datanode_2  | 2023-01-03 10:11:14,945 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: receive requestVote(ELECTION, 3719a142-1617-4d0e-a6d8-f47fd68adc81, group-9B236F254454, 2, (t:0, i:0))
datanode_2  | 2023-01-03 10:11:14,946 [grpc-default-executor-0] INFO impl.VoteContext: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FOLLOWER: reject ELECTION from 3719a142-1617-4d0e-a6d8-f47fd68adc81: our priority 1 > candidate's priority 0
datanode_2  | 2023-01-03 10:11:14,946 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:3719a142-1617-4d0e-a6d8-f47fd68adc81
datanode_2  | 2023-01-03 10:11:14,946 [grpc-default-executor-0] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: shutdown 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState
datanode_2  | 2023-01-03 10:11:14,947 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO impl.FollowerState: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState was interrupted
datanode_2  | 2023-01-03 10:11:14,947 [grpc-default-executor-0] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: start 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState
datanode_2  | 2023-01-03 10:11:14,948 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2  | 2023-01-03 10:11:14,949 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-03 10:11:14,950 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454 replies to ELECTION vote request: 3719a142-1617-4d0e-a6d8-f47fd68adc81<-482b5314-1cac-4df1-beda-20c463470dc2#0:FAIL-t2. Peer's state: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454:t2, leader=null, voted=null, raftlog=Memoized:482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-03 10:11:16,406 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-FollowerState] INFO impl.FollowerState: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5144483812ns, electionTimeout:5136ms
datanode_2  | 2023-01-03 10:11:16,407 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-FollowerState] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: shutdown 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-FollowerState
datanode_2  | 2023-01-03 10:11:16,409 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-FollowerState] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2  | 2023-01-03 10:11:16,418 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2  | 2023-01-03 10:11:16,418 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-FollowerState] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: start 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1
datanode_2  | 2023-01-03 10:11:16,446 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO impl.LeaderElection: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-03 10:11:16,448 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO impl.LeaderElection: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2  | 2023-01-03 10:11:16,459 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: shutdown 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1
datanode_2  | 2023-01-03 10:11:16,465 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2  | 2023-01-03 10:11:16,468 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-06CFDBB51D5D with new leaderId: 482b5314-1cac-4df1-beda-20c463470dc2
datanode_2  | 2023-01-03 10:11:16,471 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D: change Leader from null to 482b5314-1cac-4df1-beda-20c463470dc2 at term 1 for becomeLeader, leader elected after 5338ms
datanode_2  | 2023-01-03 10:11:16,527 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.3.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.77.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.2-8b8bdda-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.77.Final.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_5  | STARTUP_MSG:   build = https://github.com/apache/ozone/1c830c5bc7b0be75001a5e8bcef69a8810ca74f1 ; compiled by 'runner' on 2023-01-03T09:55Z
datanode_5  | STARTUP_MSG:   java = 11.0.14.1
datanode_5  | ************************************************************/
datanode_5  | 2023-01-03 10:09:44,198 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_5  | 2023-01-03 10:09:44,868 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_5  | 2023-01-03 10:09:45,786 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_5  | 2023-01-03 10:09:47,299 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_5  | 2023-01-03 10:09:47,316 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_5  | 2023-01-03 10:09:48,527 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:8e31dcea51a8 ip:172.18.0.6
datanode_5  | 2023-01-03 10:09:52,128 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_5  | 2023-01-03 10:09:53,561 [main] INFO reflections.Reflections: Reflections took 1175 ms to scan 2 urls, producing 97 keys and 217 values 
datanode_5  | 2023-01-03 10:09:54,622 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_5  | 2023-01-03 10:09:56,550 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_5  | 2023-01-03 10:09:56,735 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_5  | 2023-01-03 10:09:56,761 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_5  | 2023-01-03 10:09:56,795 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_5  | 2023-01-03 10:09:57,321 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_5  | 2023-01-03 10:09:57,495 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5  | 2023-01-03 10:09:57,500 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_5  | 2023-01-03 10:09:57,520 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_5  | 2023-01-03 10:09:57,521 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_5  | 2023-01-03 10:09:57,531 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_5  | 2023-01-03 10:09:57,900 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_5  | 2023-01-03 10:09:57,919 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_5  | 2023-01-03 10:10:09,987 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_5  | 2023-01-03 10:10:11,648 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5  | 2023-01-03 10:10:12,329 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_5  | 2023-01-03 10:10:13,488 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_5  | 2023-01-03 10:10:13,517 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_5  | 2023-01-03 10:10:13,518 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_5  | 2023-01-03 10:10:13,532 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_5  | 2023-01-03 10:10:13,533 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_5  | 2023-01-03 10:10:13,541 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_5  | 2023-01-03 10:10:13,562 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_5  | 2023-01-03 10:10:13,563 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5  | 2023-01-03 10:10:13,577 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_5  | 2023-01-03 10:10:13,578 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5  | 2023-01-03 10:10:13,691 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5  | 2023-01-03 10:10:13,718 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_5  | 2023-01-03 10:10:13,747 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_5  | 2023-01-03 10:10:16,533 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = NETTY (custom)
datanode_5  | 2023-01-03 10:10:16,815 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
datanode_5  | 2023-01-03 10:10:16,828 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
datanode_5  | 2023-01-03 10:10:16,828 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
datanode_5  | 2023-01-03 10:10:16,844 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
datanode_5  | 2023-01-03 10:10:16,956 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
datanode_5  | 2023-01-03 10:10:16,956 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
datanode_5  | 2023-01-03 10:10:17,017 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
datanode_5  | 2023-01-03 10:10:17,018 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
datanode_5  | 2023-01-03 10:10:17,036 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
datanode_5  | 2023-01-03 10:10:17,037 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
datanode_5  | 2023-01-03 10:10:17,617 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_5  | 2023-01-03 10:10:17,623 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_5  | 2023-01-03 10:10:17,657 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5  | 2023-01-03 10:10:17,669 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5  | 2023-01-03 10:10:17,730 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5  | 2023-01-03 10:10:17,818 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x42c9a2f0] REGISTERED
datanode_5  | 2023-01-03 10:10:17,896 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x42c9a2f0] BIND: 0.0.0.0/0.0.0.0:0
datanode_5  | 2023-01-03 10:10:17,936 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x42c9a2f0, L:/0.0.0.0:44527] ACTIVE
datanode_5  | 2023-01-03 10:10:18,168 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_5  | 2023-01-03 10:10:20,648 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_5  | 2023-01-03 10:10:20,754 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_5  | 2023-01-03 10:10:21,055 [main] INFO util.log: Logging initialized @51717ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_5  | 2023-01-03 10:10:22,101 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_5  | 2023-01-03 10:10:22,220 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_5  | 2023-01-03 10:10:22,334 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_5  | 2023-01-03 10:10:22,376 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_5  | 2023-01-03 10:10:22,376 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_5  | 2023-01-03 10:10:22,403 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_5  | 2023-01-03 10:10:22,982 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_5  | 2023-01-03 10:10:22,999 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_5  | 2023-01-03 10:10:23,425 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_5  | 2023-01-03 10:10:23,451 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_5  | 2023-01-03 10:10:23,466 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_5  | 2023-01-03 10:10:23,615 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5e557671{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_5  | 2023-01-03 10:10:23,628 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7962c1d5{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_5  | 2023-01-03 10:10:24,714 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@34588991{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-11131132678626274735/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_5  | 2023-01-03 10:10:24,813 [main] INFO server.AbstractConnector: Started ServerConnector@2dd46693{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_5  | 2023-01-03 10:10:24,813 [main] INFO server.Server: Started @55475ms
datanode_5  | 2023-01-03 10:10:24,823 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_5  | 2023-01-03 10:10:24,823 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_5  | 2023-01-03 10:10:24,857 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_5  | 2023-01-03 10:10:24,892 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_5  | 2023-01-03 10:10:25,392 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5ad2830c] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_5  | 2023-01-03 10:10:26,399 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.18.0.3:9891
datanode_5  | 2023-01-03 10:10:27,224 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_5  | 2023-01-03 10:10:29,480 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:29,483 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:30,481 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:30,484 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:31,482 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:31,485 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:32,483 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:32,486 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:33,484 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:33,487 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:34,485 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2  | 2023-01-03 10:11:16,541 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2  | 2023-01-03 10:11:16,545 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2  | 2023-01-03 10:11:16,566 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2  | 2023-01-03 10:11:16,566 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2  | 2023-01-03 10:11:16,567 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2  | 2023-01-03 10:11:16,591 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2  | 2023-01-03 10:11:16,602 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2  | 2023-01-03 10:11:16,612 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: start 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderStateImpl
datanode_2  | 2023-01-03 10:11:16,742 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2  | 2023-01-03 10:11:16,871 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-LeaderElection1] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D: set configuration 0: peers:[482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-03 10:11:17,212 [482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 482b5314-1cac-4df1-beda-20c463470dc2@group-06CFDBB51D5D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/22c9380f-296a-4792-92f3-06cfdbb51d5d/current/log_inprogress_0
datanode_2  | 2023-01-03 10:11:20,069 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO impl.FollowerState: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5121750616ns, electionTimeout:5119ms
datanode_2  | 2023-01-03 10:11:20,069 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: shutdown 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState
datanode_2  | 2023-01-03 10:11:20,070 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_2  | 2023-01-03 10:11:20,070 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2  | 2023-01-03 10:11:20,070 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: start 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2
datanode_2  | 2023-01-03 10:11:20,081 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2 ELECTION round 0: submit vote requests at term 3 for -1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-03 10:11:20,089 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2  | 2023-01-03 10:11:20,092 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-03 10:11:20,091 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 74492474-3f40-4050-829e-f08eb3e4f292
datanode_2  | 2023-01-03 10:11:20,092 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 3719a142-1617-4d0e-a6d8-f47fd68adc81
datanode_2  | 2023-01-03 10:11:20,245 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: receive requestVote(ELECTION, 3719a142-1617-4d0e-a6d8-f47fd68adc81, group-9B236F254454, 3, (t:0, i:0))
datanode_2  | 2023-01-03 10:11:20,249 [grpc-default-executor-0] INFO impl.VoteContext: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-CANDIDATE: reject ELECTION from 3719a142-1617-4d0e-a6d8-f47fd68adc81: already has voted for 482b5314-1cac-4df1-beda-20c463470dc2 at current term 3
datanode_2  | 2023-01-03 10:11:20,250 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454 replies to ELECTION vote request: 3719a142-1617-4d0e-a6d8-f47fd68adc81<-482b5314-1cac-4df1-beda-20c463470dc2#0:FAIL-t3. Peer's state: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454:t3, leader=null, voted=482b5314-1cac-4df1-beda-20c463470dc2, raftlog=Memoized:482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-03 10:11:20,347 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_2  | 2023-01-03 10:11:20,348 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection:   Response 0: 482b5314-1cac-4df1-beda-20c463470dc2<-74492474-3f40-4050-829e-f08eb3e4f292#0:FAIL-t3
datanode_2  | 2023-01-03 10:11:20,349 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection:   Response 1: 482b5314-1cac-4df1-beda-20c463470dc2<-3719a142-1617-4d0e-a6d8-f47fd68adc81#0:FAIL-t3
datanode_2  | 2023-01-03 10:11:20,349 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2] INFO impl.LeaderElection: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2 ELECTION round 0: result REJECTED
datanode_2  | 2023-01-03 10:11:20,349 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode_2  | 2023-01-03 10:11:20,349 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: shutdown 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2
datanode_2  | 2023-01-03 10:11:20,350 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection2] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: start 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState
datanode_2  | 2023-01-03 10:11:20,369 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2  | 2023-01-03 10:11:20,370 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-03 10:11:25,310 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: receive requestVote(ELECTION, 74492474-3f40-4050-829e-f08eb3e4f292, group-9B236F254454, 4, (t:0, i:0))
datanode_2  | 2023-01-03 10:11:25,311 [grpc-default-executor-0] INFO impl.VoteContext: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FOLLOWER: reject ELECTION from 74492474-3f40-4050-829e-f08eb3e4f292: our priority 1 > candidate's priority 0
datanode_2  | 2023-01-03 10:11:25,311 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:74492474-3f40-4050-829e-f08eb3e4f292
datanode_2  | 2023-01-03 10:11:25,311 [grpc-default-executor-0] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: shutdown 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState
datanode_2  | 2023-01-03 10:11:25,311 [grpc-default-executor-0] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: start 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState
datanode_2  | 2023-01-03 10:11:25,312 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO impl.FollowerState: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState was interrupted
datanode_2  | 2023-01-03 10:11:25,326 [grpc-default-executor-0] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454 replies to ELECTION vote request: 74492474-3f40-4050-829e-f08eb3e4f292<-482b5314-1cac-4df1-beda-20c463470dc2#0:FAIL-t4. Peer's state: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454:t4, leader=null, voted=null, raftlog=Memoized:482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-03 10:11:25,338 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2  | 2023-01-03 10:11:25,338 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-03 10:11:30,403 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO impl.FollowerState: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5091060649ns, electionTimeout:5064ms
datanode_2  | 2023-01-03 10:11:30,403 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: shutdown 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState
datanode_2  | 2023-01-03 10:11:30,403 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
datanode_2  | 2023-01-03 10:11:30,403 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2  | 2023-01-03 10:11:30,403 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-FollowerState] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: start 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3
datanode_2  | 2023-01-03 10:11:30,410 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO impl.LeaderElection: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3 ELECTION round 0: submit vote requests at term 5 for -1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2  | 2023-01-03 10:11:30,411 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2  | 2023-01-03 10:11:30,411 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2  | 2023-01-03 10:11:30,433 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO impl.LeaderElection: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2  | 2023-01-03 10:11:30,433 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO impl.LeaderElection:   Response 0: 482b5314-1cac-4df1-beda-20c463470dc2<-3719a142-1617-4d0e-a6d8-f47fd68adc81#0:OK-t5
datanode_2  | 2023-01-03 10:11:30,434 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO impl.LeaderElection: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3 ELECTION round 0: result PASSED
datanode_2  | 2023-01-03 10:11:30,434 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: shutdown 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3
datanode_2  | 2023-01-03 10:11:30,434 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: changes role from CANDIDATE to LEADER at term 5 for changeToLeader
datanode_2  | 2023-01-03 10:11:30,434 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9B236F254454 with new leaderId: 482b5314-1cac-4df1-beda-20c463470dc2
datanode_1  | 2023-01-03 10:11:30,418 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: receive requestVote(ELECTION, 482b5314-1cac-4df1-beda-20c463470dc2, group-9B236F254454, 5, (t:0, i:0))
datanode_1  | 2023-01-03 10:11:30,419 [grpc-default-executor-2] INFO impl.VoteContext: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FOLLOWER: accept ELECTION from 482b5314-1cac-4df1-beda-20c463470dc2: our priority 0 <= candidate's priority 1
datanode_1  | 2023-01-03 10:11:30,419 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:482b5314-1cac-4df1-beda-20c463470dc2
datanode_1  | 2023-01-03 10:11:30,419 [grpc-default-executor-2] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: shutdown 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState
datanode_1  | 2023-01-03 10:11:30,419 [grpc-default-executor-2] INFO impl.RoleInfo: 74492474-3f40-4050-829e-f08eb3e4f292: start 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState
datanode_1  | 2023-01-03 10:11:30,420 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO impl.FollowerState: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState was interrupted
datanode_1  | 2023-01-03 10:11:30,421 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1  | 2023-01-03 10:11:30,421 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1  | 2023-01-03 10:11:30,426 [grpc-default-executor-2] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454 replies to ELECTION vote request: 482b5314-1cac-4df1-beda-20c463470dc2<-74492474-3f40-4050-829e-f08eb3e4f292#0:OK-t5. Peer's state: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454:t5, leader=null, voted=482b5314-1cac-4df1-beda-20c463470dc2, raftlog=Memoized:74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-03 10:11:30,714 [74492474-3f40-4050-829e-f08eb3e4f292-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9B236F254454 with new leaderId: 482b5314-1cac-4df1-beda-20c463470dc2
datanode_1  | 2023-01-03 10:11:30,714 [74492474-3f40-4050-829e-f08eb3e4f292-server-thread1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: change Leader from null to 482b5314-1cac-4df1-beda-20c463470dc2 at term 5 for appendEntries, leader elected after 27439ms
datanode_1  | 2023-01-03 10:11:30,720 [74492474-3f40-4050-829e-f08eb3e4f292-server-thread1] INFO server.RaftServer$Division: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454: set configuration 0: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1  | 2023-01-03 10:11:30,725 [74492474-3f40-4050-829e-f08eb3e4f292-server-thread1] INFO segmented.SegmentedRaftLogWorker: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2023-01-03 10:11:30,733 [74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 74492474-3f40-4050-829e-f08eb3e4f292@group-9B236F254454-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454/current/log_inprogress_0
om_1        | 2023-01-03 10:11:21,054 [pool-26-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1        | 2023-01-03 10:11:21,070 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1        | 2023-01-03 10:11:21,074 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1        | 2023-01-03 10:11:21,075 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1        | 2023-01-03 10:11:21,075 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1        | 2023-01-03 10:11:21,075 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1        | 2023-01-03 10:11:21,076 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1        | 2023-01-03 10:11:21,089 [main] INFO om.OzoneManager: Creating RPC Server
om_1        | 2023-01-03 10:11:21,115 [pool-26-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1        | 2023-01-03 10:11:21,119 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1        | 2023-01-03 10:11:21,152 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1        | 2023-01-03 10:11:21,157 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1        | 2023-01-03 10:11:21,226 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1        | 2023-01-03 10:11:21,258 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1        | 2023-01-03 10:11:21,260 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1        | 2023-01-03 10:11:21,620 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1        | 2023-01-03 10:11:21,622 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1        | 2023-01-03 10:11:21,624 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1        | 2023-01-03 10:11:21,627 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1        | 2023-01-03 10:11:21,629 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1        | 2023-01-03 10:11:22,381 [main] INFO reflections.Reflections: Reflections took 976 ms to scan 8 urls, producing 23 keys and 538 values [using 2 cores]
om_1        | 2023-01-03 10:11:22,710 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1        | 2023-01-03 10:11:22,723 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1        | 2023-01-03 10:11:23,686 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1        | 2023-01-03 10:11:23,717 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1        | 2023-01-03 10:11:23,717 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1        | 2023-01-03 10:11:23,809 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.18.0.4:9862
om_1        | 2023-01-03 10:11:23,810 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1        | 2023-01-03 10:11:23,817 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1        | 2023-01-03 10:11:23,824 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@8d15445c285b
om_1        | 2023-01-03 10:11:23,848 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1        | 2023-01-03 10:11:23,855 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1        | 2023-01-03 10:11:23,886 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1        | 2023-01-03 10:11:23,887 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1        | 2023-01-03 10:11:23,890 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1        | 2023-01-03 10:11:23,892 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1        | 2023-01-03 10:11:23,897 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1        | 2023-01-03 10:11:23,905 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1        | 2023-01-03 10:11:23,906 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1        | 2023-01-03 10:11:23,916 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1        | 2023-01-03 10:11:23,917 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1        | 2023-01-03 10:11:23,918 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1        | 2023-01-03 10:11:23,920 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1        | 2023-01-03 10:11:23,921 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1        | 2023-01-03 10:11:23,926 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1        | 2023-01-03 10:11:23,928 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1        | 2023-01-03 10:11:23,928 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2023-01-03 10:11:30,439 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: change Leader from null to 482b5314-1cac-4df1-beda-20c463470dc2 at term 5 for becomeLeader, leader elected after 27661ms
datanode_2  | 2023-01-03 10:11:30,439 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2  | 2023-01-03 10:11:30,439 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2  | 2023-01-03 10:11:30,440 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2  | 2023-01-03 10:11:30,440 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2  | 2023-01-03 10:11:30,440 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2  | 2023-01-03 10:11:30,441 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2  | 2023-01-03 10:11:30,441 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2  | 2023-01-03 10:11:30,441 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2  | 2023-01-03 10:11:30,496 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2  | 2023-01-03 10:11:30,497 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-03 10:11:30,498 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2  | 2023-01-03 10:11:30,514 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2  | 2023-01-03 10:11:30,514 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2023-01-03 10:11:30,517 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2023-01-03 10:11:30,517 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2  | 2023-01-03 10:11:30,518 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2  | 2023-01-03 10:11:30,528 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2  | 2023-01-03 10:11:30,530 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2023-01-03 10:11:30,530 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2  | 2023-01-03 10:11:30,532 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2  | 2023-01-03 10:11:30,532 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2023-01-03 10:11:30,533 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2023-01-03 10:11:30,533 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2  | 2023-01-03 10:11:30,534 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2  | 2023-01-03 10:11:30,538 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO impl.RoleInfo: 482b5314-1cac-4df1-beda-20c463470dc2: start 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderStateImpl
datanode_2  | 2023-01-03 10:11:30,539 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2  | 2023-01-03 10:11:30,545 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/87a5acf5-c74f-4d1d-8b72-9b236f254454/current/log_inprogress_0
datanode_2  | 2023-01-03 10:11:30,557 [482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454-LeaderElection3] INFO server.RaftServer$Division: 482b5314-1cac-4df1-beda-20c463470dc2@group-9B236F254454: set configuration 0: peers:[74492474-3f40-4050-829e-f08eb3e4f292|rpc:172.18.0.8:9856|admin:172.18.0.8:9857|client:172.18.0.8:9858|dataStream:172.18.0.8:9858|priority:0|startupRole:FOLLOWER, 482b5314-1cac-4df1-beda-20c463470dc2|rpc:172.18.0.7:9856|admin:172.18.0.7:9857|client:172.18.0.7:9858|dataStream:172.18.0.7:9858|priority:1|startupRole:FOLLOWER, 3719a142-1617-4d0e-a6d8-f47fd68adc81|rpc:172.18.0.9:9856|admin:172.18.0.9:9857|client:172.18.0.9:9858|dataStream:172.18.0.9:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5  | 2023-01-03 10:10:34,489 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:35,486 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.18.0.3:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:35,492 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:36,492 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:37,494 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:38,495 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:39,496 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:40,497 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:40,551 [EndpointStateMachine task thread for recon/172.18.0.3:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_5  | java.net.SocketTimeoutException: Call From 8e31dcea51a8/172.18.0.6 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.6:60124 remote=recon/172.18.0.3:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_5  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_5  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_5  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_5  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_5  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_5  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_5  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_5  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_5  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_5  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_5  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_5  | 	at com.sun.proxy.$Proxy43.submitRequest(Unknown Source)
datanode_5  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_5  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_5  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_5  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_5  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5  | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.18.0.6:60124 remote=recon/172.18.0.3:9891]
datanode_5  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_5  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_5  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_5  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_5  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_5  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_5  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_5  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_5  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_5  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_5  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_5  | 2023-01-03 10:10:41,499 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:42,500 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:43,503 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:43,504 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_5  | java.net.ConnectException: Call From 8e31dcea51a8/172.18.0.6 to scm:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
datanode_5  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_5  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_5  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_5  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_5  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
om_1        | 2023-01-03 10:11:23,929 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1        | 2023-01-03 10:11:23,945 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1        | 2023-01-03 10:11:23,946 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1        | 2023-01-03 10:11:23,962 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1        | 2023-01-03 10:11:23,963 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1        | 2023-01-03 10:11:23,964 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1        | 2023-01-03 10:11:23,979 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1        | 2023-01-03 10:11:23,980 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1        | 2023-01-03 10:11:23,989 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1        | 2023-01-03 10:11:23,993 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1        | 2023-01-03 10:11:23,997 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1        | 2023-01-03 10:11:24,004 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1        | 2023-01-03 10:11:24,005 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1        | 2023-01-03 10:11:24,006 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1        | 2023-01-03 10:11:24,008 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1        | 2023-01-03 10:11:24,009 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1        | 2023-01-03 10:11:24,010 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1       | 2023-01-03 10:10:41,052 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.cached = false (default)
scm_1       | 2023-01-03 10:10:41,053 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.request.thread.pool.size = 32 (default)
scm_1       | 2023-01-03 10:10:41,055 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.async.write.thread.pool.size = 16 (default)
scm_1       | 2023-01-03 10:10:41,057 [main] INFO server.RaftServerConfigKeys: raft.server.data-stream.client.pool.size = 10 (default)
scm_1       | 2023-01-03 10:10:41,062 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.use-epoll = false (default)
scm_1       | 2023-01-03 10:10:41,062 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.boss-group.size = 0 (default)
scm_1       | 2023-01-03 10:10:41,069 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.worker-group.size = 0 (default)
scm_1       | 2023-01-03 10:10:41,071 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.server.tls.conf = null (default)
scm_1       | 2023-01-03 10:10:41,073 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.host = null (default)
scm_1       | 2023-01-03 10:10:41,074 [main] INFO netty.NettyConfigKeys$DataStream: raft.netty.dataStream.port = 0 (default)
scm_1       | 2023-01-03 10:10:41,140 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1       | 2023-01-03 10:10:41,141 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1       | 2023-01-03 10:10:41,142 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1       | 2023-01-03 10:10:41,142 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1       | 2023-01-03 10:10:41,160 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1       | 2023-01-03 10:10:41,181 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServer: 8036be97-5432-4d54-a213-e0da532bfc11: found a subdirectory /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68
scm_1       | 2023-01-03 10:10:41,207 [main] INFO server.RaftServer: 8036be97-5432-4d54-a213-e0da532bfc11: addNew group-9DD877FC0E68:[] returns group-9DD877FC0E68:java.util.concurrent.CompletableFuture@260e3837[Not completed]
scm_1       | 2023-01-03 10:10:41,225 [8036be97-5432-4d54-a213-e0da532bfc11-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x96980600] REGISTERED
scm_1       | 2023-01-03 10:10:41,229 [8036be97-5432-4d54-a213-e0da532bfc11-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x96980600] BIND: 0.0.0.0/0.0.0.0:0
scm_1       | 2023-01-03 10:10:41,243 [8036be97-5432-4d54-a213-e0da532bfc11-NettyServerStreamRpc-bossGroup--thread1] INFO logging.LoggingHandler: [id: 0x96980600, L:/0.0.0.0:37483] ACTIVE
scm_1       | 2023-01-03 10:10:41,291 [pool-16-thread-1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11: new RaftServerImpl for group-9DD877FC0E68:[] with SCMStateMachine:uninitialized
scm_1       | 2023-01-03 10:10:41,296 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1       | 2023-01-03 10:10:41,297 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1       | 2023-01-03 10:10:41,297 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1       | 2023-01-03 10:10:41,297 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1       | 2023-01-03 10:10:41,297 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1       | 2023-01-03 10:10:41,298 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1       | 2023-01-03 10:10:41,311 [pool-16-thread-1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1       | 2023-01-03 10:10:41,315 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1       | 2023-01-03 10:10:41,322 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1       | 2023-01-03 10:10:41,327 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1       | 2023-01-03 10:10:41,352 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1       | 2023-01-03 10:10:41,358 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1       | 2023-01-03 10:10:41,358 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1       | 2023-01-03 10:10:41,441 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1       | 2023-01-03 10:10:41,442 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1       | 2023-01-03 10:10:41,444 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1       | 2023-01-03 10:10:41,445 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1       | 2023-01-03 10:10:41,446 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1       | 2023-01-03 10:10:41,460 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1       | 2023-01-03 10:10:41,461 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1       | 2023-01-03 10:10:41,461 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm_1       | 2023-01-03 10:10:41,493 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm_1       | 2023-01-03 10:10:41,680 [main] INFO reflections.Reflections: Reflections took 153 ms to scan 3 urls, producing 121 keys and 272 values 
scm_1       | 2023-01-03 10:10:41,836 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1       | 2023-01-03 10:10:41,837 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1       | 2023-01-03 10:10:41,848 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1       | 2023-01-03 10:10:41,851 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1       | 2023-01-03 10:10:41,951 [main] WARN server.ServerUtils: ozone.scm.dead.node.interval value = 45000 is smaller than min = 60000 based on the key value of ozone.scm.stale.node.interval, reset to the min value 60000.
om_1        | 2023-01-03 10:11:24,012 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1        | 2023-01-03 10:11:24,019 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1        | 2023-01-03 10:11:24,044 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1        | 2023-01-03 10:11:24,051 [Listener at om/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1        | 2023-01-03 10:11:24,057 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1        | 2023-01-03 10:11:24,175 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1        | 2023-01-03 10:11:24,176 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1        | 2023-01-03 10:11:24,260 [Listener at om/9862] INFO util.log: Logging initialized @32008ms to org.eclipse.jetty.util.log.Slf4jLog
om_1        | 2023-01-03 10:11:24,901 [Listener at om/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1        | 2023-01-03 10:11:24,961 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1        | 2023-01-03 10:11:25,038 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1        | 2023-01-03 10:11:25,043 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1        | 2023-01-03 10:11:25,043 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1        | 2023-01-03 10:11:25,043 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1        | 2023-01-03 10:11:25,292 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1        | 2023-01-03 10:11:25,302 [Listener at om/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om_1        | 2023-01-03 10:11:25,449 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1        | 2023-01-03 10:11:25,449 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1        | 2023-01-03 10:11:25,454 [Listener at om/9862] INFO server.session: node0 Scavenging every 600000ms
om_1        | 2023-01-03 10:11:25,517 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@119636bf{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1        | 2023-01-03 10:11:25,519 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4d031311{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1        | 2023-01-03 10:11:25,777 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@519552ae{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-12906738313773034042/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1        | 2023-01-03 10:11:25,794 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@3e71f228{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1        | 2023-01-03 10:11:25,795 [Listener at om/9862] INFO server.Server: Started @33543ms
om_1        | 2023-01-03 10:11:25,814 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1        | 2023-01-03 10:11:25,815 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1        | 2023-01-03 10:11:25,818 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1        | 2023-01-03 10:11:25,820 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1        | 2023-01-03 10:11:25,841 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1        | 2023-01-03 10:11:26,361 [Listener at om/9862] INFO om.TrashPolicyOzone: The configured checkpoint interval is 0 minutes. Using an interval of 1 minutes that is used for deletion instead
om_1        | 2023-01-03 10:11:26,368 [Listener at om/9862] INFO om.TrashPolicyOzone: Ozone Manager trash configuration: Deletion interval = 1 minutes, Emptier interval = 1 minutes.
om_1        | 2023-01-03 10:11:26,750 [Listener at om/9862] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om_1        | 2023-01-03 10:11:26,776 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7fed4326] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1        | 2023-01-03 10:11:29,045 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5048158166ns, electionTimeout:5037ms
om_1        | 2023-01-03 10:11:29,046 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1        | 2023-01-03 10:11:29,047 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1        | 2023-01-03 10:11:29,052 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1        | 2023-01-03 10:11:29,052 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1        | 2023-01-03 10:11:29,063 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1        | 2023-01-03 10:11:29,064 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1        | 2023-01-03 10:11:29,065 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1        | 2023-01-03 10:11:29,067 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1        | 2023-01-03 10:11:29,067 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 7841ms
om_1        | 2023-01-03 10:11:29,107 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1        | 2023-01-03 10:11:29,119 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1        | 2023-01-03 10:11:29,121 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1        | 2023-01-03 10:11:29,133 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1        | 2023-01-03 10:11:29,135 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1       | 2023-01-03 10:10:41,964 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1       | 2023-01-03 10:10:41,989 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1       | 2023-01-03 10:10:41,999 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1       | 2023-01-03 10:10:42,012 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1       | 2023-01-03 10:10:42,077 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1       | 2023-01-03 10:10:42,077 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1       | 2023-01-03 10:10:42,088 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1       | 2023-01-03 10:10:42,088 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1       | 2023-01-03 10:10:42,093 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1       | 2023-01-03 10:10:42,094 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1       | 2023-01-03 10:10:42,105 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1       | 2023-01-03 10:10:42,107 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1       | 2023-01-03 10:10:42,189 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1       | 2023-01-03 10:10:42,223 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1       | 2023-01-03 10:10:42,339 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1       | 2023-01-03 10:10:42,374 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1       | 2023-01-03 10:10:42,379 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1       | 2023-01-03 10:10:42,394 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1       | 2023-01-03 10:10:42,402 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2023-01-03 10:10:42,405 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1       | 2023-01-03 10:10:43,481 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1       | 2023-01-03 10:10:43,542 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2023-01-03 10:10:43,604 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1       | 2023-01-03 10:10:43,780 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1       | 2023-01-03 10:10:43,799 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2023-01-03 10:10:43,808 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1       | 2023-01-03 10:10:43,851 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1       | 2023-01-03 10:10:43,867 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2023-01-03 10:10:43,868 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1       | 2023-01-03 10:10:43,996 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1       | 2023-01-03 10:10:44,000 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1       | Container Balancer status:
scm_1       | Key                            Value
scm_1       | Running                        true
scm_1       | Container Balancer Configuration values:
scm_1       | Key                                                Value
scm_1       | Threshold                                          10
scm_1       | Max Datanodes to Involve per Iteration(percent)    20
scm_1       | Max Size to Move per Iteration                     500GB
scm_1       | Max Size Entering Target per Iteration             26GB
scm_1       | Max Size Leaving Source per Iteration              26GB
scm_1       | 
scm_1       | 2023-01-03 10:10:44,001 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1       | 2023-01-03 10:10:44,008 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1       | 2023-01-03 10:10:44,026 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1       | 2023-01-03 10:10:44,040 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68/in_use.lock acquired by nodename 6@1898d909ae22
scm_1       | 2023-01-03 10:10:44,052 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=8036be97-5432-4d54-a213-e0da532bfc11} from /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68/current/raft-meta
scm_1       | 2023-01-03 10:10:44,118 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: set configuration 0: peers:[8036be97-5432-4d54-a213-e0da532bfc11|rpc:1898d909ae22:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1       | 2023-01-03 10:10:44,126 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1       | 2023-01-03 10:10:44,157 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1       | 2023-01-03 10:10:44,157 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | 2023-01-03 10:10:44,160 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1       | 2023-01-03 10:10:44,162 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1       | 2023-01-03 10:10:44,172 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1       | 2023-01-03 10:10:44,183 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1       | 2023-01-03 10:10:44,183 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1       | 2023-01-03 10:10:44,207 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68
scm_1       | 2023-01-03 10:10:44,208 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,077 [IPC Server handler 99 on default port 9891] WARN ipc.Server: IPC Server handler 99 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.6:40192: output error
recon_1     | 2023-01-03 10:10:56,088 [IPC Server handler 99 on default port 9891] INFO ipc.Server: IPC Server handler 99 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,076 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.9:60966: output error
recon_1     | 2023-01-03 10:10:56,091 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
datanode_5  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
datanode_5  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_5  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_5  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_5  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_5  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_5  | 	at com.sun.proxy.$Proxy42.submitRequest(Unknown Source)
datanode_5  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_5  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_5  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_5  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_5  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5  | Caused by: java.net.ConnectException: Connection refused
datanode_5  | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
datanode_5  | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
datanode_5  | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
datanode_5  | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
datanode_5  | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
datanode_5  | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
datanode_5  | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
datanode_5  | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
datanode_5  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
datanode_5  | 	... 12 more
datanode_5  | 2023-01-03 10:10:44,517 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.18.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5  | 2023-01-03 10:10:51,101 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-14143503-c595-4722-b95c-9dd877fc0e68/DS-0a728619-868f-4edd-9cf9-a8bf65b7e978/container.db to cache
datanode_5  | 2023-01-03 10:10:51,117 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-14143503-c595-4722-b95c-9dd877fc0e68/DS-0a728619-868f-4edd-9cf9-a8bf65b7e978/container.db for volume DS-0a728619-868f-4edd-9cf9-a8bf65b7e978
datanode_5  | 2023-01-03 10:10:51,122 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_5  | 2023-01-03 10:10:51,156 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_5  | 2023-01-03 10:10:51,463 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_5  | 2023-01-03 10:10:51,481 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7
datanode_5  | 2023-01-03 10:10:51,640 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.RaftServer: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7: start RPC server
datanode_5  | 2023-01-03 10:10:51,655 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7: GrpcService started, listening on 9858
datanode_5  | 2023-01-03 10:10:51,673 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7: GrpcService started, listening on 9856
datanode_5  | 2023-01-03 10:10:51,684 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO server.GrpcService: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7: GrpcService started, listening on 9857
datanode_5  | 2023-01-03 10:10:51,780 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7: Started
datanode_5  | 2023-01-03 10:10:51,784 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7 is started using port 9858 for RATIS
datanode_5  | 2023-01-03 10:10:51,784 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7 is started using port 9857 for RATIS_ADMIN
datanode_5  | 2023-01-03 10:10:51,788 [EndpointStateMachine task thread for scm/172.18.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7 is started using port 9856 for RATIS_SERVER
datanode_5  | 2023-01-03 10:10:53,922 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_5  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_5  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_5  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_5  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,074 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.18.0.7:44682: output error
recon_1     | 2023-01-03 10:10:56,099 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,036 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,036 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,036 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
datanode_5  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_5  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_5  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:661)
datanode_5  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:309)
datanode_5  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:509)
datanode_5  | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5  | Caused by: java.util.concurrent.TimeoutException
datanode_5  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_5  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_5  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_5  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5  | 	... 1 more
om_1        | 2023-01-03 10:11:29,137 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1        | 2023-01-03 10:11:29,151 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1        | 2023-01-03 10:11:29,154 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1        | 2023-01-03 10:11:29,163 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1        | 2023-01-03 10:11:29,243 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1        | 2023-01-03 10:11:29,418 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1        | 2023-01-03 10:11:29,558 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1        | 2023-01-03 10:11:29,799 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1        | [id: "om1"
om_1        | address: "om:9872"
om_1        | startupRole: FOLLOWER
om_1        | ]
om_1        | 2023-01-03 10:11:41,095 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol-yhcxp for user:hadoop
om_1        | 2023-01-03 10:11:42,336 [qtp1545008464-50] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1        | 2023-01-03 10:11:42,382 [qtp1545008464-50] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1672740702350 in 32 milliseconds
om_1        | 2023-01-03 10:11:42,530 [qtp1545008464-50] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 145 milliseconds
om_1        | 2023-01-03 10:11:42,530 [qtp1545008464-50] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1672740702350
om_1        | 2023-01-03 10:11:48,583 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: buc-vkweg of layout LEGACY in volume: vol-yhcxp
scm_1       | 2023-01-03 10:10:44,209 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1       | 2023-01-03 10:10:44,210 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1       | 2023-01-03 10:10:44,211 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1       | 2023-01-03 10:10:44,211 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1       | 2023-01-03 10:10:44,214 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1       | 2023-01-03 10:10:44,215 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1       | 2023-01-03 10:10:44,219 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1       | 2023-01-03 10:10:44,245 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1       | 2023-01-03 10:10:44,245 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | 2023-01-03 10:10:44,273 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1       | 2023-01-03 10:10:44,277 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1       | 2023-01-03 10:10:44,277 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1       | 2023-01-03 10:10:44,337 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: set configuration 0: peers:[8036be97-5432-4d54-a213-e0da532bfc11|rpc:1898d909ae22:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1       | 2023-01-03 10:10:44,339 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68/current/log_inprogress_0
scm_1       | 2023-01-03 10:10:44,343 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm_1       | 2023-01-03 10:10:44,343 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1       | 2023-01-03 10:10:44,470 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: start as a follower, conf=0: peers:[8036be97-5432-4d54-a213-e0da532bfc11|rpc:1898d909ae22:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1       | 2023-01-03 10:10:44,470 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1       | 2023-01-03 10:10:44,472 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO impl.RoleInfo: 8036be97-5432-4d54-a213-e0da532bfc11: start 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState
scm_1       | 2023-01-03 10:10:44,478 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1       | 2023-01-03 10:10:44,483 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1       | 2023-01-03 10:10:44,485 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9DD877FC0E68,id=8036be97-5432-4d54-a213-e0da532bfc11
scm_1       | 2023-01-03 10:10:44,489 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1       | 2023-01-03 10:10:44,489 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1       | 2023-01-03 10:10:44,490 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1       | 2023-01-03 10:10:44,491 [8036be97-5432-4d54-a213-e0da532bfc11-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1       | 2023-01-03 10:10:44,503 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 8036be97-5432-4d54-a213-e0da532bfc11: start RPC server
scm_1       | 2023-01-03 10:10:44,525 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 8036be97-5432-4d54-a213-e0da532bfc11: GrpcService started, listening on 9894
scm_1       | 2023-01-03 10:10:44,535 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-8036be97-5432-4d54-a213-e0da532bfc11: Started
scm_1       | 2023-01-03 10:10:44,542 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [8036be97-5432-4d54-a213-e0da532bfc11|rpc:1898d909ae22:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1       | 2023-01-03 10:10:44,543 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1       | 2023-01-03 10:10:44,801 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1       | 2023-01-03 10:10:44,849 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1       | 2023-01-03 10:10:44,849 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1       | 2023-01-03 10:10:45,244 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1       | 2023-01-03 10:10:45,245 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1       | 2023-01-03 10:10:45,247 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1       | 2023-01-03 10:10:45,397 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1       | 2023-01-03 10:10:45,398 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1       | 2023-01-03 10:10:45,400 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1       | 2023-01-03 10:10:45,403 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1       | 2023-01-03 10:10:45,515 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@365eaef] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1       | 2023-01-03 10:10:45,557 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1       | 2023-01-03 10:10:45,558 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,036 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,036 [IPC Server handler 12 on default port 9891] INFO ipc.Server: IPC Server handler 12 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,036 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
datanode_5  | 2023-01-03 10:10:57,242 [Command processor thread] INFO server.RaftServer: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7: addNew group-9DFE4A1CFFC7:[005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:1|startupRole:FOLLOWER] returns group-9DFE4A1CFFC7:java.util.concurrent.CompletableFuture@2abc9b6c[Not completed]
datanode_5  | 2023-01-03 10:10:57,572 [pool-22-thread-1] INFO server.RaftServer$Division: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7: new RaftServerImpl for group-9DFE4A1CFFC7:[005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5  | 2023-01-03 10:10:57,583 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5  | 2023-01-03 10:10:57,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5  | 2023-01-03 10:10:57,623 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5  | 2023-01-03 10:10:57,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5  | 2023-01-03 10:10:57,636 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5  | 2023-01-03 10:10:57,647 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5  | 2023-01-03 10:10:57,761 [pool-22-thread-1] INFO server.RaftServer$Division: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7: ConfigurationManager, init=-1: peers:[005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5  | 2023-01-03 10:10:57,767 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5  | 2023-01-03 10:10:57,825 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5  | 2023-01-03 10:10:57,832 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5  | 2023-01-03 10:10:57,906 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5  | 2023-01-03 10:10:57,940 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5  | 2023-01-03 10:10:57,941 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5  | 2023-01-03 10:10:58,059 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5  | 2023-01-03 10:10:58,060 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5  | 2023-01-03 10:10:58,060 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5  | 2023-01-03 10:10:58,061 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5  | 2023-01-03 10:10:58,062 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5  | 2023-01-03 10:10:58,065 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/385c10ae-b648-4de6-a81c-9dfe4a1cffc7 does not exist. Creating ...
datanode_5  | 2023-01-03 10:10:58,076 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/385c10ae-b648-4de6-a81c-9dfe4a1cffc7/in_use.lock acquired by nodename 7@8e31dcea51a8
datanode_5  | 2023-01-03 10:10:58,093 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/385c10ae-b648-4de6-a81c-9dfe4a1cffc7 has been successfully formatted.
datanode_5  | 2023-01-03 10:10:58,114 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-9DFE4A1CFFC7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5  | 2023-01-03 10:10:58,140 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5  | 2023-01-03 10:10:58,191 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5  | 2023-01-03 10:10:58,193 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5  | 2023-01-03 10:10:58,222 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5  | 2023-01-03 10:10:58,223 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5  | 2023-01-03 10:10:58,232 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5  | 2023-01-03 10:10:58,258 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5  | 2023-01-03 10:10:58,259 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5  | 2023-01-03 10:10:58,287 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/385c10ae-b648-4de6-a81c-9dfe4a1cffc7
datanode_5  | 2023-01-03 10:10:58,291 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5  | 2023-01-03 10:10:58,294 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5  | 2023-01-03 10:10:58,296 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5  | 2023-01-03 10:10:58,299 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5  | 2023-01-03 10:10:58,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5  | 2023-01-03 10:10:58,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5  | 2023-01-03 10:10:58,306 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5  | 2023-01-03 10:10:58,310 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5  | 2023-01-03 10:10:58,393 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5  | 2023-01-03 10:10:58,394 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5  | 2023-01-03 10:10:58,439 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5  | 2023-01-03 10:10:58,450 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5  | 2023-01-03 10:10:58,455 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5  | 2023-01-03 10:10:58,473 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5  | 2023-01-03 10:10:58,473 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5  | 2023-01-03 10:10:58,482 [pool-22-thread-1] INFO server.RaftServer$Division: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7: start as a follower, conf=-1: peers:[005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm_1       | 2023-01-03 10:10:45,605 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @14461ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1       | 2023-01-03 10:10:46,144 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1       | 2023-01-03 10:10:46,162 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1       | 2023-01-03 10:10:46,186 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1       | 2023-01-03 10:10:46,194 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1       | 2023-01-03 10:10:46,195 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1       | 2023-01-03 10:10:46,196 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1       | 2023-01-03 10:10:46,283 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1       | 2023-01-03 10:10:46,284 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm_1       | 2023-01-03 10:10:46,352 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1       | 2023-01-03 10:10:46,352 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1       | 2023-01-03 10:10:46,356 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm_1       | 2023-01-03 10:10:46,385 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1ccdbae4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1       | 2023-01-03 10:10:46,387 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@24090832{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1       | 2023-01-03 10:10:46,578 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7be95197{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-12548774640583974128/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1       | 2023-01-03 10:10:46,588 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@59cc46a4{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1       | 2023-01-03 10:10:46,589 [Listener at 0.0.0.0/9860] INFO server.Server: Started @15445ms
scm_1       | 2023-01-03 10:10:46,592 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1       | 2023-01-03 10:10:46,592 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1       | 2023-01-03 10:10:46,594 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1       | 2023-01-03 10:10:49,519 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO impl.FollowerState: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5047493132ns, electionTimeout:5034ms
scm_1       | 2023-01-03 10:10:49,521 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO impl.RoleInfo: 8036be97-5432-4d54-a213-e0da532bfc11: shutdown 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState
scm_1       | 2023-01-03 10:10:49,523 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1       | 2023-01-03 10:10:49,531 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm_1       | 2023-01-03 10:10:49,531 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-FollowerState] INFO impl.RoleInfo: 8036be97-5432-4d54-a213-e0da532bfc11: start 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1
scm_1       | 2023-01-03 10:10:49,554 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO impl.LeaderElection: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[8036be97-5432-4d54-a213-e0da532bfc11|rpc:1898d909ae22:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1       | 2023-01-03 10:10:49,556 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO impl.LeaderElection: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1       | 2023-01-03 10:10:49,557 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO impl.RoleInfo: 8036be97-5432-4d54-a213-e0da532bfc11: shutdown 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1
scm_1       | 2023-01-03 10:10:49,558 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1       | 2023-01-03 10:10:49,558 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1       | 2023-01-03 10:10:49,559 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1       | 2023-01-03 10:10:49,562 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: change Leader from null to 8036be97-5432-4d54-a213-e0da532bfc11 at term 2 for becomeLeader, leader elected after 8207ms
scm_1       | 2023-01-03 10:10:49,575 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1       | 2023-01-03 10:10:49,582 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1       | 2023-01-03 10:10:49,583 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1       | 2023-01-03 10:10:49,590 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1       | 2023-01-03 10:10:49,590 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1       | 2023-01-03 10:10:49,591 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1       | 2023-01-03 10:10:49,601 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1       | 2023-01-03 10:10:49,606 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1       | 2023-01-03 10:10:49,613 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO impl.RoleInfo: 8036be97-5432-4d54-a213-e0da532bfc11: start 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderStateImpl
scm_1       | 2023-01-03 10:10:49,620 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1       | 2023-01-03 10:10:49,626 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68/current/log_inprogress_0 to /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68/current/log_0-0
scm_1       | 2023-01-03 10:10:49,652 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-LeaderElection1] INFO server.RaftServer$Division: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68: set configuration 1: peers:[8036be97-5432-4d54-a213-e0da532bfc11|rpc:1898d909ae22:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1       | 2023-01-03 10:10:49,657 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/14143503-c595-4722-b95c-9dd877fc0e68/current/log_inprogress_1
scm_1       | 2023-01-03 10:10:49,668 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1       | 2023-01-03 10:10:49,669 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1       | 2023-01-03 10:10:49,675 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1     | 2023-01-03 10:10:56,036 [IPC Server handler 18 on default port 9891] INFO ipc.Server: IPC Server handler 18 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,036 [IPC Server handler 13 on default port 9891] INFO ipc.Server: IPC Server handler 13 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,036 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
datanode_5  | 2023-01-03 10:10:58,484 [pool-22-thread-1] INFO server.RaftServer$Division: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5  | 2023-01-03 10:10:58,489 [pool-22-thread-1] INFO impl.RoleInfo: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7: start 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-FollowerState
datanode_5  | 2023-01-03 10:10:58,492 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5  | 2023-01-03 10:10:58,492 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5  | 2023-01-03 10:10:58,502 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9DFE4A1CFFC7,id=005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7
datanode_5  | 2023-01-03 10:10:58,518 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5  | 2023-01-03 10:10:58,519 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5  | 2023-01-03 10:10:58,521 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5  | 2023-01-03 10:10:58,525 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5  | 2023-01-03 10:10:58,594 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=385c10ae-b648-4de6-a81c-9dfe4a1cffc7
datanode_5  | 2023-01-03 10:10:58,599 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=385c10ae-b648-4de6-a81c-9dfe4a1cffc7.
datanode_5  | 2023-01-03 10:11:03,614 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-FollowerState] INFO impl.FollowerState: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5126328570ns, electionTimeout:5120ms
datanode_5  | 2023-01-03 10:11:03,616 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-FollowerState] INFO impl.RoleInfo: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7: shutdown 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-FollowerState
datanode_5  | 2023-01-03 10:11:03,617 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-FollowerState] INFO server.RaftServer$Division: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5  | 2023-01-03 10:11:03,622 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_5  | 2023-01-03 10:11:03,623 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-FollowerState] INFO impl.RoleInfo: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7: start 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1
datanode_5  | 2023-01-03 10:11:03,668 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO impl.LeaderElection: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5  | 2023-01-03 10:11:03,675 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO impl.LeaderElection: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_5  | 2023-01-03 10:11:03,679 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO impl.RoleInfo: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7: shutdown 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1
datanode_5  | 2023-01-03 10:11:03,682 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO server.RaftServer$Division: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5  | 2023-01-03 10:11:03,684 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9DFE4A1CFFC7 with new leaderId: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7
datanode_5  | 2023-01-03 10:11:03,691 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO server.RaftServer$Division: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7: change Leader from null to 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7 at term 1 for becomeLeader, leader elected after 5783ms
datanode_5  | 2023-01-03 10:11:03,737 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5  | 2023-01-03 10:11:03,830 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5  | 2023-01-03 10:11:03,831 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_5  | 2023-01-03 10:11:03,889 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5  | 2023-01-03 10:11:03,889 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_5  | 2023-01-03 10:11:03,893 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_5  | 2023-01-03 10:11:03,990 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5  | 2023-01-03 10:11:04,018 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_5  | 2023-01-03 10:11:04,042 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO impl.RoleInfo: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7: start 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderStateImpl
datanode_5  | 2023-01-03 10:11:04,357 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5  | 2023-01-03 10:11:04,915 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-LeaderElection1] INFO server.RaftServer$Division: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7: set configuration 0: peers:[005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7|rpc:172.18.0.6:9856|admin:172.18.0.6:9857|client:172.18.0.6:9858|dataStream:172.18.0.6:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5  | 2023-01-03 10:11:05,276 [005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7@group-9DFE4A1CFFC7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/385c10ae-b648-4de6-a81c-9dfe4a1cffc7/current/log_inprogress_0
scm_1       | 2023-01-03 10:10:49,677 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1       | 2023-01-03 10:10:49,677 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1       | 2023-01-03 10:10:49,678 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1       | 2023-01-03 10:10:49,688 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1       | 2023-01-03 10:10:49,689 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1       | 2023-01-03 10:10:50,052 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.10:39804: output error
scm_1       | 2023-01-03 10:10:50,053 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
scm_1       | java.nio.channels.ClosedChannelException
scm_1       | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1       | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1       | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1       | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1       | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1       | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1       | 2023-01-03 10:10:50,079 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.9:39234: output error
scm_1       | 2023-01-03 10:10:50,083 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.8:58594: output error
scm_1       | 2023-01-03 10:10:50,083 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm_1       | java.nio.channels.ClosedChannelException
scm_1       | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1       | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1       | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1       | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1       | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1       | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1       | 2023-01-03 10:10:50,119 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.7:52126: output error
scm_1       | 2023-01-03 10:10:50,119 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
scm_1       | java.nio.channels.ClosedChannelException
scm_1       | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1       | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1       | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1       | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1       | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1       | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1       | 2023-01-03 10:10:50,133 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm_1       | java.nio.channels.ClosedChannelException
scm_1       | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1       | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,036 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,036 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1       | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1       | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1       | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1       | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1       | 2023-01-03 10:10:50,120 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.18.0.6:48218: output error
scm_1       | 2023-01-03 10:10:50,155 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm_1       | java.nio.channels.ClosedChannelException
scm_1       | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1       | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1       | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1       | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1       | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1       | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1       | 2023-01-03 10:10:52,336 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/74492474-3f40-4050-829e-f08eb3e4f292
scm_1       | 2023-01-03 10:10:52,336 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c950fbed-bbb7-451f-b5e1-adf39d3a6a32
scm_1       | 2023-01-03 10:10:52,411 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c950fbed-bbb7-451f-b5e1-adf39d3a6a32{ip: 172.18.0.10, host: ozone-legacy-bucket_datanode_4.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1       | 2023-01-03 10:10:52,411 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1       | 2023-01-03 10:10:52,484 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1       | 2023-01-03 10:10:52,691 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-8036be97-5432-4d54-a213-e0da532bfc11: Detected pause in JVM or host machine (eg GC): pause of approximately 105235470ns.
scm_1       | GC pool 'ParNew' had collection(s): count=1 time=161ms
scm_1       | 2023-01-03 10:10:52,701 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1       | 2023-01-03 10:10:52,716 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1       | 2023-01-03 10:10:52,755 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 5 required.
scm_1       | 2023-01-03 10:10:52,767 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 5 required.
scm_1       | 2023-01-03 10:10:52,771 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1       | 2023-01-03 10:10:52,780 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e4d2410b-8e21-4447-8fa8-ad68f8d0a56d to datanode:74492474-3f40-4050-829e-f08eb3e4f292
scm_1       | 2023-01-03 10:10:53,171 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3719a142-1617-4d0e-a6d8-f47fd68adc81
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,036 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1     | java.nio.channels.ClosedChannelException
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1     | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1     | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1     | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1     | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1     | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1     | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1     | 2023-01-03 10:10:56,310 [IPC Server handler 29 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7
recon_1     | 2023-01-03 10:10:56,402 [IPC Server handler 29 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7{ip: 172.18.0.6, host: ozone-legacy-bucket_datanode_5.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:10:56,816 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7 to Node DB.
recon_1     | 2023-01-03 10:10:58,280 [IPC Server handler 46 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozone-legacy-bucket_datanode_5.ozone-legacy-bucket_default
recon_1     | 2023-01-03 10:10:58,290 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=385c10ae-b648-4de6-a81c-9dfe4a1cffc7. Trying to get from SCM.
recon_1     | 2023-01-03 10:10:58,685 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 385c10ae-b648-4de6-a81c-9dfe4a1cffc7, Nodes: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7{ip: 172.18.0.6, host: ozone-legacy-bucket_datanode_5.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7, CreationTimestamp2023-01-03T10:10:55.409Z[UTC]] to Recon pipeline metadata.
recon_1     | 2023-01-03 10:10:58,755 [IPC Server handler 11 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c950fbed-bbb7-451f-b5e1-adf39d3a6a32
recon_1     | 2023-01-03 10:10:58,806 [IPC Server handler 11 on default port 9891] INFO node.SCMNodeManager: Registered Data node : c950fbed-bbb7-451f-b5e1-adf39d3a6a32{ip: 172.18.0.10, host: ozone-legacy-bucket_datanode_4.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:10:58,757 [IPC Server handler 21 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/74492474-3f40-4050-829e-f08eb3e4f292
recon_1     | 2023-01-03 10:10:58,844 [IPC Server handler 21 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:10:58,806 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node c950fbed-bbb7-451f-b5e1-adf39d3a6a32 to Node DB.
recon_1     | 2023-01-03 10:10:58,847 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 74492474-3f40-4050-829e-f08eb3e4f292 to Node DB.
recon_1     | 2023-01-03 10:10:59,140 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 385c10ae-b648-4de6-a81c-9dfe4a1cffc7, Nodes: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7{ip: 172.18.0.6, host: ozone-legacy-bucket_datanode_5.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7, CreationTimestamp2023-01-03T10:10:55.409Z[UTC]].
recon_1     | 2023-01-03 10:10:59,915 [IPC Server handler 20 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3719a142-1617-4d0e-a6d8-f47fd68adc81
recon_1     | 2023-01-03 10:10:59,915 [IPC Server handler 20 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:10:59,925 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 3719a142-1617-4d0e-a6d8-f47fd68adc81 to Node DB.
recon_1     | 2023-01-03 10:11:00,557 [IPC Server handler 76 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/482b5314-1cac-4df1-beda-20c463470dc2
recon_1     | 2023-01-03 10:11:00,558 [IPC Server handler 76 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:00,563 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 482b5314-1cac-4df1-beda-20c463470dc2 to Node DB.
recon_1     | 2023-01-03 10:11:01,716 [IPC Server handler 11 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default
recon_1     | 2023-01-03 10:11:01,719 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=e4d2410b-8e21-4447-8fa8-ad68f8d0a56d. Trying to get from SCM.
recon_1     | 2023-01-03 10:11:01,739 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: e4d2410b-8e21-4447-8fa8-ad68f8d0a56d, Nodes: 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:74492474-3f40-4050-829e-f08eb3e4f292, CreationTimestamp2023-01-03T10:10:52.775Z[UTC]] to Recon pipeline metadata.
recon_1     | 2023-01-03 10:11:01,757 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e4d2410b-8e21-4447-8fa8-ad68f8d0a56d, Nodes: 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:74492474-3f40-4050-829e-f08eb3e4f292, CreationTimestamp2023-01-03T10:10:52.775Z[UTC]].
recon_1     | 2023-01-03 10:11:01,776 [IPC Server handler 46 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozone-legacy-bucket_datanode_4.ozone-legacy-bucket_default
recon_1     | 2023-01-03 10:11:01,828 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d15fb565-ac34-40f7-b405-495fb9adc940. Trying to get from SCM.
recon_1     | 2023-01-03 10:11:01,884 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d15fb565-ac34-40f7-b405-495fb9adc940, Nodes: c950fbed-bbb7-451f-b5e1-adf39d3a6a32{ip: 172.18.0.10, host: ozone-legacy-bucket_datanode_4.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:c950fbed-bbb7-451f-b5e1-adf39d3a6a32, CreationTimestamp2023-01-03T10:10:55.444Z[UTC]] to Recon pipeline metadata.
recon_1     | 2023-01-03 10:11:01,953 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d15fb565-ac34-40f7-b405-495fb9adc940, Nodes: c950fbed-bbb7-451f-b5e1-adf39d3a6a32{ip: 172.18.0.10, host: ozone-legacy-bucket_datanode_4.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:c950fbed-bbb7-451f-b5e1-adf39d3a6a32, CreationTimestamp2023-01-03T10:10:55.444Z[UTC]].
recon_1     | 2023-01-03 10:11:03,124 [IPC Server handler 4 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozone-legacy-bucket_datanode_5.ozone-legacy-bucket_default
recon_1     | 2023-01-03 10:11:03,265 [IPC Server handler 76 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default
recon_1     | 2023-01-03 10:11:03,268 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454. Trying to get from SCM.
scm_1       | 2023-01-03 10:10:53,173 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1       | 2023-01-03 10:10:53,175 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1       | 2023-01-03 10:10:53,219 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 5 required.
scm_1       | 2023-01-03 10:10:53,968 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/482b5314-1cac-4df1-beda-20c463470dc2
scm_1       | 2023-01-03 10:10:53,969 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1       | 2023-01-03 10:10:53,969 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1       | 2023-01-03 10:10:53,976 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 4 DataNodes registered, 5 required.
scm_1       | 2023-01-03 10:10:54,115 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:10:54,129 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:10:54,309 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7
scm_1       | 2023-01-03 10:10:54,314 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7{ip: 172.18.0.6, host: ozone-legacy-bucket_datanode_5.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1       | 2023-01-03 10:10:54,369 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 5 DataNodes registered, 5 required.
scm_1       | 2023-01-03 10:10:54,379 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1       | 2023-01-03 10:10:54,379 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1       | 2023-01-03 10:10:54,383 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1       | 2023-01-03 10:10:54,375 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1       | 2023-01-03 10:10:54,385 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1       | 2023-01-03 10:10:54,564 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e4d2410b-8e21-4447-8fa8-ad68f8d0a56d, Nodes: 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-03T10:10:52.775Z[UTC]].
scm_1       | 2023-01-03 10:10:54,577 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2023-01-03 10:10:54,877 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 to datanode:74492474-3f40-4050-829e-f08eb3e4f292
scm_1       | 2023-01-03 10:10:54,908 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 to datanode:3719a142-1617-4d0e-a6d8-f47fd68adc81
scm_1       | 2023-01-03 10:10:54,908 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 to datanode:482b5314-1cac-4df1-beda-20c463470dc2
scm_1       | 2023-01-03 10:10:54,966 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:10:55,055 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 87a5acf5-c74f-4d1d-8b72-9b236f254454, Nodes: 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-03T10:10:54.877Z[UTC]].
scm_1       | 2023-01-03 10:10:55,062 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2023-01-03 10:10:55,076 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=22c9380f-296a-4792-92f3-06cfdbb51d5d to datanode:482b5314-1cac-4df1-beda-20c463470dc2
scm_1       | 2023-01-03 10:10:55,140 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 22c9380f-296a-4792-92f3-06cfdbb51d5d, Nodes: 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-03T10:10:55.076Z[UTC]].
scm_1       | 2023-01-03 10:10:55,144 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2023-01-03 10:10:55,152 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1       | 2023-01-03 10:10:55,247 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b7fb08b7-ba15-458b-85fd-4562a4fbd783 to datanode:3719a142-1617-4d0e-a6d8-f47fd68adc81
scm_1       | 2023-01-03 10:10:55,398 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b7fb08b7-ba15-458b-85fd-4562a4fbd783, Nodes: 3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-03T10:10:55.243Z[UTC]].
scm_1       | 2023-01-03 10:10:55,404 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2023-01-03 10:10:55,409 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=385c10ae-b648-4de6-a81c-9dfe4a1cffc7 to datanode:005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7
scm_1       | 2023-01-03 10:10:55,441 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 385c10ae-b648-4de6-a81c-9dfe4a1cffc7, Nodes: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7{ip: 172.18.0.6, host: ozone-legacy-bucket_datanode_5.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-03T10:10:55.409Z[UTC]].
scm_1       | 2023-01-03 10:10:55,441 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2023-01-03 10:10:55,444 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d15fb565-ac34-40f7-b405-495fb9adc940 to datanode:c950fbed-bbb7-451f-b5e1-adf39d3a6a32
scm_1       | 2023-01-03 10:10:55,488 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d15fb565-ac34-40f7-b405-495fb9adc940, Nodes: c950fbed-bbb7-451f-b5e1-adf39d3a6a32{ip: 172.18.0.10, host: ozone-legacy-bucket_datanode_4.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-03T10:10:55.444Z[UTC]].
scm_1       | 2023-01-03 10:10:55,491 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2023-01-03 10:10:55,497 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1       | 2023-01-03 10:10:55,611 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:10:56,052 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:10:58,199 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:10:58,211 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 385c10ae-b648-4de6-a81c-9dfe4a1cffc7, Nodes: 005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7{ip: 172.18.0.6, host: ozone-legacy-bucket_datanode_5.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:005fe1cb-e8b5-41d9-892f-64b6a6c8cbf7, CreationTimestamp2023-01-03T10:10:55.409Z[UTC]] moved to OPEN state
scm_1       | 2023-01-03 10:10:58,218 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1     | 2023-01-03 10:11:03,288 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 87a5acf5-c74f-4d1d-8b72-9b236f254454, Nodes: 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-03T10:10:54.877Z[UTC]] to Recon pipeline metadata.
recon_1     | 2023-01-03 10:11:03,303 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 87a5acf5-c74f-4d1d-8b72-9b236f254454, Nodes: 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-03T10:10:54.877Z[UTC]].
recon_1     | 2023-01-03 10:11:03,306 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 reported by 3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:03,504 [IPC Server handler 81 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default
recon_1     | 2023-01-03 10:11:03,506 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 reported by 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:03,963 [IPC Server handler 22 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default
recon_1     | 2023-01-03 10:11:03,965 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 reported by 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:06,502 [IPC Server handler 81 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozone-legacy-bucket_datanode_4.ozone-legacy-bucket_default
recon_1     | 2023-01-03 10:11:07,712 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 reported by 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:08,171 [IPC Server handler 76 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default
recon_1     | 2023-01-03 10:11:08,172 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 reported by 3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:08,709 [IPC Server handler 14 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default
recon_1     | 2023-01-03 10:11:08,711 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 reported by 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:11,148 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 reported by 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:11,148 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=22c9380f-296a-4792-92f3-06cfdbb51d5d. Trying to get from SCM.
recon_1     | 2023-01-03 10:11:11,185 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 22c9380f-296a-4792-92f3-06cfdbb51d5d, Nodes: 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-03T10:10:55.076Z[UTC]] to Recon pipeline metadata.
scm_1       | 2023-01-03 10:10:58,228 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2023-01-03 10:10:58,750 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:10:58,756 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:10:59,907 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:00,559 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:01,651 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:01,652 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e4d2410b-8e21-4447-8fa8-ad68f8d0a56d, Nodes: 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:74492474-3f40-4050-829e-f08eb3e4f292, CreationTimestamp2023-01-03T10:10:52.775Z[UTC]] moved to OPEN state
scm_1       | 2023-01-03 10:11:01,669 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2023-01-03 10:11:01,670 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2023-01-03 10:11:01,780 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:01,787 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d15fb565-ac34-40f7-b405-495fb9adc940, Nodes: c950fbed-bbb7-451f-b5e1-adf39d3a6a32{ip: 172.18.0.10, host: ozone-legacy-bucket_datanode_4.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:c950fbed-bbb7-451f-b5e1-adf39d3a6a32, CreationTimestamp2023-01-03T10:10:55.444Z[UTC]] moved to OPEN state
scm_1       | 2023-01-03 10:11:01,796 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2023-01-03 10:11:01,818 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2023-01-03 10:11:03,121 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:03,252 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:03,468 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2023-01-03 10:11:03,469 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:03,765 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:03,765 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2023-01-03 10:11:03,922 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:06,527 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:07,738 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2023-01-03 10:11:07,742 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:07,990 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:07,993 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2023-01-03 10:11:08,195 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:08,732 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:08,758 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:11,171 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:11,172 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 22c9380f-296a-4792-92f3-06cfdbb51d5d, Nodes: 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:482b5314-1cac-4df1-beda-20c463470dc2, CreationTimestamp2023-01-03T10:10:55.076Z[UTC]] moved to OPEN state
scm_1       | 2023-01-03 10:11:11,203 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2023-01-03 10:11:11,204 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2023-01-03 10:11:11,284 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:11,287 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b7fb08b7-ba15-458b-85fd-4562a4fbd783, Nodes: 3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3719a142-1617-4d0e-a6d8-f47fd68adc81, CreationTimestamp2023-01-03T10:10:55.243Z[UTC]] moved to OPEN state
scm_1       | 2023-01-03 10:11:11,312 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2023-01-03 10:11:11,333 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2023-01-03 10:11:12,712 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:12,930 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:13,718 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:16,156 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:16,278 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:16,497 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:16,497 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2023-01-03 10:11:16,657 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:16,657 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2023-01-03 10:11:17,710 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:17,937 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:18,699 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:21,482 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:21,664 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:22,708 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:22,932 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:23,697 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:25,525 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1       | 2023-01-03 10:11:26,474 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:26,654 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:27,710 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:27,934 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:28,698 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:30,443 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:30,444 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 87a5acf5-c74f-4d1d-8b72-9b236f254454, Nodes: 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:482b5314-1cac-4df1-beda-20c463470dc2, CreationTimestamp2023-01-03T10:10:54.877Z[UTC]] moved to OPEN state
scm_1       | 2023-01-03 10:11:30,457 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1       | 2023-01-03 10:11:30,460 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1       | 2023-01-03 10:11:30,463 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1       | 2023-01-03 10:11:30,467 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1       | 2023-01-03 10:11:30,467 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1       | 2023-01-03 10:11:30,467 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1       | 2023-01-03 10:11:30,468 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1       | 2023-01-03 10:11:30,471 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1       | 2023-01-03 10:11:30,472 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1       | 2023-01-03 10:11:30,523 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1       | 2023-01-03 10:11:31,656 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:32,714 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:32,937 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:33,702 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:35,446 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:36,650 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:37,710 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:37,931 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:38,700 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:40,443 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:41,649 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:42,709 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:42,930 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:43,699 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:45,447 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:46,650 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:47,710 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:47,933 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:48,700 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:50,446 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:51,652 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:52,716 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:52,931 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:53,700 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:55,240 [IPC Server handler 8 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1       | 2023-01-03 10:11:55,355 [8036be97-5432-4d54-a213-e0da532bfc11@group-9DD877FC0E68-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1       | 2023-01-03 10:11:55,365 [IPC Server handler 8 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1       | 2023-01-03 10:11:55,450 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:55,526 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1       | 2023-01-03 10:11:56,649 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:57,714 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
recon_1     | 2023-01-03 10:11:11,190 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 22c9380f-296a-4792-92f3-06cfdbb51d5d, Nodes: 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-01-03T10:10:55.076Z[UTC]].
recon_1     | 2023-01-03 10:11:11,208 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=22c9380f-296a-4792-92f3-06cfdbb51d5d reported by 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:11,212 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 22c9380f-296a-4792-92f3-06cfdbb51d5d, Nodes: 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:482b5314-1cac-4df1-beda-20c463470dc2, CreationTimestamp2023-01-03T10:10:55.076Z[UTC]] moved to OPEN state
recon_1     | 2023-01-03 10:11:11,302 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 reported by 3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:11,304 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=b7fb08b7-ba15-458b-85fd-4562a4fbd783. Trying to get from SCM.
recon_1     | 2023-01-03 10:11:11,322 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: b7fb08b7-ba15-458b-85fd-4562a4fbd783, Nodes: 3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:3719a142-1617-4d0e-a6d8-f47fd68adc81, CreationTimestamp2023-01-03T10:10:55.243Z[UTC]] to Recon pipeline metadata.
recon_1     | 2023-01-03 10:11:11,325 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b7fb08b7-ba15-458b-85fd-4562a4fbd783, Nodes: 3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:3719a142-1617-4d0e-a6d8-f47fd68adc81, CreationTimestamp2023-01-03T10:10:55.243Z[UTC]].
recon_1     | 2023-01-03 10:11:16,486 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 reported by 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:16,648 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 reported by 3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:30,456 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=87a5acf5-c74f-4d1d-8b72-9b236f254454 reported by 482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1     | 2023-01-03 10:11:30,458 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 87a5acf5-c74f-4d1d-8b72-9b236f254454, Nodes: 74492474-3f40-4050-829e-f08eb3e4f292{ip: 172.18.0.8, host: ozone-legacy-bucket_datanode_1.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3719a142-1617-4d0e-a6d8-f47fd68adc81{ip: 172.18.0.9, host: ozone-legacy-bucket_datanode_3.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}482b5314-1cac-4df1-beda-20c463470dc2{ip: 172.18.0.7, host: ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:482b5314-1cac-4df1-beda-20c463470dc2, CreationTimestamp2023-01-03T10:10:54.877Z[UTC]] moved to OPEN state
recon_1     | 2023-01-03 10:11:41,105 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1     | 2023-01-03 10:11:41,106 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1     | 2023-01-03 10:11:42,621 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1672740701106
recon_1     | 2023-01-03 10:11:42,636 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1     | 2023-01-03 10:11:42,640 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1     | 2023-01-03 10:11:42,873 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1672740701106.
recon_1     | 2023-01-03 10:11:42,995 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1     | 2023-01-03 10:11:43,016 [pool-50-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
scm_1       | 2023-01-03 10:11:57,928 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
recon_1     | 2023-01-03 10:11:43,029 [pool-50-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1     | 2023-01-03 10:11:43,607 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1     | 2023-01-03 10:11:43,607 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1     | 2023-01-03 10:11:43,609 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1     | 2023-01-03 10:11:43,610 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1     | 2023-01-03 10:11:43,630 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1     | 2023-01-03 10:11:43,631 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.023 seconds to process 0 keys.
recon_1     | 2023-01-03 10:11:43,669 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1     | 2023-01-03 10:11:43,673 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1     | 2023-01-03 10:11:59,619 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #1 got from ozone-legacy-bucket_datanode_2.ozone-legacy-bucket_default.
recon_1     | 2023-01-03 10:11:59,765 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1     | 2023-01-03 10:12:43,691 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1     | 2023-01-03 10:12:43,697 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1     | 2023-01-03 10:12:43,697 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1     | 2023-01-03 10:12:43,697 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1     | 2023-01-03 10:12:43,703 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1     | 2023-01-03 10:12:43,703 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1     | 2023-01-03 10:12:43,703 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1     | 2023-01-03 10:12:43,706 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1     | 2023-01-03 10:12:43,706 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 5 
recon_1     | 2023-01-03 10:12:43,801 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 6, SequenceNumber diff: 20, SequenceNumber Lag from OM 0.
recon_1     | 2023-01-03 10:12:43,801 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 20 records
recon_1     | 2023-01-03 10:12:43,814 [pool-28-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1     | 2023-01-03 10:12:43,835 [pool-28-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1     | 2023-01-03 10:12:44,034 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1     | 2023-01-03 10:12:44,058 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 4 OM DB update event(s).
recon_1     | 2023-01-03 10:12:44,087 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
scm_1       | 2023-01-03 10:11:58,703 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:59,663 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:59,818 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:11:59,997 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:02,931 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:03,701 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:04,526 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:04,793 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:04,962 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:07,932 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:08,702 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:09,529 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:09,795 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:09,962 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:12,932 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:13,705 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:14,528 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:14,790 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:14,954 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:17,929 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:18,699 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:19,525 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:19,791 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:19,962 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:22,934 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:23,701 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:24,532 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:24,796 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:24,959 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:25,528 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1       | 2023-01-03 10:12:27,935 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:28,703 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:29,535 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:29,792 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:29,965 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:32,931 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:33,703 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:34,524 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:34,793 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:34,960 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:37,931 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:38,701 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:39,529 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:39,801 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:39,959 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:42,932 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:43,705 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:44,526 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:44,793 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:44,967 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:47,930 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:48,700 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:49,527 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:49,790 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:49,963 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:52,931 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:53,703 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:54,531 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:54,801 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:54,963 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:55,532 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
scm_1       | 2023-01-03 10:12:57,929 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
scm_1       | 2023-01-03 10:12:58,714 [EventQueue-CommandQueueReportForCommandQueueReportHandler] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=DatanodeDetails, name='Datanode_Command_Queue_Updated'}
